[
    {
        "basename": "0708.2566",
        "fulltext": 33,
        "footnote_size": 4,
        "footnote_max": 8,
        "reference": 38,
        "authors": []
    },
    {
        "title": "",
        "abstract": "We introduce S-DUDE, a new algorithm for denoising DMC-corrupted data. The algorithm, which generalizes the recently introduced DUDE (Discrete Universal DEnoiser) of Weissman et al., aims to compete with a genie that has access, in addition to the noisy data, also to the underlying clean data, and can choose to switch, up to m times, between sliding window denoisers in a way that minimizes the overall loss. When the underlying data form an individual sequence, we show that the S-DUDE performs essentially as well as this genie, provided that m is sub-linear in the size of the data. When the clean data is emitted by a piecewise stationary process, we show that the S-DUDE achieves the optimum distribution-dependent performance, provided that the same sub-linearity condition is imposed on the number of switches. To further substantiate the universal optimality of the S-DUDE, we show that when the number of switches is allowed to grow linearly with the size of the data, any (sequence of) scheme(s) fails to compete in the above senses. Using dynamic programming, we derive an efficient implementation of the S-DUDE, which has complexity (time and memory) growing only linearly with the data size and the number of switches m. Preliminary experimental results are presented, suggesting that S-DUDE has the capacity to significantly improve on the performance attained by the original DUDE in applications where the nature of the data abruptly changes in time (or space), as is often the case in practice. Index Terms-Discrete denoising, competitive analysis, individual sequence, universal algorithms, piecewise stationary processes, dynamic programming, discrete memoryless channel (DMC), switching experts, forward-backward recursions. Introduction Discrete denoising is the problem of reconstructing the components of a finite-alphabet sequence based on the entire observation of its Discrete Memoryless Channel (DMC)-corrupted version. The quality of the reconstruction is evaluated via a user-specified (single-letter) loss function. Universal discrete denoising, in which no statistical or other properties are known a priori about the underlying clean data and the goal is to attain optimum performance, was considered and solved in [1] . The main problem setting there is the \"semi-stochastic\" one, in which the underlying signal is assumed to be an \"individual sequence,\" and the randomness is due solely to the channel noise. In this setting, it is unreasonable to expect to attain the best performance among all the denoisers in the world, since for every given sequence, there exists a denoiser that recovers all the sequence components perfectly. Thus, [1] limits the comparison class, a.k.a. expert class, and uses the competitive analysis approach. Specifically, it is shown that regardless of what the underlying individual sequence may be, the Discrete Universal DEnoiser (DUDE) essentially attains the performance of the best sliding window denoiser that would be chosen by a genie with access to the * Authors are with the",
        "": "underlying clean sequence, in addition to the observed noisy sequence. This semi-stochastic setting result is shown in  #b0  to imply the stochastic setting result, i.e., that for any underlying stationary signal, the DUDE attains the optimal distribution-dependent performance.The setting of an arbitrary individual sequence, combined with competitive analysis, has been very popular in many other research areas, especially for problems of sequential decision-making. Examples include universal compression  #b3 , universal prediction  #b4 , universal filtering  #b1 , repeated game playing  #b5  #b6  #b7 , universal portfolios  #b8 , online learning  #b9  #b10 , zero-delay coding  #b11  #b12 , and much more. A comprehensive account of this line of research can be found in  #b13 . The beauty of this approach is the fact that it leads to the construction of schemes that perform, on every individual sequence, essentially as well as the best in a class of experts, which is the performance of a genie that had hindsight on the entire sequence before selecting his actions. Moreover, if the expert class is judiciously chosen, the relative sense of such a performance guarantees can, in many cases, imply optimum performance in absolute senses as well.One extension to this approach is competition with an expert class and a genie that has the freedom to form a compound action, which breaks the sequence into a certain (limited) number of segments, applies different experts in each segment, and achieves an even better performance overall. Note that the optimal segmentation of the sequence and the choice of the best expert in each segment is also determined by hindsight. Clearly, competing with the best compound action is more challenging, since the number of possible compound actions is exponential in the sequence length n, and the brute-force vanilla implementation of the ordinary universal scheme requires prohibitive complexity. However, clever schemes with linear complexity that successfully track the best segments and experts have been devised in many different areas, such as online learning, universal prediction  #b14  #b15 , universal compression  #b16  #b17 , online linear regression  #b18 , universal portfolios  #b19 , and zero-delay lossy source coding  #b21 .In this paper, we expand the idea of compound actions and apply it to the discrete denoising problem. The motivation of this expansion is natural: the characteristics of the underlying data in the denoising problem often tend to be time-or space-varying. In this case, determining the best segmentation and the best expert for each segment requires complete knowledge of both clean and noisy sequences. Therefore, whereas the challenge in sequential decision-making problems is to track the shift of the best expert based on the past, true observation, the challenge in the denoising problem is to learn the shift based on the entire, but noisy, observation. We extend DUDE to meet this challenge and provide results that parallel and strengthen those of  #b0 .Specifically, we introduce the S-DUDE and show first that, for every underlying noiseless sequence, it attains the performance of the best compound finite-order sliding window denoiser (concretely defined later), both in expectation and in a high probability sense. We develop our scheme in the semi-stochastic setting as in  #b0 . The toolbox for the construction and analysis of our scheme draws on ideas developed in  #b1 . We circumvent the difficulty of not knowing the exact true loss by using an observable unbiased estimate of it. This kind of an estimate has proved to be very useful in  #b1  and  #b2  to devise schemes for filtering and for denoising with dynamic contexts. Building on this semi-stochastic setting result, we also establish a stochastic setting result, which can be thought of as a generalization and strengthening of the stochastic setting results of  #b0 , from the world of stationary processes to that of piecewise stationary processes.Our stochastic setting has connections to other areas, such as change-point detection problems in statistics  #b22  #b23  and switching linear dynamical systems in machine learning and signal processing  #b24  #b25 . Both of these lines of research share a common approach with S-DUDE, in that they try to learn the change of the underlying time-varying parameter or state of stochastic models, based on noisy observations of the parameter or state. One difference is that, whereas our goal is the noncausal estimation, i.e., denoising, of the general underlying piecewise stationary process, the change-point detection problems mainly focus on sequentially detecting the time point where the change of model happened. Another difference is in that the switching linear dynamical systems focus on a special class of underlying processes, the linear dynamical system. In addition, they deal with continuous-valued signals, whereas our focus is the discrete case, with finite-alphabet signals.As we explain in detail, the S-DUDE can be practically implemented using a two-pass algorithm with complexity (both space and time) linear in the sequence length and the number of switches. We also present initial experimental results that demonstrate the S-DUDE's potential to outperform the DUDE on both simulated and real data.The remainder of the paper is organized as follows. Section 2 provides the notation, preliminaries and background for the paper; in Section 3 we present our scheme and establish its strong universality properties via an analysis of its performance in the semi-stochastic setting. Section 4 establishes the universality of our scheme in a fully stochastic setting, where the underlying noiseless sequence is emitted by a piecewise stationary process. Algorithmic aspects and complexity of the actual implementation of the scheme is considered in Section 5, and some experimental results are displayed in Section 6. In Section 7 we conclude with a summary of our findings and some possible future research directions.",
        "Notation, Preliminaries, and Motivation": "",
        "2-A Notation": "We use a combination of notation of  #b0  and  #b1 . Let X , Z,X denote, respectively, the alphabet of the clean, noisy, and reconstructed sources, which are assumed to be finite. As in  #b0  and  #b1 , the noisy sequence is a DMC-corrupted version of the clean one, where the channel matrix \u03a0 = {\u03a0(x, z)} x\u2208X ,z\u2208Z , \u03a0(x, z) denoting the probability of a noisy symbol z when the underlying clean symbol is x, is assumed to be known and fixed throughout the paper, and of full row rank. The z-th column of \u03a0 will be denoted as \u03c0 z . Upper case letters will denote random variables as usual; lower case letters will denote either individual deterministic quantities or specific realizations of random variables.Without loss of generality, the elements of any finite set V will be identified with {0, 1, \u00b7 \u00b7 \u00b7 , |V| \u2212 1}. We let V \u221e denote the set of one-sided infinite sequences with V-valued components, i.e., v \u2208 V \u221e is of the form v = (v 1 , v 2 , \u00b7 \u00b7 \u00b7 ), v i \u2208 V, i \u2265 1. For v \u2208 V \u221e , let v n = (v 1 , \u00b7 \u00b7 \u00b7 , v n ) and v n m = (v m , \u00b7 \u00b7 \u00b7 , v n ). Furthermore, we let v n\\t denote the sequence v t\u22121 v n t+1 . R V is a space of |V|-dimensional column vectors with real-valued components indexed by the elements of V. The a-th component of q \u2208 R V will be denoted by either q a or q[a]. Subscripting a vector or a matrix by \"max\" will represent the difference between the maximum and minimum of all its components. Thus, for example, if \u0393 is a |Z| \u00d7 |X | matrix, then \u0393 max stands for max x\u2208X ,z\u2208Z \u0393(z, x) \u2212 min x\u2208X ,z\u2208Z \u0393(z, x) (in particular, if the components of \u0393 are nonnegative and \u0393(z, x) = 0 for some z and x, then \u0393 max = max z\u2208Z,z\u2208X \u0393(z, x).) In addition, 1 {\u00b7} denotes an indicator of the event inside {\u00b7}.Generally, let the finite sets Y, A be, respectively, a source alphabet and an action space. For a general loss function l : Y \u00d7 A \u2192 R, a Bayes response for \u03b6 \u2208 R Y under the loss function l is given asb l (\u03b6) = arg min a\u2208A \u03b6 T \u00b7 L a ,(1)where L a denotes the column of the matrix of the loss function l corresponding to the a-th action, and ties are resolved lexicographically. The corresponding Bayes envelope is denoted asU l (\u03b6) = min a\u2208A \u03b6 T \u00b7 L a .(2)Note that when \u03b6 is a probability, namely, it has non-negative components summing to one, U l (\u03b6) is the minimum achievable expected loss (as measured under the loss function l) in guessing the value of Y \u2208 Y which is distributed according to \u03b6. The associated optimal guess is b l (\u03b6).An n-block denoiser is a collection of n mappingsX n = {X t } 1\u2264t\u2264n , whereX t : Z n \u2192X . We assume a given loss function \u039b : X \u00d7X \u2192 [0, \u221e), where the maximum single-letter loss is denoted by \u039b max , and \u03bbx denotes thex-th column of the loss matrix. The normalized cumulative loss of the denoiserX n on the individual sequence pair (x n , z n ) is represented asLX n (x n , z n ) = 1 n n t=1\u039b(x t ,X t (z n )).In words, LX n (x n , z n ) is the normalized (per-symbol) loss, as measured under the loss function \u039b, when using the denoiserX n and when the observed noisy sequence is z n while the underlying clean one is x n . The notation LX n is extended for 1 \u2264 i \u2264 j \u2264 n,LX n (x j i , z n ) = 1 j \u2212 i + 1 j t=i \u039b(x t ,X t (z n ))denoting the normalized (per-symbol) loss between (and including) locations i and j. Now, consider the set S = {s : Z \u2192X }, which is the (finite) set of mappings that take Z intoX . We refer to elements of S as \"single-symbol denoisers\", since each s \u2208 S can be thought of as a rule for estimating X \u2208 X on the basis of Z \u2208 Z. Now, for any s \u2208 S, an unbiased estimator for \u039b(x, s(Z)) (based on Z only), where x is a deterministic symbol and Z is the output of the DMC when the input is x, can be obtained as in  #b1 . First, pick a function h : Z \u2192 R X with the property that, for a, b \u2208 X ,E a h b (Z) = z\u2208Z h b (z)\u03a0(a, z) = \u03b4(a, b) 1, if a = b 0, otherwise ,(3)where E a denotes expectation over the channel output Z when the underlying channel input is a, and h b (z) denotes the b-th component of h(z). Let H denote the |Z| \u00d7 |X | matrix whose z-th row is h T (z), i.e., H(z, b) = h b (z). To see that our assumption of a channel matrix with full row rank guarantees the existence of such an h, note that (3) can equivalently be stated in matrix form as\u03a0H = I,(4)where I is the |X | \u00d7 |X | identity matrix. Thus, e.g., any H of the form H = \u0393 T (\u03a0\u0393 T ) \u22121 , for any \u0393 such that \u03a0\u0393 T is invertible, satisfies (4). In particular, \u0393 = \u03a0 is a valid choice (\u03a0\u03a0 T is invertible, since \u03a0 is of full row rank) corresponding to the Moore-Penrose generalized inverse  #b26 . Now, for any s \u2208 S, \u03c1(s) \u2208 R X denotes the column vector with x-th component\u03c1 x (s) = z \u039b(x, s(z))\u03a0(x, z) = E x \u039b(x, s(Z)).(5)In words, \u03c1 x (s) is the expected loss using the single-symbol denoiser s, while the underlying symbol is x. Considering S as an action space alphabet, we define a loss function \u2113 : Z \u00d7 S \u2192 R as\u2113(z, s) = h(z) T \u00b7 \u03c1(s).(6)We observe from (3) and (5) that \u2113(Z, s) is an unbiased estimate of \u039b(x, s(Z)) sinceE x \u2113(Z, s) = E x h(Z) T \u00b7 \u03c1(s) = x \u2032 E x h x \u2032 (Z)\u03c1 x \u2032 (s) = x \u2032 \u03b4(x, x \u2032 )\u03c1 x \u2032 (s) = \u03c1 x (s) = E x \u039b(x, s(Z)) \u2200x \u2208 X .(7)For \u03be \u2208 R Z , let B H (\u03be, \u00b7) \u2208 S be defined byB H (\u03be, z) = arg min x \u03be T \u00b7 H \u00b7 [\u03bbx \u2299 \u03c0 z ],(8)where, for vectors v 1 and v 2 of equal dimensions, v 1 \u2299 v 2 denotes the vector obtained by component-wise multiplication. Note that, similarly as in [2, (88),(89) ],B H (\u03be, \u00b7) = arg min s\u2208S z \u03be T \u00b7 H \u00b7 [\u03bb s(z) \u2299 \u03c0 z ] = arg min s\u2208S \u03be T \u00b7 H \u00b7 \u03c1(s) = arg min s\u2208S z \u03be z \u00b7 [h T (z) \u00b7 \u03c1(s)] = arg min s\u2208S z \u03be z \u00b7 \u2113(z, s) = b \u2113 (\u03be).(9)Thus, B H (\u03be, \u00b7) is a Bayes response for \u03be under the loss function \u2113 defined in (6).",
        "2-B Preliminaries": "In this section, we summarize the results from  #b0  and motivate the approach underlying the construction of our new class of denoisers. Analogously as in  #b1 , the n-block denoiserX n = {X t } 1\u2264t\u2264n can be associatedwith F n = {F t } 1\u2264t\u2264n , where F t : Z n\\t \u2192 S is defined as follows: F t (z n\\t , \u00b7) is the single-symbol denoiser in S satisfyingX t (z n ) = F t (z n\\t , z t ) \u2200z t .(10)Therefore, we can adopt the view that at each time t, an n-block denoiser is choosing a single-symbol denoiser based on all the noisy sequence components but z t , and applying that single-symbol denoiser on z t to yield the t-th reconstructionx t . Conversely, any sequence of mappings into single-symbol denoisers F n defines a denoiserX n , again via (10). We will adhere to this viewpoint in what follows. One special class of widely used n-block denoisers is that of k-th order \"sliding window\" denoisers, which we denote byX n,S k . Such denoisers are of the formX s k t (z n ) = s k (z t+k t\u2212k ), t = k + 1, \u00b7 \u00b7 \u00b7 , n \u2212 k,(11)where s k is an element of S k = {s k : Z 2k+1 \u2192X }, the (finite) set of mappings from Z 2k+1 intoX .  $b1  We also refer to s k \u2208 S k as a \"k-th order denoiser\". Note that S 0 = S. From the definition  #b10 , it follows thatX s k i (z n ) =X s k j (z n ) whenever z i+k i\u2212k = z j+k j\u2212k .(12)Following the association in (10), we can adopt an alternative view that the k-th order sliding window denoiser chooses a single-symbol denoiser s k (z t\u22121 t\u2212k , z t+k t+1 , \u00b7) \u2208 S at time t on the basis of the context, and X s k t (z n ) = s k (z t\u22121 t\u2212k , z t+k t+1 , z t ). We denote c t (z t\u22121 t\u2212k , z t+k t+1 ) as a (two-sided) context for z t , and define the set of all possible k-th order contexts,C k {(u \u22121 \u2212k , u k 1 ) : (u \u22121 \u2212k , u k 1 ) \u2208 Z 2k }.Then, for given z n and for each c \u2208 C k , we defineT (c) t : c t = c, k + 1 \u2264 t \u2264 n \u2212 k = {t : (z t\u22121 t\u2212k , z t+k t+1 ) = c, k + 1 \u2264 t \u2264 n \u2212 k ,(13)1 The value ofX s k t (z n ) for t \u2264 k and t > n \u2212 k is defined, for concreteness and simplicity, as an arbitrary fixed symbol in X .the set of indices where the context equals c. Now, an equivalent interpretation for  #b11  is that for each c \u2208 C k , the k-th order sliding window denoiser employs a time-invariant single-symbol denoiser, s k (c, \u00b7), at all points t \u2208 T (c). In other words, the sequence z n is partitioned into the subsequences associated with the various contexts, and on each such subsequence a time-invariant single-symbol scheme is employed.In  #b0 , for integers k \u2265 0 and n > 2k, the k-th order minimum loss of (x n , z n ) is defined byD k (x n , z n ) min X n \u2208X n,S k LX n (x n\u2212k k+1 , z n ) = min s k \u2208S k 1 n \u2212 2k n\u2212k t=k+1 \u039b(x t , s k (c t , z t )).(14)The identity of the element s k \u2208 S k that achieves  #b13  depends not only on z n , but also on x n , since  #b13  can be expressed as1 n \u2212 2k c\u2208C k min s\u2208S \u03c4 \u2208T (c)\u039b(x \u03c4 , s(z \u03c4 )) , and at each time t, the best k-th order sliding window denoiser that achieves  #b13  will employ the singlesymbol denoiser arg mins\u2208S \u03c4 \u2208T (ct) \u039b(x \u03c4 , s(z \u03c4 )),(15)which is determined from the joint empirical distribution of pairs {(x \u03c4 , z \u03c4 ) : \u03c4 \u2208 T (c t )}. It was shown in  #b0  that, despite the lack of knowledge of x n , D k (x n , Z n ) is achievable in a sense made precise below, in the limit of growing n, by a scheme that only has access to Z n . This scheme is dubbed in  #b0  as the Discrete Universal DEnoiser (DUDE),X n,k univ . The algorithm is defined b\u0177X k univ,t (z n ) = B H (m(z n , z t\u22121 t\u2212k , z t+k t+1 ), z t ),(16)where m(z n , c) is the vector of counts of the appearances of the various symbols within the context c along the sequence z n . That is, for all \u03b2 \u2208 Z, m(z n ,z \u22121 \u2212k ,z k 1 ) is the |Z|-dimensional column vector whose \u03b2-th component ism(z n ,z \u22121 \u2212k ,z k 1 )[\u03b2] = {t : k + 1 \u2264 t \u2264 n \u2212 k, z t+k t\u2212k =z \u22121 \u2212k \u03b2z k 1 } ,namely, the number of appearances ofz \u22121 \u2212k \u03b2z k 1 along the sequence z n . The main result of  #b0  is the following theorem, pertaining to the semi-stochastic setting of an individual sequence x = (x 1 , x 2 , . . .) corrupted by a DMC that yields the stochastic noisy sequence Z = (Z 1 , Z 2 , . . .).Theorem 1 ([1, Theorem 1]) Take k = k n satisfying k n |Z| 2kn = o(n/ log n). Then, for all x \u2208 X \u221e , the sequence of denoisers {X n,kn univ } defined in (16) satisfies:a) lim n\u2192\u221e LXn,k n univ (x n , Z n ) \u2212 D kn (x n , Z n ) = 0 a.s. b) E LXn,k n univ (x n , Z n ) \u2212 D kn (x n , Z n ) = O k n |Z| 2kn n .Theorem 1 was further shown in  #b0  to imply the universality of the DUDE in the fully stochastic setting where the underlying sequence is emitted by a stationary source (and the goal is to attain the performance of the optimal distribution-dependent denoiser). From  #b15 , it is apparent that the DUDE ends up employing a k-th order sliding window denoiser (where the sliding window scheme the DUDE chooses depends on z n ). Moreover,  #b8  implies that, at each time t, DUDE is merely employing the single-symbol denoiser B H (m(z n , z t\u22121 t\u2212k , z t+k t+1 ), \u00b7) \u2208 S, which can be obtained by finding the Bayes response b \u2113 m(z n , z t\u22121 t\u2212k , z t+k t+1 ) or, equivalently, the mapping in S given by arg mins\u2208S \u03c4 \u2208T (ct) \u2113(z \u03c4 , s),(17)where \u2113(z, s) is the loss function defined in  #b5 . By comparing  #b14  with  #b16 , and from Theorem 1, we observe that working with the estimated loss \u2113(z \u03c4 , s) in lieu of the genie-aided \u039b(x \u03c4 , s(z \u03c4 )) allows us to essentially achieve the genie-aided performance in  #b13 .",
        "2-C Motivation": "Our motivation for this paper is based on the observation that the k-th order sliding window denoisers ignore the time-varying nature of the underlying sequence x n . That is, as discussed above, for time instances with the same contexts, the single-symbol denoiser employed along the associated subsequence is time-invariant. In other words, for each t, only the empirical distribution of the sequence {(x \u03c4 , z \u03c4 ) : \u03c4 \u2208 T (c t )} matters, but its order of composition, i.e., its time-varying nature, is not considered. It is clear, however, that when the characteristics of the underlying clean sequence x n are changing, the (normalized) cumulative loss that is achieved by sliding window denoisers that can shift from one rule to another along the sequence may be strictly lower (better) than  #b13 . We now devise and analyze our new scheme that achieves this more ambitious target performance.",
        "The Shifting Denoiser (S-DUDE)": "In this section, we derive our new class of denoisers and analyze their performance. In Subsection 3-A, we begin with the simplest case, competing with shifting symbol-by-symbol denoisers, or, in other words, shifting 0-th order denoisers. The argument is generalized to shifting k-th order denoisers in Subsection 3-B, and the framework and results include Subsection 3-A as a special case. We will use the notation S 0 , instead of S, for consistency in denoting the class of single-symbol denoisers. Throughout this section, we assume the semi-stochastic setting.",
        "3-A Switching between symbol-by-symbol (0-th order) denoisers": "Consider an n-tuple of single-symbol denoisers S = {s 1 , \u00b7 \u00b7 \u00b7 , s n } \u2208 S n 0 . Then, as mentioned in Section 2-B, for such S, we can define the associated n-block denoiserX n,S a\u015dX S t (z n ) = s t (z t ).(18)Note that in this case, the single-symbol denoiser applied at each time may depend on the time t (but not on z n\\t , as would be the case for a general denoiser). We also denote the estimated normalized cumulative loss asLS (z n ) 1 n n t=1 \u2113(z t , s t ),(19)whose property is given in the following lemma, which parallels [2, Theorem 4].Lemma 1 Fix \u01eb > 0. For fixed S \u2208 S n 0 , and all x n \u2208 X n ,P LX n,S (x n , Z n ) \u2212L S (Z n ) > \u01eb \u2264 exp \u2212 n 2\u01eb 2 L 2 max and (20) P L S (Z n ) \u2212 LX n,S (x n , Z n ) > \u01eb \u2264 exp \u2212 n 2\u01eb 2 L 2 max ,(21)where L max = \u039b max + \u2113 max .In words, the lemma shows that for every S \u2208 S n 0 , the estimated lossL S (Z n ) is concentrated around the true loss LX n,S (x n , Z n ) with high probability, as n becomes large, regardless of the underlying sequence x n .Proof of Lemma 1: See Appendix 8-A. Now, let the integer 0 \u2264 m \u2264 \u230a n 2 \u230b denote the maximum number of shifts allowed along the sequence. Then, define a set S n 0,m \u2286 S n 0 asS n 0,m = S \u2208 S n 0 : n t=2 1 {s t\u22121 =st} \u2264 m ,(22)namely, S n 0,m is the set of n-tuples of single-symbol denoisers with at most m shifts from one mapping to another.  $b2  Analogously to  #b13 , for the class of n-block denoisersX n,S with S \u2208 S n 0,m , we defineD 0,m (x n , z n ) min S\u2208S n 0,m LX n,S (x n , z n ) = min S\u2208S n 0,m 1 n n t=1 \u039b(x t , s t (z t )),(23)which is the minimum normalized cumulative loss that can be achieved for (x n , z n ) by the sequence of n single-symbol denoisers that allow at most m shifts. Our goal in this section is to build a universal scheme that only has access to Z n , but still essentially achieves D 0,m (x n , Z n ). As hinted by the DUDE, we build our universal scheme by working with the estimated loss. That is, define\u015c=\u015c(z n ) arg min S\u2208S n 0,mL S (z n ),(24)and our (0, m)-Shifting Discrete Universal DEnoiser (S-DUDE),X n,0,m univ , is defined asX n,\u015c . It is clear that, by definition, LX n,\u015c (x n , z n ) \u2265 D 0,m (x n , z n ) for all x n and z n , but we can also show that, with high probability, LX n,\u015c (x n , Z n ) does not exceed D 0,m (x n , Z n ) by much, as stated in the following theorem.Theorem 2 LetX n,0,m univ be defined asX n,\u015c , where\u015c is given in  #b23 . Then, for all \u01eb > 0 and x n \u2208 X n ,P LXn,0,m univ (x n , Z n ) \u2212 D 0,m (x n , Z n ) > \u01eb \u2264 2 exp \u2212n \u01eb 2 2L 2 max \u2212 2 h m n + (m + 1) ln N n , where h(x) = \u2212x ln x \u2212 (1 \u2212 x) ln(1 \u2212 x) for 0 \u2264 x \u2264 1, and N = |S| = |Z| |X | .In particular, the right-hand side of the inequality is exponentially small, provided m = o(n).Remark: It is reasonable to expect this theorem to hold, given Lemma 1. That is, since, for fixed S \u2208 S n 0,m , L S (Z n ) is concentrated on LX n,S (x n , Z n ), it is plausible that\u015c that achieves min S\u2208S n 0,mL S (Z n ) will have a loss LX n,\u015c (x n , Z n ) close to min S\u2208S n 0,m LX n,S (x n , Z n ), i.e., D 0,m (x n , Z n ). Proof of Theorem 2: See Appendix 8-B.",
        "3-B Switching between k-th order denoisers": "Now, we extend the result from Subsection 3-A to the case of shifting between k-th order denoisers. The argument parallels that of Subsection 3-A. Let {s k,t } n\u2212k t=k+1 be an arbitrary sequence of the k-th order denoiser mappings, i.e., s k,t \u2208 S k for k + 1 \u2264 t \u2264 n \u2212 k. Now, for given z n , define an (n \u2212 2k)-tuple of (k-th order denoiser induced) single-symbol denoisersS k (z n ) {s k,t (c t , \u00b7)} n\u2212k t=k+1 \u2208 S n\u22122k 0 ,(25)where, to recall, c t = (z t\u22121 t\u2212k , z t+k t+1 ), and s k,t (c t , \u00b7) is the single-symbol denoiser induced from s k,t \u2208 S k and c t . For brevity of notation, we will suppress the dependence on z n in S k (z n ) and denote it as S k . Then, as in (18), we define the associated n-block denoiserX n,S k as 3X S k t (z n ) = s k,t (c t , z t ).(26)In addition, extending  #b18 , the estimated normalized cumulative loss is given asL S k (z n ) = 1 n \u2212 2k n\u2212k t=k+1 \u2113(z t , s k,t (c t , \u00b7)).(27)Then, we have the following lemma, which parallels Lemma 1.Lemma 2 Fix \u01eb > 0. For any fixed sequence {s k,t } n\u2212k t=k+1 , and all x n \u2208 X n ,Pr LXn,S k (x n\u2212k k+1 , Z n ) \u2212L S k (Z n ) > \u01eb \u2264 (k + 1) exp \u2212 2(n \u2212 2k)\u01eb 2 (k + 1)L 2 max and(28)Pr L S k (Z n ) \u2212 LXn,S k (x n\u2212k k+1 , Z n ) > \u01eb \u2264 (k + 1) exp \u2212 2(n \u2212 2k)\u01eb 2 (k + 1)L 2 max ,(29)where L max = \u039b max + \u2113 max .Remark: Note that when k = 0, this lemma coincides with Lemma 1. The proof of this lemma combines Lemma 1 and the de-interleaving argument in the proof of [1, Theorem 2]. Namely, we de-interleave Z n into (k + 1) subsequences consisting of symbols separated by blocks of k symbols, and exploit the conditional independence of symbols in each subsequence, given all symbols not in that subsequence, to use Lemma 1.Proof of Lemma 2: See Appendix 8-C. Now, for an integer 0 \u2264 m \u2264 \u230a n\u22122k 2 \u230b and given z n , let n(c) |T (c)|, and m(c) min{n(c), m} for c \u2208 C k . Then, analogously as in (22), we defineS n k,m (z n ) = S k (z n ) \u2208 S n\u22122k 0 : {s k,\u03c4 (c, \u00b7)} \u03c4 \u2208T (c) \u2208 S n(c) 0,m(c) for all c \u2208 C k .(30)In words, S n k,m (z n ) is the set of (n \u2212 2k)-tuples of (k-th order denoiser induced) single-symbol denoisers that allow at most m(c) shifts within the subsequence {t : t \u2208 T (c)} for each context c \u2208 C k . 4 Again, for brevity, the dependence on z n in S n k,m (z n ) is suppressed, and we write simply S n k,m . It is worth noting that S n k,m is a larger class than the class of k-th order 'sliding window' denoisers that are allowed to shift at most m times. The reason is that in S n k,m , the shift within each subsequence associated with each context can occur at any time, regardless of the shifts in other subsequences, whereas in the latter class, the shifts in each subsequence occur together with other shifts in other subsequences.For integers k \u2265 0 and n > 2k, we now define, for the class of n-block denoisersX n,S with S \u2208 S n k,m ,D k,m (x n , z n ) min S\u2208S n k,m LX n,S (x n\u2212k k+1 , z n ) = min S\u2208S n k,m 1 n \u2212 2k n\u2212k t=k+1 \u039b(x t , s k,t (c t , z t )),(31)the minimum normalized cumulative loss of (x n , z n ) that can be achieved by the sequence of k-th order denoisers that allow at most m shifts within each context. Now, to build a legitimate (non genie-aided) universal scheme achieving (31) on the basis of Z n only, we defin\u00eaS k,m = arg min S\u2208S n k,mL S (z n ),(32)and the (k, m)-S-DUDE,X n,k,m univ , is defined asX n,\u015c k,m . Note that when m = 0,X n,\u015c k,m coincides with the DUDE in  #b0 . The following theorem generalizes Theorem 2 to the case of general k \u2265 0.Theorem 3 LetX n,k,m univ be given byX n,\u015c k,m , where\u015c k,m is defined in  #b31 . Then, for all \u01eb > 0 and x n \u2208 X n ,Pr LXn,k,m univ (x n\u2212k k+1 , Z n ) \u2212 D k,m (x n , Z n ) > \u01eb (33) \u2264 2(k + 1) exp \u2212 (n \u2212 2k) \u00b7 \u01eb 2 2(k + 1)L 2 max \u2212 2|Z| 2k \u00b7 h m n \u2212 2k + (m + 1) ln N n \u2212 2k ,(34)whereh(x) = \u2212x ln x \u2212 (1 \u2212 x) ln(1 \u2212 x) for 0 \u2264 x \u2264 1, and N = |S| = |Z| |X | .Remark: Note that when k = 0, this theorem coincides with Theorem 2. Similarly to the way Theorem 2 was plausible given Lemma 1, Theorem 3 can be expected given Lemma 2, since\u015c k,m achieves min S\u2208S n k,mL S (Z n ), and we expect LX n,\u015c k,m (x n\u2212k k+1 , Z n ) to be close to D k,m (x n , Z n ) from the concentrationofL S (Z n ) to LX n,S (x n\u2212k k+1 , Z n ) for all S \u2208 S n k,m . Proof of Theorem 3: See Appendix 8-D.From Theorem 3, we now easily obtain one of the main results of the paper, which extends Theorem 1 from the case m = 0 to the case of general 0 \u2264 m \u2264 \u230a n\u22122k 2 \u230b. That is, the following theorem asserts that, for every underlying sequence x \u2208 X \u221e , our (k, m)-S-DUDE performs essentially as well as the best shifting k-th order denoiser that allows at most m shifts within each context, both in high probability and expectation sense, provided a growth condition on k and m is satisfied.Theorem 4 Suppose k = k n and m = m n are such that the right-hand side of (34) is summable in n. Then, for all x \u2208 X \u221e , the sequence of denoisers {X n,k,m univ } satisfies a)lim n\u2192\u221e LXn,k,m univ (x n , Z n ) \u2212 D k,m (x n , Z n ) = 0 a.s. (35) b) For any \u03b4 > 0, E LXn,k,m univ (x n , Z n ) \u2212 D k,m (x n , Z n ) = O k n |Z| 2kn m n n 1\u2212\u03b4 .(36)Remark: It will be seen in Claim 1 below that the stipulation in the theorem implies lim n\u2192\u221e k n |Z| 2kn mn n 1\u2212\u03b4 = 0, which, when combined with  #b35 , implies that the expected difference on the left hand side of (36) vanishes with increasing n. That in itself, however, can easily be deduced from  #b34  and bounded convergence. The more significant value of (36) is in providing a rate of convergence result for the 'redundancy' in the S-DUDE's performance, as a function of both k and m. In particular, note that for any \u03b7 > 0, O(n \u22121/2+\u03b7 ) is achievable provided k n = c log n and m n = n \u03be , for small enough positive constants c, \u03be.In what follows, we specify the maximal growth rates for k = k n and m = m n under which the summability condition stipulated in Theorem 4 holds.",
        "Claim 1": "a) Maximal growth rate for k: The summability condition in Theorem 4 is satisfied provided k n = c 1 log n with c 1 < 1 2 log |Z| and m n grows at any sub-polynomial rate. On the other hand, the condition is not satisfied for k n = c 1 log n with any c 1 \u2265 1 2 log |Z| , even when m is fixed (not growing with n). b) Maximal growth rate for m: The summability condition in Theorem 4 is satisfied for any sub-linear growth rate of m n , provided k n is taken to increase sufficiently slowly that k n |Z| 2kn = o((n/m n ) 1\u2212\u03b4 ) for some \u03b4 > 0. On the other hand, the condition is not satisfied whenever m n grows linearly with n, even when k is fixed.Proof of Claim 1: See Appendix 8-E.Proof of Theorem 4: See Appendix 8-F.",
        "3-C A \"strong converse\"": "In Claim 1, we have shown the necessity of m = o(n) for the condition required in Theorem 4 to hold. However, we can prove the necessity of m = o(n) in a much stronger sense, described in the following theorem.Theorem 5 Suppose that X =X , that \u039b(x,x) \u2265 0 for all x,x with equality if and only if x =x, and that \u03a0(x, z) > 0 for all x, z. If m = \u0398(n), then for any sequence of denoisers {X n }, there exists x \u221e \u2208 X \u221e such that lim sup n\u2192\u221e E LX n (x n , Z n ) \u2212 D 0,m (x n , Z n ) > 0.(37)with a genie that shifts among single-symbol denoisers so, a fortiori, it implies that for any fixed k > 0 or k that grows with n, lim supn\u2192\u221e E LX n (x n , Z n ) \u2212 D k,m (x n , Z n ) > 0(38)also holds since, by definition, D 0,m (x n , z n ) \u2265 D k,m (x n , z n ) for all x n , z n and k \u2265 0. Therefore, the theorem asserts that for any sequence of denoisers to compete with D k,m (x n , Z n ), even in expectation sense, m = o(n) is necessary. Finally, we mention that the conditions stipulated in the statement of the theorem regarding the loss function and the channel can be considerably relaxed without compromising the validity of the theorem. These conditions are made to allow for the simple proof that we give in Appendix 8-G.",
        "The Stochastic Setting": "In  #b0 , the semi-stochastic setting result,  #b0 Theorem 1], was shown to imply the result for the stochastic setting as well. That is, when the underlying data form a stationary process, [1, Section VI] shows that the DUDE attains optimum distribution-dependent performance. Analogously, we can now use the results from the semi-stochastic setting of the previous section to generalize the results of [1, Section VI] and show that our S-DUDE attains optimum distribution-dependent performance when the underlying data form a piecewise stationary process. We first define the precise notion of the class of piecewise stationary processes in Subsection 4-A, and discuss the richness of this class in Subsection 4-B. Subsection 4-C gives the main result of this section: the stochastic setting optimality of the S-DUDE.",
        "4-A Definition of the class of processes": "P{m n } Let P (1) X , \u00b7 \u00b7 \u00b7 , P (M ) Xbe a finite collection of M probability distributions of stationary processes, with components taking the values in X . Let A be a process with components taking the values in {1, . . . , M }. Then, a piecewise stationary process X is generated by shifting between the M processes in a way specified by the \"switching process\" A, as we now describe.First, denote r(A n ) as the number of shifts that have occurred along the n-tuple A n , i.e.,r(A n ) n\u22121 j=1 1 {A j =A j+1 } .Thus, there are r(A n ) + 1 \"blocks\" in A n , where each block is a tuple of constant values that are different from the values of adjacent blocks. Now, for each 1 \u2264 i \u2264 r(A n ) + 1, we define\u03c4 i (A n ) inf{t : t j=1 1 {A j =A j+1 } = i} if 1 \u2264 i \u2264 r(A n ) n if i = r(A n ) + 1as the last time instance of the i-th block in A n . In addition, define \u03c4 0 (A n ) 0. Clearly, r(A n ) and \u03c4 i (A n ) depend on A n and, thus, are random variables. However, for brevity, we suppress the dependence on A n when there is no confusion, and write simply r and \u03c4 i , respectively. Using these definitions, and by denoting P A n as the n-th order marginal distribution of A, we define a piecewise stationary process X by characterizing its n-th order marginal distribution P X n as P X n (X n = x n ) = a n P A n (a n )P (X n = x n |A n = a n ) = a n P A n (a n )r+1 i=1 P (a\u03c4 i ) X (x \u03c4 i \u03c4 i\u22121 +1 ),(39)for each n. The corresponding distribution of the process X is denoted as P X .  $b5  In words, X is constructed by following one of the M probability distributions in each block, switching from one to another depending on A. Furthermore, conditioned on the realization of A, each stationary block is independent of other blocks, even if the distribution of distinct blocks is the same. This property of conditional independence is reasonable for modeling many types of data arising in practice, since we can think of the M distributions as different 'modes'; if the process returns to the same mode, it is reasonable to model the new block as a new independent realization of that same distribution. In other words, the 'mode' may represent the kind of 'texture' in a certain region of the data, but two different regions with the same 'texture' should have independent realizations from the texture-generating source. Our notion of a piecewise stationary process almost coincides with that developed in  #b20 . The main difference is that we allow an arbitrary distribution for the process A. Now, we define P{m n } to be the class of all process distributions that can be constructed as in (39) for some M , some collection P (1) X , \u00b7 \u00b7 \u00b7 , P (M ) X of stationary processes, and some switching process A whose number of shifts satisfies r(A n ) \u2264 m n a.s. \u2200n.In words, a process X belongs to 6 P{m n } if and only if it can be formed by switching between a finite collection of independent processes in which the number of switches by time n does not exceed m n .",
        "4-B Richness of P{m n }": "In this subsection, we examine how rich the class P{m n } is, in terms of the growth rate m n and the existence of denoising schemes that are universal with respect to P{m n }. First, given any distribution on a noiseless n-tuple, P X n , we defineD(P X n , \u03a0) min X n \u2208Dn ELX n (X n , Z n ),(41)where D n is the class of all n-block denoisers. The expectation on the right-hand side of (41) assumes that X n is generated from P X n and that Z n is the output of the DMC, \u03a0, whose input is X n . Thus, D(P X n , \u03a0) is the optimum denoising performance (in the sense of expected per-symbol loss) attainable when the source distribution P X n is known. What happens when the source distribution is unknown? Theorem 3 of  #b0  established the fact that 7 lim n\u2192\u221e ELX n DUDE (X n , Z n ) \u2212 D(P X n , \u03a0) = 0 for all stationary P X .Note that our newly-defined class of processes, P{m n }, is simply the class of all stationary processes if one takes the sequence m n to be m n \u2261 0 for all n. Thus, assuming m n \u2261 0, (42) is equivalent tolim n\u2192\u221e ELX n DUDE (X n , Z n ) \u2212 D(P X n , \u03a0) = 0 for all P X \u2208 P{m n }.(43)At the other extreme, when m n = n, P{m n } consists of all possible (not necessarily stationary) processes. We can observe this equivalence by having M = |X | processes each be a constant process at a different symbol in X , and creating any process by switching to the appropriate symbol. In this case, not only does  $b5  {PXn } n\u22651 is readily verified to be a consistent family of distributions and, thus, by Kolmogorov's extension theorem, uniquely defines the distribution of the process X.  $b6  The phrase \"the process X belongs to P{mn}\" is shorthand for \"the distribution of the process X, PX, belongs to P{mn}\".  $b7  When PX is stationary, the limit limn\u2192\u221e D(PXn , \u03a0) \u25b3 = D(PX, \u03a0) was shown to exist in  #b0 . Thus, (42) was equivalently stated as limn\u2192\u221e ELXn (43) not hold for the DUDE, but clearly (43) cannot hold under any sequence of denoisers. In other words, P{m n } is far too rich to allow for the existence of schemes that are universal with respect to it.It is obvious then that P{m n } is significantly richer than the family of stationary processes whenever m n grows with n. It is of interest then to identify the maximal growth rate of m n that allows for the existence of schemes that are universal with respect to P{m n }, and to find such a universal scheme. In what follows, we offer a complete answer to these questions. Specifically, we show that if the growth rate of m n allows for the existence of any scheme which is universal with respect to P{m n }, the S-DUDE is universal, too.",
        "4-C Universality of S-DUDE": "Here, we state our stochastic setting result, which establishes the universality of (k, m)-S-DUDE with respect to the class P{m n }.Theorem 6 Let k = k n and m = m n satisfy the growth rate condition stipulated in Theorem 4, in addition to lim n\u2192\u221e k n = \u221e. Then, the sequence of denoisers {X n,k,m univ } defined in Section 3 satisfy lim n\u2192\u221e ELXn,k,m univ (X n , Z n ) \u2212 D(P X n , \u03a0) = 0 for all P X \u2208 P{m n }.Remark 1: Recall that, as noted in Claim 1, m n = o(n) together with appropriately slowly growing k = k n is sufficient to guarantee the growth rate condition stipulated in Theorem 4. Hence, by Theorem 6, m = o(n) and the sufficiently slowly growing k = k n suffices for (44) to hold. Therefore, Theorem 6 implies the existence of schemes that are universal with respect to P{m n } whenever m n increases sublinearly in n. Since, as discussed in Subsection 4-B, no universal scheme exists for P{m n } when m n is linear in n, we conclude that the sub-linearity of m n is the necessary and sufficient condition for a universal scheme to exist with respect to P{m n }. Moreover, Theorem 6 establishes the strong sense of optimality of the S-DUDE, as it shows that whenever P{m n } is universally \"competable\", the S-DUDE does the job. This fact is somewhat analogous to the situation in  #b20 , where the optimality of the universal lossless coding scheme presented therein for piecewise stationary sources was established under the condition that m = o(n). for all P X \u2208 P{m n }, which is analogous to  #b0 Theorem 4], can also be derived. However, we omit such a result here since the details required for stating it rigorously would be convoluted, and its added value over the strong point-wise result we have already established in the semi-stochastic setting would be little. Proof of Theorem 6: See Appendix 8-H.",
        "Algorithm and Complexity": "",
        "5-A An Efficient Implementation of S-DUDE": "In the preceding two sections, we gave strong asymptotic performance guarantees for the new class of schemes, the S-DUDE. However, the question regarding the practical implementation of (32), i.e., obtainin\u011dS k,m = arg min S\u2208S n k,mL S (z n ),for fixed k, m and n remains and, at first glance, may seem to be a difficult combinatorial optimization problem. In this section, we devise an efficient two-pass algorithm, which yields (32) and performs denoising with linear complexity in the sequence length n. A recursion similar to that in the first pass of the algorithm we present appears also in the study of tracking the best expert in on-line learning  #b14  #b15 . From the definition of S n k,m , (30), we can see that obtaining (32) is equivalent to obtaining the best combination of single-symbol denoisers with at most m(c) shifts that minimizes the cumulative estimated loss along {t : t \u2208 T (c)}, for each c \u2208 C k . Thus, our problem breaks down to |C k | independent problems, each being a problem of competing with the best combination of single-symbol schemes allowing m switches.To describe an algorithm that implements this parallelization efficiently, we first define variables. For (k, m)-S-DUDE, let I = m + 1, J = N + 1, where N = |S| = |Z| |X | . Then, a matrix M t \u2208 R I\u00d7J is defined for k + 1 \u2264 t \u2264 n \u2212 2k, where M t (i, j) for 1 \u2264 i \u2264 I and 1 \u2264 j \u2264 J \u2212 1 represents the minimum (un-normalized) cumulative estimated loss of the sequence of single-symbol denoisers along the time index {\u03c4 : \u03c4 \u2264 t, c \u03c4 = c t }, allowing at most (i \u2212 1) shifts between single-symbol denoisers and applying s t = j. Moreover, M t (i, J), for 1 \u2264 i \u2264 I, is the symbol-by-symbol denoiser that attains the minimum value of the i-th row of M t , i.e., arg min 1\u2264j\u2264J\u22121 M t (i, j). A time pointer T \u2208 R D , where D = |C k | = |Z| 2k , is defined to store the closest time index that has the same context as current time, during the first and second pass. That is,T (c t )max{\u03c4 : \u03c4 < t, c \u03c4 = c t }, when first pass min{\u03c4 : \u03c4 > t, c \u03c4 = c t }, when second pass (45)We also define r \u2208 R D and q \u2208 R D as variables for storing the pointer enabling our scheme to follow the best combination of single-symbol denoisers during the second pass. Thus, the total memory size required is O(mN n + |Z| 2k ) = O(mn) (assuming that k satisfies the growth rate stipulated in the previous sections, which implies |Z| 2k = o(n)).Our two-pass algorithm has ingredients from both the DUDE and from the forward-backward recursions of hidden Markov models  #b27  and, in fact, the algorithm becomes equivalent to DUDE when m = 0. The first pass of the algorithm runs forward from t = k + 1 to t = n \u2212 k, and updates the elements of M t recursively. The recursions have a natural dynamic programming structure.For 2 \u2264 i \u2264 I, 1 \u2264 j \u2264 J \u2212 1, M t (i, j) is determined by M t (i, j) = \u2113(z t , j) + min M T (ct) (i, j), M T (ct ) (i \u2212 1, M T (ct) (i \u2212 1, J)) ,(46)that is, adding the current loss to the best cumulative loss up to T (c t ) along {\u03c4 : \u03c4 < t, c \u03c4 = c t }. When i = 1, the second term in the minimum of (46) is not defined, and M t (i, j) just becomes \u2113(z t , j)+M T (ct) (i, j). The validity of (46) can be verified by observing that there are two possible cases in achieving M t (i, j): either the (i \u2212 1)-th shift to the single-symbol denoiser j occurred before t, or it occurred at time t.We can see that the first term in the minimum of (46) corresponds to the former case; the second term corresponds to the latter. Obviously, the minimum of these two (where ties may be resolved arbitrarily), leads to the value of M t (i, j) as in (46). After updating all M t 's during the first pass, the second pass runs backwards from t = n \u2212 k to t = k + 1, and extracts\u015c k,m from {M t } n\u22122k t=k+1 by following the best shifting between single-symbol denoisers. The actual denoising (i.e., assembling the reconstruction sequenceX n ) is also performed in that pass. The pointers r(c t ) and q(c t ) are updated recursively, and they track the best shifting point and combination of single-symbol denoisers, respectively, for each of the subsequences associated with the various contexts. A succinct description of the algorithm is provided in Algorithm 1. The time complexity of the algorithm is readily seen to be O(mn) as well.Algorithm 1 The (k, m)-Shifting Discrete Denoising Algorithm  #b31  and the denoised outputRequire: M t (i, j) \u2208 R I\u00d7J , k + 1 \u2264 t \u2264 n \u2212 2k, 1 \u2264 i \u2264 I, 1 \u2264 j \u2264 J, T \u2208 R D ,r \u2208 R D , q \u2208 R D , L \u2208 R Ensure:\u015c k = {s k,t (c t , \u00b7)} n\u22122k t=k+1 in{x t } n\u2212k t=k+1 \u03c4 (c) \u21d0 \u03c6 for all c \u2208 C k for t = k + 1 to n \u2212 2k do if T (c t ) = \u03c6 then M t (i, j) \u21d0 \u2113(z t , j) for 1 \u2264 i \u2264 I, 1 \u2264 j \u2264 J \u2212 1 M t (i, J) \u21d0 arg min 1\u2264j\u2264J\u22121 M t (i, j) for 1 \u2264 i \u2264 I else M * T (ct) (i, j) \u21d0 M T (ct) (i, j) for i = 1, 1 \u2264 j \u2264 J \u2212 1 min M T (ct) (i, j), M T (ct ) (i \u2212 1, M T (ct) (i \u2212 1, J)) for 2 \u2264 i \u2264 I, 1 \u2264 j \u2264 J \u2212 1 M t (i, j) \u21d0 M * T (ct) (i, j) + \u2113(z t , j) for 1 \u2264 i \u2264 I, 1 \u2264 j \u2264 J \u2212 1 M t (i, J) \u21d0 arg min 1\u2264j\u2264J\u22121 M t (i, j) for 1 \u2264 i \u2264 I end if T (c t ) \u21d0 t end for T (c) \u21d0 \u03c6 for all c \u2208 C k for t = n \u2212 2k to k + 1 do if T (c t ) = \u03c6 then r(c t ) \u21d0 I, q(c t ) \u21d0 M t (r(c t ), J) else L \u21d0 M T (ct) (r(c t ), q(c t )) \u2212 \u2113(z t , q(c t )) if L < M t (r(c t ), q(c t )) then r(c t ) \u21d0 r(c t ) \u2212 1, q(c t ) \u21d0 M t (r(c t ), J) end if end if T (c t ) \u21d0 t, s k,t (c t , \u00b7) \u21d0 q(c t ) x t \u21d0 s k,t (c t , z t ) end for",
        "5-B Extending the S-DUDE to Multi-Dimensional Data": "As noted, our algorithm is essentially separately employing the same algorithm to compete with the best shifting single-symbol denoisers, on each subsequence associated with each context. The overall algorithm is the result of parallelizing the operations of the schemes for the different subsequences, which allows for a more efficient implementation than if these schemes were to be run completely independently of one another. This characteristic of running the same algorithm in parallel along each subsequence enables us to extend S-DUDE to the case of multi-dimensional data: run the same algorithm along each subsequence associated with each (this time multi-dimensional) context. It should be noted, however, that the extension of the S-DUDE to the multidimensional case is not as straightforward as the extension of the DUDE was, since, whereas the DUDE's output is independent of the ordering of the data within each context, this ordering may be very significant in its effect on the output and, hence, the performance of S-DUDE. Therefore, the choice of a scheme for scanning the data and capturing its local spatial stationarity, e.g., Peano-Hilbert scanning  #b28 , is an important ingredient in extending S-DUDE to the denoising of multidimensional data. Findings from the recent study on universal scanning reported in  #b29  #b30  can be brought to bear on such an extension.",
        "Experimentation": "In this section, we report some preliminary experimental results obtained by applying S-DUDE to several kinds of noise-corrupted data.",
        "6-A Image denoising": "In this subsection, we report some experimental results of denoising a binary image under the Hamming loss function. The first and most simplistic experiment is with the 400 \u00d7 400 black-and-white binary image shown in Figure 1. The first figure is the clean underlying image. The image is passed through a binary symmetric channel (BSC) with crossover probability \u03b4 = 0.1, to obtain the noisy image (second image in Figure 1). Note that in this case, there are only four symbol-by-symbol denoisers, namely, S = {0, 1, z,z}, representing always-say-0, always-say-1, say-what-you-see, and flip-what-you-see, respectively. The third image in Figure 1 is the DUDE output with k = 0, and the last image is the output of our S-DUDE with k = 0, m = 1. The DUDE with k = 0 is competes with the best time-invariant symbol-by-symbol denoiser which, in this case, is the say-what-you-see denoiser, since the empirical distribution of the clean image is (0.5, 0.5) and \u03b4 = 0.1. Thus, the DUDE output is the same as the noisy image; hence, no denoising is performed. However, it is clear that, for this image, the best compound action of the symbol-by-symbol denoisers is always-say-0 for the first half and then a shift to always-say-1 for the remainder. We can see that our (0, 1)-S-DUDE successfully captures this shift from the noisy observations, and results in perfect denoising with zero bit errors. Now, we move on to a more realistic example. The first image in Figure 2, a concatenation of a halftoned Einstein image (300 \u00d7 300) and scanned Shannon's 1948 paper (300 \u00d7 300), is the clean image. We pass the image through a binary symmetric channel (BSC) with crossover probability \u03b4 = 0.1, to obtain the second noisy image, which we raster scan and employ the S-DUDE on the resulting one-dimensional sequence. Since the two concatenated images are of a very different nature, we expect our S-DUDE to perform better than the DUDE, because it is designed to adapt to the possibility of employing different schemes in different regions of the data. The plot shows the performance of our (k, m)-S-DUDE with various values of k and m. The horizontal axis reflects k, and the vertical axis represents the ratio of bit error per symbol (BER) to \u03b4 = 0.1. Each curve represents the BER of schemes with different m = 0, 1, 2, 3. Note that m = 0 corresponds to the DUDE. We can see that S-DUDE with m > 0 mostly dominates the DUDE, with an additional BER reduction of \u223c 11%, including when k = 6, the best k value for the DUDE. The bottom three figures show the denoised images with (k, m) = (4, 0), (4, 2), (6, 1), achieving BERs of \u03b4 \u00d7 (0.744, 0.6630, 0.4991), respectively. Thus, in this example, (4, 2)-S-DUDE achieves an additional BER reduction of 11% over the DUDE with k = 4, and the overall best performance is achieved by (6, 1)-S-DUDE. Given the nature of the image, which is a concatenation of two completely different types of images, each reasonably uniform in texture, it is not surprising to find that the S-DUDE with m = 1 performs the best.  ",
        "6-B State estimation for a switching binary hidden Markov process": "Here, we give a stochastic setting experiment. A switching binary hidden Markov process in this example is defined as a binary symmetric Markov chain observed through a BSC, where the transition probabilities of the Markov chain switches over time. The goal of a denoiser here is to estimate the underlying Markov chain based on the noisy output.In our example, we construct a simple switching binary hidden Markov process of length n = 10 6 , in which the transition probability of the underlying binary symmetric Markov source switches from p = 0.01 to p = 0.2 at the midpoint of the sequence, and the crossover probability of BSC is \u03b4 = 0.1. Then, we estimate the state of the underlying Markov chain based on the BSC output. The goodness of the estimation is again measured by the Hamming loss, i.e., the fraction of errors made. Slightly better than the optimal Bayesian distribution-dependent performance for this case can be obtained by employing the forward-backward recursion scheme, incorporating the varying transition probabilities with the help of a genie that knows the exact location of the change in the process distribution. Figure 3 plots the BER of (k, m)-S-DUDE with various k and m, compared to the genie-aided Bayes optimal BER. The horizontal axis represents k, and the two curves refer to m = 0 (DUDE) and m = 1. The vertical axis is the ratio of BER to \u03b4 = 0.1. We can observe that the optimal Bayesian BER is (lower bounded by) 0.4865\u00d7\u03b4. The best performance of the DUDE was achieved when k = 6 with a BER of 0.5738 \u00d7 \u03b4, which is far above (18% more than) the optimal BER. It is clear that, despite the size of the data, the DUDE fails to converge to the optimum, as it is confined to be employing the same sliding-window scheme throughout the whole data. However, we can see that the (4, 1)-S-DUDE achieves a BER of .4979 \u00d7 \u03b4, which is within 2.3% of the optimal BER. This example shows that our S-DUDE is competent in attaining the optimum performance for a class richer than that of the stationary processes. Specifically, it attains the optimum performance for piecewise stationary processes, on which the DUDE generally fails.",
        "Conclusion and Some Future Directions": "Inspired by the DUDE algorithm, we have developed a generalization that accommodates switching between sliding window rules. We have shown a strong semi-stochastic setting result for our new scheme in competing with shifting k-th order denoisers. This result implies a stochastic setting result as well, asserting that the S-DUDE asymptotically attains the optimal distribution-dependent performance for the case in which the underlying data is piecewise stationary. We also described an efficient low-complexity implementation of the algorithm, and presented some simple experiments that demonstrate the potential benefits of employing S-DUDE in practice.There are several future research directions related to this work. The S-DUDE can be thought of as a generalization of the DUDE, with the introduction of a new component captured by the non-negative integer parameter m. Many previous extensions of the DUDE, such as the settings of channel with memory  #b33 , channel uncertainty  #b32 , applications to channel decoding  #b36 , discrete-input, continuous-output data  #b34 , denoising of analog data  #b31 , and decoding in the Wyner-Ziv problem  #b35 , may stand to benefit from a revision that would incorporate the viewpoint of switching between time-invariant schemes. Particularly, extending S-DUDE to the case where the the data are analog as in  #b31  will be non-trivial and interesting from both a theoretical and a practical viewpoint. In addition, as mentioned in Section 5, an extension of the S-DUDE to the case of multi-dimensional data is not as straightforward as the extension of the DUDE was. Such an extension should prove interesting and practically important. Finally, it would be useful to devise guidelines, in the spirit of those in  #b37  #b2 , for the choice of k and m based on n and the noisy observation sequence z n .",
        "Acknowledgments": "The first author is grateful to Professor Manfred Warmuth for introducing him to a substantial amount of related work on the expert tracking problems in online learning.",
        "Appendix": "",
        "8-A Proof of Lemma 1": "We first establish the fact that for all x n \u2208 X n , and for fixed S \u2208 S n 0 ,n LX n,S (x n , Z n ) \u2212L S (Z n ) n\u22651is a {Z n }-martingale. This is not hard to see by following:E n[LX n,S (x n , Z n ) \u2212L S (Z n )] Z n\u22121 = E n t=1 \u039b(x t , s t (Z t )) \u2212 n t=1 \u2113(Z t , s t ) Z n\u22121 = (n \u2212 1)[LX n\u22121,S (x n\u22121 , Z n\u22121 ) \u2212L S (Z n\u22121 )] + E \u039b(x n , s n (Z n )) \u2212 \u2113(Z n , s n ) Z n\u22121 = (n \u2212 1)[LX n\u22121,S (x n\u22121 , Z n\u22121 ) \u2212L S (Z n\u22121 )],(47)where (47) follows from the fact that Z n is independent of Z n\u22121 , and E\u039b(x n , s n (Z n )) = E\u2113(Z n , s n ). Therefore, L S (x n , Z n ) \u2212L S (Z n ) is a normalized sum of bounded martingale differences; therefore the inequalities  #b19  and (21) follow directly from the Hoeffding-Azuma inequality  #b13 Lemma A.7].",
        "8-B Proof of Theorem 2": "Consider following chain of inequalities:P LX n,\u015c (x n , Z n ) \u2212 D 0,m (x n , Z n ) > \u01eb = P max S\u2208S n 0,m LX n,\u015c (x n , Z n ) \u2212 LX n,S (x n , Z n ) > \u01eb \u2264 S\u2208S n 0,m P LX n,\u015c (x n , Z n ) \u2212 LX n,S (x n , Z n ) > \u01eb (48) \u2264 S\u2208S n 0,m P LX n,\u015c (x n , Z n ) \u2212L\u015c(Z n ) > \u01eb/2 (i) + S\u2208S n 0,m P L\u015c (Z n ) \u2212 LX n,S (x n , Z n ) > \u01eb/2 (ii) ,(49)where (48) follows from the union bound, and (49) follows from adding and subtractingL\u015c(Z n ), and the union bound. For term (i) in (49),(i) \u2264 S\u2208S n 0,m P max S\u2208S n 0,m LX n,S (x n , Z n ) \u2212L S (Z n ) > \u01eb/2 (50) \u2264 S\u2208S n 0,m S\u2208S n 0,m exp \u2212 n \u01eb 2 2L 2 max ,(51)where (50) follows from LX n,\u015c (x n , Z n ) \u2212L\u015c(Z n ) \u2264 max S\u2208S n 0,m LX n,S (x n , Z n ) \u2212L S (Z n ) , and (51) follows from the union bound and  #b19 . Similarly, for term (ii) in (49),(ii) \u2264 S\u2208S n 0,m P L S (Z n ) \u2212 LX n,S (x n , Z n ) > \u01eb/2 (52) \u2264 S\u2208S n 0,m exp \u2212 n \u01eb 2 2L 2 max ,(53)where (52) follows fromL\u015c(Z n ) \u2264L S (Z n ) a.s., and (53) follows from  #b20 . Therefore, continuing (49), we obtain(49) \u2264 2 S\u2208S n 0,m S\u2208S n 0,m exp \u2212 n \u01eb 2 2L 2 max = 2 m k=0 n \u2212 1 k N (N \u2212 1) k 2 exp \u2212 n \u01eb 2 2L 2 max (54) \u2264 2 exp \u2212n \u01eb 2 2L 2 max \u2212 2h m n \u2212 2(m + 1) ln N n ,(55)where (54) follows from |S n 0,m | = m k=0 n\u22121 k N (N \u22121) k , and (55) follows from |S n 0,m | \u2264 N m+1 exp nh( m n ) . Hence, the theorem is proved.",
        "8-C Proof of Lemma 2": "We will prove (28) since the proof of (29) is essentially identical. As in  #b0 , defineI d {t : k + 1 \u2264 t \u2264 n \u2212 k, t \u2261 d mod (k + 1)},whose cardinality is denoted n d = \u230a(n \u2212 d \u2212 k)/(k + 1)\u230b. Then, by denoting C t = (Z t\u22121 t\u2212k , Z t+k t+1 ), we start the chain of inequalities,Pr LXn,S k (x n\u2212k k+1 , Z n ) \u2212L S k (Z n ) > \u01eb \u2264 Pr k d=0 \u03c4 \u2208I d \u039b x \u03c4 , s k,\u03c4 (C \u03c4 , Z\u03c4 ) \u2212 \u2113 Z \u03c4 , s k,\u03c4 (C \u03c4 , \u00b7) > (n \u2212 2k)\u01eb (56) \u2264 k d=0 Pr \u03c4 \u2208I d \u039b x \u03c4 , s k,\u03c4 (C \u03c4 , Z\u03c4 ) \u2212 \u2113 Z \u03c4 , s k,\u03c4 (C \u03c4 , \u00b7) > (n \u2212 2k)\u03b3 d \u01eb ,(57)where (56) follows from the triangle inequality, (57) follows from the union bound, and {\u03b3 d } is a set of nonnegative constants (to be specified later) satisfying d \u03b3 d = 1. In the sequel, for simplicity, we will denote \u039b x \u03c4 , s k,\u03c4 (C \u03c4 , Z\u03c4 ) and \u2113 Z \u03c4 , s k,\u03c4 (C \u03c4 , \u00b7) in (48) as \u039b \u03c4 and \u2113 \u03c4 , respectively. Now, the collection of random variables Z(d) is defined to beZ(d) {Z t : 1 \u2264 t \u2264 n, t / \u2208 I d },and z(d) \u2208 Z n\u2212n d denotes a particular realization of Z(d). Then, by conditioning, we have(57) \u2264 k d=0 z(d)\u2208Z n\u2212n d Pr(Z(d) = z(d))Pr \u03c4 \u2208I d \u039b \u03c4 \u2212 \u2113 \u03c4 > (n \u2212 2k)\u03b3 d \u01eb Z(d) = z(d) ,(58)and let P d denote the conditional probability of (58). Now, conditioned on Z(d) = z(d), {Z \u03c4 } \u03c4 \u2208I d are all independent, and the summation in P d beomes\u03c4 \u2208I d \u039b x \u03c4 , s k,\u03c4 (c \u03c4 , Z\u03c4 ) \u2212 \u2113 Z \u03c4 , s k,\u03c4 (c \u03c4 , \u00b7) ,which is the sum of the absolute differences of the true and estimated losses of the symbol-by-symbol denoisers s k,\u03c4 (c \u03c4 , \u00b7) over \u03c4 \u2208 I d . Thus, we can apply  #b19 , and obtainP d = Pr \u03c4 \u2208I d \u039b \u03c4 \u2212 \u2113 \u03c4 > n d \u00b7 (n \u2212 2k)\u03b3 d \u01eb n d Z(d) = z(d) \u2264 exp \u2212 2(n \u2212 2k) 2 \u03b3 2 d \u01eb 2 L 2 max n d .(59)Following  #b0 , we choose \u03b3 d = \u221a n d P j \u221a n j , and from the Cauchy-Schwartz inequality and d n d = n \u2212 2k, we arrive at n d \u03b3 2 d \u2264 (k + 1) k d=0 n d = (k + 1)(n \u2212 2k), and, hence,P d \u2264 exp \u2212 2(n \u2212 2k)\u01eb 2 (k + 1)L 2 max .(60)Therefore, plugging (60) into (58), we finally have(58) \u2264 (k + 1) exp \u2212 2(n \u2212 2k)\u01eb 2 (k + 1)L 2 max ,which proves the lemma.",
        "8-D Proof of Theorem 3": "The proof resembles that of Theorem 2. ConsiderPr LX n,\u015c k,m (x n\u2212k k+1 , Z n ) \u2212 D k,m (x n , Z n ) > \u01eb = P max S\u2208S n k,m LX n,\u015c k,m (x n\u2212k k+1 , Z n ) \u2212 LX n,S (x n\u2212k k+1 , Z n ) > \u01eb \u2264 S\u2208S n k,m P LX n,\u015c k,m (x n\u2212k k+1 , Z n ) \u2212 LX n,S (x n\u2212k k+1 , Z n ) > \u01eb (61) \u2264 S\u2208S n k,m P LX n,\u015c k,m (x n\u2212k k+1 , Z n ) \u2212L\u015c k,m (Z n ) > \u01eb 2 + P L\u015c k,m (Z n ) \u2212 LX n,S (x n\u2212k k+1 , Z n ) > \u01eb 2 (62) \u2264 2(k + 1) S\u2208S n k,m S\u2208S n k,m exp \u2212 (n \u2212 2k)\u01eb 2 2(k + 1)L 2 max (63) = 2(k + 1) m(c) k=0 n(c) \u2212 1 k N (N \u2212 1) k 2|C k | exp \u2212 (n \u2212 2k)\u01eb 2 2(k + 1)L 2 max ,(64)where (61) N (N \u2212 1) k |C k | . Now, for all c \u2208 C k , m(c) k=0 n(c) \u2212 1 k N (N \u2212 1) k \u2264 N m+1 exp n(c)h m(c) n(c) \u2264 N m+1 exp (n \u2212 2k)h m(c) n \u2212 2k (65) \u2264 N m+1 exp (n \u2212 2k)h m n \u2212 2k ,(66)where (65) is based on the fact that exp(nh( m n )) is an increasing function in n, and (66) follows from m \u2264 \u230a n\u22122k 2 \u230b. Therefore, together with |C k | = |Z| 2k , we have(64) \u2264 2(k + 1) exp \u2212 (n \u2212 2k) \u00b7 \u01eb 2 2(k + 1)L 2 max \u2212 2|Z| 2k \u00b7 h m n \u2212 2k + (m + 1) ln N n \u2212 2k ,(67)which proves the theorem.",
        "8-E Proof of Claim 1": "For part a), to show the necessity first, suppose c 1 \u2265 ( m n ) 1\u2212\u03b4 , which will grow to infinity as n grows, even when m is fixed. Therefore, the right-hand side of (34) is not summable. On the other hand, k = c 1 log n with c 1 < 1 2 log |Z| is readily verified to suffice for the summability, provided that m = m n grows at any sub-polynomial rate, i.e., grows more slowly than n \u03b1 for any \u03b1 > 0 (e.g., c 2 log n).For part b), to show the necessity, suppose m = \u0398(n). Then, h( m n\u22122k ) + (m+1) ln N n\u22122k = \u0398(1), and, thus, for sufficiently small \u01eb,\u01eb 2 2(k+1)L 2 max \u2212 |Z| 2k \u00b7 h m n\u22122k + (m+1) log N n\u22122k< 0 even for k fixed. Therefore, the right-hand side of (34) is not summable. Hence, m = o(n) is necessary for the summability. For sufficiency, suppose m = m n is any rate, such that lim n\u2192\u221e mn n = 0. Then,\u01eb 2 2(k + 1)L 2 max \u2212 2|Z| 2k \u00b7 h m n \u2212 2k + (m + 1) log N n \u2212 2k = 1 k \u01eb 2 2(1 + 1 k L 2 max ) \u2212 2k|Z| 2k \u00b7 O m n n 1\u2212\u03b4 .(68)Thus, if k grows sufficiently slowly that k|Z| 2k = o ( n mn ) 1\u2212\u03b4 , then (68) becomes positive for sufficiently large n, and the right-hand side of (34) becomes summable.",
        "8-F Proof of Theorem 4": "First, denote the random variable A n k,m LXn,k,m univ (x n\u2212k k+1 , Z n ) \u2212 D k,m (x n , Z n ). Then, for part a), we haveLXn,k,m univ (x n , Z n ) \u2212 D k,m (x n , Z n ) \u2264 2k\u039b max n + A n k,m a.s.Since the maximal rate for k is c 1 log n as specified in Claim 1, lim n\u2192\u221e 2k\u039bmax n = 0. Furthermore, from the summability condition on k and m, Theorem 3, and the Borel-Cantelli lemma, we get lim n\u2192\u221e A n k,m = 0 with probability 1, which proves part a). To prove part b), note that, for any \u01eb > 0,E LXn,k,m univ (x n , Z n ) \u2212 D k,m (x n , Z n ) \u2264 2k\u039b max n + E(A n k,m ) = 2k\u039b max n + E(A n k,m |A n k,m \u2264 \u01eb)P r(A n k,m \u2264 \u01eb) + E(A n k,m |A n k,m > \u01eb)P r(A n k,m > \u01eb) \u2264 2k\u039b max n + \u01eb + \u039b max \u00b7 P r(A n k,m > \u01eb)\u2264 2k\u039b max n + \u01eb + \u039b max \u00b7 (right-hand side of (34)).From the proof of Claim 1, the condition of Theorem 4 requires k = k n and m = m n to satisfy lim n\u2192\u221e k n |Z| 2kn ( m n n ) 1\u2212\u03b4 = 0.Therefore, if we set \u01eb 2 = \u0398(k n |Z| 2kn ( mn n ) 1\u2212\u03b4 ) with sufficiently large constant then, from (68), we can see that the right-hand side of (34) will decay almost exponentially, which is much faster than \u0398(k n |Z| 2kn ( mn n ) 1\u2212\u03b4 ). Hence, from (69), we conclude that E(A n k,m ) = O k n |Z| 2kn ( mn n ) 1\u2212\u03b4 , which results in part b).",
        "8-G Proof of Theorem 5": "The fact that m = \u0398(n) implies the existence of \u03b1 > 0, such that m \u2265 n\u03b1 for all sufficiently large n. Let X be the process formed by concatenating i.i.d. blocks of length \u23081/\u03b1\u2309, each block consisting of the same repeated symbol chosen uniformly from X . The first observation to note is that, for all n large enough that m \u2265 n\u03b1, D 0,m (X n , Z n ) = 0 a.s.This is because, by construction, X n is, with probability 1, piecewise constant with constancy sub-blocks of length, at least, \u23081/\u03b1\u2309. Thus, a genie with access to X n can choose a sequence of symbol-by-symbol schemes (in fact, ignoring the noisy sequence), with less than n\u03b1 (and, therefore, less than m) switches, that perfectly recover X n (and, therefore, by our assumption on the loss function, suffers zero loss). On the other hand, the assumptions on the loss function and the channel imply that, for the process X just constructed, lim sup n\u2192\u221e min X n ELX n (X n , Z n ) > 0,since even the Bayes-optimal scheme for this process incurs a positive loss, with a positive probability, on each \u23081/\u03b1\u2309 super-symbol. Thus, we getE lim sup n\u2192\u221e E LX n (X n , Z n ) \u2212 D 0,m (X n , Z n )|X n (72) \u2265 lim sup n\u2192\u221e E LX n (X n , Z n ) \u2212 D 0,m (X n , Z n ) (73) = lim sup n\u2192\u221e ELX n (X n , Z n ) (74) \u2265 lim sup n\u2192\u221e min X n ELX n (X n , Z n ) > 0,(75)where (73) follows from Fatou's lemma; (74) follows from (70); and (75) follows from (71). In particular, there must be one particular individual sequence x \u2208 X \u221e for which the expression inside the curled brackets of (72) is positive, i.e., lim supn\u2192\u221e E LX n (X n , Z n ) \u2212 D 0,m (X n , Z n )|X n = x n > 0,(76)which is equivalent to (37).",
        "8-H Proof of Theorem 6": "First, by adding and subtracting the same terms, we obtainELXn,k,m univ (X n , Z n ) \u2212 D(P X n , \u03a0)= ELXn,k,m univ (X n , Z n ) \u2212 min S\u2208S n k,m ELX n,S (X n , Z n )(i) + min S\u2208S n k,m ELX n,S (X n , Z n ) \u2212 D(P X n , \u03a0) (ii) .(77)We will consider term (i) and term (ii) separately. For term (i),(i) = ELXn,k,m univ (X n , Z n ) \u2212 min S\u2208S n k,m ELX n,S (X n , Z n ) \u2264 2k\u039b max n + n \u2212 2k n \u00b7 ELXn,k,m univ (X n\u2212k k+1 , Z n ) \u2212 min S\u2208S n k,m ELX n,S (X n\u2212k k+1 , Z n ) (78) \u2264 2k\u039b max n + n \u2212 2k n \u00b7 E LXn,k,m univ (X n\u2212k k+1 , Z n ) \u2212 min S\u2208S n k,m LX n,S (X n\u2212k k+1 , Z n ) (79) \u2264 2k\u039b max n + E LXn,k,m univ (X n\u2212k k+1 , Z n ) \u2212 D k,m (X n , Z n ) ,(80)where (78) follows from upper bounding and omitting the losses for time instances t \u2264 k and t > n \u2212 k in the first and second terms of (i), respectively; (79) follows from exchanging the minimum with the expectation, and (80) follows from the definition (31) and n\u22122k n \u2264 1.For term (ii), we bound the first term in (ii) as min S\u2208S n k,m ELX n,S (X n , Z n ) \u2264 2k(m + 1)\u039b max n + 1 n minS\u2208S n k,m E E r+1 i=1 \u03c4 i \u2212k j=\u03c4 i\u22121 +k+1\u039b(X j , s k,j (Z j+k j\u2212k )) A n ,by upper bounding the losses with \u039b max on the boundary of the shifting points. Now, let P X j |Z l i ,A n \u2208 R |X | denote the |X |-dimensional probability vector whose x-th component is P r(X j = x|Z l i , A n ). Then, we can bound the second term in (81) by the following chain of inequalities:1 n min S\u2208S n k,m E E r+1 i=1 \u03c4 i \u2212k j=\u03c4 i\u22121 +k+1\u039b(X j , s k,j (Z j+k j\u2212k )) A n (82)= 1 n E r+1 i=1 \u03c4 i \u2212k j=\u03c4 i\u22121 +k+1 min s k \u2208S k E \u039b(X j , s k (Z j+k j\u2212k )) A n (83) = 1 n E r+1 i=1 \u03c4 i \u2212k j=\u03c4 i\u22121 +k+1 z k \u2212k \u2208Z 2k+1 P (Z j+k j\u2212k = z k \u2212k |A n ) min x\u2208X E \u039b(X j ,x)|Z j+k j\u2212k = z k \u2212k , A n (84) = 1 n E r+1 i=1 \u03c4 i \u2212k j=\u03c4 i\u22121 +k+1 z k \u2212k \u2208Z 2k+1 P (Z j+k j\u2212k = z k \u2212k |A n )U \u039b (P X j |Z j+k j\u2212k =z k \u2212k ,A n ) (85) = 1 n E r+1 i=1 \u03c4 i \u2212k j=\u03c4 i\u22121 +k+1 E U \u039b (P X j |Z j+k j\u2212k ,A n ) A n ] = 1 n E r+1 i=1 \u03c4 i \u2212k j=\u03c4 i\u22121 +k+1 E U \u039b (P (A\u03c4 i ) X 0 |Z k \u2212k )|A n (86) \u2264 1 n E r+1 i=1 \u03c4 i j=\u03c4 i\u22121 +1 E U \u039b (P (A\u03c4 i ) X 0 |Z k \u2212k )|A n ,(87)where (83) follows from the stationarity of the distribution in each block as well as the fact that the combination of the best k-th order sliding window denoiser for each block is in S n k,m and achieves the minimum in (82); (84) follows from conditioning; (85) follows from the definition (2); (86) follows from the stationarity of the distribution in each i-th block; and (87) follows from adding more nonnegative terms.For the second term in (ii), we first definen i (A n ) \u03c4 i (A n ) \u2212 \u03c4 i\u22121 (A n )as the length of the i-th block, for 1 \u2264 i \u2264 r(A n ) + 1. Obviously, n i (A n ) also depends on A n , and, thus, is a random variable, but we again suppress A n for brevity and denote it as n i . Then, similar to the first term above, we obtain D(P X n , \u03a0) = min X n \u2208Dn ELX n (X n , Z n ) = 1 n minX n \u2208Dn E E r+1 i=1 \u03c4 i j=\u03c4 i\u22121 +1 \u039b(X j ,X j (Z n )) A n = 1 n E r+1 i=1 \u03c4 i j=\u03c4 i\u22121 +1 min X:Z n \u2192X E \u039b(X j ,X(Z n )) A n = 1 n E r+1 i=1 \u03c4 i j=\u03c4 i\u22121 +1 min X:Z n i \u2192X E \u039b(X j ,X(Z \u03c4 i \u03c4 i\u22121 +1 )) A n (88) = 1 n E r+1 i=1 \u03c4 i j=\u03c4 i\u22121 +1 E U \u039b (P X j |Z \u03c4 i \u03c4 i\u22121 +1 ,A n ) A n = 1 n E r+1 i=1 \u03c4 i j=\u03c4 i\u22121 +1 E U \u039b (P (A\u03c4 i ) X 0 |Z n i \u2212j 1\u2212j ) A n (89) \u2265 1 n E r+1 i=1 \u03c4 i j=\u03c4 i\u22121 +1 E U \u039b (P (A\u03c4 i ) X 0 |Z \u221e \u2212\u221e ) A n ,(90)where (88) follows from the conditional independence between different blocks, given A n ; (89) follows from the stationarity of the distribution in each block, and (90)  \u2264 2k(m + 1)\u039b max n+ 1 n E r+1 i=1 \u03c4 i j=\u03c4 i\u22121 +1 E U \u039b (P (A\u03c4 i ) X 0 |Z k \u2212k )|A n \u2212 E U \u039b (P (A\u03c4 i ) X 0 |Z \u221e \u2212\u221e ) A n = 2k(m + 1)\u039b max n + E r+1 i=1 n i n \u00b7 E U \u039b (P (A\u03c4 i ) X 0 |Z k \u2212k )|A n \u2212 E U \u039b (P (A\u03c4 i ) X 0 |Z \u221e \u2212\u221e ) A n .(91)Now, observe that, regardless of A n , the sequence of numbers { n i n } r+1 i=1 form a probability distribution, since r+1 i=1 n i n = 1 and n i n \u2265 0 for all i, with probability 1. Then, based on the fact that the average is less than the maximum, we obtain the further upper bound(91) \u2264 2k(m + 1)\u039b max n + E max i\u2208{1,\u00b7\u00b7\u00b7 ,M } E U \u039b (P (i) X 0 |Z k \u2212k ) \u2212 E U \u039b (P (i) X 0 |Z \u221e \u2212\u221e ) .(92)The remaining argument to prove the theorem is to show that the upper bounds (80) and (92) converge to 0 as n tends to infinity. First, from the given condition on k = k n and m = m n , the maximal allowable growth rate for k is k = c 1 log n, which leads to lim n\u2192\u221e 2k\u039bmax n = 0. In addition, the condition requires m = o(n), and k to be sufficiently slow, such that k|Z| 2k = o ( n m ) 1\u2212\u03b4 , which implies k = o( n m ). Therefore, lim n\u2192\u221e 2k(m+1)\u039bmax n = 0. Furthermore, from conditioning on X n , bounded convergence theorem, and part b) of Theorem 4, we obtain lim n\u2192\u221e E[LXn,k,m univ (X n\u2212k k+1 , Z n ) \u2212 D k,m (X n , Z n )] = 0. Thus, we have lim sup n\u2192\u221e ELXn,k,m univ (X n , Z n ) \u2212 D(P X n , \u03a0)\u2264 lim sup n\u2192\u221e E max i\u2208{1,\u00b7\u00b7\u00b7 ,M } E U \u039b (P (i) X 0 |Z k \u2212k ) \u2212 E U \u039b (P (i) X 0 |Z \u221e \u2212\u221e ) \u2264 E lim sup n\u2192\u221e max i\u2208{1,\u00b7\u00b7\u00b7 ,M } E U \u039b (P (i) X 0 |Z k \u2212k ) \u2212 E U \u039b (P (i) X 0 |Z \u221e \u2212\u221e ) (93) = 0,(94)where (93) follows from the reverse Fatou's lemma, and (94) follows from [1, Lemma 4(2)] and M being finite. Since it is clear that lim inf n\u2192\u221e [ELXn,k,m univ (X n , Z n ) \u2212 D(P X n , \u03a0)] \u2265 0 by definition of D(P X n , \u03a0), the theorem is proved.Remark: As in [1, Theorem 3], the convergence rate in (44) may depend on P X , and there is no vanishing upper bound on this rate that holds for all P X \u2208 P{m n }. However, we can glean some insight into the convergence rate from (i) and (ii): whereas the term (i) is uniformly upper bounded for all P X \u2208 P{m n },  $b8  the rate at which term (ii) vanishes depends on P X . In general, we observe that the slower the rate of increase of k = k n , the faster the convergence in (i), but the convergence in (ii) is slower. With respect to the rate of increase of m n , the slower it is, the faster the convergence in (i), but whether or not the convergence in (ii) is accelerated by a slower rate of increase of m n may depend on the underlying process distribution P X ."
    },
    {
        "2": "Note that, when m = 0, S n 0,0 is the set of constant n-tuples consisting of the same single-symbol denoiser.",
        "3": "Again, the value ofX S k t (z n ) for t \u2264 k and t > n \u2212 k can be defined as an arbitrary fixed symbol, since it will be inconsequential in subsequent development.",
        "4": "When m = 0, S n k,0 (z n ) becomes the set of n-block k-th order 'sliding window' denoisers.",
        "8": "Recall part b) of Theorem 4, where a uniform bound (uniform in the underlying individual sequence) on E\u02c6LXn,k,m univ (x n , Z n ) \u2212 D k,m (x n , Z n )\u02dcwas provided in the semi-stochastic setting. Clearly, in the stochastic setting the same bound holds on E\u02c6LXn,k,m univ (X n , Z n ) \u2212 D k,m (X n , Z n )\u02dc, regardless of the distribution of X n ."
    },
    {
        "b0": [
            "Universal discrete denoising: Known channel",
            "",
            "",
            "",
            "Weissman",
            "Ordentlich",
            "Seroussi",
            "Verd\u00fa",
            "Weinberger"
        ],
        "b1": [
            "Universal filtering via prediction",
            "",
            "",
            "",
            "Weissman",
            "Ordentlich",
            "Weinberger",
            "Somekh-Baruch",
            "Merhav"
        ],
        "b2": [
            "Multi-directional context sets with applications to universal denoising and compression",
            "",
            "",
            "",
            "Ordentlich",
            "Weinberger",
            "Weissman"
        ],
        "b3": [
            "Compression of individual sequences via variable-rate coding",
            "",
            "",
            "",
            "Ziv",
            "Lempel"
        ],
        "b4": [
            "Universal prediction",
            "",
            "",
            "",
            "Merhav",
            "Feder"
        ],
        "b5": [
            "Controlled random walks",
            "",
            "",
            "",
            "Blackwell"
        ],
        "b6": [
            "An analog of the minimax theorem for vector payoffs",
            "",
            "",
            "",
            "Blackwell"
        ],
        "b7": [
            "Approximation to Bayes risk in repeated play",
            "",
            "",
            "",
            "Hannan"
        ],
        "b8": [
            "The cost of achieving the best portfolio in hindsight",
            "",
            "",
            "",
            "Ordentlich",
            "Cover"
        ],
        "b9": [
            "",
            "",
            "The weighted majority algorithm",
            ""
        ],
        "b10": [
            "Aggregating strategies",
            "",
            "",
            "",
            "Vovk"
        ],
        "b11": [
            "Efficient algorithms and minimax bounds for zero-delay lossy source coding",
            "",
            "",
            "",
            "Gyorgy",
            "Linder",
            "Lugosi"
        ],
        "b12": [
            "Universal zero-delay joint source-channel coding",
            "",
            "",
            "",
            "Matloub",
            "Weissman"
        ],
        "b13": [
            "",
            "",
            "Prediction, learning, and games",
            ""
        ],
        "b14": [
            "Tracking the best expert",
            "",
            "",
            "",
            "Herbster",
            "Warmuth"
        ],
        "b15": [
            "Tracking a small set of experts by mixing past posteriors",
            "",
            "",
            "",
            "Bousquet",
            "Warmuth"
        ],
        "b16": [
            "Low-complexity sequential lossless coding for piecewise-stationary memoryless sources",
            "",
            "",
            "",
            "Shamir",
            "Merhav"
        ],
        "b17": [
            "Coding for binary independent piecewise identically distributed source",
            "",
            "",
            "",
            "Willems"
        ],
        "b18": [
            "Universal piecewise constant and least squares prediction",
            "",
            "",
            "",
            "Kozat",
            "Singer"
        ],
        "b19": [
            "Swithcing portfolios",
            "",
            "",
            "",
            "Singer"
        ],
        "b20": [
            "On the redundancy of universal lossless coding for general piecewise stationary sources",
            "",
            "",
            "",
            "Shamir",
            "Costello"
        ],
        "b21": [
            "Tracking the best quantizer",
            "",
            "",
            "",
            "Gy\u00f6rgy",
            "Linder",
            "Lugosi"
        ],
        "b22": [
            "Confidence sets in change-point problems",
            "",
            "",
            "",
            "Siegmund"
        ],
        "b23": [
            "Using the generalized likelihood ratio statistic for sequential detection of a change-point",
            "",
            "",
            "",
            "Siegmund",
            "Venkatraman"
        ],
        "b24": [
            "Parameterized duration modeling for switching linear dynamic systems",
            "",
            "",
            "",
            "Oh",
            "Rehg",
            "Dellaert"
        ],
        "b25": [
            "Switching linear dynamical systems for noise robust speech recognition",
            "",
            "",
            "",
            "Mesot",
            "Barber"
        ],
        "b26": [
            "",
            "",
            "The theory of matrices",
            ""
        ],
        "b27": [
            "Hidden Markov processes",
            "",
            "",
            "",
            "Ephraim",
            "Merhav"
        ],
        "b28": [
            "Compression of two-dimensional data",
            "",
            "",
            "",
            "Lempel",
            "Ziv"
        ],
        "b29": [
            "",
            "",
            "Scanning and sequential decision making for multidimensional data -Part I: the noiseless Case",
            ""
        ],
        "b30": [
            "Scanning and sequential decision making for multidimensional data -Part II: the noisy case",
            "",
            "",
            "",
            "Cohen",
            "Merhav",
            "Weissman"
        ],
        "b31": [
            "Universal denoising of discrete-time continuous-amplitude signals",
            "http://www.stanford.edu/~tsachy/ieee_it_draft.pdf",
            "",
            "",
            "Sivaramakrishnan",
            "Weissman"
        ],
        "b32": [
            "Universal minimax discrete denoising under channel uncertainty",
            "",
            "",
            "",
            "Gemelos",
            "Sigurjonsson",
            "Weissman"
        ],
        "b33": [
            "Discrete denoising for channels with memory",
            "",
            "",
            "",
            "Zhang",
            "Weissman"
        ],
        "b34": [
            "Universal denoising for the finite-input-general-output channel",
            "",
            "",
            "",
            "Dembo",
            "Weissman"
        ],
        "b35": [
            "A universal Wyner-Ziv scheme for discrete sources",
            "",
            "",
            "",
            "Jalali",
            "Verd\u00fa",
            "Weissman"
        ],
        "b36": [
            "Universal algorithms for channel decoding of uncompressed sources",
            "",
            "",
            "",
            "Ordentlich",
            "Seroussi",
            "Verd\u00fa",
            "Viswanathan"
        ],
        "b37": [
            "Schemes for bidirectional modeling of discrete stationary sources",
            "",
            "",
            "",
            "Yu",
            "Verd\u00fa"
        ]
    },
    {
        "tab_0": "1 21log |Z| . Then, from |Z| 2k = n2k log |Z| \nlog n \n\n, we have \n\n2|Z| 2k \u00b7 {h( m \nn\u22122k ) + (m+1) ln N \n\nn\u22122k \n\n} = \u2126 n \n\n2k log |Z| \nlog n \n\n"
    }
]