Testing k-Monotonicity (The Rise and Fall of Boolean Functions)

Abstract
A Boolean k-monotone function defined over a finite poset domain D alternates between the values 0 and 1 at most k times on any ascending chain in D. Therefore, k-monotone functions are natural generalizations of the classical monotone functions, which are the 1-monotone functions. Motivated by the recent interest in k-monotone functions in the context of circuit complexity and learning theory, and by the central role that monotonicity testing plays in the context of property testing, we initiate a systematic study of k-monotone functions, in the property testing model. In this model, the goal is to distinguish functions that are k-monotone (or are close to being k-monotone) from functions that are far from being k-monotone. Our results include the following: 1. We demonstrate a separation between testing k-monotonicity and testing monotonicity, on the hypercube domain {0, 1} d , for k ≥ 3; 2. We demonstrate a separation between testing and learning on {0, 1} d , for k = ω(log d): testing k-monotonicity can be performed with 2 O( √ d·log d·log 1/ε) queries, while learning k-monotone functions requires 2 Ω(k· √ d·1/ε) queries (Blais et al. (RANDOM 2015)). 3. We present a tolerant test for functions f : [n] d → {0, 1} with complexity independent of n, which makes progress on a problem left open by Berman et al. (STOC 2014). Our techniques exploit the testing-by-learning paradigm, use novel applications of Fourier analysis on the grid [n] d , and draw connections to distribution testing techniques.

Introduction
A function f : D → {0, 1}, defined over a finite domain D equipped with a partial order, is said to be k-monotone, for some integer k ≥ 0, if there does not exist x 1 x 2 . . . x k+1 in D such that f (x 1 ) = 1 and f (x i ) = f (x i+1 ) for all i ∈ [k]. Note that 1-monotone functions are the classical monotone functions, satisfying f (x 1 ) ≤ f (x 2 ), whenever x 1 x 2 .Monotone functions have been well-studied on multiple fronts in computational complexity due to their natural structure. They have been celebrated for decades in the property testing literature [GGL + 00, DGL + 99, FLN + 02, BCGM12, CS13b, CS13a, CS14], where we have recently witnessed ultimate results  #b31  #b13  #b4 , in the circuit complexity literature, where we now have strong lower bounds  #b45  #b43 , and in computational learning, where we now have learning algorithms in numerous learning models  #b12  #b3  #b34  #b46  #b40  #b41 .The generalized notion of k-monotonicity has also been studied in the context of circuit lower bounds for more than 50 years. In particular, Markov  #b36  showed that any k-monotone function (even with multiple outputs) can be computed using circuits containing only log k negation gates. The presence of negation gates appears to be a challenge in proving circuit lower bounds: "the effect of such gates on circuit size remains to a large extent a mystery"  #b29 . The recent results of Blais et al. [BCO + 15] on circuit lower bounds have prompted renewed interest in understanding k-monotone functions from multiple angles, including cryptography, circuit complexity, learning theory, and Fourier analysis ([Ros15,  #b27  #b26  #b35 ).Motivated by the exponential lower bounds on PAC learning k-monotone functions due to [BCO + 15], we initiate the study of k-monotonicity in the closely related Property Testing model. In this model, given query access to a function, one must decide if the function is k-monotone, or is far from being k-monotone, by querying the input only in a small number of places.

Our results
We focus on testing k-monotonicity of Boolean functions defined over the d-dimensional hypegrid [n] d , and the hypercube {0, 1} d . We begin our presentation with the results for the hypercube, in order to build intuition into the difficulty of the problem, while comparing our results with the current literature on testing monotonicity. Our stronger results concern the hypegrid [n] d .

Testing k-monotonicity on the hypercube {0, 1} d
In light of the recent results of  #b31  that provide a O( √ d)-query tester for monotonicity, we first show that testing k-monotonicity is strictly harder than testing monotonicity on {0, 1} d , for k ≥ 3. On the upper bounds side, while the monotonicity testing problem is providing numerous potential techniques for approaching this new problem [GGL + 00, DGL + 99, CS13a, BCGM12, CST14,  #b31 , most common techniques appear to resist generalizations to k-monotonicity. However, our upper bounds demonstrate a separation between testing and PAC learning k-monotonicity, for large enough values of k = ω(log d). Indeed, in the related PAC learning model, [BCO + 15] shows that learning k-monotone functions on the hypercube requires 2 Ω(k· √ d·1/ε) many queries. We further observe that the recent non-adaptive and adaptive 2-sided lower bounds of  #b13  #b4 , imply the same bounds for k-monotonicity, using black box reductions. We summarize the state of the art for testing k-monotonicity on the hypercube in Table 1 The remainder of the paper focuses on functions defined over the d-dimensional hypergrid domain [n] d , where we denote by (i 1 , i 2 , . . . , i n ) (j 1 , j 2 , . . . , j n ) the partial order in which i 1 ≤ j 1 , i 2 ≤ j 2 , . . . , i n ≤ j n . Testing monotonicity has received a lot of attention over the d-dimensional hypergrids [GGL + 00, EKK + 00, Fis04, BRW05, AC06, HK08, BBM12, CS13b, CS13a,  #b17  #b11 , where the problem is well-understood, and we refer the reader to Table 4 in the appendix for a detailed review on the state of the art in the area. We summarize our results on testing k-monotonicity over [n] d in Table 2.  Testing k-monotonicity on the line and the 2-dimensional grid We begin with a study of function f : [n] → {0, 1}. As before, note that 1-sided tests should always accept k-monotone functions, and so, they must accept unless they discover a violation to k-monotonicity in the form of a sequence x 1 x 2 . . . x k+1 in [n] d , such that f (x 1 ) = 1 and f (x i ) = f (x i+1 ). Therefore, lower bounds for 1-sided k-monotonicity testing must grow at least linearly with k. We show that this is indeed the case for both adaptive and non-adaptive tests, and more over, we give a tight non-adaptive algorithm. Consequently, our results demonstrate that adaptivity does not help in testing k-monotonicity with one-sided error on the line domain. Testing with 2-sided error, however, does not require a dependence on k. In fact the problem has been well-studied in the machine learning literature in the context of testing/learning "union of intervals"  #b33  #b5 , and in testing geometric properties, in the context of testing surface area  #b32  #b38 , 1 resulting in an O(1/ε 7/2 )-query algorithm. Namely, the starting point of  #b5  (later improved by  #b32 ) is a "Buffon Needle's"-type argument, where the crucial quantity to analyze is the noise sensitivity of the function that is the probability that a randomly chosen pair of nearby points cross a "boundary" -i.e., have different values. (Moreover, the algorithm of  #b5  works in the active testing setting: it only requires a weaker access model that the standard query model. ) We provide an alternate proof of a poly(1/ε) bound (albeit with a worse exponent) that reveals a surprising connection with distribution testing, namely with the problem of estimating the support size of a distribution.Theorem 1.5. There exists a two-sided non-adaptive tester for k-monotonicity of functions f : [n] → {0, 1} with query complexity q(n, ε, k) =Õ 1/ε 7 , independent of k.An immediate implication of Theorem 1.5 is that one can test even n 1−α -monotonicity of f : [n] → {0, 1}, for every α > 0, with a constant number of queries. Hence, there is a separation between 1-sided and 2-sided testing, for k = ω(1).Turning to the 2-dimensional grid, we show that 2-monotone functions can be tested with the minimum number of queries one could hope for: Theorem 1.6. There exists a two-sided adaptive tester for 2-monotonicity of functions f : [n] 2 → {0, 1} with query complexity q(n, ε) = O 1 ε .We also discuss possible generalizations of Theorem 1.6 to general k or d section, Section 5.2.Testing k-monotonicity on [ with query complexity q(n, d, ε, k) = min(Õ 1ε 2 5kd ε d , 2Õ (k √ d/ε 2 ) ).In fact, we obtain more general testing algorithms than in Theorem 1.7, namely our results hold for tolerant testers.The notion of tolerant testing was first introduced in  #b42  to account for the possibility of noisy data. In this notion, a test should accept inputs that are ε 1 -close to the property, and reject inputs that are ε 2 -far from the property, where ε 1 and ε 2 are given parameters. Tolerant testing is intimately connected to the notion of distance approximation: given tolerant testers for every (ε 1 , ε 2 ), there exists an algorithm that estimates the distance to the property within any (additive) ε, while incurring only aÕ log 1 ε factor blow up in the number of queries. Furthermore,  #b42  shows that both tolerant testing and distance approximation are no harder than agnostic learning. We prove the following general result. , ε 2 , k) =Õ 1 (ε 2 −ε 1 ) 2 5kd ε 2 −ε 1 d ;• a non-adaptive tolerant tester for k-monotonicity of functions f :[n] d → {0, 1} with query complexity q(n, d, ε 1 , ε 2 , k) = 2Õ (k √ d/(ε 2 −3ε 1 ) 2 ) , under the restriction that ε 2 > 3ε 1 .To the best of our knowledge, the only previous results for tolerant testing for monotonicity on [n] d are due to Fattal and Ron  #b24 . They give both additive and multiplicative distance approximations algorithms, and obtain O(d)-multiplicative and ε-additive approximations with query complexity poly( 1 ε ). While very efficient, there results only give fully tolerant testers for dimensions d = 1 and d = 2. Our results generalize the work of  #b24  showing existence of tolerant testers for k-monotonicity (and hence for monotonicity) for any dimension d ≥ 1, and any k ≥ 1, but paying the price in the query complexity.As a consequence to Theorem 1.8, we make progress on an open problem of Berman et al.  #b11 , as explained next.Testing k-monotonicity under L p distance The property of being a monotone Boolean function has a natural extension to real-valued functions. Indeed, a real-valued function defined over a finite domain D is monotone if f (x) ≤ f (y) whenever x y. For real-valued functions the more natural notion of distance is L p distance, rather than Hamming distance. The study of monotonicity has been extended to real-valued functions in a recent work by Berman et al.  #b11 . They give tolerant testers for grids of dimension d = 1 and d = 2, and leave open the problem of extending the results to general d, as asked explicitly at the recent Sublinear Algorithms Workshop 2016 [Sub16].We make progress towards solving this open problem, by combining our Theorem 1.8 with a reduction from L p testing to Hamming testing from  #b11 . Theorem 1.9. There exists a non-adaptive tolerant L 1 -tester for monotonicity of functions f :[n] d → {0, 1} with query complexity •Õ 1 (ε 2 −ε 1 ) 2 5d ε 2 −ε 1 d , for any 0 ≤ ε 1 < ε 2 ≤ 1; • 2Õ ( √ d/(ε 2 −3ε 1 ) 2 ), for any 0 ≤ 3ε 1 < ε 2 ≤ 1.

Proofs overview and technical contribution
Structural properties and the separation between testing and learning on {0, 1} d . We first observe that basic structural properties, such as extendability (i.e. the feature that a function that is monotone on a sub-poset of [n] d can be extended into a monotone function on the entire poset domain), and properties of the violation graph (i.e., the graph whose edges encode the violations to monotonicity), extend easily to k-monotonicity (see Appendix B). These properties help us to argue the separation between testing and learning (Theorem 1.2). However, unlike the case of monotonicity testing, these properties do not seem to be enough for showing upper bounds that grow polynomially in d.Grid coarsening and testing by implicit/explicit learning. One pervading technique, which underlies all the hypergrid upper bounds in this work, is that of gridding: i.e., partitioning the domain into "blocks" whose size no longer depends on the main parameter of the problem, n. This technique generalizes the approach of  #b24  who performed a similar gridding for dimension d = 2. By simulating query access to the "coarsened" version of the unknown function (with regard to these blocks), we are able to leverage methods such as testing-by-learning (either fully or partially learning the function), or reduce our testing problem to a (related) question on these nicer "coarsenings." (The main challenge here lies in providing efficient and consistent oracle access to the said coarsenings.)At a high-level, the key aspect of k-monotonicity which makes this general approach possible is reminiscent of the concept of heredity in property testing. Specifically, we rely upon the fact that "gridding preserves k-monotonicity:" if f is k-monotone, then so will be its coarsening g -but now g is much simpler to handle. This allows us to trade the domain [n] d for what is effectively [m] d , with m ≪ n. We point out that this differs from the usual paradigm of dimension reduction: indeed, the latter would reduce the study of a property of functions on [n] d to that of functions on [n] d ′ for d ′ ≪ d (usually even d ′ = 1) by projecting f on a lower-dimensional domain. In contrast, we do not take the dimension down, but instead reduce the size of the alphabet. Moreover, it is worth noting that this gridding technique is also orthogonal to that of range reduction, as used e.g. in [DGL + 99]. Indeed, the latter is a reduction of the range of the function from [R] to {0, 1}, while gridding is only concerned about the domain size.Estimating the support of distributions. Our proof of the poly(1/ε) upper bound for testing k-monotonicity on the line (Theorem 1.5) rests upon an unexpected connection to distribution testing, namely to the question of support size estimation of a probability distribution. In more detail, we describe how to reduce k-monotonicity testing to the support size estimation problem in (a slight modification of) the Dual access model introduced by Canonne and Rubinfeld  #b14 , where the tester is granted samples from an unknown distribution as well as query access to its probability mass function.For our reduction to go through, we first describe how any function f : [n] → {0, 1} determines a probability distribution D f (on [n]), whose effective support size is directly related to the kmonotonicity of f . We then show how to implement dual access to this D f from queries to f : in order to avoid any dependence on k and n in this step, we resort both to the gridding approach outlined above (allowing us to remove n from the picture) and to a careful argument to "cap" the values of D f returned by our simulated oracle. Indeed, obtaining the exact value of D f (x) for arbitrary x may require Ω(k) queries to f , which we cannot afford; instead, we argue that only returning D f (x) whenever this value is "small enough" is sufficient. Finally, we show that implementing this "capped" dual access oracle is possible with no dependence on k whatsoever, and we can now invoke the support size estimation algorithm of  #b14  to conclude. Fourier analysis on the hypergrid. We give an algorithm for fully tolerantly testing kmonotonicity whose query complexity in exponential in d. We also describe an alternate tester (with a slightly worse tolerance guarantee) whose query complexity is instead exponential in O(k √ d) for constant distance parameters. As mentioned above, we use our gridding approach combined with tools from learning theory. Specifically, we employ an agnostic learning algorithm of  #b30  using polynomial regression. Our coarsening methods allow us to treat the domain as if it were [m] d for some m that is independent of n. To prove that this agnostic learning algorithm will succeed, we turn to Fourier analysis over [m] d . We extend the bound on average sensitivity of k-monotone functions over the Boolean hypercube from [BCO + 15] to the hypergrid, and we show that this result implies that the Fourier coefficients are concentrated on "simple" functions.

Discussion and open problems
This is the first work to study k-monotonicity, a natural and well-motivated generalization of monotonicity. Hence this work opens up many intriguing questions in the area of property testing, with potential applications to learning theory, circuit complexity and cryptography. As previously mentioned, the main open problem prompted by our work is the following:Can k-monotonicity on the hypercube {0, 1} d be tested with poly(d k ) queries?A natural 1-sided tester for k-monotonicity is a chain tester: it queries points along a random chain, and rejects only if it finds a violation to k-monotonicity, in the form of a sequence x 1 Answering this question would imply further progress on the L 1 -testing question for monotonicity, left open in  #b11 Sub16].x 2 . . . x k+1 in {0, 1} d , such that f (x 1 ) = 1 and f (x i ) = f (x i+1There also remains the question of establishing two-sided lower bounds that would go beyond those of monotonicity. Specifically:Is there an d Ω(k) -query two-sided lower bound for k-monotonicity on the hypercube {0, 1} d ?In this work we also show surprising connections to distribution testing (e.g. in the proof of Theorem 1.5), and to testing union of intervals and testing surface area (as discussed in Section 4.2). An intriguing direction is to generalize this connection to union of intervals and surface area in higher dimensions, to leverage or gain insight on k-monotonicity on the d-dimensional hypergrid.Finally, while we only stated here a few directions, we emphasize that every question that is relevant to monotonicity is also relevant and interesting in the case of k-monotonicity.

Related work
As mentioned, k-monotonicity has deep connections with the notion of negation complexity of functions, which is the minimum number of negation gates needed in a circuit to compute a given function. The power of negation gates is intriguing and far from being understood in the context of circuit lower bounds. Quoting from Jukna's book  #b29 , the main difficulty in proving nontrivial lower bounds on the size of circuits using AND, OR, and NOT is the presence of NOT gates: we already know how to prove even exponential lower bounds for monotone functions if no NOT gates are allowed. The effect of such gates on circuit size remains to a large extent a mystery.This gap has motivated the study of circuits with few negations. Two notable works successfully extend lower bounds in the monotone setting to negation-limited setting: in  #b2 , Amano and Maruoka show superpolynomial circuit lower bounds for (1/6) log log n negations using the Clique function; and recently the breakthrough work of Rossman  #b44  establishes circuit lower bounds for NC 1 with roughly 1 2 log n negations by drawing upon his lower bound for monotone NC 1 . The divide between the understanding of monotone and non-monotone computation exists in general: while we usually have a fairly good understanding of the monotone case, many things get murky or fail to hold even when a single negation gate is allowed. In order to get a better grasp on negation-limited circuits, a body of recent work has been considering this model in various contexts: Blais et al. [BCO + 15] study negation-limited circuits from a computational learning viewpoint, Guo et al.  #b27  study the possibility of implementing cryptographic primitives using few negations, and Lin and Zhang  #b35  are interested in verifying whether some classic Boolean function conjectures hold for the subset of functions computed by negation-limited circuits.Many of these results implicitly or explicitly rely on a simple but powerful tool: the decomposition of negation-limited circuits into a composition of some "nice" function with monotone components. Doing so enables one to apply results on separate monotone components, and finally to carefully combine the outcomes (e.g.,  #b26 ). Though these techniques can yield results for as many as O(log n) negations, they also leave open surprisingly basic questions: In contexts where the circuit size is not the quantity of interest, the equivalent notion of 2monotone functions is more natural than that of circuits allowing only one negation. Albeit seemingly simple, even the class of 2-monotone functions remains largely a mystery: as exemplified above, many basic yet non-trivial questions, ranging from the structure of their Fourier spectrum to their expressive power of k-monotone functions, remain open.

Organization of the paper
After recalling some notations and definitions in Section 2, we consider the case of the Boolean hypercube in Section 3, where we establish lower bounds on testing k-monotonicity of functions f : {0, 1} d → {0, 1} for both one-and two-sided algorithms, and provide an algorithm which "beats" the testing-by-learning approach, showing that testing is provably easier than learning.Next, we establish our results for functions on the line in Section 4, starting with the lower and upper bounds for one-sided testers before turning in Section 4.2 to the two-sided upper bound of Theorem 1.3. We then describe in Section 5 our results for functions on the grid [n] 2 , focusing on the case k = 2; and discussing possible extensions in Section 5.2. Section 6 contains our general algorithms for k-monotonicity on the hypergrid [n] d , for arbitrary k and d. We prove Theorem 1.8 in two parts. We establishing its first item (general tolerant testing algorithm with exponential dependence in d) in Section 6.1 (Proposition 6.2). The second item (with query complexity exponential in k √ d) is proven in Section 6.2, where we analyze the Fourier-based tolerant tester of Proposition 6.11. We then apply these results to the question of tolerant L 1 -testing of monotonicity in Section 7, after describing a reduction between monotonicity of functions [n] d → [0, 1] and of [n] d+1 → {0, 1}.Except maybe Section 7 which depends on Section 6, all sections are independent and selfcontained, and the reader may choose to read them in any order.

Preliminaries
We denote by log the binary logarithm, and useÕ(·) to hide polylogarithmic factors in the argument (so thatÕ(f ) = O(f log c f ) for some c ≥ 0).Given two functions f, g : X → Y on a finite domain X , we write dist(f, g) for the (normalized) Hamming distance between them, i.e.dist(f, g) = 1 |X | x∈X 1 {f (x) =g(x)} = Pr x∼X [ f (x) = g(x) ]where x ∼ X refers to x being drawn from the uniform distribution on X . A property of functions from X to Y is a subset P ⊆ X Y of these functions; we define the distance of a function f to P as the minimum distance of f to any g ∈ P:dist(f, P) = inf g∈P dist(f, g) .For some of our applications, we will also use another notion of distance specific to real-valued functions, the L 1 distance (as introduced in the context of property testing in  #b11 ). For f, g : X → [0, 1], we writeL 1 (f, g) = 1 |X | x∈X |f (x) − g(x)| = E x∼X [|f (x) − g(x)|] ∈ [0, 1]and extend the definition to L 1 (f, P), for P ⊆ X [0,1] , as before.Property testing. We recall the standard definition of testing algorithms, as well as some terminology:Definition 2.1. Let P be a property of functions from X to Y. A q-query testing algorithm for P is a randomized algorithm T which takes as input ε ∈ (0, 1] as well as query access to a function f : X → Y. After making at most q(ε) queries to the function, T either outputs ACCEPT or REJECT, such that the following holds:• if f ∈ P, then T outputs ACCEPT with probability at least 2/3; (Completeness) • if dist(f, P) ≥ ε, then T outputs REJECT with probability at least 2/3;where the probability is taken over the algorithm's randomness. If the algorithm only errs in the second case but accepts any function f ∈ P with probability 1, it is said to be a one-sided tester; otherwise, it is said to be two-sided. Moreover, if the queries made to the function can only depend on the internal randomness of the algorithm, but not on the values obtained during previous queries, it is said to be non-adaptive; otherwise, it is adaptive.Additionally, we will also be interested in tolerant testers -roughly, algorithms robust to a relaxation of the first item above:Definition 2.2. Let P, X , and Y be as above. A q-query tolerant testing algorithm for P is a randomized algorithm T which takes as input 0 ≤ ε 1 < ε 2 ≤ 1, as well as query access to a function f : X → Y. After making at most q(ε 1 , ε 2 ) calls to the oracle, T outputs either ACCEPT or REJECT, such that the following holds:• if dist(f, P) ≤ ε 1 , then T outputs ACCEPT with probability at least 2/3;(Completeness) • if dist(f, P) ≥ ε 2 , then T outputs REJECT with probability at least 2/3;where the probability is taken over the algorithm's randomness. The notions of one-sidedness and adaptivity of Definition 2.1 extend to tolerant testing algorithms as well.Note that as stated, in both cases the algorithm "knows" X , Y, and P; so that the query complexity q can be parameterized by these quantities. More specifically, when considering X = [n] d and the property P of k-monotonicity, we will allow q to depend on n, d, and k. Finally, we shall sometimes require a probability of success 1− δ instead of the (arbitrary) constant 2/3; by standard techniques, this can be obtained at the cost of a multiplicative O(log(1/δ)) in the query complexity.

PAC and agnostic learning [Val84]
A learning algorithm A for a concept class C of functions f : X → Y (under the uniform distribution) is given parameters ε, δ > 0 and sample access to some target function f ∈ C via labeled samples x, f (x) , where x is drawn uniformly at random from X . The algorithm should output a hypothesis h : X → Y such that dist(h, f ) ≤ ε with probability at least 1 − δ. The algorithm is efficient if it runs in time poly(n, 1/ε, 1/δ). If A must output h ∈ C we say it is a proper learning algorithm, otherwise, we say it is an improper learning one.Moreover, if A still succeeds when f does not actually belong to C, we say it is an agnostic learning algorithm. Specifically, the hypothesis function h that it outputs must satisfy dist(f, g) ≤ opt f + ε with probability at least 1 − δ, where opt f = min g∈C dist(f, g).

On the Boolean hypercube
In this section, we focus on k-monotonicity of Boolean functions over the hypercube {0, 1} d . We begin in Section 3.1 with a tester with query complexity 2Õ ( √ d) , establishing a strict separation between learning and testing. Section 3.2 is then dedicated to our lower bounds on k-monotonicity testing.

Upper bound: beating the learning approach
In this section, we prove the following theorem: 2 2 We note that this result is only interesting in the regime k ≤ √ d: indeed, for k = Ω √ d log 1 ε every function is ε-close to k-monotone. − √ d log C ε , d 2 + √ d log C ε ] is at most ε2 d−1 .With that fact in our hands, we now describe the following tester.(1) Sample O(1/ε) random points from the middle levels(2) For each of the queries in the first step, query all points with Hamming weight in the middle levels which fall in the subcube below and in the subcube above each such random point. We call each of these O(1/ε) collections of queries a superquery .The key idea behind the tester is the extendability lemma, Lemma B.7. The tester tries to find one of the "violated hyperedges" from the matching of violations that we know exists from Lemma B.7. Now, we analyze the tester.Proof of Theorem 1.2. Suppose k is odd. 3 In this case, given a function f ε-far from k-monotonicity we can assume without loss of generality that f and equals 0 on points with Hamming weight< d 2 − √ d log C ε ,

and equals 1 on points with Hamming weight
> d 2 + √ d log C ε . The resulting function is still ε 2 -far from being k-monotone. Now, by Lemma B.7, we know that there exists a matching M f of violations to k-monotonicity in f of size 1 2 · ε2 d k .The set of vertices that participate in these violations has cardinality|M f | · ε2 d k = ε(k + 1)2 d k ≥ ε2 d−1With constant probability, in O(1/ε) queries in the first step, the tester queries a vertex that belongs to some violation from M f . Then in the next step, the tester finds this violation. Now we bound the number of queries made in a single superquery. Since it involves only 2 √ d log C ε levels of the cube, the number of points queried in a single superquery is no more thanb = d O( √ d log 1 ε ). The total query complexity of the tester can therefore be upperbounded byb/ε = d O( √ d log 1 ε ). We emphasize that the number of queries made by this tester has no dependence on k.

Lower bounds
We now turn to lower bounds for testing k-monotonicity of Boolean functions over the hypercube {0, 1} d . In Section 3.2.1, we show that for constant k any one-sided non-adaptive tester for kmonotonicity requires Ω(d k/4 ) queries , generalizing the Ω √ d lower bound for monotonicity due to Fischer et al. [FLN + 02]. This bound suggests the problem become strictly harder when k increases: specifically, for k > 2 testing k-monotonicity requires ω( √ d) queries, while an O( √ d) one-sided non-adaptive upper bound holds for monotonicity testing  #b31 .We then describe in Section 3.2.2 a general reduction from monotonicity testing to k-monotonicity testing, for arbitrary constant k. This blackbox reduction allows us to carry any lower bound (possibly 2-sided or adaptive) for monotonicity testing to k-monotonicity. In particular, combining it with the recent lower bounds  #b13  #b4  for 2-sided monotonicity testing, we obtain an Ω(d 1/2−o(1) ) lower bound for non-adaptive k-monotonicity testers, and an Ω d 1/4 lower bound for adaptive ones.

One-sided lower bounds
Theorem 1.1. For 1 ≤ k ≤ d 1/4 /2, any one-sided non-adaptive tester for k-monotonicity of functions f : {0, 1} d → {0, 1} must make (Ω d/k 2 ) k/4 queries.Consider the family of functions {f S : S ⊆ [d] of size t} where t ≥ k is a parameter to be determined later and f S is a truncated anti-parity over t input coordinates indexed by S, namely,f S = ⊕ i∈S x i if ||x| − d/2| ≤ √ d, 0otherwise.Theorem 1.1 immediately follows from the two claims below. In particular,by Claim 3.3, for t ≤ √ d and t = 4k 2 , f S is Ω(1)-far from any k-monotone function, and by Claim 3.2, to reject every f S with Ω(1) probability, q needs to be at least d k/4 · ( k2e 2 t ) k/2 = Ω(d/k 2 ) k/4 when t = 4k 2 . Claim 3.2. For any non-adaptive q-query algorithm A, there exists f S where |S| = t such that A reveals a violation on f S with probability at most q 2 2 √ d k t k / d k . Claim 3.3. For k ≤ t ≤ √ d, f S is Ω( ⌊ t−k−1 2 ⌋ i=0 ti /2 t )-far from any k-monotone functions. Proof of Claim 3.2. Let Q be an arbitrary set of q queries. Without loss of generality, we assume every query z in Q has Hamming weight |z| ∈[ d 2 − √ d, d 2 + √ d]. We define Q x,z = { y ∈ Q : x y z } and Rej(Q) = { f S : Q contains a violation for f S }. Note that Rej(Q) = (x,z) : x z∈Q Rej(Q x,z ). Hence we can bound the size of Rej(Q) by|Rej(Q)| ≤ (x,z)∈Q 2 : x z |Rej(Q x,z )| .(1)Fix any x z ∈ Q. Because any violation for f S in Q x,z contains at least two points x ′ and z ′ such that x x ′ z ′ z, and x ′ S ′ = 0 k and z ′ S ′ = 1 k , we have x S ′ = 0 k and z S ′ = 1 k for some S ′ ⊆ S of size k. Note that x and z differ in at most 2 √ d coordinates so that there are at most 2√ d k distinct S ′ on which x S ′ is 0 k and z S ′ is 1 k . Moreover, for each S ′ there are at most d−k t−k distinct S such that S ′ ⊆ S. Therefore we can bound the size of Rej(Q x,z ) by |Rej(Q x,z )| ≤ 2 √ d k d − k t − k .(2)Combining (1) and (2) , |Rej(Q)| ≤ q 2 2 √ d k d−k t−k .It follows that for any non-adaptive algorithm making at most q queries,

S⊆[d]:|S|=t
Pr[A reveals a violation for f S ] ≤ E[|Rej(Q)|] ≤ q 2 2 √ d k d − k t − k .Hence there exists f S such that Pr[A reveals a violation forf S ] ≤ q 2 ( 2 √ d k )( d−k t−k ) ( d t ) = q 2 ( 2 √ d k )( t k ) ( d k ).Proof of Claim 3.3. Let f ′ S be the closest k-monotone function to f S . Let Z denote the set {z ∈ {0, 1} d−t : d/2 − √ d ≤ |z| ≤ d/2 + √ d − t}. For any t ≤ √ d, [ d−t 2 − √ d−t 2 , d−t 2 + √ d−t 2 ] is contained in [d/2 − √ d, d/2 + √ d − t] so that |Z| = Ω(2 d−t ). For any assignment z ∈ Z on coordinates indexed by [d] \ S, f S (·, z) agrees with ⊕ i∈S x i and f ′ S (·, z) is k-monotone.To finish the proof, it suffices to show that an anti-parity over t inputs is⌊ t−k−1 2 ⌋ i=0 t k /2 t -far from any k-monotone function over t inputs. Indeed, this will imply that, for every z ∈ Z, f S (·, z) differs from f ′ S (·, z) on ⌊ t−k−1 2 ⌋ i=0 t kpoints, and thus finally thatf S differs from f S ′ on |Z|· ⌊ t−k−1 2 ⌋ i=0 t k = Ω(2 d−t · ⌊ t−k−1 2 ⌋ i=0 t k ) points. Now we show that an anti-parity function over t inputs h(x 1 , . . . , x t ) = ⊕ t i=1 x i is ⌊ t−k−1 2 ⌋ i=0 t i /2 t - far from any k-monotone function g (over x 1 , . . . , x t ).We begin by noting that we can sample a random point from {0, 1} t by first sampling a random chain C = (x 0 = 0 t , x 1 , . . . , x t = 1 t ) (from all possible chains from 0 t to 1 t ) then outputting x i with probability t i /2 t . Thus the distance between h and g, namely Prx [h(x) = g(x)], is equal to Pr C,i [h(x i ) = g(x i )] = E C t i=0 t i 2 t · 1 {h(x i ) =g(x i )} = E C 1 2 t t i=1 t − 1 i − 1 · (1 {h(x i ) =g(x i )} + 1 {h(x i−1 ) =g(x i−1 )} ) (3)where the last inequality relies on identity ti = t−1 i−1 + t−1 i . For any fixed chain C, because g alternates at most k times, there are at least t − k choices of i ∈ [t] such that g(x i−1 ) = g(x i ), which implies 1 {h(x i ) =g(x i )} + 1 {h(x i−1 ) =g(x i−1 )} ≥ 1 (due to h(x i−1 ) = h(x i )). Thus t i=1 t−1 i−1 · (1 {h(x i ) =g(x i )} +1 {h(x i−1 ) =g(x i−1 )} ) is at least the sum of smallest t−k binomials among t−1 0 , . . . , t−1 t−1 which is ⌊ t−k−1 2 ⌋ i=0 t − 1 i + ⌊ t−k−2 2 ⌋ i=0 t − 1 t − 1 − i = ⌊ t−k−1 2 ⌋ i=0 t − 1 i + ⌊ t−k−2 2 ⌋ i=0 t − 1 i ≥ ⌊ t−k−1 2 ⌋ i=0 t i , which implies Pr x [h(x) = g(x)] ≥ 1 2 t ⌊ t−k−1 2 ⌋ i=0 ti by combining the above with (3).

Two-sided lower bounds
The following theorem gives a construction that enables us to convert monotone functions into k-monotone functions, and functions that are far from monotone into functions that are far from k-monotone. • if g is monotone, then g||h is a k-monotone function;• if g is ε-far from monotone, then g||h is Ω(ε/k)-far from being a k-monotone function;where (g||h)(x, y) def = g(x) ⊕ h(y) for any x, y ∈ {0, 1} d/2 .The above theorem reduces test monotonicity to testing k-monotonicity (for arbitrary constant k) with the same number of queries to the input function. This theorem allows us to carry any lower bound on monotonicity testing to k-monotonicity, while preserving the characteristics (twosidedness, adaptivity) of the original lower bound. In particular, combining it with the recent recent of  #b13  #b4 , we obtain the following corollary.Corollary 3.5. For any c > 0 and k ≥ 1, there exists ε = ε(k, c) > 0 such that any 2-sided nonadaptive algorithm for testing whether f is k-monotone or ε-far from it requires Ω(d 1/2−c ) queries. Any 2-sided adaptive algorithm requiresΩ d 1/4 queries.To prove Theorem 3.4, we prove following three claims. Claim 3.6 and Claim 3.7 show that the existence of a (k − 1)-monotone function h for which one can find a big enough set of vertex disjoint paths in the hypercube whose labelling under h satisfies some specific condition will imply Theorem 3.4. Finally in Claim 3.8, we establish the existence of such h, and Theorem 3.4 follows.  Claim 3.6. Let h : {0, 1} d → {0, 1} be a (k − 1)-monotone function. Then for any monotone g : {0, 1} d → {0, 1}, f = g||h is a k-monotone function.y 1 · · · y k , h(y 1 ) = 0 and h(y i ) = h(y i+1 ) for 1 ≤ i ≤ k − 1. Then, for any g : {0, 1} d → {0, 1} which is ε-far from being a monotone function, the function f = g||h is Ω( M 2 d · ε)-far from being a k-monotone function.1 · · · y k , h(y 1 ) = 0 and h(y i ) = h(y i+1 ) for 1 ≤ i ≤ k − 1. Proof of Claim 3.6. Suppose f = g||h is not k-monotone, then there exist (x 1 , y 1 ), . . . , (x k+1 , y k+1 ) such that (x 1 , y 1 ) · · · (x k+1 , y k+1 ), f (x 1 , y 1 ) = 1 and f (x i , y i ) = f (x i+1 , y i+1 ) for any 1 ≤ i ≤ k.Because g is monotone, either g is constant on x 1 , . . . , x k+1 , or there exists an index 1 < j ≤ k + 1 such that g(x i ) = 0 for i < j and g(x i ) = 1 for i ≥ j. For the first case, h(y i ) = f (x i , y i ) for any i and h alters exactly k times on points y 1 · · · y k+1 . For the second case, h(y 1 ) = f (x 1 , y 1 ) = 1, and h alters exactly k − 1 times on y 1 · · · y j−1 y j+1 · · · y k+1 . Both cases contradict h being (k − 1)-monotone.Proof of Claim 3.7. Let M h be the maximal set of paths of length k such that all paths are vertex disjoint and for every path y 1 · · · y k , h(y 1 ) = 0 and h(y i ) = h(y i+1 ) for 1 ≤ i ≤ k − 1. Let M g be the maximal set of pairs such that all pairs are vertex disjoint and every pair x 1 x 2 is a violation for g, i.e., g(x 1 ) = 1 and g(x 2 ) = 0. For each path (y 1 · · · y k ) ∈ M h and any pair (x 1 x 2 ) ∈ M g , it is easy to see following path is a violation for f ||g being k-monotone :(x 1 , y 1 ), . . . , (x 1 , y k ), (x 2 , y k ).Let f ′ be the closest k-monotone function to f . For each violating path, f and f ′ differ on at least 1 point. Because both M h and M g are vertex disjoint, violating paths constructed by taking every path in M h and every pair in M g are vertex disjoint. Thus f and f ′ differ on at least |M h | × |M g | points and f is (|M h |·|M g | /2 2d )-far from f ′ . It is known ([GGL + 00]) that for any g : {0, 1} d → {0, 1} which is ε-far from monotone, |M g | ≥ 2 d−1 ε. The desired conclusion follows.Proof of Claim 3.8. Let B 1 , B 2 , . . . , B k be consecutive blocks each consisting of consecutive layers of the hypercube such that for each i ∈ [k],(1 − k √ d ) 2 d k ≤ |B i | ≤ (1 + k √ d ) 2 d k .Because k is a constant and every layer contains at most 2 d / √ d points, we can always greedily find B 1 , . . . , B k one by one. Let h be a function such that h has constant value (i + 1 mod 2) on blockB i for 1 ≤ i ≤ k. It is easy to see that h is (k − 1)-monotone. Next we prove for any 1 ≤ j ≤ k, B 1 , . . . , B j contain at least (1−o d (1))2 d k vertex disjoint paths of length j − 1 such the ith point on every path is in B i . Claim 3.8 follows from the case j = k.For j = 1, the statement holds by taking all points inB 1 . For j > 1, assume that B 1 , . . . , B j−1 contain a set P j−1 of such (1−o d (1))2 d k vertex disjoint paths of length j − 2.Let M be the maximal matching between B j−1 and B j and let P j be the set of paths of length j −1 constructed in following way: for each path in P j−1 with an endpoint y j−1 , if there exists y j such that (y j−1 , y j ) ∈ M , we add the path appended with y j into P j . Because no points in B j will be added into two different paths in P j and P j−1 are vertex disjoint, paths in P j are vertex disjoint.Now we show |M | = min(|B j−1 | , |B j |) which implies |P j | ≥ (1−o d (1))2 d k . Suppose |B j−1 | ≤ |B j | (the argument is analogous in the other case). For any subset S of B j−1 , let f S be the indicator function of the upper closure of S denoted as N (S). It is not hard to check that f S is monotone and thus Pr[f S (x) = 1|x ∈ B j ] ≥ Pr[f S (x) = 1|x ∈ B j−1 ]. It follows |N (S) ∩ B j | = |B j | Pr[f S (x) = 1|x ∈ B j ] ≥ |B j−1 | Pr[f S (x) = 1|x ∈ B j−1 ] ≥ |S| . By Hall's theorem, |M | = |B j−1 |. By similar argument, we can show |M | = |B j | when |B j−1 | > |B j |. Thus |M | = min(|B j−1 | , |B j |).

On the line
In this section we prove our results on testing k-monotonicity on the line, that is of functions f : [n] → {0, 1}. We start with Theorem 1.4, which establishes that this can be done non-adaptively with one-sided error, with only O(k/ε) queries; we then turn to Theorem 1.3, which shows that this is the best one can hope for if insisting on one-sidedness. The last result of this section is Theorem 1.5, where we show that -perhaps unexpectedly -two-sided algorithms, even nonadaptive, can break this barrier and test k-mononicity with no dependence on k.

Upper and lower bounds for one-sided testers
We first prove the upper bound, restated below: Proof. We assume that εn 50k is an integer, 4 and partition the domain into K def = 50k ε intervals of size εn 50k , the consecutive "blocks" B 1 , . . . , B K . We then define g : [n] → {0, 1, * } as the function constant on each blockB i = {b i , . . . , b i+1 − 1}, such that • If f (b i ) = f (b i+1 − 1), then g(j) = f (b i ) for all j ∈ B i ; • otherwise, g(j) = * for all j ∈ B i .We say that a block B i such that g| B i = * is a changepoint block for g. Clearly, given query access to f one can obtain the value of g on any point j ∈ [n] with only two queries to f . Moreover, definingg : [n] → {0, 1} to be the function obtained from g by replacing * by 0, we observe the following:• If f is k-monotone, then (i) so isg, and (ii) f andg differ in at most k blocks (namely the changepoint blocks of g), so that dist(f, g) ≤ k · 1 K = ε 50 ; • If f is ε-far from k-monotone, then either (i)g is not k-monotone, or (ii) dist(f,g) > ε.We start by learning g (and thusg) exactly, using 2K = O(k/ε) non-adaptive queries. Setting m def = C k ε , we also sample m ′ ∼ Poisson(m) points 5 j 1 , . . . , j m ′ independently and uniformly from [n], where C > 0 is a constant to be determined in the course of the analysis, and query the value of f (and g) on all of them. Then, we reject if either (i)g is not k-monotone; or (ii) there exist at least k + 1 distinct blocks which contain a sample s j such thatg(s j ) = f (s j ).By definition, this tester is non-adaptive; and it is not difficult to see it accepts any k-monotone function with probability 1, since in that case f and g (and a fortiorig) differ in at most k blocks: indeed, these blocks can only be changepoint blocks for g, i.e. blocks where f changes value.It remains to argue soundness: we will show that if f is ε-far from k-monotone, the tester will reject with probability at least 2/3. By the first check made, (i), we can assume in the following thatg is k-monotone -as otherwise f is rejected with probability 1 -and we need to show that (ii) will reject with probability at least 2/3. For each block B i (where i ∈ [K]), let p i ∈ [0, 1 K ] be defined as the (normalized) number of points in B i on which f andg differ (we henceforth refer to such a point as a giveaway point):p i def = 1 n j∈B i 1 {f (j) =g(j)} .Since f is ε-far from the k-monotone functiong, we have K i=1 p i ≥ ε. Now, letting Z i be the indicator of the event that among the m ′ samples, at least one is giveaway point from B i , andZ = K i=1 Z i , we can write Z i = 1 {Y i =0} , where the (Y i ) i∈[K] are independent Poisson random variables with Y i ∼ Poisson(mp i ).The expected number of blocks in which a giveaway point is sampled is thenEZ = K i=1 EZ i = K i=1 Pr[ Y i = 0 ] = K i=1 (1 − e −mp i ) Since for every i ∈ [K] it holds that e −mp i ≤ 1 − m 2 p i (the inequality holding since 0 ≤ mp i ≤ 1, 4 If not, we consider instead ε ′ def = 50k n εn 50k > ε − 50k n > ε 2 if ε > 100k n ; while if ε ≤ 100k nwe query the entire function, for a total of n = O k ε queries. 5 The fact that we sample Poisson(m) instead of m is for ease of the analysis; note that due to the tight concentration of Poisson random variables, with probability 1 − o(1) we will have m ′ ≤ 2m. If this does not happen, the tester can output ACCEPT, incurring only a small additional error probability (and not affecting the one-sidedness).which is verified for m ≤ K), we getEZ = K i=1 EZ i ≥ K i=1 m 2 p i ≥ mε 2 ≥ C 2 k.Moreover, by a Chernoff bound, we get thatPr Z < C 4 k ≤ e − Ck 16 ≤ e − C 16which is less that 1/4 for C ≥ 23. Setting C def = 30 satisfies both conditions that m ≤ K and C ≥ 23, and results in a one-sided non-adaptive tester which rejects functions far from k-monotone with probability at least 1 − 14 + o(1) ≥ 2 3 .Turning to the lower bounds against one-sided testers, we show the following: Proof. Since a lower bound of Ω 1 ε straightforwardly holds, we can restrict ourselves to k ≥ 8, and ε < 1 12 ; moreover, we assume without loss of generality that εn k is an integer, and partition thedomain into K def = k ε intervals of size εn k , the consecutive "blocks" B 1 , . . . , B K . For v ∈ {0, 1} K/2 , we define g v : [n] → {0, 1}as the function which has constant value v i on block B 2i−1 and has constant value 1 on the remaining blocks.Consider the distribution over {g v } v where each coordinate of v is independently set to 0 with probability p def = 6ε, and 1 otherwise. We next show that g v is at least ε-far from any k-monotone function with very high probability over the choice of v. By a Chernoff bound, with probability at least 1 − e −pK/16 = 1 − e −3k/8 , g v has at least pK/4 = 3k/2 blocks that are 0 blocks. Conditioned on this, it is easy to see that g v is ε-far from k-monotone: indeed, to make it k-monotone one has to flip its value on at least k blocks, and each block contains an ε/k fraction of the domain.Fix any deterministic adaptive algorithm with query complexity q ≤ k/(24ε) queries, and denote by x 1 , . . . , x q the sequence of queries made (when given query access to some function g v ). Note that x 1 is fixed by the algorithm and that for 1 < i ≤ q, x i is uniquely determined by previous answers f (x 1 ), . . . , f (x i−1 ). We can sample the distribution {g v } v and answer queries from the given algorithm in the following "lazy way": first, by marking every even blocks with value 1 and initializing a list of queried odd blocks with their values. When a new query x comes, if x was previously queried or belongs to an even block, we return the corresponding stored value. Otherwise, we sample the value, which is 0 with probability 6ε and 1 otherwise, for the odd block which x belongs to; and mark this block as queried.Let y 1 , . . . , y r be the following subsequence of x 1 , . . . , x q : y i is the ith query made into an odd block which is not queried in y 1 , . . . , y i−1 . Clearly r ≤ q and y 1 , . . . , y r reveals a violation if and only if number of 0's in corresponding answers is at least k/2 + 1. Note, for arbitrary a ∈ {0, 1} r , determined by f (y 1 ) = a 1 , . . . , f (y i−1 ) = a i−1 and by our way of sampling f (y i ), we know that for every i it holds that PrPr[ f (y 1 ) = a 1 , . . . , f (y r ) = a r ] = Pr[ f (y 1 ) = a 1 ] · r i=2 Pr[ f (y i ) = a i | f (y 1 ) = a 1 , . . . , f (y i−1 ) = a i−1 ] . y i is[ f (y i ) = a i | f (y 1 ) = a 1 , . . . , f (y i−1 ) = a i−1 ] = 6ε if a i = 0 and Pr[ f (y i ) = a i | f (y 1 ) = a 1 , . . . , f (y i−1 ) = a i−1 ] = 1 − 6ε if a i = 1. Thus. Pr[ f (y 1 ) = a 1 , . . . , f (y r ) = a r ] = (1 − 6ε) |a| (6ε) r−|a| .(4)Let Y i be the indicator that f (y i ) = 0. We get that, writing F (i, N, p) for the cumulative distribution function of a Binomial with parameters N and p,Pr r i=1 Y i ≥ k 2 + 1 ≤ a∈{0,1} r : |ā|≥k/2 (1 − 6ε) |a| (6ε) r−|a| = r−k/2 ℓ=0 r ℓ (1 − 6ε) ℓ (6ε) r−ℓ = F r − k 2 , r, 1 − 6ε = F (r(1 − x), r, 1 − 6ε) (x def = k 2r ∈ (12ε, 1)) ≤ e −rD(1−x||1−6ε) (Relative entropy Chernoff bound 6 )where D(p || q) def = p ln p q + (1 − p) ln 1−p 1−q , and we used the fact that k 2 + 1 ≤ r ≤ k 24ε . Rewriting slightly the right-hand-side, we obtainPr r i=1 Y i ≥ k 2 + 1 ≤ e − k 2 Φ(x) for Φ(x) def = 1 x (1 − x) ln 1−x 1−6ε + x ln x 6ε. It is not hard to see that Φ is increasing on [6ε, 1), and since x ≥ 12ε the right-hand-side is at most e − k 2 Φ(12ε) . It then suffices to observe that, for ε ≤ 1 12 , it holds that Φ(12ε) ≥ Φ(0) = ln 2 − 1 2 > 1 8 to conclude thatPr r i=1 Y i ≥ k 2 + 1 ≤ e − k 16and therefore obtain Pr[ f (y 1 ), . . . , f (y r ) contains at least (k/2 + 1) zeros ] ≤ e − k 16 .Combining the two, this shows that the probability that y 1 , . . . , y r does not reveal a violation for g v while g v is ε-far from k-monotone is at least 1 − e −k/16 − e −3k/8 > 1/3 (since k ≥ 8). By Yao's principle, for any (possibly randomized) non-adaptive algorithm A making at most k/(24ε) there exists a fixed v such that g v is ε-far from k-monotone yet A rejects g v with probability less than 2/3. The desired conclusion follows.

Upper bound for two-sided testers: proof of Theorem 1.5
In this section, we prove the two-sided non-adaptive upper bound of Theorem 1.5, restated below: In what follows, we assume that k > 20/ε, as otherwise we can use for instance the O(k/ε)-query (non-adaptive, one-sided) tester of Theorem 1.4 to obtain an O 1/ε 2 query complexity.6

Testing k-monotonicity over [Ck]
We begin by giving a poly(C/ε)-query tester for k-monotonicity over the domain [Ck]. The tester proceeds by reducing to support size estimation and using (a slight variant of) an algorithm of Canonne and Rubinfeld  #b14 . Let f : [Ck] → {0, 1}, and suppose f is s-monotone but not (s−1)monotone. Then there is a unique partition of [Ck] into s + 1 disjoint intervals I 1 , I 2 , . . . , I s+1 such that f is constant on each interval; note that this constant value alternates in consecutive intervals. We define a distribution D f over [s+ 1] such that D f (i) = |I i | /(Ck).The algorithm of  #b14  uses "dual access" to D; an oracle that provides a random sample from D, and an oracle that given an element of D, returns the probability mass assigned to this element by D. We only have access to D f through query access to f . One difficulty is that, to access D f (i), we need to determine where I i lies in f . For example, finding D f (k/2) requires finding I k/2 , which might require a large number of queries to f . We circumvent this by noting that the algorithm does not require knowing the "label" of any element in the support of the distribution. The only access required is being able to randomly sample elements according to D f , and evaluate the probability mass on the sampled points.Lemma 4.2 (Sampling from D f ). Let i ∈ [n] be chosen uniformly at random, and let j be such that i ∈ I j . Then, the distribution of j is exactly D f . D f (j)). Suppose I j = {a, a + 1, . . . , b}. Given i such that i ∈ I j , we can find I j by querying

Lemma 4.3 (Evaluating
f (i + 1) = f (i + 2) = · · · = f (b) and f (b + 1) = f (b), as well as f (i − 1) = f (i − 2) = . . . = f (a) and f (a − 1) = f (a). The number of queries to f is b − a + 3 = |I j | + 3.If we straightforwardly use these approaches to emulate the required oracles to estimate the support size of D f , the number of queries is potentially very large. If we attempt to query D f (j) where |I j | = Ω(k), we will need Ω(k) queries to f . It will be enough for us to "cap" the size of the interval. D f (j) with a cap). Given i such that i ∈ I j , we will query f on every point in [i − 20C/ε, i + 20C/ε]. If |I j | ≤ 20C/ε, then I j will be determined by these queries. If these queries do not determine I j , we know |I j | > 20C/ε. Beyond querying i, this requires 40C/ε (nonadaptive) queries.

Lemma 4.4 (Evaluating
Claim 4.5. If f is ε-far from k-monotone, then it is not (1 + ε 4 )k-monotone, and in particular |supp(D f )| > (1 + ε 4 )k + 1. Proof. The last part of the statement is immediate from the first, so it suffices to prove the first implication. We show the contrapositive: assuming f is (1 + ε 4 )k-monotone, we will "fix" it into a k-monotone function by changing at most εn points. In what follows, we assume εk 4 ≥ 1, as otherwise the statement is trivial (any function that is ε-far from k-monotone is a fortiori not k-monotone).Let as before ℓ * be the minimum integer ℓ for which f is ℓ-monotone: we can assume k < ℓ * ≤ (1 + ε 4 )k (as if ℓ * ≤ k we are done.) Consider as above the maximal consecutive monochromatic intervals I 1 , . . . , I ℓ * , and let i be the index of the shortest one. In particular, it must be the case that |I i | ≤ n ℓ * +1 . Flipping the value of f on I i therefore has "cost" at most n ℓ * +1 , and the resulting function f ′ is now exactly (ℓ * − 2)-monotone if 1 < i < ℓ * , and (ℓ * − 1)-monotone if i ∈ {1, ℓ * }. This means in particular that repeating the above ε 4 k times is enough to obtain a k-monotone function, and the total cost is upperbounded byεk/4 j=0 n ℓ * + 1 − 2j ≤ εk/4 j=0 n k + 1 − 2j = k+1 j=k(1− ε 2 )+1 n j ≤ n ε 2 k + 1 (1 − ε 4 )k + 1 ≤ n 3ε 4 k (1 − ε 4 )kwhere for the last inequality (for the numerator) we used that 1 ≤ εk 4 . But this last RHS is upperbounded by εn (as 34 x ≤ x(1 − 1 4 x) for x ∈ [0, 1]),showing that Therefore, f was ε-close to k-monotone to begin with, which is a contradiction.  Proof. We use the algorithm of  #b14  for estimating support size. Inspecting their algorithm, we see that our cap of 20C/ε for interval length (and therefore 20/(εk) for maximum probability reported) might result in further error of the estimate. The algorithm interacts with the unknown function by estimating the expected value of 1/D f (j) over random choices of j with respect to D f . Our cap can only decrease this expectation by at most (εk)/20. Indeed, the algorithm works by estimating the quantityE x∼D f [ 1 D f (x) 1 {Df (x)>τ } ], for some suitable parameter τ > 0. By capping the value of 1/D f (x) to 20/(εk), we can therefore only decrease the estimate, and by at most 20/(εk) · D f ({ x : D f (x) > (εk)/20 }) ≤ 20/(εk).The condition for their algorithm to estimate support size to within ±εm is that all elements in the support have a probability mass of at least 1/m. Since each nonempty interval has length at least 1, we have min j D f (j) ≥ (1/Ck). In order for their algorithm to report an estimate within ±εk/20 of support size, we set ε ′ = (ε/20C) in their algorithm.The total error in support size is at most εk/20 + εk/20 = εk/10. By Claim 4.6, this suffices to test ε-test k-monotonicity of f .Using the algorithm of  #b14 , we need O(1/ε ′ 2 ) = O (C/ε) 2 queries to D f . For every query to D f , we need to make O(C/ε) queries to f , so the overall query complexity is O C 3 /ε 3 .

Reducing
[n] → {0, 1} to [Ck] → {0, 1}.Now we show how to reduce ε-testing k-monotonicity of f : [n] → {0, 1} to ε ′ -testing k-monotonicity of a function g : [Ck] → {0, 1} for C = poly(1/ε) and ε ′ = poly(ε), resulting in a poly(1/ε)-query algorithm for ε-testing k-monotonicity.The first step is (as before) to divide [n] in blocks (disjoint intervals) of size εn 4k if ε > 8k n (again assuming without loss of generality that εn 4k is an integer), and blocks of size 1 otherwise (in which case n ≤ 8k ε and we can directly apply the result of Claim 4.7, with C = n/k ≤ 8/ε). Let m = 4k/ε be the number of resulting blocks, and define f m : [n] → {0, 1} as the m-block-coarsening of f : namely, for any j ∈ B i , we setf m (j) = argmax b∈{0,1} Pr k∈B i [f m (k) = b](majority vote)Ordering the blocks B 1 , B 2 , . . . , B m , we also define g : [m] → {0, 1} such that g(i) = min a∈B i f m (a).

Lemma 4.8. Suppose f is k-monotone. Then f has at most k non-constant blocks, and f m is k-monotone.
Proof. The function f only changes values k times; for a block to be non-constant, the block must contain a pair of points with a value change.We call a block variable if the minority points comprise at least an ε/100-fraction of the block; formally, B is variable if min b∈{0,1} Pr j∈B [f (j) = b] ≥ ε/100. Lemma 4.10. Suppose f is promised to be either (i) k-monotone or (ii) such that f m has more than 5 4 k variable blocks. Then we can determine which with O 1 ε 2 log 1 ε queries, and probability 9/10. Proof. We first note that given any fixed block B, it is easy to detect whether it is variable (with probability of failure at most δ) by making O 1 ε log 1 δ uniformly distributed queries in B. Doing so, a variable block will be labelled as such with probability at least 1 − δ, while a constant block will never be marked as variable. (If a block is neither constant nor variable, then any answer will do.)Letting s denote the number of variable blocks, we then want to non-adaptively distinguish between s ≥ 5 4 k = 5ε 16 m and s ≤ k = ε 4 m (since if f were k-monotone, then f m had at most k variable blocks). Doing so with probability at least 19/20 can be done by checking only q = O 1 ε blocks chosen uniformly at random: by the above, setting δ = 1 20q all of the q checks will also yield the correct answer with probability no less than 9/10, so by a union bound we will distinguish (i) and (ii) with probability at least 9/10. We conclude by observing that all O q · 1 ε log 1 q = O 1 ε 2 log 1 ε queries are indeed non-adaptive. Proof. We use the estimation/test from the previous lemma as the first part of our tester. Assuming f passes, we can assume that f m has less than 5 4 k variable blocks. By Lemma 4.9, dist(f, f m ) ≤ 5k 4 /m + ε 100 = 5ε 32 + ε 100 ≤ ε 3 . This part takes O 1 ε 2 log 1 ε queries. Now, we apply the tester of Claim 4.7 (with probability of success amplified to 9/10 by standard arguments) to (ε/6)-test k-monotonicity of g : [m] → {0, 1}, where g(i) is the constant value of f m on B i , and m = (4k)/ε. Let q be the query complexity of the tester, and set δ = 1/(10q); to query g(i), we randomly query f on O 1 ε log 1 δ points in B i and take the majority vote. With probability at least 1 − δ, we get the correct value of g(i), and by a union bound all q simulated queries have the correct value with probability at least 9/10. Therefore, to get a single query to g, we use O((log q)/ε) queries. In the context of our previous section, we have C = 4/ε, so q = O(C 3 /ε 3 ) = O 1/ε 6 and the overall query complexity of this part is O((q log q)/ε) = O 1 ε 7 log 1 ε . This dominates the query complexity of the other part of the tester, from Lemma 4.10, which is O 1 ε 2 log 1 ε . By a union bound over the part from Lemma 4.10, the simulation of g, and the call to the tester of Claim 4.7, the algorithm is correct with probability at least 1 − 3/10 > 2/3.

On the grid
We now turn to the grid, and consider k-monotonicity of functions defined on [n] 2 . More specifically, in this section we prove Theorem 1.6, giving an adaptive tester for 2-mononicity with optimal query complexity, before discussing in Section 5.2 possible extensions of these ideas.

The case k = 2
Theorem 1.6. There exists a two-sided adaptive tester for 2-monotonicity of functions f : [n] 2 → {0, 1} with query complexity q(n, ε) = O 1 ε .Proof. At a high-level, the algorithm relies on two key components: the first is the observation that testing 2-monotonicity of f : [n] 2 → {0, 1} under some suitable additional assumption on f reduces to (tolerant) testing monotonicity of two one-dimensional functions (but with larger range), under the L 1 norm. The second is that, given access to an arbitrary f , one can efficiently provide query access to some function g which satisifies this additional assumption, and such that g will also be close to f whenever f is truly 2-monotone. Combining the two then enables one to test this function g for 2-monotonicity, and then check whether it is also the case that f and g are sufficiently close. The first step, by the above, can be done efficiently by simulationg query access to g, which in turn allows to (with some additional tricks) simulate access to the corresponding one-dimensional functions: and invoke on these two functions the L 1 -tester of  #b11 . (The main challenges there lies in performing this two-level simulation while keeping the number of queries to f low enough; which we achieve by carefully amortizing the queries made overall.) Details. We hereafter assume without loss of generality that f is identically 0 on the bottom and top rows, that is f (1, j) = f (n, j) = 0 for all j ∈ [n]. (Indeed, we can ensure this is the case by adding two extra columns, extending the domain of f to [n + 2] × [n]: note that if f remains 2-monotone if it was already, and can only decrease its distance to 2-monotonicity by O(1/n)). 7 For the sake of the proof, we will require the notion of 2-column-wise-monotonicity, defined below:Definition 5.1. A function f : [n] 2 → {0, 1} is said to be 2-column-wise-monotone if, for every j ∈ [n], its restriction f j : [n] × {j} → {0, 1} is 2-monotone. Given such a function f , we define the two sequences (∂f j ) j∈ [n] and (∂f j ) j∈ [n] as the sequence of "changepoints" in the columns. More formally, we definē∂f j = min { i ∈ [n] : f (i, j) = f (1, j) } − 1,∂f j = max { i ∈ [n] : f (i, j) = f (n, j) } + 1for every j such that f | [n]×{j} is not constant; and∂f j =∂f j = 1 otherwise. Note that we havē ∂f j ≤∂f j for every column j ∈ [n].As it turns out, testing 2-monotonicity of functions guaranteed to be 2-column-wise-monotone reduces to testing monotonicity of these two specific subsequences:Lemma 5.2. Let f : [n] 2 → {0, 1} be 2-column-wise-monotone. If f is 2-monotone,

then both sequences (∂f j ) j∈[n] and (∂f j ) j∈[n] are non-increasing. Moreover, if f is ε-far from 2-monotone, then at least one of the two sequences is ε 2 -far from non-increasing (in Hamming distance).
It is possible to refine the above statement to obtain, in the second case, a more precise characterization in terms of the L 1 distance of these two sequences to monotonicity. For conciseness, in the rest of this section we denote by M We defer the proof of these two lemmata to Appendix C, and turn to the proof of Theorem 1.6. We first describe a non-optimal tester making O(1/ε 2 ) queries; before explaining how to modify it in order to amortize the number of queries, to yield the desired O(1/ε) query complexity.Suppose we are given query access to an arbitrary function f : ). This uniquely defines a function g : [n] 2 → {0, 1, * }: for any point x = (i, j) ∈ [n] 2 , we let ℓ ∈ [K] be the index such that i ∈ B ℓ = {b ℓ , . . . , b ℓ+1 − 1}, and set:• g(x) = f (i, b ℓ )), if f (i, b ℓ ) = f (i, b ℓ+1 − 1); • g(x) = * , if f (i, b ℓ ) = f (i, b ℓ+1 − 1);so that g is constant on any "block" B ℓ × {i}. Note that we can provide query access to g, at the price of an overhead of 2 queries (to f ) per query (to g).However, this g may not itself be 2-column-wise-monotone; for this reason, we will instead work with a "fixed" version of g which will by construction be 2-column-wise-monotone. In more detail, we defineg to be the 2-column-wise-monotone function obtained by the following process.• First, we (arbitrarily) set the values * to 0, so that g becomes a function g : [n] 2 → {0, 1}.• Defineg by its restriction on each column: letting (∂g j ) j∈ [n] and (∂g j ) j∈[n] be as defined in Definition 5.1 (observing that the quantities are well and uniquely defined even if g is not 2-column-wise-monotone), setg(i, j) =        g(1, j) if i ≤∂g j 1 − g(1, j) if∂g j < i <∂g j g(n, j) if i ≥∂g jFrom this construction, it is clear thatg is 2-column-wise-monotone, and entirely and deterministically determined by g (and therefore by f ); moreover we haveg = g whenever g is itself 2-columnwise-monotone. Furthermore, any query tog can straightforwardly be answered by making at most queries O(1/ε) to g, and hence to f .• If f is 2-monotone, then g is ε 8 -close to f and so isg; moreover,g is 2-monotone as well. Therefore dist(f,g) ≤ ε 8 and dist g, M(2) 2 = 0.• If f is ε-far from 2-monotone, theng is either (i) ε 4 -far from f or (ii) 3ε 4 -far from 2-monotone, since if neither hold then by the triangle inequality f is ε-close to 2-monotone.The tester (first take). The algorithm now proceeds as follows:1. simulate query access tog (defined as above) to detect if dist g, M(2) 2 ≥ ε using Eq.(5), with probability of failure 1 6 . More precisely, test monotonicity in L 1 distance of both∂g and∂g with parameter ε 64 , using the (non-adaptive) algorithm of  #b11 ; and reject if any of these two tests rejects.• If dist g, M(2) 2 = 0, then L 1 (∂f, M (1) ) = L 1 (∂f, M (1) ) = 0 by Lemma 5.2, and both tests will accept.• If dist g, M (2) 2 > 3ε 4 , then max(L 1 (∂f, M (1) ), L 1 (∂f, M (1) )) > 3ε 8and at least one of the two tests rejects.2. simulate query access tog to test whether dist(f,g) ≤ ε 8 vs. dist(f,g) > ε 4 , with probability of failure 1 6 ; 3. return ACCEPT if both of the two tests above passed, REJECT otherwise. By a union bound, the tester is then correct with probability at least 2 3 ; its query complexity is2t · O 1 ε + t ′ · O 1 ε + O 1 εwhere t and t ′ are respectively the cost of simulating query access to∂g,∂g, and tog; as the first step, testing in L 1 for for functions defined on the line [n], has query complexity O(1/ε) from  #b11 . Taking t = t ′ = O(1/ε) as discussed above then results in a query complexity of O 1/ε 2 .However, as mentioned previously this is not optimal: as we shall see, we can modify this to obtain instead an O(1/ε) query complexity. In order to "amortize" the overall query complexity, we define the following process that specifies a 2-column-wise-monotone functiong:Initialization. Let j 1 , . . . , j 1/ε+1 ∈ [n] be the indices defined by j ℓ = (ℓ − 1) · εn + 1 for ℓ ∈ [1/ε], and j 1/ε+1 = n.• Obtain all the values of g on the j 1 -st column [n] × {j 1 }, at a cost of O(1/ε) queries to f , to find∂g j 1 ,∂g j 1 . Defineg on this column accordingly. • Assumingg has been defined on the j ℓ -th column, define it on the j ℓ+1 -th column: starting at the "vertical" positions of the two changepoints∂g j ℓ ,∂g j ℓ of the previous column, start querying "downwards" the values of g on the j ℓ+1 -th column until candidates values for∂g j ℓ+1 ,∂g j ℓ+1 consistent with g are found or the bottom of the column is reached (in which case the corresponding changepoint∂g j ℓ+1 or∂g ℓ+1 is set to 1). After this, define g on the j ℓ+1 -th column to be consistent with these (at most) two changepoints.Note that the queries made in this step are adaptive, and the (partial) functiong obtained at the end coincides withg if f (and thereforeg) is indeed 2-monotone. This is because in this case, by Lemma 5.2 the sequences (∂g j ) j , (∂g j ) j will be non-decreasing, and therefore the process outlined above will result in∂g j ℓ =∂g j ℓ and∂g j ℓ =∂g j ℓ for all 1 ≤ ℓ ≤ 1/ε + 1. Moreover, it is not difficult to see that the total number of queries made to f in this initialization step will be O(1/ε): this is because of the fact that we only search for the current changepoint∂g j ℓ (resp.∂g j ℓ ) starting at the position of the previous one∂g j ℓ−1 (resp.∂g j ℓ−1 ), going downwards only. Since to obtain a changepoint we only query the positions of g (and therefore f ) at block endpoints, there are in total at most 1/ε positions to query where starting at one and then only going "down." Thus, the number of queries made for each column j ℓ can be written asO(1) + m ℓ , where 1/ε+1 ℓ=1 m ℓ ≤ K = O(1/ε). Query time.When querying the value ofg on a point (i, j), first let ℓ be the index such that j ℓ ≤ j < j ℓ+1 . Then define∂g j (resp.∂g j ) by querying the value of g on the j-th column for all at most 1/ε (block) indices between∂g j ℓ and∂g j ℓ+1 (resp.∂g j ℓ and∂g j ℓ+1 ) to find the corresponding candidate changepoints:∂g j = min ∂g j ℓ+1 ≤ i ≤∂g j ℓ : g(i, j) = g(1, j) − 1, ∂g j = max ∂g j ℓ+1 ≤ i ≤∂g j ℓ : g(i, j) = g(n, j) + 1Again, note that after each query the (partial) functiong obtained so far will coincide withg if f (and thereforeg) is indeed 2-monotone; andg is uniquely determined by f (and in particular does not depend on the actual queries made nor on their order). Finally, the functiong thus defined will always by construction be 2-column-wise-monotone. The tester previously described can then be slightly modified to simulate access tog instead ofg: by the above discussion, for the completeness case we will have the same guarantees as then g =g, while the soundness case stays unchanged:• If f is 2-monotone, theng =g is ε 8 -close to f ; so dist(f,g) ≤ ε 8 and dist g, M(2) 2 ≤ ε 8 . • If f is ε-far from 2-monotone, theng is either (i) ε 4 -far from f or (ii) 3ε 4 -far from 2-monotone. Thus, the analysis of correctness of the tester carries through with this modification; it only remains to bound the query complexity. We will show that the expected number of queries made is O(1/ε); a bound on the worst-case query complexity will then follow from standard arguments, 8 at the price of a constant factor in the O(·).To give this bound on the expected number of queries, we first observe that the algorithm from  #b11  we rely on in the first stage of the tester is non-adaptive, and moreover all the queries it makes are uniformly distributed (as it works by a reduction, invoking the non-adaptive, one-sided, sample-based monotonicity tester for functions [n] → {0, 1}). Similarly, all the queries made in the second stage are uniformly distributed as well.Therefore, the expected number of queries a ℓ made to columns with indices j ℓ ≤ j < j ℓ+1 is the same for each ℓ ∈ [1/ε + 1], namelya ℓ = q 1/ε = O(1).where q = O 1 ε is the total number of queries made tog (and/or to∂g,∂g) during the second phase of "Query time." Now, letting m ℓ =∂g j ℓ −∂g j ℓ+1 and m ′ ℓ =∂g j ℓ −∂g j ℓ+1 , we have that the total expected cost of simulating these queries (in terms of queries to f ) is upperbounded by1 ε ℓ=1 a ℓ · O(1) + m ℓ + m ′ ℓ = O 1 ε + O(1) · 1 ε ℓ=1 m ℓ + O(1) · 1 ε ℓ=1 m ′ ℓ = O 1 ε .Since the total number of queries made to f is the sum of the number of queries made to partly buildg during the "Initialization phase" (which is O(1/ε) by the foregoing discussion), the number of queries made to simulate access tog or∂g,∂g during the "Query time" (which was just shown to be O(1/ε) in expectation), and the number of queries directly made to f when testing the distance of f tog (which is also O(1/ε)), the expected total number of queries is indeed O(1/ε), as claimed.

Possible extensions
We now discuss two possible extensions of the techniques underlying Theorem 1.6, namely to (i) for all j ∈ [n]. The next step would then be to obtain an analogue of the key lemma of the previous section, Lemma 5.3 to this setting. An issue is that it appears necessary to consider now the L 1 distance to (k − 1)-monotonicity of these k sequences, instead of monotonicity as before. Thus, taking this route requires to generalize the definition of k-monotonicity to real-valued functions, but also to develop L 1 -testers for k-monotonicity over the line.The testing algorithm now follows the same outline as in the previous section, with the same "amortizing" idea when invoking this newly obtained L 1 -tester for k-monotonicity in parallel on the k subsequences, each with probability of failure δ = 1/(10k) (for a union bound) and approximation parameter ε ′ = ε/(10k). (Note that some more optimizations may then help further reduce the query complexity, by "sharing" the same set of queries between the k instances of the L 1 -testing algorithm.)Extending to general d (for k = 2). At a very high-level, the tester of Section 5.1 works by reducing 2-monotonicity testing of f : [n] 2 → {0, 1} to monotonicity L 1 -testing of∂f,∂f : [n] → [0, 1]. More generally, one can hope to extend this approach to higher dimensions, reducing 2monotonicity testing of f : [n] d → {0, 1} to monotonicity L 1 -testing of∂f,∂f : [n] d−1 → [0, 1]: that is, testing monotonicity (in L 1 ) of the two (d − 1)-dimensional "surfaces" of changepoints. This in turn could be done invoking the L 1 non-adaptive tester of  #b11  for monotonicity over [n] d , which has query complexityÕ(d/ε): which may lead to a total query complexity of poly(d, 1/ε), that is polynomial in the dimension. We leave this possible extension as an interesting direction for future work.

On the high-dimensional grid
In this section, we give two algorithms for tolerant testing, that is testing whether a function f : [n] d → {0, 1} is ε 1 -close to k-monotone vs. ε 2 -far from k-monotone, establishing Theorem 1.8. The first has query complexity exponential in the dimension d and is fully tolerant, that is works for any setting of 0 ≤ ε 1 < ε 2 ≤ 1. The second applies whenever ε 2 > 3ε 1 , and has (incomparable) query complexity exponential inÕ(k √ d/(ε 2 − 3ε 1 ) 2 ). Both of these algorithms can be used for non-tolerant ("regular") testing by setting ε 1 = 0 and ε 2 = ε, which implies Theorem 1.7. query complexity q(n, d, ε 1 , ε 2 , k) =Õ 1 (ε 2 −ε 1 ) 2 5kd ε 2 −ε 1 d ;• a non-adaptive tolerant tester for k-monotonicity of functions f : [n] d → {0, 1} with query complexity q(n, d, ε 1 , ε 2 , k) = 2Õ (k √ d/(ε 2 −3ε 1 ) 2 ) , under the restriction that ε 2 > 3ε 1 .As a corollary, this implies Theorem 1.7, restated below: To keep on with the notations of the other sections, we will call these cosets blocks, and will say a function h : [n] d → {0, 1} is an m-block function if it is constant on each block. Moreover, for clarity of presentation, we will omit the subscripts on B and B −1 whenever they are not necessary.TheoremWe first establish a lemma that will be useful for the proofs of correctness of both algorithms. C x = x + ℓ · 1 d : ℓ ∈ N, x ∈ [m] d and x i = 0 for some i .There are m d − (m − 1) d ≤ dm d−1 of these chains: we will show that f can only be nonconstant on at most k blocks of each chain. By contradiction, suppose there existsx ∈ [m] d such that f is nonconstant on k + 1 different blocks B −1 (z (i) ), where z (1) ≺ z (2) ≺ . . . ≺ z (k) ≺ z (k+1) , and each z (i) ∈ C x . By construction, we have B −1 (z (i) ) ≺ B −1 (z (j) ) for i < j. For each 1 ≤ i ≤ k + 1, there are two points v (i) * , v (i) * * ∈ B −1 (z i ) such that v (i) * ≺ v (i) * * and f (v (i) * ) = f (v (i) * * ). By construction v (1) * ≺ v (1) * * ≺ v (2) * ≺ v (2) * * ≺ v (3) * ≺ v (3) * * ≺ . . . ≺ v (k+1) * ≺ v (k+1) * *, and there must be at least k + 1 pairs of consecutive points with differing function values. Out of these 2k+2 many points, there is a chain of pointsv (1) ≺v (2) ≺ . . . ≺v (k+1) where f (v (i) ) = f (v (i+1) ) for 1 ≤ i ≤ k, which is a violation of the k-monotonicity of f .Thus, in each of the dm d−1 many chains of blocks, there can only be k nonconstant blocks. It follows that there are at most kdm d−1 nonconstant blocks in total. We now define h(y) to be equal to f (y) if f is constant on B(y), and arbitrarily set h(y) = 0 otherwise. Each set B −1 (y) contains (n/m) d = n d · m −d many points, and f is not constant on at most kdm d−1 of these. It follows that dist(f, h) ≤ kdm d−1 · m −d = kd/m. 6.1 Fully tolerant testing with O(kd/(ε 2 − ε 1 ))) d queries Our first algorithm (Algorithm 1) then proceeds by essentially brute-force learning an m-block function close to the unknown function, and establishes the first item of Theorem 1.8. Proposition 6.2. Algorithm 1 accepts all functions ε 1 -close to k-monotone functions, and rejects all functions ε 2 -far from k-monotone (with probability at least 2/3). Its query complexity isO d (ε 2 −ε 1 ) 2 5kd ε 2 −ε 1 + 1 d log kd ε 2 −ε 1 .Proof. The algorithm first estimates Pr y∈B −1 (x) [ f (y) = b ] for every x ∈ [m] d and b ∈ {0, 1} to within ± α 5 . We use t = 25 ln(6m d )/2α 2 points in each block to ensure (by an additive Chernoff bound) that each estimate is correct except with probability at most m −d /3. By a union bound, the probability that all estimates are correct is at least 2/3, and we hereafter condition on this.Algorithm 1 Fully tolerant testing with O(kd/(ε 2 − ε 1 ))) d queries.

Require: Query access to
f : [n] d → {0, 1}, ε 2 > ε 1 ≥ 0, a positive integer k 1: α ← (ε 2 − ε 1 ), m ← ⌈5kd/α⌉, t ← ⌈25 ln(6m d )/(2α 2 )⌉ 2: ⊲ Define a distribution D over [m] d × {0, 1}. 3: for x ∈ [m] d do 4:Query f on t random points T x ⊆ B −1 (x). 

Completeness.
Suppose dist(f, f * ) ≤ ε 1 . Then by the triangle inequality,Pr (y,b)∼D ′ [ h * (y) = b ] ≤ Pr (y,b)∼D ′ [ h * (y) = f * (y) ]+ Pr (y,b)∼D ′ [ f * (y) = f (y) ]+ Pr (y,b)∼D ′ [ f (y) = b ] ≤ ε 1 + 2α 5 .where to bound the first term Pr (y,b)∼D ′ [ h * (y) = f * (y) ] by dist(f * , h * ) ≤ α/5 we used the fact that the marginal distribution of y is uniform when (y, b) ∼ D ′ . Thus, the algorithm will find a k-monotone m-block function close to D (without using any queries to f ) and accept.

Soundness.
Suppose dist(f, f * ) ≥ ε 2 . Then by the triangle inequalityPr (y,b)∼D ′ [h(y) = b] ≥ Pr (y,b)∼D ′ [h(y) = f (y)] − Pr (y,b)∼D ′ [f (y) = b] ≥ Pr (y,b)∼D ′ [f * (y) = f (y)] − Pr (y,b)∼D ′ [f (y) = b] ≥ ε 2 − α 5for every k-monotone m-block function h. Since ε 2 − 2α/5 ≥ ε 1 + 3α/5, the algorithm never find a k-monotone m-block function h with low error with respect to D, and the algorithm will reject. 

Tolerant testing via agnostic learning
We now present our second algorithm, Algorithm 2, proving the second item of Theorem 1.8. At its core is the use of an agnostic learning algorithm for k-monotone functions, which we first describe. 9 Proposition 6.3. There exists an agnostic learning algorithm for k-monotone functions over[r] d → {0, 1} with excess error τ with sample complexity exp( O(k √ d/τ 2 ).Algorithm 2 Multiplicative approximation with exp( O(k √ d/((ε 2 − 3ε 1 ) 2 )) queries

Require:
Query access to f : queries. 6: if the estimate is more than ε 1 + 5α 12 then return REJECT 7: end if[n] d → {0, 1}, ε 2 > 3ε 1 ≥ 0, a positive integer k 1: α ← (ε 2 − 3ε 1 ), m ← ⌈6kd/ε⌉ , t ← ⌈3d(k + 1)/ε ln m + ln 100⌉ 2: Define D to be the distribution over [m] d × {0, 1} such that D(x, b) = Pr y∈B −1 (x) [ f (y) = b ]. 3: ⊲ A D (τ, f )8: if dist(h, ℓ) = Pr x∈[m] d [ h(x) = ℓ(x) ] ≤ 2ε 1 + 5α12 for some k-monotone m-block function ℓ then return ACCEPT 9: else return REJECT 10: end ifWe will rely on tools from Fourier analysis to prove Proposition 6.3. For this reason, it will be convenient in this section to view the range as {−1, 1} instead of {0, 1}. Inf i [f ] = 2 Pr [f (x) = f (x (i) )where x = (x 1 , x 2 , . . . , x d ) is a uniformly random string over [r] d , and x (i) = (x 1 , x 2 , . . . , x i−1 , x ′ i , x i+1 , . . . , x d ) for x ′ drawn independently and uniformly from [r]. We also define Inf[f ] = d i=1 Inf i [f ].We first generalize the following result, due to Blais et al., for more general domains:Proposition 6.5 ([BCO + 15]). Let f : {0, 1} d → {−1, 1} be a k-monotone function. Then Inf [f ] ≤ k √ d.Lemma 6.6 (Generalization). Let f :[r] d → {−1, 1} be a k-monotone function. Then Inf [f ] ≤ k √ d.Proof. For any two strings y 0 , y 1 ∈ [r] d , let f y 0 ,y 1 : {0, 1} d → {−1, 1} be the function obtained by setting f y 0 ,y 1 (x) = f (y x ), where y x ∈ [r] d is defined asy x i = min{y 0 i , y 1 i } if x i = 0 max{y 0 i , y 1 i } if x i = 1Since f was a k-monotone function, so is f y 0 ,y 1 . Thus Inf [f y 0 ,y 1 ] ≤ k √ d for every choice of y 0 and y 1 . It is not hard to see that for any fixed i ∈ [d] the following two processes yield the same distribution over [r] d × [r] d :• Draw z ∈ [r] d , z ′ i ∈ [r]independently and uniformly at random, set z ′ def = (z 1 , . . . , z i−1 , z ′ i , z i+1 , . . . , z d ), and output (z, z ′ );• Draw y 0 , y 1 ∈ [r] d , x ∈ {0, 1} d independently and uniformly at random, and output (y x , y x (i) ).

This implies that
Inf [f ] = d i=1 Inf i [f ] = d i=1 2 Pr z∈[r] d [f (z) = f (z (i) )] = d i=1 2E y 0 ,y 1 ∈[r] d Pr x∈{0,1} d f (y x ) = f (y x (i) ) = E y 0 ,y 1 ∈[r] d d i=1 2 Pr x∈{0,1} d f (y x ) = f (y x (i) ) = E y 0 ,y 1 ∈[r] d d i=1 2 Pr x∈{0,1} d f y 0 ,y 1 (x) = f y 0 ,y 1 (x (i) ) = E y 0 ,y 1 [Inf [f y 0 ,y 1 ]] ≤ E y 0 ,y 1 [k √ d] = k √ d.For (x) = d i=1 φ α i (x i ). Then every f : [r] d → R has a unique representation f = α∈[r] d f (α)φ α , where f (α) = f, φ α ∈ R.Many Fourier formulae hold in arbitrary Fourier bases, an important example being Parseval's Identity: α∈[r] d f (α) 2 = 1. We will use the following property: Lemmaf (α) 2 ≤ ε. Proof. If not, then Inf [f ] = α |α| f (α) 2 ≥ α:|α|>k/ε |α| f (α) 2 ≥ k ε α:|α|>k/ε f (α) 2 > k ε · ε = k, a contradiction.Lemma 6.9. Let p be the function α:|α|≤t f (α)φ α . Then (i) p − f 2 2 = E x∈[r] d [(p(x) − f (x)) 2 ] = α:|α|>t f (α) 2 ; (ii) p isy∈B −1 (x) [ f (y) = b ] .To generate a sample (x, b) from D, we draw a uniformly random string in x ∈ [m] d , and b is the result of a query for the value of f (y) for a uniformly random y ∈ B −1 (x). From Lemma 6.9, we can take S to be the set of (k √ d/τ 2 )-way products of rd indicator functions. It follows that|S| = rd k √ d/τ 2 = exp( O(k √ d/τ 2 )).Proposition 6.11. Algorithm 2 accepts all functions ε 1 -close to k-monotone functions, and rejects all functions ε 2 -far from k-monotone, when ε 2 > 3ε 1 (with probability at least 2/3). Its query complexity is exp( O(k √ d/(ε 2 − 3ε 1 ) 2 )).Proof. By a union bound, we have that with probability at least 8/10 both Step 5 and Step 4 succeed. We hereafter condition on this.Completeness. Suppose f is ε 1 -close to k-monotone. Lemma 6.1 and the triangle inequality imply that there is a k-monotone m-block function g * such that dist(f, g * ) ≤ ε 1 + α/6. The agnostic learning algorithm thus returns a hypothesis h such that dist(f, h) ≤ ε 1 + α/4. The algorithm estimates this closeness to within α/7, so the estimate obtained inStep 5 is at most ε 1 + ε/4 + ε/7 < ε 1 + 5α/12 and the algorithm does not reject in this step. By the triangle inequality, h is (2ε 1 + 5α/12)-close to k-monotone, and the algorithm will accept. There is no estimation error here, since no queries to f are required.Soundness. Now suppose f is ε 2 -far from k-monotone, where ε 2 = 3ε 1 + α for some α > 0. Suppose the algorithm does not reject when estimating dist(f, h), where h is the hypothesis returned by the agnostic learning algorithm. Then dist(f, h) ≤ ε 1 +5α/12+α/7 < ε 1 +7α/12. By the triangle inequality, if t is a k-monotone function, dist(h, t) ≥ dist(f, t) − dist(f, h) > ε 2 − (ε 1 + 7α/12) = 2ε 1 + 5α/12. The algorithm will thus reject in the final step.Query complexity. The query complexity of the algorithm is dominated by the query complexity of the agnostic learning algorithm, which is exp( In what follows, we let X be a discrete partially ordered domain equipped with a measure µ, 10 that is a tuple (X , , µ); and for a set Y ⊆ R we denote by M (X →Y) ⊆ Y X the set of monotone functions from X to Y. O(k √ d/α 2 )) = exp(Õ k √ d/(ε 2 − 3ε 1 ) 2 ).T • f (x, t) = 1 {f (x)≥1−t} = 1 if f (x) ≥ 1 − t 0 otherwise.The next fact is immediate from this definition: We begin by the following characterization, which is immediately obtained from a corresponding theorem of Berman et al.; before stating a slightly modified version that we shall rely upon. For completeness, the proof of the former can be found in Appendix C. Φ m • f : X → R m by Φ m • f (x) = ⌈mf (x)⌉ m , x ∈ X .Then we have(i) L 1 f, M (X →[0,1]) − L 1 Φ m • f, M (X →Rm) ≤ 1 m ; (ii) L 1 Φ m • f, M (X →Rm) = dist T • Φ m • f , M (X ×Rm→{0,1}) .Proof of Proposition 7.4. Fix any m ≥ 1. We start the proof of item (i) by the simple observation that if f ∈ M (X →[0,1]) , then Φ m • f ∈ M (X →Rm) ⊆ M (X →[0,1]) , that is rounding preserves monotonicity; and that Φ m • g = g for all g : X → R m . This, along with the fact that for all f : X → [0, 1]L 1 (f, Φ m • f ) = 1 µ(X ) X µ(dx) |Φ m • f (x) − f (x)| ≤1/m ≤ 1 mimplies by the triangle inequality, for any g ∈ M (X →Rm) ⊆ M (X →[0,1]) , thatL 1 f, M (X →[0,1]) ≤ L 1 (f, g) ≤ 1 m + L 1 (g, Φ m • g) + L 1 (Φ m • f, Φ m • g) = 1 m + 0 + L 1 (Φ m • f, g).Taking g ∈ M (X →Rm) that achieves L 1 (Φ m • f, g) = L 1 Φ m • f, M (X →Rm) , we get L 1 f, M (X →[0,1]) ≤ 1 m + L 1 Φ m • f, M (X →Rm) .For the other direction, we first note that for any two functions f, g :X → [0, 1], it is the case that L 1 (f, g) ≥ L 1 (Φ m • f, Φ m • g) − 1/m (which is immediate from the definition of the rounding operator), and taking g to be the closest monotone function to f this readily yieldsL 1 f, M (X →[0,1]) ≥ L 1 (Φ m • f, Φ m • g) − 1 m ≥ L 1 Φ m • f, M (X →Rm) − 1 m .Finally, the proof of the second part, item (i), is identical to that of Proposition 7.3, replacing the Lebesgue measure on L 1 f, M (X →[0,1]) ≤ ε 1 vs. L 1 f, M (X →[0,1]) ≥ ε 2 it is enough to distinguish dist g, M (X ×Rm→{0,1}) ≤ ε 1 + 1 m vs. dist g, M (X ×Rm→{0,1}) ≥ ε 2 − 1 m .By our choice of m, we also have ε 2 − 1 m − ε 1 + 1 m ≥ ε 2 −ε 1 2 . The last step is to observe that one can view equivalently g as a function g : •Õ 1 (ε 2 −ε 1 ) 2 5d ε 2 −ε 1 d , for any 0 ≤ ε 1 < ε 2 ≤ 1; • 2Õ ( √ d/(ε 2 −3ε 1 ) 2 ), for any 0 ≤ 3ε 1 < ε 2 ≤ 1.

A Previous work on monotonicity testing
In this appendix, we summarize the state-of-the-art on monotonicity testing. We observe that this question has been considered for functions over various domains (e.g. hypergrids, hypercubes and general posets) and ranges (notably Boolean range {0, 1} and unbounded range N); as hypergrids and hypercubes are arguably the domains that have received the most attention in the literature, we will in this overview restrict ourselves on work on these, and refer readers to [FLN + 02, BGJ + 09] for other various posets. We will also focus on the Boolean range {0, 1}, which is most relevant to our work, and briefly mention the best known results (which are also tight) for unbounded range N. In the end of this section, we include known results for tolerant monotonicity testing.Before we go over those results, we recall some notation: namely, testers can make adaptive (a.) or non-adaptive (n.a.) queries and have 1-sided (1.s.) or 2-sided (2.s.) error. The best one could hope for would then be to obtain 1-sided non-adaptive upper bound, complemented with 2-sided adaptive lower bounds. We note all testers included in below except tolerant testers are 1-sided (almost all of them are non-adaptive) algorithms.   #b13 ) to Ω(d 1/2−c ) (for any constant c > 0). All these lower bounds applying to non-adaptive testers, they only imply an Ω(log d) lower bound for adaptive ones. Recently, Belovs and Blais  #b4  showed an Ω(d 1/4 ) lower bound for 2-sided adaptive testers, i.e. an exponential improvement over the previous bounds. All mentioned lower bounds hold for constant ε > 0, and are summarized in Table 3 

Hypercubes with Boolean


B Structural results
In this section, we will prove that the distance to k-monotonicity of a Boolean function f can be expressed in a combinatorial way -which does not require measuring the distance between f and the closest k-monotone function to f . We will prove this for a general finite poset domain buiding up on the ideas of [FLN + 02]. In the rest of this section, we denote by P = (V, ) an arbitrary poset, the underlying domain of the function.Definition B.1. We define the forbidden pattern K 10 as the sequence of alternating bits K 10 = (b 1 , b 2 , · · · , b k , b k+1 ) of length (k + 1) , where b 1 = 1 and all the bits in the sequence alternate, i.e.,b i = b i+1 ∀i ∈ [k].A function f : P → {0, 1} is k-monotone only if it avoids K 10 . That is, for any x 1 ≺ x 2 ≺ . . . ≺ x k+1 ∈ P we have f (x i ) = K 10 (i) for some i ∈ [k + 1].Using insights from the literature on monotonicity testing, we show that functions far from kmonotonicity have a large matching of "violated hyperedges" in the "violation hypergraph" which we define shortly. Let us recall the definition of "violation graph"which has been extremely useful with monotonicity testing as seen in [EKK + 00, PRR06, HK08, ACCL07, FLN + 02].

Definition B.2 (Violation graph). Given a function
f : P → {0, 1}, the violation graph of f is defined as G viol (f ) = (P, E(G viol )) where (x, y) ∈ E(G viol ) if x, y ∈ P form a monotonicity violating pair in f -that is x y but f (x) > f (y).The following theorem about violation graphs has been extremely useful in monotonicity testing literature. where (x 1 , x 2 , · · · , x k ) ∈ E(H viol ) if the ordered (k + 1)-tuple x 1 < x 2 < . . . < x k+1 (which is a (k + 1)-uniform hyperedge) forms a violation to k-monotonicity in f . Now, we state the main theorem that we intend to prove in this section. This theorem offers an alternate characterization of distance to k-monotonicity that we seek. We recall that a set of edges forms a matching in a hypergraph if any pair of hyperedges is pairwise disjoint.Theorem B.5. Let f : P → {0, 1} be function that is ε-far from k-monotone. Then, there exists a matching of (k + 1)-uniform hyperedges of size at least ε|P| k+1 in the violation hypergraph. To prove this theorem we first exploit the key notion of extendability which we define below. Later we will show that k-monotone functions are extendable. Definition B.6 (Extendability). A property of Boolean functions is said to be extendable over a poset domain if the following holds for any set X ⊆ P: given a function f : X → {0, 1} which has the property (on X), it is possible to define a function g : P → {0, 1} such that g(x) = f (x), ∀x ∈ X and g has the property.In other words, a property is extendable if for any subset X ⊆ P, given a function defined over the set X which respects the property, it is possible to fill in values outside X such that the new function obtained continues to respect the property. Next, we show that k-monotonicity is an extendable property: Lemma B.7. k-monotonicity is an extendable property.Proof of Lemma B.7. Consider X ⊆ P and some function f : X → {0, 1} which is k-monotone over X. That is, for anyx 1 < x 2 < . . . < x k+1 ∈ X there exists i ∈ [k + 1] such that f (x i ) = K 10 [i].Take a minimal point v ∈ P \ X. That is, for any other point v ′ ∈ P \ X either v ≤ v ′ or v and v ′ are not comparable. We will use the following result:Claim B.8. There exists a function g : X ∪ {v} → {0, 1} such that g(x) = f (x) for all x ∈ X, and g respects k-monotonicity over its domain.Before proving this claim, we show how it implies Lemma B.7. Namely, starting with any function f : X → {0, 1} which is k-monotone on its domain X, we just keep applying the Claim B.8 inductively until we get a function defined over the entire poset which respects k-monotonicity.Proof of Claim B.8. We will show this by contradiction. Suppose there is no good assignment available for g (v), that is that both the choices g(v) = 0 and g(v) = 1 lead to a violation to kmonotonicity in g. Consider the choice g(v) = 0. Since this results in a violation to k-monotonicity, we know that there is a path P 0 = (x 1 ≺ x 2 ≺ . . . ≺ x k+1 ) which is a violation to k-monotonicity. It is clear that v ∈ P 0 ; let i be such that x i = v. Similarly, there is path P 1 = (y 1 ≺ y 2 ≺ . . . ≺ y k+1 ) corresponding to g(v) = 1 which also contains the forbidden pattern, and some j such that y j = v. And thus, g(x t ) = g(y t ), for all t ∈ [k + 1] (as both of the paths indexed by x and y form a violation to k-monotonicity). By the above discussion 2 paths, P 0 and P 1 , meet at v. We will see that one of the two paths P ′ 0 = (x 1 ≺ x 2 ≺ . . . < x i−1 ≺ y i ≺ y i+1 ≺ . . . ≺ y k+1 ) or P ′ 1 = (y 1 ≺ y 2 ≺ . . . < y j−1 ≺ x j ≺ x j+1 ≺ . . . ≺ x k+1 ) is already a violation to k-monotonicity in f . To see this, let us begin by recalling that we let v be the i th vertex on P 0 and the j th vertex on P 1 . Now it is clear that i = j. Without loss of generality, suppose i < j. In this case, the evaluations of f along path P ′ 1 form the forbidden pattern. This is because the function values alternate along the segment (y 1 ≺ y 2 ≺ . . . ≺ y j−1 ). Also, the function values alternate along the segment (x i ≺ x i+1 ≺ x i+2 ≺ . . . ≺ x k+1 ). And finally note that since f (y j−1 ) = f (y j ) and f (y j ) = f (x j ) we get that f (y j−1 ) = f (x j ) as well. So, the path P ′ 1 indeed contains a violation to k-monotonicity as claimed. The other case, i > j, is analogous. Hence the claim follows.In the next lemma, we show that there is a nice characterization of distance to k-monotonicity in terms of the size of the minimum vertex cover of the violation hypergraph. 11 Lemma B.9. Let M k denote the set of k-monotone functions over the poset P, and f : P → {0, 1}.Then dist(f, M k ) = ε f if, and only if, the size of the minimum vertex cover in H viol (f ) is ε f |P|.Proof of Lemma B.9. We establish separately the two inequalities. Claim B.10. dist(f, M k ) ≥ |V C min (H viol )| Proof. Suppose the distance to k-monotonicity is ε f , and let g be a k-monotone achieving it, so that dist(f, g) = ε f . Define X = { x ∈ P : f (x) = g(x) } (thus, |X| = ε f |P|). Let us consider the violation hypergraph for f given as H viol (f ) = (P, E(H viol )). Now, delete vertices in X and the hyperedges containing any vertex v ∈ X from this hypergraph. Because X is the smallest set of vertices changing values at which gives a k-monotone function, it follows that every hyperedge in E(H viol ) must contain a vertex in X. Thus, X indeed forms a vertex cover in H viol (f ).Claim B.11. dist(f, M k ) ≤ |V C min (H viol )| Proof. Suppose the minimum vertex cover in the violation hypergraph has size ε f |P|. We will show that the distance of the function to k-monotonicity is ε f . To see this, let C ⊆ P be the smallest vertex cover of the violation hypergraph of the said size. Observe that deleting C from H viol (f ) removes all the hyperedges, and therefore that the function f restricted to X = P \ C is k-monotone. And by the extendability of k-monotone functions established in Lemma B.7, it follows that the function can be extended to the rest of the domain (by providing values in the cover C) such that it keeps respecting k-monotonicity. Thus, the distance to k-monotonicity is at most |C| / |P|.Having characterized distance to k-monotonicity as the size of the smallest vertex cover in the violation graph, we are ready to establish Theorem B.5. To do so, we will require the following standard fact: Proof of Theorem B.5. By Lemma B.9, we know that the violation hypergraph, H viol (f ) has a minimum vertex cover of size at least ε f |P|. And by Fact B.12, it is seen that it contains a matching of t-uniform hyperedges of size at least ε f t |P|.

C Omitted proofs
Proof of Lemma 5.2. The first part of the theorem is straightforward (by contrapositive, if at least one of the two sequences is not non-increasing then we can find a violation of 2-monotonicity). We thus turn to the second part, and show the contrapositive; for this purpose, we require the following result: Proof. By contradiction, suppose there exists a 2-column-wise monotone function f satisfying (i) and (ii), which is not 2-monotone. This last point implies there exists a triple of comparable elements x = (i x , j x ) ≺ y = (i y , j y ) ≺ z = (i z , j z ) constituting a violation, i.e. such that (f (x), f (y), f (z)) = (1, 0, 1). Moreover, since (i) holds we must have 1 < i x ≤ i y ≤ i z < n; more precisely, 1 ≤∂f jx < i x ≤ i y ≤ i z <∂f jz ≤ n. As x ≺ y ≺ z, we have j x ≤ j y ≤ j z , which by the non-increasing assumption (ii) implies that∂f jx ≥∂f jy and∂f jy ≥∂f jz . But this is not possible, as altogether this leads to∂f jy < i y <∂f jy , i.e. f (y) = 1.Assume both sequences (∂f j ) j∈ [n] , (∂h j ) j∈[n] ⊆ [n] are ε 2 -close to non-increasing, and let L, H ⊂ [n] (respectively) be the set of indices where the two sequences need to be changed in order to become non-increasing. By assumption, |L| , |H| ≤ εn 2 , so |L ∪ H| ≤ εn. But to "fix" a value of (∂f j ) j∈ [n] or (∂f j ) j∈ [n] requires to change the values of the function f inside a single column -and this can be done preserving its 2-column-wise-monotonicity, so that changing the value of f on at most n points is enough. It follows that making both (∂f j ) j∈ [n] and (∂f j ) j∈[n] non-increasing requires to change f on at most εn 2 points, and with Claim C.1 this results in a function which is 2-monotone. Thus, f is ε-close to 2-monotone.Proof of Lemma 5.3. Recall that we aim at establishing the following: For notational convenience, we will view in this proof the sequences (∂f ) j , (∂f ) j ) as functions ∂f,∂f : [n] → [n]. Let ℓ, h : [n] → [n] (for "low" and "high," respectively) be monotone functions achieving L 1 (∂f, M (1) ) and L 1 (∂f, M (1) ), respectively.• As∂f (j) ≤∂f (j) for all j ∈ [n], we will assume ℓ(j) ≤ h(j) for all j. Otherwise, one can consider instead the functions ℓ ′ = min(ℓ, h) and h ′ = max(ℓ, h): both will still be monotone (non-increasing), and by construction ℓ ′ (j) −∂f (j) + h ′ (j) −∂f (j) ≤ |ℓ(j) −∂f (j)| + h(j) −∂f (j)for all j ∈ [n], so that L 1 (∂f, ℓ ′ ) + L 1 (∂f, h ′ ) ≤ L 1 (∂f, ℓ) + L 1 (∂f, h). • From ℓ and h, we can define a 2-column-wise monotone function g : [n] 2 → [n] such that ∂g = ℓ and∂g = h: that is,g(i, j) =        0 if i ≥ h(j) 1 if ℓ(j) < i < h(j) 0 if i ≤ ℓ(j) for (i, j) ∈ [n] 2 .It is clear that g is 2-column-wise monotone with g(1, j) = g(n, j) = 0 for all j ∈ [n]; since by construction∂g,∂g are non-decreasing, we can invoke Claim C.1 to conclude g is 2-monotone. It remains to bound the distance between f and g: writing ∆ j ∈ {0, . . . , n} for the number of points on which f and g differ in the j-th column, we have For any fixed t ∈ [0, 1], let g t ∈ M (X →{0,1}) be any function achieving L 1 (T • f (·, t), g t ) = L 1 T • f (·, t), M (X →{0,1}) , and define g ∈ [0, 1] X by g ′ (x) = 1 0 dtg t (x) for all x ∈ X : note that g is then monotone by construction. 12 Moreover, choose h ∈ M (X ×[0,1]→{0,1}) as a function achieving L 1 (T • f, h) = L 1 T • f, M (X ×[0,1]→{0,1}) . Then we have where we applied Fact 7.2 (and the definition of g ′ = 1 0 g t ) for the first equality, and for the third inequality the fact that h induces (for every fixed t ∈ [0, 1]) a monotone function h(·, t) ∈ M (X →{0,1}) : so that L 1 (T • f (·, t), g t ) ≤ L 1 (T • f (·, t), h(·, t)) for all t.L 1 f, M (X →[0,1]) ≤ L 1 f, g ′ = 1 µ(X ) X µ(dx) 1 0 dt(T • f (x, t) − g t (x)) ≤ 1 µ(X ) X µ(dx) 1 0 dt |T • f (x, t) − g t (x)| = 1 0 dt 1 µ(X ) X µ(dx) |T • f (x, t) − g t (x)| =For the other direction of the inequality, fix any f : X → [0, 1], and let g ∈ M (X →[0,1]) be (any) function achieving L 1 (f, g) = L 1 f, M (X →[0,1]) . We can write, unrolling the definitions,L 1 f, M (X →[0,1]) = 1 µ(X ) X µ(dx)|f (x) − g(x)| = 1 µ(X ) X µ(dx) 1 0 dt(T • f (x, t) − T • g(x, t)) = 1 µ(X ) X µ(dx) 1 0 dt(T • f (x, t) − T • g(x, t)) = 1 µ(X ) X µ(dx) 1 0 dt(T • f (x, t) − T • g(x, t))1 {f (x)>g(x)} + (T • g(x, t) − T • f (x, t))1 {g(x)>f (x)} = 1 µ(X ) X 1 0 dtµ(dx) (T • f (x, t) − T • g(x, t))1 {f (x)>g(x)} + (T • g(x, t) − T • f (x, t))1 {g(x)>f (x)} = 1 ν(X × [0, 1]) X ×[0,1] ν(dx, dt) |T • f (x, t) − T • g(x, t)| = L 1 (T • f, T • g) ≥ L 1 T • f, M (X ×[0,1]→{0,1})where we applied Fact 7.2 for the second equality, the definition of L 1 distance for the second-tolast; and to handle the absolute values we used the fact that |a − b| = (a 

Footnote
1 : We thank Eric Blais for mentioning the connection, and pointing us to these works
3 : The case k even is similar. In this case, one may assume that f evaluates to 0 on points with hamming weight everywhere outside the middle levels.
7 : This will be used in the proof of Lemma 5.2 and Lemma 5.3.
8 : Namely, stopping the algorithm and outputting REJECT if the number of queries made exceeds C ε for some absolute constant C > 0, found by applying Markov's inequality.
9 : Recall that an agnostic learner with excess error τ for some class of functions C is an algorithm that, given an unknown distribution D, an unknown arbitrary function f , and access to random labelled samples x, f (x) where x ∼ D, satisfies the following. It outputs a hypothesis functionĥ such that Prx∼D f (x) =ĥ(x) ≤ optD + τ with probability at least 2/3, where optD = min h∈C Prx∼D [ f (x) = h(x) ] (i.e., it performs "almost as well as the best function in C").
10 : We will only require that (X , µ) be a measurable space with finite measure, that is µ(X ) < ∞, and shall only hereafter concern ourselves with measurable functions.
11 : Recall that a vertex cover in a hypergraph is just a set of vertices such that every hyperedge contains at least one of the vertices from this set.
12 : Additionally, since we restrict ourselves to finite X , there are only finitely many distinct functions T • f (·, t) (for t ∈ [0, 1], and therefore only finitely many distinct functions gt.