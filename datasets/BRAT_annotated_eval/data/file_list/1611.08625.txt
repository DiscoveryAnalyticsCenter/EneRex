Directional Mean Curvature for Textured Image Demixing

Abstract
Approximation theory plays an important role in image processing, especially image deconvolution and decomposition. For piecewise smooth images, there are many methods that have been developed over the past thirty years. The goal of this study is to devise similar and practical methodology for handling textured images. This problem is motivated by forensic imaging, since fingerprints, shoeprints and bullet ballistic evidence are textured images. In particular, it is known that texture information is almost destroyed by a blur operator, such as a blurred ballistic image captured from a low-cost microscope. The contribution of this work is twofold: first, we propose a mathematical model for textured image deconvolution and decomposition into four meaningful components, using a high-order partial differential equation approach based on the directional mean curvature. Second, we uncover a link between functional analysis and multiscale sampling theory, e.g., harmonic analysis and filter banks. Both theoretical results and examples with natural images are provided to illustrate the performance of the proposed model.

Introduction
Processing textured images is difficult, but such images are common in many applications. For example, as part of the 2015-2016 forensic statistics program at the Statistical and Applied Mathematical Sciences Institute, much of the research dealt with images of fingerprints, balistic striations on bullets, and shoeprints, and all of these are typically textured.There is an extensive literature on methods for processing piecewise smooth images; e.g., image restoration by functional analysis  #b0  #b1  #b2  #b3  #b4  #b5  #b6  #b7 , image representation by harmonic analysis  #b8  #b9  #b10  #b11  #b12  #b13  #b14  #b15  and image segmentation  #b16  #b17  #b18  #b19  #b20  #b21  #b22  #b23  #b24  #b25  #b26 . Much less has been done with textured images.Instead of being piecewise smooth, textured images have fine scale discontinuities, so that neighboring pixels in the image may take very different values in gray scale or color space. Examples are a shoeprint in sand, where the grainy structure of the sand provides the texture, or a fingerprint on leather, where the leather's texture interacts with the signal, see  #b27 .To demix (i.e., to simultaneously deblur and decompose) textured images, we substantially extend the work in Thai and Gottschlich  #b28 . That paper provided a directional decomposition of gray-scale images f into three parts, consisting of piecewise smooth structure u, texture v, and fine scale residual structure ρ. The solution required minimization of the directional total variation norm, which entails the solution to a second-order partial differential equation.The new approach is based upon  #b29 , and also focuses upon gray-scale images. But now it decomposes the image f into four parts, adding fine-scale noise structure to the previous three distinctions. It also models a blur operator h (for simplicity, h is assumed to be known, which is reasonable since in principle it can be measured for any camera in a specific application). Thus the demixing problem is formulated asf = h * (u + v + ρ) + ,where * is a convolution operator.The solution minimizes a function of several well-chosen norms and uses directional mean curvature rather than a directional decomposition. This entails solution of a fourth-order partial differential equation (cf. Zhu and Chan  #b5  #b21 ) which addresses the "staircase effect" in the TV-L2 model  #b2 . Other high-order partial differential equation (PDE) approaches for image reconstruction are given in  #b30  #b31  #b32  #b33  #b34 . Importantly, this work finds a new connection between the minimization problem in demixing and multiscale harmonic analysis. This connection enables a more general theory and a new solution strategy.Our model realistically reflects the mechanism of image capture. The true image is a combination of both smooth regions, texture, and fine scale structure. Blurring is inevitable, and the additive noise term accounts for other distortions (e.g., threshold variation in the CMOS or CCD light sensors, edge effects between pixels). The reconstruction error (the pixel-wise difference between the true image and the estimated image) is especially useful since it permits a quantitative basis for comparing the quality of demixing algorithms. When two algorithms make comparable assumptions about smoothness and the blur operator, then any structure that persists after demixing appears in the reconstruction error. Its mean squared error, or the eigenvalues of the estimated covariance matrix, enable one to determine which algorithm has successfully extracted more signal.Despite the ability of the Euler-Lagrange equations associated with the variational model to achieve advanced performance in image analysis, numerical solution of this high order PDE is difficult. Following  #b21  #b33 , a numerical augmented Lagrangian method (ALM) is applied to split the directional mean curvature norm into several 2 -and 1 -norms, which are solvable by iterative shrinkage/thresholding (IST) algorithms  #b35  #b36  #b37 . Wu et al.  #b38  proved the equivalence between ALM, dual methods (e.g., Chambolle's projection  #b39 ), and the splitting Bregman method  #b7  for solving the convex optimization. (Note that ALM or the splitting Bregman method can be employed in the Douglas-Rachford splitting scheme  #b40 .)We focus on understanding the advantages of the fourth-order PDE approach to reconstruct a smooth image with sharp edges, and make two contributions:1. we provide a general solution to a mathematical model for textured image deconvolution and decomposition with four meaningful components, and 2. we find a link between functional analysis and multiscale sampling theory in harmonic analysis and filter banks.This employs a novel Directional Mean Curvature Demixing (DMCD) method for textured images corrupted by i.i.d. noise with a given blur kernel.To develop these ideas, section 2 introduces the mathematical framework, and section 3 describes the use of directional mean curvature for demixing and the algorithm needed to fit the model. Section 4 makes a quantitative comparison between the demixing results from the proposed algorithm and results from a standard alternative algorithm. Section 5 describes the new model's correspondence with multiscale harmonic analysis. Section 6 summarizes our conclusions. For readability, mathematical proofs are provided in the Appendix and we also refer the reader to  #b41  #b28  #b42  #b43  for more detailed notation and literature review.

Notation and Mathematical Preliminaries
In this section we define the image and present the directional forward/backward difference operators. We also specify the directional gradient and divergence operators, the directional Laplacian operator, and the discrete directional G S -norm. Using the curvelet transform and pointwise shrinkage operators, these enable the mathematical derivation of the new demixing algorithm.Given a discrete grayscale image f [k] : Ω → R + of size d 1 × d 2 , with the latticeΩ = k = [k 1 , k 2 ] ∈ [0 , d 1 − 1] × [0 , d 2 − 1] ⊂ Z 2 ,let X be the Euclidean space whose dimension is given by the size of the lattice Ω; i.e., X = R |Ω| . The 2D discrete Fourier transform F acting on f [k] isf [k] F ←→ F (e jω ) = k∈Ω f [k] · e −j k ,ω 2 ,where the Fourier coordinates ω ∈ [−π , π] 2 are defined on the lattice asI = ω = [ω 1 , ω 2 ] = 2πd 2 d 2 , 2πd 1 d 1 | (d 2 , d 1 ) ∈ − d 2 2 , d 2 2 × − d 1 2 , d 1 2 ⊂ Z 2 .We let z = [z 1 , z 2 ] = e jω1 , e jω2 denote the discrete coordinates of the Fourier transform.Given the matrix k∈Ω ∈ X, areD 1 =         −∂ + l f = cos( πl L )f D T 2 + sin( πl L )D 1 f F ←→ cos πl L (z 2 − 1) + sin πl L (z 1 − 1) F (z) , ∂ − l f = − cos πl L f D 2 + sin πl L D T 1 f F ←→ − cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) F (z) .The transposed matrices of D 2 and D 1 are D T 2 and D T 1 , respectively. Their adjoint operators are ∂ ± l * = −∂ ∓ l . The directional gradient and divergence operators are, respectively,∇ ± L f = ∂ ± l f L−1 l=0∈ X L and div ± L g = L−1 l=0 ∂ ± l g l ∈ X.Note that the adjoint operator of ∇ ± L is ∇ ± L * = −div ∓ L , i.e.,∇ ± L f , g 2 = f , −div ∓ L =(∇ ± L ) * g 2 .The directional Laplacian operator is∆ dL f = div − L ∇ + L f = L−1 l=0 ∂ − l ∂ + l f F ←→ − L−1 l=0 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 F (z).Remind that given x ∈ R 2 , the impulse responses of directional derivative and directional Laplacian operators in a continuous setting are∂ l δ(x) = cos πl L ∂ x + sin πl L ∂ y δ(x) F ←→ cos πl L jω x + sin πl L jω y , ∆ L δ(x) = L−1 l=0 ∂ 2 l δ(x) F ←→ − L−1 l=0 cos πl L ω x + sin πl L ω y 2 , 4with (ω x , ω y ) are continuous version of (ω 2 , ω 1 ) which is numerically used instead.Extending  #b44  #b45  #b46 , and due to the discrete nature of images, Thai and Gottschlich  #b28  defines the discrete directional G S -norm with S ∈ N + in the anisotropic version asv G S = inf g 1 = S−1 s=0 g s 1 , v = div − S g , g = g s S−1 s=0 ∈ X S .(1)As stated in  #b44  and [46, p. 87], the space of bounded variation BV is suitable for the piecewise smooth image component u, the G-space is suitable for the texture component v, and the dual Besov spaceḂ −1 ∞,∞ is suitable for the noise component , wherėB 1 1,1 ⊂ḂV ⊂ L 2 ⊂ G ⊂Ḃ −1 ∞,∞ .Since natural images are better described in terms of multi-scale and multi-direction, we apply the curvelet transform  #b9  #b8  #b11  instead of the wavelet transform in the dual Besov space, see  #b43  #b28 . Motivated by the Dantzig selector  #b47 , Aujol and Chambolle  #b45  and Thai and Gottschlich  #b28 , find that the residual is better captured by C{·} ∞ (bounded by a constant ν) in the curvelet domain, which can be represented by an indicator function on a feasible convex setG (ν) = ∈ X : C{ } ∞ ≤ ν as G * ν = 0 , ∈ G (ν) +∞ , else.(2)This measure controls the maximum curvelet coefficient of the residual . Due to the curvlet transform, no assumption on the kind of noise is needed (e.g. Gaussian, Laplacian or weakly correlated noise), see  #b42  #b41 . Following  #b48 , Thai and Gottschlich  #b43  proposes a threshold based on extreme value theory for  #b1 . Note that if is normally distributed, the random variable C{ } ∞ has the Gumbel distribution (since the curvelet of a Gaussian process is weakly correlated), see  #b48 . In general, for correlated noise, its distribution is a max-stable process  #b49  #b50 .It is known that the solution of 1 minimization is a shrinkage operator, see  #b35  #b36  #b51 . It can be defined in a matrix form asShrink(f , α) := f |f | · × max |f | − α , 0 ,with the point-wise operators. For example, a multiply pointwise operator offunctions f , d ∈ X is f · × d = f [k]d[k] k∈Ω .The discrete convolution of two functions f , g ∈ X is defined in a matrix form asf * d = (f * d)[k] k∈Ω and (f * d)[k] = n∈Ω f [n]d[k − n] .For simplicity, given t , r ∈ X L+1 , we denotet , r X = L l=0 t l · × r l ∈ X and f · × r = f · × r l L l=0 ∈ X L+1 .Given a curvelet transform C and its inverse version C *  #b9  and a function f , the curvelet shrinkage thresholding operator CST(· , ·) is defined asCST(f , ν) = C * Shrink C{f } , α .Note that one can easily apply other kinds of harmonic analysis than the curvelet, e.g., the shearlet  #b52 , steerable wavelet  #b14  #b53  #b54 , contourlet  #b13  #b55  and dual-tree complex wavelet  #b56 . Given a function f ∈ X, its time reversed functionf = f [k] k∈Ω ∈ X is defined ašf [k] = f [−k] F ←→ F (z −1 ) .For more mathematical background and notation, we refer the reader to  #b51  #b43  #b28 .

Directional mean curvature for image demixing (DMCD)
We assume that the original image f , which consists of piecewise smooth regions u, texture v and fine scale structure ρ, is blurred by the operator h and corrupted by noise asf [k] = h * (u + v + ρ) [k] + [k] , k ∈ Ω .Instead of the total variation norm  #b2 , and following the higher order approaches  #b57  #b30  #b5  #b21  #b58 , we propose a discrete directional mean curvature (DMC) norm to reconstruct the piecewise-smooth image u asκ d L u 1 = k∈Ω κ d L u [k] and κ d L {u} = div − L ∇ + L u 1 + ∇ + L u ·2with the matrix of ones 1 (size d 1 × d 2 ). According to  #b58 , a discrete DMC is rewritten asκ d L (u) = div − L ∇ + L u , 1 ∇ + L u , 1 .As in the DG3PD model  #b28 , and following the image generation mechanism shown in Fig. 1, we define the discrete DMCD model for image deconvolution and decomposition problem asmin (u,v,ρ, )∈X 4 κ d L u 1 + µ 1 v G S + µ 2 v 1 s.t. f = h * (u + v + ρ) + , C{ρ} ∞ ≤ ν ρ , C{ } ∞ ≤ ν .(3)Following  #b43  #b46  #b45  #b28  #b59  #b44  #b8 , the discrete directional G S -norm measures texture in several directions, and the dual of a generalized Besov space in the curvelet domain C captures the residual does not exist in general, but the reconstructed image is obtained by the proposed minimization (4). structure ρ and noise . Note that the ∞ -norm of the curvelet transform is a good measure for fine scale oscillating patterns (i.e., residual structure and noise), which can be either independent or "weakly" correlated and need not follow a Gaussian distribution. The G S -norm for texture is handled by the approach of Vese and Osher  #b46 . According to Meyer  #b44  (or [61]), the oscillating components do not have small L 2 or L 1 -norm.From the definition of the directional G S -norm (1) and the curvelet space for noise measurement (2), the constrained minimization (3) is rewritten asmin (u,v,ρ, , g)∈X 4+S κ d L u 1 + µ 1 S−1 s=0 g s 1 + µ 2 v 1 + G * ρ ν ρ + G * ν s.t. f = h * u + h * v + h * ρ + , v = div − S g .(4)Similar to  #b28  #b21  #b58 , in order to solve (4) we introduce five new variables d ,r = r l L l=0 , t = t l L l=0 , y = y l L l=0 , w = w s S−1 s=0 where                      | r| − y , r X = 0 , r = ∇ + L u , 1 ∈ X L+1 , d = div − L t ∈ X , t = y ∈ X L+1 , y = y l L l=0 ∈ R , w = g ∈ X S ,with the indicator function on its feasible convex setR * ( y) = 0 , y ∈ R +∞ , else , R = y ∈ X L+1 : y l [k] ≤ 1 , l = 0, . . . , L , k ∈ Ω .The augmented Lagrangian method (ALM) is applied to (4) by introducing Lagrange multipliersλ 1 ∈ X , λ 2 = λ 2l L l=0 ∈ X L+1 , λ 3 ∈ X , λ 4 = λ 4l L l=0 ∈ X L+1 , λ 5 ∈ X , λ 6 = λ 6s S−1 s=0 ∈ X S , λ 7 ∈ X and positive parameters β i 7 i=1 > 0 as min (u,v,ρ, ,d, r, t, y, w, g)∈X 5+3(L+1)+2S L(u, v, ρ, , d, r, t, y, w, g ; λ 1 , λ 2 , λ 3 , λ 4 , λ 5 , λ 6 , λ 7 )(5)and the Lagrange function isL(· ; ·) = d 1 + µ 1 S−1 s=0 w s 1 + µ 2 v 1 + G * ρ ν ρ + G * ν + R * ( y) + λ 1 + β 1 ,| r| − y , r X 2 + β 2 2 L−1 l=0 r l − ∂ + l u + λ 2l β 2 2 2 + β 2 2 r L − 1 + λ 2L β 2 2 2 + β 3 2 d − div − L t + λ 3 β 3 2 2 + β 4 2 t − y + λ 4 β 4 2 2 + β 5 2 f − h * u − h * v − h * ρ − + λ 5 β 5 2 2 + β 6 2 S−1 s=0 w s − g s + λ 6s β 6 2 2 + β 7 2 v − div − S g + λ 7 β 7 2 2 .Due to multi-variable minimization, the numerical solution of (5) is obtained by applying the alternating directional method of multipliers through iteration τ = 1, 2, . . . to findu (τ ) , v (τ ) , ρ (τ ) , (τ ) , d (τ ) , r (τ ) , t (τ ) , y (τ ) , w (τ ) , g (τ ) = argmin L u, v, ρ, , d, r, t, y, w, g ; λ (τ −1) 1 , λ (τ −1) 2 , λ (τ −1) 3 , λ (τ −1) 4 , λ (τ −1) 5 , λ (τ −1) 6 , λ (τ −1) 7 .Given the initialization asu (0) = f , v (0) = ρ (0) = (0) = d (0) = r (0) = t (0) = y (0) = w (0) = g (0) = λ (0) 1 = λ (0) 2 = λ (0) 3 = λ (0) 4 = λ (0) 5 = λ (0) 6 = λ(0)7 = 0, we solve the following ten subproblems and then update the seven Lagrange multipliers in each iteration.The "u-problem": Fix v, ρ, , d, r, t, y, w, g and then solvemin u∈X β 2 2 L−1 l=0 r l − ∂ + l u + λ 2l β 2 2 2 + β 5 2 f − h * u − h * v − h * ρ − + λ 5 β 5 2 2 .(6)Given k ∈ Ω, we denote the discrete Fourier transforms of h[k] , r l [k] , λ 2l [k] , f [k] , v[k] , ρ[k] , [k] and λ 5 [k] by H(z) , R l (z) , Λ 2l (z) , F (z) , V (z) , P (z), E(z) and Λ 5 (z), respectively. The minimizer of (6) is solved in the Fourier domain asu * = Re F −1 Y(z) X (z) , 8 with Y(z) = β 2 L−1 l=0 cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) R l (z) + Λ 2l (z) β 2 + β 5 H(z −1 ) F (z) − H(z)V (z) − H(z)P (z) − E(z) + Λ 5 (z) β 5 ,andX (z) = β 2 L−1 l=0 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 + β 5 H(z) 2 .The "v-problem": Fix u, ρ, , d, r, t, y, w, g and then solve min v∈Xµ 2 v 1 + β 5 2 h * v − f − h * u − h * ρ − + λ 5 β 5 2 2 + β 7 2 v − div − S g − λ 7 β 7 2 2 .(7)Let δ denote the matrix-valued Dirac delta function evaluated at k ∈ Ω. Then a solution of (7) isv (τ ) = Shrink t (τ ) v , µ 2 α (τ ) β 5 + α (τ ) β 7 , τ = 1, . . . with t (τ ) v = β 5 β 5 + α (τ ) β 7 δ − α (τ )ȟ * h * v (τ −1) + α (τ )ȟ * f − h * u − h * ρ − + λ 5 β 5 + β 7 α (τ ) β 5 + α (τ ) β 7 − S−1 s=0 cos πs S g s D 2 + sin πs S D T 1 g s =div − S g − λ 7 β 7 .Using the inverse cumulative distribution function to get the quantile corresponding to probability α, one can choose an adaptive µ 2 at each iteration τ asµ 2 = α µ2 t (τ ) v ∞ (β 5 + α (τ ) β 7 ) α (τ ) .The "d-problem": Fix u, v, ρ, , r, t, y, w, g and then solvemin d∈X d 1 + β 3 2 d − div − L t − λ 3 β 3 2 2 .(8)9The solution of the 1 minimization (8) is obtained by the shrinkage operator asd * = Shrink − L l=0 cos πl L t l D 2 + sin πl L D T 1 t l =div − L t − λ 3 β 3 , 1 β 3 .(9)The " r-problem": Fix u, v, ρ, , d, t, y, w, g and solve min r∈X L+1λ 1 + β 1 ,| r| − y , r X 2 + β 2 2 L−1 l=0 r l − ∂ + l u + λ 2l β 2 2 2 + β 2 2 r L − 1 + λ 2L β 2 2 2 .(10)The minimizer of problem (10) is obtained by the shrinkage operator asr * l =                Shrink cos πl L uD T 2 + sin πl L D 1 u =∂ + l u − λ 2l β 2 + λ 1 + β 1 β 2 · × y l , λ 1 + β 1 β 2 , l = 0, . . . , L − 1 Shrink 1 − λ 2L β 2 + λ 1 + β 1 β 2 · × y L , λ 1 + β 1 β 2 , l = L .The " t-problem": Fix u, v, ρ, , d, r, y, w, g and solvemin t∈X L+1 β 3 2 d − div − L t + λ 3 β 3 2 2 + β 4 2 t − y + λ 4 β 4 2 2 .(11)Similar to the "u-problem", we denote Y l (z) , Λ 4l (z) , D(z) , T l (z) and Λ 3 (z) as the discrete Fourier transforms of y l [k] , λ 4l [k] , d[k] , t l [k] and Λ 3 [k] with k ∈ Ω, respectively. The solution of this " tproblem"  #b10  for each separable problem l = 0, . . . , L is       t l = Re F −1 M l (z) N l (z) , l = 0, . . . , L − 1 , t L = y L − λ 4L β 4 , with N l (z) = β 4 + β 3 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 , M l (z) = β 4 Y l (z) − Λ 4l (z) β 4 − β 3 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) ×   D(z) + l =[0,L−1]\{l} cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) T l (z) + Λ 3 (z) β 3   .Note that D(z) is similar to the auto-correlation function in the Riesz basis [62].The " y-problem": Fix u, v, ρ, , d, r, t, w, g and solve min y∈X L+1R * ( y) + λ 1 + β 1 ,| r| − y , r X 2 + β 4 2 t − y + λ 4 β 4 2 2 .(12)Due to its separability, we consider the problem at l = 0, . . . , L − 1 and the solution of (12) isy * l =      y l , y ≤ 1 y l y , y > 1 , l = 0, . . . , L , and          y l = t l + λ 4l β4 + r l · × λ1+β1 β4 , y = L l=0 t l + λ 4l β4 + r l · × λ1+β1 β4 ·2 .The " w-problem": Fix u, v, ρ, , d, r, t, y, g and solvemin w∈X S µ 1 S−1 s=0 w s 1 + β 6 2 S−1 s=0 w s − g s + λ 6s β 6 2 2 .(13)Due to the 1 -minimization, a solution of (13) for each separable problem s = 0, . . . , S − 1 isw * s = Shrink g s − λ 6s β 6 , µ 1 β 6 .Note that µ 1 can be adaptively chosen as µ 1 = β 6 α µ1 g s − λ6sβ6 ∞ .The " g-problem": Fix u, v, ρ, , d, r, t, y, w and solvemin g∈X S β 6 2 S−1 s=0 w s − g s + λ 6s β 6 2 2 + β 7 2 v − div − S g + λ 7 β 7 2 2 .(14)Due to the higher order partial differential equation (in a discrete setting) of the Euler-Lagrange equation for  #b13 , the solution is obtained in the Fourier domain, which is similar to the "uproblem" and the " t-problem". We denote W s (z) , Λ 6s (z) , G s (z) and Λ 7 (z) as the discrete Fourier transform of w s [k] , λ 6s [k] , g s [k] and λ 7 [k] with k ∈ Ω, respectively. The solution of this " gproblem"  #b13  for each separable problem s = 0, . . . , S − 1 isG s (z) = B s (z) A s (z) , with A s (z) = β 6 + β 7 cos πs S (z 2 − 1) + sin πs S (z 1 − 1) 2 , B s (z) = β 6 W s (z) + Λ 6s (z) β 6 − β 7 cos πs S (z 2 − 1) + sin πs S (z 1 − 1) ×   V (z) + s =[0,S−1]\{s} cos πs S (z −1 2 − 1) + sin πs S (z −1 1 − 1) G s (z) + Λ 7 (z) β 7   .The "ρ-problem": Fix u, v, , d, r, t, y, w, g and solvemin ρ∈X G * ρ ν ρ + β 5 2 h * ρ − f − h * u − h * v − + λ 5 β 5 2 2 .(15)Minimization of (15) is approximated by the first-order Taylor expansion asmin ρ∈X G * ( ρ ν ρ ) + β 5 2α (τ ) ρ − δ − α (τ )ȟ * h * ρ (τ −1) + α (τ )ȟ * f − h * u − h * v − + λ 5 β 5 =ρ 2 2 .

Its solution is
ρ =ρ − CST ρ , ν ρ ,where ν ρ can be selected by the α-quantile as ν ρ = α ρ C{ρ} ∞ . Note that without component ρ in minimization  #b3 , it is difficult to separate texture v and some very fine scale structures in an original image because of a blurring kernel.The " -problem": Fix u, v, ρ, d, r, t, y, w, g and solvemin ∈X G * ( ν ) + β 5 2 − f − h * u − h * v − h * ρ + λ 5 β 5 =˜ 2 2 .(16)Similar to the "ρ-problem" (15) and given a possible choice ν = α C{˜ } ∞ , the solution of (16)is * =˜ − CST ˜ , ν .The final solution is found by iteratively updating the Lagrange multipliers λ 1 , λ 2 , λ 3 , λ 4 , λ 5 , λ 6 , λ 7 ∈ X 4+2(L+1)+S λ (τ )1 = λ (τ −1) 1 + β 1 | r| − y , r X , | r| = L l=0 r ·2 l , y , r X = L l=0 y l · × r l λ (τ ) 2l =    λ (τ −1) 2l + β 2 r l − cos πl L uD T 2 − sin πl L D 1 u , l = 0, . . . , L − 1 λ (τ −1) 2l + β 2 r l − 1 , l = L λ (τ ) 3 = λ (τ −1) 3 + β 3 d + L−1 l=0 cos πl L t l D 2 + sin πl L D T 1 t l λ (τ ) 4l = λ (τ −1) 4l + β 4 t l − y l , l = 0, . . . , L λ (τ ) 5 = λ (τ −1) 5 + β 5 f − h * u − h * v − h * ρ − λ (τ ) 6s = λ (τ −1) 6s + β 6 w s − g s , s = 0, . . . , S − 1 λ (τ ) 7 = λ (τ −1) 7 + β 7 v + S−1 s=0 cos πs S g s D 2 + sin πs S D T 1 g s =−div − S g .As in  #b28  #b43 , relative error on the log scale shows the convergence of the algorithm asErr v (τ ) = log v (τ ) − v (τ −1) 2 v (τ −1) 2 , τ = 1, . . . .(17)Note that the convergence can be performed by other criteria, see  #b21  #b58  #b33 . We use this criterion because the problem is convex in v, and because our method emphasizes texture recovery. Figure 2 illustrates a performance of our demixing model in terms of simultaneously decomposing and deblurring through directional mean curvature. This result will be clearly explained in later sections after we set up the link between the DMCD model and filter banks in harmonic analysis. The Algorithms 1-4 in Appendix C summarize a numerical solution of the DMCD model (3).

Variational Analysis and Filter Banks
In this section, we establish a deep connection between the proposed demixing model and multiscale harmonic analysis. Specifically, we analyze filter banks generated by the DMCD model at iteration τ in the ordering for the (u , v , ρ , )-problems according to the Algorithms 1-4 in Appendix C. These filter banks are similar to a wavelet-like operator [63,  #b14  #b54 . We then generalize the concept of filter banks in the u-problem and the g-problem to continuous and discrete multiscale sampling versions. The mathematical proofs are described in proposition 6.12-6.15 in Appendix A. To summarize the filter banks of these solutions, we refer the reader to Algorithms 5-7 in Appendix C and Figure 3, 13, 14 (for a version of sampling theory) and Figure 8 (for multiscale version). Although the concept of filter banks and scaling/wavelet functions are different in harmonic analysis, we shall combine these two concepts in this section.

The "u-problem"
Consider the sampling theory form for the u-problem (see proposition 6.12 in Appendix A). Given k ∈ Ω and taking β 2 = c 25 β 5 , β 3 = c 34 β 4 , a solution of the u-problem (6) at iteration τ can be rewritten in a form of sampling theory asu (τ ) [k] =   φ L,c25 * ȟ * f − h * v (τ −1) − h * ρ (τ −1) − (τ −1) + λ (τ −1) 5 β 5   [k] + L−1 l=0   ψ L,c25 l *   Shrink ψ L l * u (τ −1) − λ (τ −1) 2l β 2 + λ (τ −1) 1 + β 1 β 2 y (τ −1) l , λ (τ −1) 1 + β 1 β 2 + λ (τ −1) 2l β 2      [k](18)with three definitions.Definition the of y(τ −1) l [k]-component. At a direction l = 0, . . . , L − 1, we have y (τ −1) l [k] = Proj [−1,1] y (τ −1) [k] with y ( τ −1) [k] = y l (τ −1) [k] L−1 l=0 and y ( τ −1) l [k] =   ξ L,c34 l * y (τ −2) l − λ (τ −2) 4l β 4   [k] + λ (τ −2) 4l [k] β 4 +     θ L,c34 l *   Shrink   L−1 l =0θ L l * t (τ −2) l − λ (τ −2) 3 β 3 , 1 β 3   + λ (τ −2) 3 β 3 − l =[0,L−1]\{l}θ L l * t (τ −2) l        [k] + λ (τ −2) 1 [k] + β 1 β 4 Shrink ∂ + l u (τ −2) − λ (τ −2) 2l β 2 + λ (τ −2) 1 + β 1 β 2 y (τ −2) l , λ (τ −2) 1 + β 1 β 2 [k].(19)Note that due to the high order PDE behind directional mean curvature, u (τ ) in (18) is updated from u (τ −1) and u (τ −2) at every iteration τ and a thresholding value adaptively depends on Lagrange multiplier λ(τ −1) 1 and λ (τ −2) 1 .14 Definition of frames in  #b17  at direction l = 0, . . . , L − 1. To simply the notation, we use a parameter c > 0 instead of c 25 to define frames asφ L,c [k] F ←→ Φ L,c (z) = 1 H(z) 2 + c L−1 l =0 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 ,(20)ψ L,c l [k] = −c∂ − l φ L,c [k] F ←→Ψ L,c l (z −1 ) = c cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) c L−1 l =0 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 + H(z) 2 ,(21)ψ L l [k] = ∂ + l δ[k] F ←→ Ψ L l (z) = cos πl L (z 2 − 1) + sin πl L (z 1 − 1)(22)where δ[k] denotes the Dirac delta function evaluated at position k. Note that given the spectrum of an impulse response for a blur operator H(z) with H(e j0 ) 2 > 0 (usually H(e j0 = 1) due to its lowpass-like kernel), these bounded frames satisfy the unity condition:H(z) 2 Φ L,c (z) + L−1 l=0Ψ L,c l (z −1 )Ψ L l (z) = 1 , Φ L,c (e j0 ) = 1 H(e j0 ) 2 andΨ L,c l (e j0 ) = Ψ L l (e j0 ) = 0 .Definition of frames in  #b18  at direction l = 0, . . . , L − 1. We also simplify the notation by using c > 0 instead of c 34 , soξ L,c l [k] = 1 − c∂ − l ∂ + l −1 δ[k] F ←→ Ξ L,c l (z) = 1 1 + c cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 , (23) θ L,c l [k] = −c∂ + l ξ L,c l [k] F ←→ Θ L,c l (z) = −c cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 1 + c cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 , (24) θ L l [k] = ∂ − l δ[k] F ←→Θ L l (z −1 ) = − cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) .(25)Due to the splitting method for minimization in (4), there is no effect from a blur operator H(z) on these frames. Moreover, it is easy to see that these frames are bounded above from zero for ω ∈ [−π , π] 2 and also satisfy the unity condition in the Fourier domain asΞ L,c l (z) + Θ L,c l (z)Θ L l (z −1 ) = 1 ,(26)Ξ L,c l (e j0 ) = 1 and Θ L,c l (e j0 ) =Θ L l (e j0 ) = 0 .  Figure 14 illustrates a 3-dimensional version of Φ L,c (z) and Φ L,c (z) without the blurring effect; i.e., H(z) = 1. Now consider the multiscale sampling version of the u-problem (see proposition 6.14). Since a solution of the u-problem can be described in a form of the sampling theory in harmonic analysis, we generalize this form to its (continuous and discrete) multi-scale version (with some simplified notation as in the proof of proposition 6.14 in Appendix A).For the discrete case, given a function f ∈ 2 (R 2 ), a constant a > 0 and k ∈ Ω, the discrete multiscale sampling theory at scale I and direction L isf [k] = (f * φ)[k] + I−1 i=0 L−1 l=0 (f * ψ il * ψ il )[k] F ←→ F (z) = F (z)Φ(z) + I−1 i=0 L−1 l=0 F (z)Ψ il (z −1 )Ψ il (z) .(27)Their frames are defined in the Fourier domain (see Figure 8(b) for their spectra) asφ[k] F ←→ Φ(z) = I −1 I−1 i=0 Φ int (z a i ) , ψ il [k] F ←→ Ψ il (z) = I − 1 2 Ψ l (z a i ) , ψ il [k] F ←→Ψ il (z −1 ) = I − 1 2Ψ l (z −a i ) ,with the discrete version of the interpolant φ int (·) and the directional mother dual/primal wavelet ψ l (·) (with l = 0, . . . , L − 1) asφ int [k] = c(−∆ dL ) + 1 −1 δ[k] F ←→ Φ int (z) = 1 1 + c L−1 l =0 sin πl L (z 1 − 1) + cos πl L (z 2 − 1) 2 , ψ l [k] = −c c(−∆ dL ) + 1 −1 ∂ − l δ[k] = −c∂ − l φ int [k] F ←→Ψ l (z −1 ) = c sin πl L (z −1 1 − 1) + cos πl L (z −1 2 − 1) 1 + c L−1 l =0 sin πl L (z 1 − 1) + cos πl L (z 2 − 1) 2 , ψ l [k] = ∂ + l δ[k] F ←→ Ψ l (z) = sin πl L (z 1 − 1) + cos πl L (z 2 − 1) .Note that these discrete frames are bounded and also satisfy the unity condition, sinceΦ(z) + I−1 i=0 L−1 l=0Ψ il (z −1 )Ψ il (z) = 1 , Φ(e j0 ) = 1 , Ψ il (e j0 ) =Ψ il (e j0 ) = 0 .For the continuous case, given a constant a > 0, k ∈ Ω and u ∈ 2 (Ω) whose discrete Fourier transform is F (e jω ) or F (z), the multiscale sampling theory at scale I and direction L isf [k] = (f * φ)[k] + I−1 i=0 L−1 l=0 (f * ψ il * ψ il )[k] F ←→ F (e jω ) = F (e jω ) φ(ω) + I−1 i=0 L−1 l=0 F (e jω ) ψ * il (ω) ψ il (ω)(28)and their frames are defined in the Fourier domain with x ∈ R 2 (see Figure 8(a) for their spectra) asφ(x) = I −1 I−1 i=0 a −i φ int (a −1 x) F ←→ φ(ω) = I −1 I−1 i=0 φ int (a i ω) , ψ il (x) = I − 1 2 a −i ψ l (a −i x) F ←→ ψ il (ω) = I − 1 2 ψ l (a i ω) , ψ il (x) = I − 1 2 a −iψ l (a −i x) F ←→ ψ * il (ω) = I − 1 2 ψ * l (a i ω) ,with the interpolant φ int (·) in the continuous setting and, for l = 0, . . . , L − 1, the directional dual/primal waveletψ l (·) and ψ l (·), soφ int (x) = c(−∆ L ) + 1 −1 δ(x) F ←→ φ int (ω) = 1 1 + c L−1 l =0 cos( πl L )ω 2 + sin( πl L )ω 1 2 , ψ l (x) = −c∂ l φ int (x) F ←→ ψ * l (ω) = −c cos πl L jω 2 + sin πl L jω 1 1 + c L−1 l =0 cos πl L ω 2 + sin πl L ω 1 2 , ψ l (x) = ∂ l δ(x) F ←→ ψ l (ω) = cos πl L jω 2 + sin πl L jω 1 .Similar to its discrete version, these bounded frames in a continuous setting also satisfy the unity condition asφ(ω) + I−1 i=0 L−1 l=0 ψ * il (ω) ψ il (ω) = 1 , φ(0) = 1 , ψ il (0) = ψ il (0) = 0 .Also note that, unlike the continuous filter banks in Figure 8(a), there are aliasing effects in the discrete version in Figure 8(b) because of the exponential operator in the complex domain; i.e., z = e jω .

The "v-problem"
The solution of the v-problem (7) is rewritten in a sampling theory form with two shrinkage operators (due to v 1 and g s 1 in the DMCD model (4)), the Lagrange multipliers (λ 5 , λ 6 , λ 7 ) and the blur kernel h asv (τ ) = Shrink t (τ ) v , µ 2 α (τ ) β 5 + α (τ ) β 7(29)with t(τ ) v = t (τ ) v [k] k∈Ω and t (τ ) v [k] = β 5 β 5 + α (τ ) β 7   δ − α (τ )ȟ * h * v (τ −1) + α (τ )ȟ * f − h * u (τ ) − h * ρ (τ −1) − (τ −1) + λ (τ −1) 5 β 5   [k] + β 7 α (τ ) β 5 + α (τ ) β 7   S−1 s=0θ S s * g (τ ) s − λ (τ −1) 7 β 7   [k].By choosing β 7 = c 67 β 6 , the solution of the g-problem (14) at iteration τ isg (τ ) s [k] =   ξ S,c67 s *   Shrink g (τ −1) s − λ (τ −1) 6s β 6 , µ 1 β 6 + λ (τ −1) 6s β 6      [k] +   θ S,c67 s * θS s * g (τ −1) s + λ (τ −1) 7 β 7   [k] .(30)Note that the sampling theory form for (30) is more obvious if we simplify the equation by removing the shrinkage operator and the Lagrange multipliers, sog (τ ) s [k] = ξ S,c67 s * g (τ −1) s [k] + θ S,c67 s * θ S s * g (τ −1) s [k] .Frames ξ S,c67 s (·) , θ S,c67 s (·) andθ S s (·) are well defined in (23)-(26), see proposition 6.13 in Appendix A.The multiscale sampling version of the g-problem is similar to the u-problem; see proposition 6.15 in Appendix A. Given a discrete function (data) f ∈ X, constant a > 0 and k ∈ Ω, the discrete multiscale sampling theory at scale I and direction L isf [k] = (f * ξ)[k] + I−1 i=0 S−1 s=0 (f * θ si * θ si )[k] F ←→ F (z) = F (z)Ξ(z) + I−1 i=0 S−1 s=0 F (z)Θ si (z −1 )Θ si (z)(31)and their frames are defined in the Fourier domain as (23)-  #b24 . These multiscale frames also satisfy the unity condition in the Fourier domain asξ[k] F ←→ Ξ(z) = 1 SI I−1 i=0 S−1 s=0 Ξ s (z a i ) , θ si [k] F ←→ Θ si (z) = 1 √ SI Θ s (z a i ) andθ si [k] F ←→Θ si (z) = 1 √ SIΘ s (z a i ) (see Figure 8(d) for their spectra). The Ξ s (z) = Ξ S,c67 s (z) , Θ s (z) = Θ S,c67 s (z) andΘ s (z) =Θ S s (z) are defined inΞ(z) + I−1 i=0 S−1 s=0Θ si (z −1 )Θ si (z) = 1 , Ξ(e j0 ) = 1 and Θ si (e j0 ) =Θ si (e j0 ) = 0 .Given a constant a > 0 , k ∈ Ω and a discrete function f [k] F ←→ F (e jω ), the continuous multiscale sampling theory form at scale I and direction L isf [k] = (f * ξ)[k] + I−1 i=0 S−1 s=0 (f * θ si * θ si )[k] F ←→ F (e jω ) = F (e jω ) ξ(ω) + I−1 i=0 S−1 s=0 F (e jω ) θ * si (ω) θ si (ω)(32)withξ(x) = 1 SI I−1 i=0 S−1 s=0 a −i ξ s (a −i x) F ←→ ξ(ω) = 1 SI I−1 i=0 S−1 s=0 ξ s (a i ω) θ si (x) = 1 √ SI a −i θ s (a −i x) F ←→ θ si (ω) = 1 √ SI θ s (a i ω) θ si (x) = 1 √ SI a −iθ s (a −i x) F ←→ θ si (ω) = 1 √ SI θ s (a i ω)and framesξ s (x) = 1 − c∂ 2 s −1 δ(x) F ←→ ξ s (ω) = 1 1 + c cos πs S ω 2 + sin πs S ω 1 2 , θ s (x) = −c∂ s ξ s (x) F ←→ θ s (ω) = −c cos( πs S )jω 2 + sin( πs S )jω 1 1 + c cos πs S ω 2 + sin πs S ω 1 2 , θ s (x) = ∂ s δ(x) F ←→ θ * s (ω) = cos πs S jω 2 + sin πs S jω 1(see Figure 8(d) for their spectra). These bounded frames satisfy the unity condition in the Fourier domain since ξ(ω) + I−1 i=0 S−1 s=0 θ * si (ω) θ si (ω) = 1 , ξ(0) = 1 , θ si (0) = θ si (0) = 0 .

The "ρ and -problems"
Multi-scale and multi-direction analysis in the curvelet domain is especially useful for demixing the noise and residuals. We minimize (4) in terms of the supremum norms of the curvlet coefficients corresponding to the indicator functions for the residual structure and noise terms, G * ( ρ νρ ) and G * ( ν ), respectively.There are two terms in every solution of the ρ-problem  #b14  or the -problems  #b15 , namely an updated remainder and its curvelet smoothing term CST [· , ·] determined by the soft-thresholding operator. We describe the case of the -problem, but the explanation is the same for the ρ-problem. A solution of (16) at iteration τ is(τ ) =˜ (τ ) − CST ˜ (τ ) , ν(33)20 where˜ (τ ) , the remainder at iteration τ , can be approximated as(τ ) = f − h * u (τ ) − h * v (τ ) − h * ρ (τ ) + λ (τ −1) 5 β 5 ≈ (τ ) .In (33), we call˜ (τ ) ≈ (τ ) the updated term and CST ˜ (τ ) , ν the smoothing term found by curvelet soft-thresholding CST [· , ·] (up to a level ν ). If, at iteration τ , the updated term (τ ) still contains both signal and noise, then the noise in˜ (τ ) is removed by the CST-operator. Finally, by subtraction, (τ ) contains almost pure noise (up to a degree ν ). This "smoothing and subtraction" procedure results from the constraint C{ } ∞ ≤ ν , which takes advantage of the sparsity assumption in the curvelet transform; i.e., the multi-scale and multi-directional partition of the Fourier domain.

Experimental Results and Comparison
To quantify the improvement obtained by the proposed algorithm, we perform our demixing algorithm upon two images: the "Barbara" image Figure 2(a), and a fingerprint image used in the SAMSI program on forensic statistics. We use Barbara image because it contains many key apects of image analysis, i.e., homogeneous areas and texture at different scales (on the scarf, trouser and tablecloth). We compare our proposed algorithm to the TV-2 deblurring algorithm [64] and to the blind deblurring by the Matlab function deconvblind.m. The criterion for comparing peformance is the mean squared error (MSE) in the pixel-wise difference between the true image f 0 and a combination of the signal components obtained from the demixing algorithms f re = u + v + ρ:MSE = f 0 − f re 2 2 d 1 d 2 .In order to evaluate the performance of the algorithms, besides simple visual comparison and the calculation of the mean square error, we also use the eigenvalues of the estimated covariance matrix under the assumption that any structure which persists after demixing appears in the reconstruction error and the eigenvalues of the estimated covariance matrix enable one to determine which algorithm successfully extracted more signal. In the same vein, we denote {e l } l≥1 as a vectorized sample of 10 × 10 non-overlapping blocks of the error (f 0 − f re ), i.e. e l ∈ R 100 . The maximum eigenvalue of the estimated covariance matrix (MEC) for sample {e l } l≥1 is defined asMEC = max s=1,...,100 eig {Σ} [s]with sample mean and sample covariance, respectively, as F ←→ H(z) in (c)) is simultaneously reconstructed and decomposed into the piecewise smooth image u (d), the sparse texture v (e) and its binarized version (j) and the residual structure ρ (f). The convergence of the algorithm is measured by the relative error of texture v in log scale (k). It shows that the proposed DMCD model can reconstruct a blurred image and simultaneously decompose it into different components, including sparse texture v and piecewise smooth u with sharp edges, while preserving contrast. So, the reconstructed image with f re = u + v + ρ in Figure  2(h) can preserve contrast and texture with small mean squared error in comparison with the original image Figure 2  . This artifact is due to the large bandwidth of a lowpass Φ(z) which covers texture information, see (m). For the homogeneous areas, a comparison of a reconstructed image between L = 2 and L = 10 directions is depicted in (e-l). We see that the reconstructed components u by DMC (h,l) are smoother in approximating the function f (f, j), and MC produces the "staircase" effect, see (u, k). An explanation for this benefit of DMC is that increasing L makes the bandwidth of the lowpass Φ(z) smaller (see (m) and (o) for L = 2 and L = 10, respectively) while small wavelet coefficients are eliminated in different directions, see equation  #b17 . This effect makes a cartoon u smoother and removes oscillating patterns, such as texture and noise. The highpass Ψ(z) are depicted in (n) and (p) for L = 2 and L = 10, respectively. The stair case effect is due to the assumption of sparse signal under the gradient operator, and the directional version of the total variation norm  #b28  #b42  #b41  is known to handle this limitation. The proposed directional mean curvature norm benefits from the advantages of both the high-order PDE approach and the ability of directional methods to enhance sparse signal while preserve sharp edges in the restored image. Figure 5 compares our demixing method to TV-2 deblurring and the blind deblurring by the Matlab function deconvblind.m. We see that TV-2 (c, f) can recover very sharp edges, but it also eliminates texture. The blind deblurring can recover texture, but it also produces "ringing" effects (i.e., the larger a kernel size is, the more artifacts there are in the reconstructed image), see (b, e). We observed that a kernel of size 7 is the best choice in terms of minimizing ringing. The Matlab function can directly estimate an unknown blur kernel, which our method does not, but it cannot decompose an image into different components while deblurring, and its performance on deblurring still has problematic ringing.e = 1 # {e l } l≥1 l≥1 e l and Σ = 1 # {e l } l≥1 l≥1 (e l −ē)(e l −ē) T .Besides the Barbara image, we also demix a fingerprint image which contains small scale objects (noise) together with fingerprint patterns (texture), see Figure 6. DMC removes the texture component in the piecewise smooth component u (d) while preserving sharp edges, and the texture component v (e, j) is sparse. Also, small scale structure is separated in ρ. Finally, the reconstructed by DMCD (h) achieves good performance in terms of mean squared error and visualization. Figure 7 illustrates the performance of our method when signal is corrupted by noise. We add an i.i.d. Gaussian noise N (0 , σ 2 ) with σ = 10 to a blurred signal (b). By choosing a threshold ν = 6.5, the noise component can be separated by the DMCD model-see its QQ-plot (d). And a reconstructed image (i) still preserves texture. Note that mathematically selecting an optimal threshold for this Gaussian noise is beyond this paper. We use the QQ-plot to evaluate this threshold instead. The DMCD model for other textured images are depicted in Figure 10 

Conclusion
We provide the DMCD method to demix a blurred image f (with a known blurred kernel h) into four meaningful components: piecewise smooth u, texture v, fine scale residual structure ρ, and noise , so f = h * (u + v + ρ) + .A cornerstone of the DMCD analysis is the assumption that signal is sparse under some transformed domains. Using novel norms as key ingredients, we address some transformed domains to enforce on these components:• The directional mean curvature (DMC) norm eliminates texture from a piecewise smooth u while keeping edges and preserving its contrast. This property is due to the multi-directional and high-order approach which enhances sparsity of the objects under DMC.• The directional G S -norm is applied to capture texture v and the 1 -norm v 1 obtains sparse coefficients which are mainly due to repeated pattern.• The fine scale residual structure ρ and noise are measured in the ∞ -norm of curvelet coefficients C{ } ∞ . Since this ∞ -norm takes the advantage of the multi-scale and multidirectional curvelet transform, oscillating components can be independent (e.g., white noise ) or weakly correlated (fine scale residual structure ρ).We also apply our DMCD model to real images to find superior results compared to other state-ofthe-art methods, as measured by mean squared error and the maximum eigenvalue of the estimated covariance matrix. Moreover, DMCD simultaneously solves the decomposition and deblurring problems. Finally, we uncover a link between functional analysis and multiscale sampling theory, e.g., between harmonic analysis and filter banks.Due to high-order PDE problem, following  #b58 , an augmented Lagrangian method is applied to split DMC into several 1 -and 2 -norms. The advantage of this splitting method is to approximate complicated norms, but it also introduces new parameters which are chosen beforehand to speed of the convergence of the algorithm. [64] P. Getreuer. Total variation deconvolution using split bregman. Image Processing On Line,    The third row shows sample covariance matrices of 10 × 10 non-overlapping blocks of the errors on the second row. We see that TV-2 can recover very sharp edges, but it also eliminates texture. The blind deblurring (with Matlab function "deconvblind.m") can recover texture, but it also produces "ringing effects" (the larger a kernel size is, the more artifacts on its reconstructed image are). We observed that a size of kernel 7 as its recommendation is the best choice in terms of "ringing effects" on its reconstructed image by visualization (although this Matlab function can estimate an unknown blur kernel itself which is more advantage than us, it cannot do demixing to decompose an image into different components while deblurring).   and direction L = 4 in continuous and discrete settings from the "u-problem" (a, b) and the " g-problem" (c, d) with a = 2 and c = 1. The mathematical calculations for these frames are explained in proposition 6.14 and 6.15 in Appendix A. Note that the aliasing effect in a discrete setting (b) and (d) is due to the exponential operator of the discrete Fourier transform: z = e jω . Their scaling and wavelet coefficients of a fingerprint image are depicted in Figure 9, respectively. Figure 9: Visualization of scaling, wavelet coefficients and its synthesis image by multiscale projection obtained from the "u-problem" and the " g-problem" in continuous and discrete setting (a)-(d). For visualization, the scaling and wavelet coefficients are displayed in log scale. Their corresponding wavelet and scaling functions are illustrated in Figure 8 .

36


Appendix A. Propositions and Proofs
Proposition 6.1. The solution of the " t-problem" ist * = argmin t∈X L+1    F ( t) = β 3 2 d − div − L t + λ 3 β 3 2 2 + β 4 2 t − y + λ 4 β 4 2 2    =          Re F −1 M l (z) N l (z) , l = 0, . . . , L − 1 , y L − λ 4L β 4 . with N l (z) = β 4 + β 3 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 , M l (z) = β 4 Y l (z) − Λ 4l (z) β 4 − β 3 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) ×   D(z) + l =[0,L−1]\{l} cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) T l (z) + Λ 3 (z) β 3   .Proof. The Euler-Lagrange equation is0 = ∂F ( t) ∂ t = −β 3 ∂{div − L t} ∂ t (div − L ) * ∂ t ∂ t =−∇ + L δ d − div − L t + λ 3 β 3 + β 4 t − y + λ 4 β 4 = β 3 ∇ + L d − div − L t + λ 3 β 3 + β 4 t − y + λ 4 β 4 we have ∇ + L = ∂ + l L−1 l=0 , div − L t = L−1 l=0 ∂ − l t l , t = [t l ] L l=0 , y = [ y l ] L l=0 , λ 4 = [λ 4l ] L l=0 Thus,            β 3 ∂ + l   d − L−1 l =0 ∂ − l t l + λ 3 β 3   + β 4 t l − y l + λ 4a β 4 = 0 , l = 0, . . . , L − 1. β 4 t L − y L + λ 4L β 4 = 0 ⇔            β 4 − β 3 ∂ + l ∂ − l Id t l = −β 3 ∂ + l   d − l =[0,L−1]\{l} ∂ − l t l + λ 3 β 3   − β 4 −y l + λ 4l β 4 , l = 0, . . . , L − 1. (a) t L = y L − λ 4L β 4 (b)The Fourier transform of equation (a) ist l = β 4 − β 3 ∂ + l ∂ − l Id −1   −β3∂ + l   d − l =[0,L−1]\{l} ∂ − l t l + λ 3 β 3   − β 4 −y l + λ 4l β 4    F ←→ T l (z) := M l (z) N l (z) =   β 4 + β 3 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2   −1 × − β 4 −Y l (z) + Λ 4l (z) β 4 − β 3 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) ×   D(z) + l =[0,L−1]\{l} cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) T l (z) + Λ 3 (z) β 3   .Proposition 6.2. The solution of the " r = r l L−1 l=0 -problem" isr * = argmin r∈X L+1    F ( r) = λ 1 + β 1 ,| r| − y , r X 2 + β 2 2 L−1 l=0 r l − ∂ + l u + λ 2l β 2 2 2 + β 2 2 r L − 1 + λ 2L β 2 2 2    =        Shrink ∂ + l u − λ 2l β 2 + λ 1 + β 1 β 2 · × y l , λ 1 + β 1 β 2 , l = 0, . . . , L − 1 , Shrink 1 − λ 2L β 2 + λ 1 + β 1 β 2 · × y L , λ 1 + β 1 β 2 , l = L .Proof.Remark 6.3. Given f , d , u ∈ X and t = [t l ] L l=0 ∈ X L+1 , we have t 2 2 = L l=0 t l 2 2 , f , L l=0 t l 2 = L l=0 f , t l 2 and f , d · × u 2 = d · × f , u 2 .The proofs for this remark aret 2 2 = L l=0 k∈Ω t 2 l [k] = t l 2 2 , f , L l=0 t l 2 = k∈Ω f [k]   L l=0 t l [k]   = L l=0 k∈Ω f [k]t l [k] = f ,t l 2 and f , d · × u 2 = k∈Ω f [k] d · × u [k] = k∈Ω f [k]d[k] =(f · × d)[k] u[k] .Let A = ∇ + L u , 1 ∈ X L , the objective function is rewritten asF ( r) = λ 1 + β 1 ,| r| 2 + β 2 2 L l=0 r l − A l + λ 2l β 2 2 2 − λ 1 + β 1 , L l=0 y l · × r l 2 = λ 1 + β 1 ,| r| 2 + β 2 2 L l=0 r l − A l + λ 2l β 2 2 2 − 2 λ 1 + β 1 β 2 · × y l , r l 2 = λ 1 + β 1 ,| r| 2 + β 2 2 L l=0 r l 2 2 + 2 r l , −A l + λ 2l β 2 − λ 1 + β 1 β 2 · × y l 2 + −A l + λ 2l β 2 − λ 1 + β 1 β 2 · × y l 2 2 + −A l + λ 2l β 2 2 2 − −A l + λ 2l β 2 − λ 1 + β 1 β 2 · × y l 2 2 = λ 1 + β 1 ,| r| 2 + β 2 2 r − A + λ 2 β 2 − λ 1 + β 1 β 2 · × y 2 2 + β 2 2   − A + λ 2 β 2 2 2 − − A + λ 2l β 2 − λ 1 + β 1 β 2 · × y 2 2   = constantUnder a minimization, a constant term can be removed and the " r-problem" is rewritten asmin r∈X L+1    F ( r) = k∈Ω   (λ 1 [k] + β 1 ) r[k] + β 2 2 r[k] − A[k] + λ 2 [k] β 2 − λ 1 [k] + β 1 β 2 y[k] 2     Due to separability, consider at k = m ∈ Ω, we haver * [m] = argmin r[m]∈R L+1    F ( r[m]) = r[m] + 1 2 β 2 λ 1 [m] + β 1 r[m] − A[m] + λ 2 [m] β 2 − λ 1 [m] + β 1 β 2 y[m] 2    = Shrink A[m] − λ 2 [m] β 2 + λ 1 [m] + β 1 β 2 y[m] , λ 1 [m] + β 1 β 2 .We haver = [r l ] L l=0 ∈ X L+1 , A = ∂ + l u L−1 l=0 , 1 ∈ X L+1 , y = [y l ] L l=0 ∈ X L+1 , λ 1 ∈ X , λ 2 = [λ 2l ] L l=0 ∈ X L+1 .The matrix form isr * = Shrink A − λ 2 β 2 + λ 1 + β 1 β 2 · × y , λ 1 + β 1 β 2 =        Shrink ∂ + l u − λ 2l β 2 + λ 1 + β 1 β 2 · × y l , λ 1 + β 1 β 2 , l = 0, . . . , L − 1 , Shrink 1 − λ 2L β 2 + λ 1 + β 1 β 2 · × y L , λ 1 + β 1 β 2 , l = L .Lemma 6.4. Given y ∈ X L+1 and µ > 0, a solution of 1 -minimization (a primal problem) in a matrix form isy * p = argmin yp∈X L+1 F ( y p ) := µ y p 1 + 1 2 y p − y 2 2(34)= y y · × max y − µ , 0 := Shrink y , µ ,with y = L l=0 y l ·2 . Denote y d as a dual variable of y p , the Legendre-Fenchel transform of J ( y p ) = µ y p 1 on a convex set isR * y d µ = 0 , y d ∈ R(µ) +∞ , else , R(µ) =      y d ∈ X L+1 : y d ∞ := max k∈Ω L l=0 y 2 dl [k] ≤ µ      .(36)

40
A dual problem of (34) isy * d = argmin y d ∈X L+1 R * y d µ + 1 2 y d − y 2 2 = Proj R(µ) y (37) =      y , y ≤ µ µ y y , else = µ y max µ , y (by 1 -projection) (38) = y − Shrink y , µ = y * p (by primal-dual relation) .(39)Proof. Proof for a solution of 1 -minimization (34) with a vector-valued form:To be self-contained, a proof of (35) is provided for vector-valued data from  #b7  and [65]. An objective function in (34) is rewritten asF ( y p ) = k∈Ω   µ L l=0 y 2 pl [k] + 1 2 L l=0 y pl [k] − y l [k] 2    .The optimal condition of F ( y p ) at k ∈ Ω and l ∈ {0, . . . , L} is If y p = 0, then we have0 = ∂F ( y p ) ∂y pl [k ] = µ y pl [k ] y p [k ] + y pl [k ] − y l [k ] ,y [k ] = µ y p [k ] + 1 y p [k ] ⇔ y [k ] = µ y p [k ] + 1 y p [k ] = µ + y p [k ] .Thus, we havey p [k ] = y [k ] y [k ] y [k ] − µ .Finally, we have a solution of (34) with k ∈ Ω asy * p [k ] =        0 , y [k ] ≤ µ y [k ] y [k ] y [k ] − µ , else = y [k ] y [k ] max y [k ] − µ , 0 ,or its matrix form isy * p = y y · × max y − µ , 0 with y = L l=0 y l ·2 .Proof for a Lengendre-Fenchel transform of J ( y p ) = µ y p 1 in (36): The epigraph of a convex function J ( y p ) is a convex set asepi(J ) = y p , ν ∈ X L+1 × R : J ( y p ) ≤ ν , ∀ y p . A hyperplane (defined by ( y d , ν) ∈ X L+1 ×R) lying below J (·), i.e. y d , y p 2 −ν ≤ J ( y p ) (∀ y p ∈ X L+1 ), results in the Legendre-Fenchel transform of J ( y p ) as a convex functionJ * ( y d ) = sup yp∈X L+1 y d , y p 2 − J ( y p ) = µ sup yp∈X L+1 H ( y p ) := y d µ , y p 2 − y p 1 := µR * y d µ ≤ ν .(40)The optimal condition of its objective functionH ( y p ) = k∈Ω    1 µ L l=0 y dl [k]y pl [k] − L l=0 y 2 pl [k]   at k ∈ Ω and l ∈ {0, . . . , L} is0 = ∂H ( y p ) ∂y pl [k ] = 1 µ y dl [k ] − y pl [k ] y p [k ] ⇔ y dl [k ] = µ y pl [k ] y p [k ] , with y p [k ] = L l=0 y 2 pl [k ] .If y p = 0, then by sub-differential we choose c[k ] = yp[k ] | yp[k ]| ∈ R L+1 such that c[k ] ≤ 1 , so H ( y p ) = 0 and y d [k ] = µ c[k ] with y d [k ] = µ c[k ] ≤ µ , ∀k ∈ Ω which is equivalent to y d ∞ := max k ∈Ω y d [k ] ≤ µ and y d [k ] = L l=0 y 2 dl [k ] .Thus,R * y d µ = 0 with y d ∞ ≤ µ .If y p = 0, we haveH ( y p ) = k∈Ω    1 µ L l=0 y dl [k]y pl [k] − L l=0 y 2 pl [k]    ≥ k∈Ω L l=0 1 µ y dl [k]y pl [k] − y pl [k] =              k∈Ω L l=0 1 µ y dl [k] − 1 y pl [k] , y pl [k] ≥ 0 , ∀l , k , k∈Ω L l=0 1 µ y dl [k] + 1 y pl [k] , y pl [k] < 0 , ∀l , k .We observe that given y d [k] > µ, sup yp∈X L+1 H ( y p ) = +∞ when y p → ±∞. Thus,R * y d µ = +∞ with y d [k] > µ , ∀k ∈ Ω .So, we have the Legendre-Fenchel transform of J ( y p ) = µ y p 1 on a convex set isJ * ( y d ) = R * y d µ = 0 , y d ∈ R(µ) +∞ , else , R(µ) =      y d ∈ X L+1 : y d ∞ := max k∈Ω L l=0 y 2 dl [k] ≤ µ      .Proof for a solution of dual problem of (34), Eq. (37)-(39):We define epigraph of the Legendre-Fenchel transform J * ( y d ) in (40) as a set of hyperplane lying below a convex function J ( y p ) asepi(J * ) = ( y d , ν) ∈ X L+1 × R : J * ( y d ) ≤ ν , ∀ y d .By bi-conjugate of a convex function, we haveJ * * ( y p ) = sup y d ∈X L+1 y d , y p 2 − J * ( y d ) = J ( y p ) ⇔ J * ( y d ) + J ( y p ) ≥ y d , y p 2 , ∀ y d , y p ∈ X L+1 × X L+1 .

43
The equality happens, i.e. J * ( y d ) + J ( y p ) = y d , y p 2 , if y p is the sub-differential of J * ( y d ) or y d is the sub-differential of J ( y p ) as∂J ( y p ) ∂ y p = y d ⇔ ∂J * ( y d ) ∂ y d = y p .(41)Equivalently, the objective function in (34) is rewritten asF ( y p ) = J ( y p ) + 1 2 y p − y 2 2 = y d , y p 2 − J * ( y d ) + 1 2 y p − y 2 2 .Since Legendre-Fenchel transform of J ( y p ) is J * ( y d ) = R * y d µ , we build a dual relation of a non-smooth minimization (34) by considering the optimal condition of an objective function F (·) at ( y * p , y * d ) with the sub-differential  #b45 66] asy = y * d + y * p and 0 ∈ y * p − ∂J * ( y d ) ∂ y d y d = y * d Thus, we have 0 ∈ ∂J * ( y d ) ∂ y d y d = y * d + y * d − y ,where y * d is a solution of a dual problem of (34) by a primal/dual relation asy * d = argmin y d ∈X L+1 R * y d µ + 1 2 y d − y 2 2(42)= y − Shrink y , µ .Note that a convex set R(µ) is equivalent to 1 -ball asR(µ) = y d ∈ X L+1 : y d ∞ := max k∈Ω y d [k] ≤ µ ⇔ y d [k] ≤ µ , ∀k ∈ Ω .Thus, a solution of a dual problem (42) is a projection of y onto an 1 -ball R(µ) asy * d = argmin y d ∈R(µ) y d − y 2 2 = Proj R(µ) y =      y , y ≤ µ µ y y , else = µ y max µ , y .Proposition 6.5. The solution of the " y = [y l ]L−1 l=0 -problem" is y * = argmin y∈X L+1    F ( y) = R * ( y) + λ 1 + β 1 ,| r| − y , r X 2 + β 4 2 L l=0 t l − y l + λ 4l β 4 2 2    =      y l , y ≤ 1 y l y , y > 1 , l = 0, . . . , L and          y l = t l + λ 4l β4 + r l · × λ1+β1 β4 , y = L l=0 t l + λ 4l β4 + r l · × λ1+β1 β4 ·2 .Proof. Under a minimization, the objective function is rewritten asF ( y) = R * ( y) − L l=0 λ 1 + β 1 , y l · × r l 2 + β 4 2 L l=0 y l − t l + λ 4l β 4 2 2 = R * ( y) + β 4 2 L l=0 y l 2 2 − 2 y l , t l + λ 4l β 4 + r l · × λ 1 + β 1 β 4 2 + t l + λ 4l β 4 + r l · × λ 1 + β 1 β 4 2 2 − β 4 2 L l=0 t l + λ 4l β 4 + r l · × λ 1 + β 1 β 4 2 2 + β 4 2 L l=0 t l + λ 4l β 4 2 2 = R * ( y) + β 4 2 y − t + λ 4 β 4 + r · × λ 1 + β 1 β 4 2 2 + β 4 2   t + λ 4 β 4 2 2 − t + λ 4 β 4 + r · × λ 1 + β 1 β 4 2 2   .According to Lemma 6.4, a solution of a minimization of the " y-problem" isy * = argmin y∈X L+1            R * ( y) + β 4 2 y − t + λ 4 β 4 + r · × λ 1 + β 1 β 4 = y 2 2            =      y , y ≤ 1 y y , else with y = y l L l=0 ∈ X L+1 , y l = t l + λ 4l β 4 + r l · × λ 1 + β 1 β 4 and y = L l=0 t l + λ 4l β 4 + r l · × λ 1 + β 1 β 4 ·2 .Proposition 6.6. The solution of the "u-problem" is as followsu * = min u∈X    F (u) := β 2 2 L−1 l=0 r l − ∂ + l u + λ 2l β 2 2 2 + β 5 2 f − h * u − h * v − h * ρ − + λ 5 β 5 2 2    = Re F −1 Y(z) X (z) . with Y(z) = β 2 L−1 l=0 cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) R l (z) + Λ 2l (z) β 2 + β 5 H(z −1 ) F (z) − H(z)V (z) − H(z)P (z) − E(z) + Λ 5 (z) β 5 , X (z) = β 2 L−1 l=0 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 + β 5 H(z) 2 .Proof. The Euler-Lagrange equation is0 = ∂F (u) ∂u = −β 2 L−1 l=0 ∂ ∂ + l u ∂u =(∂ + l ) * δ[k]=−∂ − l δ[k] r l − ∂ + l u + λ 2l β 2 − β 5ȟ * f − h * u − h * v − h * ρ − + λ 5 β 5 ⇔   −β 2 L−1 l=0 ∂ − l ∂ + l δ + β 5ȟ * h   * u = −β 2 L−1 l=0 ∂ − l r l + λ 2l β 2 + β 5ȟ * f − h * v − h * ρ − + λ 5 β 5 F ←→   β 2 L−1 l=0 cos( πl L )(z 2 − 1) + sin( πl L )(z 1 − 1) 2 + β 5 H(z) 2   U (z) = β 2 L−1 l=0 cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) R l (z) + Λ 2l (z) β 2 + β 5 H(z −1 ) F (z) − H(z)V (z) − H(z)P (z) − E(z) + Λ 5 (z) β 5By applying the Fourier transform, a solution of the "u-problem" isU (z) := Y(z) X (z) =   β 2 L−1 l=0 cos( πl L )(z 2 − 1) + sin( πl L )(z 1 − 1) 2 + β 5 H(z) 2   −1 β 2 L−1 l=0 cos( πl L )(z −1 2 − 1) + sin( πl L )(z −1 1 − 1) R l (z) + Λ 2l (z) β 2 + β 5 H(z −1 ) F (z) − H(z)V (z) − H(z)P (z) − E(z) + Λ 5 (z) β 5 .Proposition 6.7. The sum of two 2 functions is rewritten asF (v) := β 1 v − f 1 2 2 + β 2 v − f 2 2 2 = (β 1 + β 2 ) v − β 1 β 1 + β 2 f 1 − β 2 β 1 + β 2 f 2 2 2 + c f1f2Proof.F (v) = (β 1 + β 2 ) v 2 2 − 2 v , β 1 f 1 + β 2 f 2 2 + β 1 f 1 2 2 + β 2 f 2 2 2 = (β 1 + β 2 ) v − β 1 β 1 + β 2 f 1 − β 2 β 1 + β 2 f 2 2 2 + (β 1 + β 2 ) − β 1 β 1 + β 2 f 1 + β 2 β 1 + β 2 f 2 2 2 + β 1 β 1 + β 2 f 1 2 2 + β 2 β 1 + β 2 f 2 2 2:= c f 1 f 2 Lemma 6.8. Given a non-smooth convex function f 1 , e.g. f 1 (·) = · 1 and a smooth convex function f 2 , e.g. f 2 (·) = · 2 2 , we linearize a convex minimization asv * = argmin v∈X µf 1 (v) + f 2 (v) ⇔ v (τ ) = argmin v∈X µf 1 (v) + 1 2α (τ ) v − v (τ −1) − α (τ ) ∇ v f 2 (v (τ −1) ) 2 2 , τ = 1, . . .

Proof.
Remark 6.9. 1-dimensional Taylor expansion of a function f (x) at a isf (x) | x=a ≈ f (1) (a) 1! (x − a) + f (2) (a) 2! (x − a) 2 + f (3) (a) 3! (x − a) 3 + · · · with f (i) (a) is a i-th order derivative of f (x) at x = a, i.e. f (i) (a) = d i f (x) dx i x=a .The 1st order of the 2-dimensional Taylor expansion of a function f (x) atx (τ −1) with x = (x , y) is f (x) | x=x (τ −1) ≈ f (x (τ −1) ) + x − x (τ −1) , ∇ x f (x (τ −1) ) 2 + 1 2α (τ ) x − x (τ −1) 2 2(43)As in ISTA  #b35  or FISTA  #b36 , given a convex minimizationmin v∈X µf 1 (v) + f 2 (v)(44)f 1 is a non-smooth (non-differentiable) convex function, e.g. f 1 (·) = · 1 and f 2 is a smooth (differentiable) convex function, e.g. f 2 (·) = · (v) + ≈ f 2 (v) | v=v (τ −1) f 2 (v (τ −1) ) + v − v (τ −1) , ∇ v f 2 (v (τ −1) ) 2 + 1 2α (τ ) v − v (τ −1) 2 2 = 1 2α (τ ) v − v (τ −1) − α (τ ) ∇ v f 2 (v (τ −1) ) 2 2 + const = argmin v∈X µf 1 (v) + 1 2α (τ ) v − v (τ −1) − α (τ ) ∇ v f 2 (v (τ −1) ) 2 2 .(45)Proposition 6.10. The solution of the "v-problem" is as followsv * = argmin v∈X    µ 2 v 1 + β 5 2 h * v − f − h * u − h * ρ − + λ 5 β 5 2 2 + β 7 2 v − div − S g − λ 7 β 7 2 2    ⇔ v (τ ) = Shrink t (τ ) v , µ 2 α (τ ) β 5 + α (τ ) β 7 , τ = 1, . . . with t (τ ) v = β 5 β 5 + α (τ ) β 7 δ − α (τ )ȟ * h * v (τ −1) + α (τ )ȟ * f − h * u − h * ρ − + λ 5 β 5 + β 7 α (τ ) β 5 + α (τ ) β 7 − S−1 s=0 cos( πs S )g s D 2 + sin( πs S )D T 1 g s =div − S g − λ 7 β 7 .Proof. According to Lemma 6.8, a linearized version of the "v-problem" isv * = argmin v∈X µ 2 β 5 v 1 + 1 2 h * v − f − h * u − h * ρ − + λ 5 β 5 2 2 = q(v) + β 7 2β 5 v − div − S g − λ 7 β 7 2 2 ⇔ v (τ ) = argmin v∈X µ 2 β 5 v 1 + 1 2α (τ ) v − v (τ −1) − α (τ ) = ∂q(v) ∂v | v=v (τ −1) ȟ * h * v (τ −1) − f − h * u − h * ρ − + λ 5 β 5 2 2 + β 7 2β 5 v − div − S g − λ 7 β 7 2 2 , τ = 1, . . . = argmin v∈X v 1 + β 5 + α (τ ) β 7 2µ 2 α (τ ) v − t v 2 2 with t v = β 5 β 5 + α (τ ) β 7 v (τ −1) − α (τ )ȟ * h * v (τ −1) − f − h * u − h * ρ − + λ 5 β 5 + β 7 α (τ ) β 5 + α (τ ) β 7 div − S g − λ 7 β 7 .Thus, a solution of the v-problem is defined at iteration τ asv (τ ) = Shrink t (τ ) v , µ 2 α (τ ) β 5 + α (τ ) β 7 , τ = 1, . . .Proposition 6.11. The " g-problem"min g∈X S F ( g) := β 6 2 w − g + λ 6 β 6 2 2 + β 7 2 v − div − S g + λ 7 β 7 2 2has a minimizer asG s (z) = B s (z) A s (z) , s = 0, . . . , S − 1 . with A s (z) = β 6 + β 7 cos πs S (z 2 − 1) + sin πs S (z 1 − 1) 2 , B s (z) = β 6 W s (z) + Λ 6s (z) β 6 − β 7 cos πs S (z 2 − 1) + sin πs S (z 1 − 1) ×   V (z) + s =[0,S−1]\{s} cos πs S (z −1 2 − 1) + sin πs S (z −1 1 − 1) G s (z) + Λ 7 (z) β 7   . Proof. Given g = g s S−1 s=0 , w = w s S−1 s=0 , λ 6 = λ 6s S−1 s=0 and ∇ + S = ∂ + s S−1 s=0 , the Euler-Lagrange equation is 0 = ∂F ( g) ∂ g = −β 6 w − g + λ 6 β 6 − β 7 ∂ div − S g ∂ g = div − S * δ=−∇ + S δ v − div − S g + λ 7 β 7Given s = 1, . . . , S − 1, we have0 = −β 6 w s − g s + λ 6s β 6 + β 7 ∂ + s v − S−1 s =0 ∂ − s g s =div − S g + λ 7 β 7 ⇔ g s = β 6 − β 7 ∂ + s ∂ − s Id −1 − β 7 ∂ + s v − s =[0,S−1]\{s} ∂ − s g s + λ 7 β 7 + β 6 w s + λ 6s β 6Thus, a solution of the " g-problem" is obtained in the Fourier domain.Proposition 6.12. A solution of the "u-problem" (6) can be rewritten in a form of sampling theory as in equation  #b17 .Proof. we analyze filter banks generated by the DMCD model at iteration τ in the listed order of Algorithm 1-4 in Appendix C.a. The " t-problem":From (9) and a solution of the " t-problem" (11), we haveN l (z) F −1 ←→ β 4 δ[k] − β 3 ∂ − l ∂ + l δ[k] M (τ ) l (z) F −1Frames at a direction l = 0, . . . , L − 1 areΞ L,c34 l (z) = 1 1 + c 34 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 F −1 ←→ ξ L,c34 l [k] = 1 − c 34 ∂ − l ∂ + l −1 δ[k] ,(48)Θ L,c34 l (z) = −c 34 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 1 + c 34 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 F −1 ←→ θ L,c34 l [k] = −c 34 ∂ + l ξ L,c34 l [k] ,(49)Θ L l (z −1 ) = − cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) F −1 ←→θ L l [k] = ∂ − l δ[k] .(50)Note that these frames satisfy the unity condition as  b. The " y-problem":Ξ L,c34 l (z) + Θ L,c34 l (z)Θ L l (z −1 ) = 1 ,(51)Given k ∈ Ω, a solution of the " y-problem" (12) (at iteration τ ) is rewritten asy (τ ) l =      y (τ ) l , y (τ ) ≤ 1 y (τ ) l | y (τ ) | , y (τ ) > 1, l = 0, . . . , L , and       y (τ ) l = t (τ ) l + λ (τ −1) 4l β4 + β1+λ (τ −1) 1 β4 · × r (τ ) l y = L l=0 t (τ ) l + λ (τ −1) 4l β4 + β1+λ (τ −1) 1 β4 · × r (τ ) l ·2 ⇔ y (τ ) [k] = Proj [−1,1] t (τ ) [k] + λ (τ −1) 4 [k] β 4 + β 1 + λ (τ −1) 1 [k] β 4 r (τ ) [k] = y (τ ) [k] = y (τ ) l [k] L l=0 . By denoting y (τ ) [k] = y (τ ) l [k] L l=0 , t (τ ) [k] = t (τ ) l [k] L l=0 , r (τ ) [k] = r (τ ) l [k] L l=0, and from  #b46  and a solution of the " r-problem" at direction l = 0, . . . , L−1, we rewrite y (τ ) [k] = Proj [−1,1]  c. The "u-problem":y (τ ) [k] 52 with y (τ ) l [k] = t (τ ) l [k] + λ (τ −1) 4l [k] β 4 + β 1 + λ (τ −1) 1 [k] β 4 r (τ ) l [k] =   ξ L,c34 l * y (τ −1) l − λ (τ −1) 4l β 4   [k] + λ (τ −1) 4l [k] β 4 +     θ L,c34 l *   Shrink   L−1 l =0θ L l * t (τ −1) l − λ (τ −1) 3 β 3 , 1 β 3   + λ (τ −1) 3 β 3 − l =[0,L−1]\{l}θ L l * t (τ −1) l        [k] + λ (τ −1) 1 [k] + β 1 β 4 Shrink ∂ + l u (τ −1) [k] − λ (τ −1) 2l [k] β 2 + λ (τ −1) 1 [k] + β 1 β 2 y (τ −1) l [k] , λ (τ −1) 1 [k] + β 1 β 2 ,(52)From a solution of the "u-problem (6), we haveX (z) F −1 ←→ β 5 h * ȟ [k] − β 2 L−1 l=0 ∂ − l ∂ + l δ[k] Y (τ ) (z) F −1 ←→ β 5 ȟ * f − h * v (τ −1) − h * ρ (τ −1) − (τ −1) + λ (τ −1) 5 β 5 [k] − β 2 L−1 l=0 ∂ − l Shrink ∂ + l u (τ −1) [k] − λ (τ −1) 2l [k] β 2 + λ (τ −1) 1 [k] + β 1 β 2 y (τ −1) l [k] , λ (τ −1) 1 [k] + β 1 β 2 =r (τ ) l [k] + λ (τ −1) 2l [k] β 2To avoid singularity, we check X (z) at ω = 0 asX (e j0 ) = β 2 L−1 l=0 cos πl L (e j0 − 1) + sin πl L (e j0 − 1) 2 + β 5 H(e j0 ) 2 = β 5 H(e j0 ) 2 .If a blur operator has H(e j0 ) 2 > 0 (usually H(e j0 ) = 1 because H(e jω ) often plays as a lowpass kernel in the Fourier domain), then the function X (z) satisfies0 < β 5 H(e j0 ) 2 ≤ X (z) < +∞ , ∀ω ∈ [−π , π] 2 .Note that this is similar to a condition of Riesz basis.Thus, a solution of the "u-problem" is rewritten asU (τ ) (z) = X −1 (z)Y (τ ) (z) = Φ L,c25 (z)H(z −1 ) F (z) − H(z)V (τ −1) (z) − H(z)P (τ −1) (z) − E (τ −1) (z) + Λ (τ −1) 5 (z) β 5 + L−1 l=0Ψ L,c25 l (z −1 ) R (τ ) l (z) + Λ (τ −1) 2l (z) β 2 .(53)Given k ∈ Ω, the inverse Fourier transform of (53) isu (τ ) [k] = φ L,c25 * ȟ * f − h * v (τ −1) − h * ρ (τ −1) − (τ −1) + λ (τ −1) 5 β 5 [k] + L−1 l=0ψ L,c25 l [k] * Shrink ψ L l * u (τ −1) [k] − λ (τ −1) 2l [k] β 2 + λ (τ −1) 1 [k] + β 1 β 2 y (τ −1) l [k] , λ (τ −1) 1 [k] + β 1 β 2 + λ (τ −1) 2l [k] β 2 .(54)At a direction l = 0, . . . , L−1, we have y(τ −1) l [k] = P [−1,1] y (τ −1) [k] with y (τ −1) [k] = y (τ −1) l [k] L−1 l=0as defined in (52) at iteration (τ − 1).Note that u (τ ) in (54) is updated from u (τ −1) and u (τ −2) at every iteration τ . Frames (at direction l = 0, . . . , L − 1) are well defined in the Fourier domain Φ L,c25 (z) = 1c 25 L−1 l=0 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 + H(z) 2 F −1 ←→ φ L,c25 [k] ,(55)Ψ L,c25 l (z −1 ) = c 25 cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) c 25 L−1 l =0 cos πl L (z 2 − 1) + sin πl L (z 1 − 1) 2 + H(z) 2 F −1 ←→ψ L,c25 l [k] = −c 25 ∂ − l φ L,c25 [k] ,(56)Ψ L l (z) = cos πl L (z 2 − 1) + sin πl L (z 1 − 1) F −1 ←→ ψ L l [k] = ∂ + l δ[k] .(57)Proposition 6.14. The discrete and continuous versions of the multiscale sampling theory for the "u-problem" are defined in  #b26  and (28), respectively.Proof. 1. Proof for the discrete multiscale sampling theory (27):In order to build a form of multiscale sampling version generated by a solution of the "uproblem", for easy calculation we consider (18) without blur operator, i.e. h(·) = δ(·), and simplify its notation by (3)) • removing shrinkage operator, the Lagrange multipliers (λ 1 , λ 2 , λ 5 ) and y (τ −1) ,• replacing h * u = f − h * v − h * ρ − (due to a condition of a decomposition in• denoting c = c 25 ,ψ l =ψ L,c l , ψ l = ψ L l and interpolant φ int = φ L,c .Thus, we rewrite (18) asu (τ ) [k] = u (τ −1) * φ int [k] + L−1 l=0 u (τ −1) * ψ l * ψ l [k] .(60)At a convergence, i.e. when iteration τ goes to infinity, we have u (τ ) = u (τ −1) = u and (60) is rewritten as u[k] = u * φ int [k] + L−1 l=0 u * ψ l * ψ l [k] F ←→ U (z) = U (z)Φ int (z) + L−1 l=0 U (z)Ψ l (z −1 )Ψ l (z) .φ int [k] = c(−∆ dL ) + 1 −1 δ[k] F ←→ Φ int (z) = 1 1 + c L−1 l=0 sin πl L (z 1 − 1) + cos πl L (z 2 − 1) 2 ,(62)ψ l [k] = −c c(−∆ dL ) + 1 −1 ∂ − l δ[k] = −c∂ − l φ int [k] F ←→Ψ l (z −1 ) = c sin πl L (z −1 1 − 1) + cos πl L (z −1 2 − 1) 1 + c L−1 l=0 sin πl L (z 1 − 1) + cos πl L (z 2 − 1) 2 ,(63)ψ l [k] = ∂ + l δ[k] F ←→ Ψ l (z) = sin πl L (z 1 − 1) + cos πl L (z 2 − 1) .(64)56 Note that the directional mother dual/primal wavelet plays as an operator-like wavelet of interpolant asψ[k] = c(−∆ dL )φ int [k] F ←→ Ψ(z) = L−1 l=0 Ψ l (z)Ψ l (z −1 )and these frame elements satisfy a perfect reconstruction scheme, i.e.Φ int (z) + L−1 l=0 Ψ l (z)Ψ l (z −1 ) = 1 .(65)From (61), we defined a multiscale projection of a function u ∈ X to scaling space and its orthogonal wavelet space, i.e. V {φ int } ⊥ W il ψ il ,ψ il with scale i = 0, . . . I − 1 and direction l = 0, . . . , L − 1 asu[k] = n∈Z 2 u[n]φ[k − n] =(u * φ)[k] + I−1 i=0 L−1 l=0 n∈Z 2 u,ψ il [· − n] 2 ψ il [k − n] =(u * ψ il * ψ il )[k] (66) F ←→ U (z) = U (z)Φ(z) + I−1 i=0 L−1 l=0 U (z)Ψ il (z −1 )Ψ il (z)where definition of frames on spaces V and W il are defined in the Fourier domain from (62)-(65), as (see Figure 8(b) for their spectra)φ[k] F ←→ Φ(z) = I −1 I−1 i=0 Φ int (z a i ) , ψ il [k] F ←→ Ψ il (z) = I − 1 2 Ψ l (z a i ) anď ψ il [k] F ←→Ψ il (z −1 ) = I − 1 2Ψ l (z −a i ) ,which satisfy the unity condition asΦ(z) + I−1 i=0 L−1 l=0Ψ il (z −1 )Ψ il (z) = 1 .Given n ∈ Ω, wavelet coefficients in direction l and scale i isc il [n] = u ,ψ il [· − n] 2 F ←→ C il (z) = U (z)Ψ il (z −1 ) .

Proof for the continuous multiscale sampling theory (28):
The impulse response of a continuous directional Laplacian operator is∆ L δ(x) = L−1 l=0 ∂ 2 l δ(x) F ←→ − L−1 l=0 cos πl 2 ω 2 + sin πl L ω 1 2Note that the 1st order Maclaurin approximation is a link between a continuous and discrete version of the operator. Similar to a discrete domain (62)-(65), the continuous version of the interpolant φ int (·) and the directional dual/primal waveletψ l (·) and ψ l (·) (with l = 0, . . . , L − 1) areφ int (x) = c(−∆ L ) + 1 −1 δ(x) F ←→ φ int (ω) = 1 1 + c L−1 l=0 cos πl L ω 2 + sin πl L ω 1 2 , ψ l (x) = −c∂ l φ int (x) F ←→ ψ * l (ω) = −c cos πl L jω 2 + sin πl L jω 1 1 + c L−1 l=0 cos πl L ω 2 + sin πl L ω 1 2 , ψ l (x) = ∂ l δ(x) F ←→ ψ l (ω) = cos πl L jω 2 + sin πl L jω 1 .Similarly, this directional wavelet also plays as an operator-like wavelet of the interpolant  #b14  #b54 63] asψ(x) = c(−∆ L )φ int (x) F ←→ ψ(ω) = L−1 l=0 ψ l (ω) ψ * l (ω) .Note that we have a pair of the continuous Fourier transform as |c| −1 φ(c −1 x) F ←→ φ(c ω) with a constant c and x ∈ R 2 . Thus, definition of frames is (see Figure 8(a) for their spectrum)φ(x) = I −1 I−1 i=0 a −i φ int (a −1 x) F ←→ φ(ω) = I −1 I−1 i=0 φ int (a i ω) , ψ il (x) = I − 1 2 a −i ψ l (a −i x) F ←→ ψ il (ω) = I − 1 2 ψ l (a i ω) anď ψ il (x) = I − 1 2 a −iψ l (a −i x) F ←→ ψ * il (ω) = I − 1 2 ψ * l (a i ω) ,and they satisfy the unity conditionφ(ω) + I−1 i=0 L−1 l=0 ψ * il (ω) ψ il (ω) = 1 , φ(0) = 1 , ψ il (0) = ψ il (0) = 0 .A multiscale decomposition in a continuous setting for u ∈ X isu[k] = n∈Z 2 u[n]φ(k − n) =(u * φ)[k] + I−1 i=0 L−1 l=0 n∈Z 2 u ,ψ il (· − n) 2 ψ il (k − n) = u * ψ il * ψ il [k] F ←→ U (e jω ) = U (e jω ) φ(ω) + I−1 i=0 L−1 l=0 U (e jω ) ψ * il (ω) ψ il (ω) .Proposition 6.15. The continuous and discrete versions of the multiscale sampling theory for the " g-problem" are defined in (31) and (32).Proof. 1. Proof for the discrete multiscale sampling theory (31):To ease the calculation, we denote c = c 67 , ξ s = ξ S,c67 s , θ s = θ S,c67 s ,θ s =θ S s . Given s = 0 , . . . , S−1, a simplified version of (30) in the g-problem is obtained by removing a shrinkage operator and Lagrange multipliers ( λ 6 , λ 7 ) asg (τ ) s [k] = ξ s * g (τ −1) s [k] + θ s * θ s * g (τ −1) s [k] .Denote Ξ s (z) , Θ s (z) andΘ s (z) as discrete Fourier transform of ξ s [k] , θ s [k] andθ s [k], respectively. At convergence when the iteration τ goes to infinity, i.e. g (τ )s = g (τ −1) s = g s , we have g s [k] = (ξ s * g s ) [k] + θ s * θ s * g s [k] F ←→ G s (z) = Ξ s (z)G s (z) + Θ s (z)Θ s (z −1 )G s (z)and the unity condition isΞ s (z) + Θ s (z)Θ s (z −1 ) = 1 .Given f ∈ X whose discrete Fourier transform is F (z), a multiscale sampling theory in the Fourier domain is described asF (z) = 1 SI I−1 i=0 S−1 s=0 F (z)Ξ s (z a i ) + 1 SI I−1 i=0 S−1 s=0 F (z)Θ s (z −a i )Θ s (z a i ) = F (z)Ξ(z) + I−1 i=0 S−1 s=0 F (z)Θ si (z −1 )Θ si (z) F ←→ f [k] = n∈Z 2 f [n]ξ[k − n] =(ξ * f )[k] + I−1 i=0 S−1 s=0 n∈Z 2 f ,θ si (· − n) 2 θ si [k − n] =(θsi * θsi * f )[k] ,where the frames (see Figure 8(d) for their spectra)ξ[k] F ←→ Ξ(z) = 1 SI I−1 i=0 S−1 s=0 Ξ s (z a i ) , θ si [k] F ←→ Θ si (z) = 1 √ SI Θ s (z a i ) andθ si [k] F ←→Θ si (z) = 1 √ SIΘ s (z a i ) .satisfy the unity condition Ξ(z) + Similar to the u-problem and by some simplified notations as in the previous explanation, we derive a continuous version for multiscale sampling theory in the g-problem by considering continuous operators in (23)-(25) as These bounded frames satisfy the unity condition in the Fourier domain as (s = 0, . . . , S − 1) ξ s (ω) + θ s (ω) θ * s (ω) = 1 , ξ s (0) = 1 , θ s (0) = θ s (0) = 0 . Given a constant a > 0 and a discrete function f ∈ 2 (R 2 ) whose discrete Fourier transform is F (e jω ), a multiscale sampling theory in the continuous setting is defined asξ s (x) = 1 − c∂ 2 s −1 δ(x) F ←→ ξ s (ω) = 1 1 + c cos πs S ω 2 + sin πs S ω 1 2 , θ s (x) = −c∂ s ξ s (x) F ←→ θ s (ω) = −F (e jω ) = 1 SI I−1 i=0 S−1 s=0 F (e jω ) ξ s (a i ω) + 1 SI I−1 i=0 S−1 s=0 F (e jω ) θ * s (a i ω) θ s (a i ω)Thus, we reformulate the above formulae asf [k] = n∈Z 2 f [n]ξ(k − n) =(f * ξ)[k] + I−1 i=0 S−1 s=0 n∈Z 2 f ,θ si (· − n) 2 θ si (k − n) =(f * θsi * θsi)[k] . F ←→ F (e jω ) = F (e jω ) ξ(ω) + I−1 i=0 S−1 s=0 F (e jω ) θ * si (ω) θ si (ω)with frames (see Figure 8(c) for their spectra)ξ(x) = 1 SI I−1 i=0 S−1 s=0 a −i ξ s (a −i x) F ←→ ξ(ω) = 1 SI I−1 i=0 S−1 s=0 ξ s (a i ω) θ si (x) = 1 √ SI a −i θ s (a −i x) F ←→ θ si (ω) = 1 √ SI θ s (a i ω) θ si (x) = 1 √ SI a −iθ s (a −i x) F ←→ θ si (ω) = 1 √ SI θ s (a i ω) .Note that these multiscale frames are also bounded and satisfy the unity condition in the Fourier domain as Initialization:      ξ(ω) + I−1 i=0 S−1 s=0 θ * si (ω) θ si (ω) = 1 ,u (0) = f , v (0) = (0) = d (0) = r (0) = t (0) = y (0) = w (0) = g (0) = λ (0) 1 = λ (0) 2 = λ (0) 3 = λ (0) 4 = λ (0) 5 = λ (0) 6 = λ (0) 7 = λ1. d (τ ) = Shrink − L l=0 cos πl L t (τ −1) l D2 + sin πl L D T 1 t (τ −1) l =div − L t (τ −1) − λ (τ −1) 3 β3 , 1 β3 2. t (τ ) l =            Re   F −1 M

r
(τ ) l =                Shrink cos( πl L )u (τ −1) D T 2 + sin( πl L )D1u (τ −1) =∂ + l u − λ (τ −1) 2l β2 + λ (τ −1) 1 + β1 β2 · × y (τ −1) l , λ (τ −1) 1 + β1 β2, l = 0, . . . , LShrink 1 − λ (τ −1) 2L β2 + λ (τ −1) 1 + β1 β2 · × y (τ −1) L , λ (τ −1) 1 + β1 β2 , l = L 4. y (τ ) l =      y (τ ) l , y (τ ) ≤ 1 y (τ ) l | y (τ ) | , y (τ ) > 1, l = 0, . . . , L , and        y (τ ) l = t (τ ) l + λ (τ −1) 4l β 4 + β 1 +λ (τ −1) 1 β 4 · × r (τ ) l , y = L l=0 t (τ ) l + λ (τ −1) 4l β 4 + β 1 +λ (τ −1) 1 β 4 · × r (τ ) l ·2 . 5. w (τ ) s = Shrink g (τ −1) s − λ (τ −1) 6s β6 , µ1 β6 , s = 0, . . . , S − 1 (z) β7   7. u (τ ) = Re   F −1 Y (τ ) (z) X (z)   Y (τ ) (z) = β2 L−1 l=0 cos πl L (z −1 2 − 1) + sin πl L (z −1 1 − 1) R (τ ) l (z) + Λ (τ −1) 2l (z) β2µ2α (τ ) β5 + α (τ ) β7 t (τ ) v = β5 β5 + α (τ ) β7   δ − α (τ )ȟ * h * v (τ −1) + α (τ )ȟ * f − h * u (τ ) − h * ρ (τ −1) − (τ −1) + λ (τ −1) 5 β5   + β7α (τ ) β5 + α (τ ) β7 − S−1 s=0 cos( πs S )g (τ ) s D2 + sin( πs S )D T 1 g (τ ) s =div − S g − λ (τ −1) 7 β7 9. ρ (τ ) =ρ (τ ) − CST ρ (τ ) , ν (τ ) ρ , ρ (τ ) = δ − α (τ )ȟ * h * ρ (τ −1) + α (τ )ȟ * f − h * u (τ ) − h * v (τ ) − (τ −1) + λ (τ −1) 5 β5 68 Algorithm 4 DMCD (III) 10. (τ ) =˜ (τ ) − CST ˜ (τ ) , ν (τ ) ,˜ (τ ) = f − h * u (τ ) − h * v (τ ) − h * ρ (τ ) + λ (τ −1) 5 β5II. Update Lagrange multipliers:  φint(a i ω) λ (τ ) 1 = λ (τ −1) 1 + β1 r (τ ) − y (τ ) , r (τ ) X , | r| = L l=0 r ·2 l , y , r X = L l=0 y l · × r l λ (τ ) 2l =      λ (τ −1) 2l + β2 rΦ(z) = I −1 I−1 i=0 Φint(z a i ) , Ψ il (z) = I − 1 2 Ψ l (z a i ) ,Ψ il (z −1 ) = I − 1 2Ψ l (z a i ) , Ξ(z) = 1 IL I−1 i=0 L−1 l=0 Ξ l (z a i ) , Θ il (z) = 1 √ IL Θs(z a i ) ,Θ il (z) = 1 √ ILΘ l (z a i ) .ψ il (x) = I − 1 2 a −i ψ l (a −i x) F ←→ ψ il (ω) = I − 1 2 ψ l (a i ω) ψ il (x) = I − 1 2 a −iψ l (a −i x) F ←→ ψ il (ω) = I − 1 2 ψ l (a i ω) ξ(x) = 1 IL I−1 i=0 L−1 l=0 a −i ξ l (a −i x) F ←→ ξ(ω) = 1 IL I−1 i=0 L−1 l=0 ξ l (a i ω) θ il (x) = 1 √ IL a −i θ l (a −i x) F ←→ θ il (ω) = 1 √ IL θ l (a i ω) θ il (x) = 1 √ IL a −iθ l (a −i x)