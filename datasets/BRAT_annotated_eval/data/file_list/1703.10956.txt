InverseFaceNet: Deep Monocular Inverse Face Rendering

Abstract
10m 10mm 0mm m 0mm Input Estimate Geometry Error Input Estimate Geometry Error Figure 1 . Our single-shot deep inverse face renderer InverseFaceNet obtains a high-quality geometry, reflectance and illumination estimate from just a single input image. We jointly recover the facial pose, shape, expression, reflectance and incident scene illumination. From left to right: the input photo, our estimated face model, its geometry, and the pointwise Euclidean geometry error compared to Garrido et al. [19] . Abstract We introduce InverseFaceNet, a deep convolutional inverse rendering framework for faces that jointly estimates facial pose, shape, expression, reflectance and illumination from a single input image. By estimating all parameters from just a single image, advanced editing possibilities on a single face image, such as appearance editing and relighting, become feasible in real time. Most previous learning-based face reconstruction approaches do not jointly recover all dimensions, or are severely limited in terms of visual quality. In contrast, we propose to recover high-quality facial pose, shape, expression, reflectance and illumination using a deep neural network that is trained using a large, synthetically created training corpus. Our approach builds on a novel loss function that measures model-space similarity directly in parameter space and significantly improves reconstruction accuracy. We further propose a self-supervised bootstrapping process in the network training loop, which iteratively updates the synthetic training corpus to better reflect the distribution of real-world imagery. We demonstrate that this strategy outperforms completely synthetically trained networks. Finally, we show high-quality reconstructions and compare our approach to several state-of-the-art approaches.


 Figure 1. Our single-shot deep inverse face renderer InverseFaceNet obtains a high-quality geometry, reflectance and illumination estimate from just a single input image. We jointly recover the facial pose, shape, expression, reflectance and incident scene illumination. From left to right: the input photo, our estimated face model, its geometry, and the pointwise Euclidean geometry error compared to Garrido et al.  #b19 .

Abstract
We introduce InverseFaceNet, a deep convolutional inverse rendering framework for faces that jointly estimates facial pose, shape, expression, reflectance and illumination from a single input image. By estimating all parameters from just a single image, advanced editing possibilities on a single face image, such as appearance editing and relighting, become feasible in real time. Most previous learning-based face reconstruction approaches do not jointly recover all dimensions, or are severely limited in terms of visual quality. In contrast, we propose to recover high-quality facial pose, shape, expression, reflectance and illumination using a deep neural network that is trained using a large, synthetically created training corpus. Our approach builds on a novel loss function that measures model-space similarity directly in parameter space and significantly improves reconstruction accuracy. We further propose a self-supervised bootstrapping process in the network training loop, which iteratively updates the synthetic training corpus to better reflect the distribution of real-world imagery. We demonstrate that this strategy outperforms completely synthetically trained networks. Finally, we show high-quality reconstructions and compare our approach to several state-of-the-art approaches.

Introduction
Inverse rendering aims to reconstruct scene properties such as geometry, reflectance and illumination from image data. This reconstruction is fundamentally challenging, as it inevitably requires inverting the complex real-world image formation process. It is also an ill-posed problem as certain effects, such as low-frequency reflectance and illumination, can be indistinguishable  #b45 . Inverse rendering, for example, enables relighting of faces by modifying the scene illumination and keeping the face reflectance and geometry fixed.Recently, optimization-based approaches for inverse face rendering were introduced with convincing results [2, #b19  #b28  #b34  #b60 . One of the key ingredients that enables to disentangle pose, geometry (both related to shape and facial expression), reflectance and illumination are specific priors that constrain parameters to plausible values and distributions. Formulating such priors accurately for real faces is difficult, as they are unknown a priori. The priors could be learned by applying inverse rendering to a large dataset of real face images, but this is highly challenging without having the priors a priori.We take a different approach to solve this chicken-and-egg problem. Instead of formulating explicit priors, we directly learn inverse face rendering with a deep neural network that implicitly learns priors based on the training corpus. As annotated training data is hard to come by, we train on synthetic face images with known model parameters (geometry, reflectance and illumination). This is similar to existing approaches  #b46  #b47  #b52 , but the used parameter distribution does not match that of real-world faces and environments. As a result, the learned implicit priors are rather weak and do not generalize well to in-the-wild images.The approach of Li et al.  #b38  introduces a self-augmented procedure for training a CNN to regress the spatially varying surface appearance of planar exemplars. Our self-supervised bootstrapping approach extends their training strategy to handle unknown, varying geometry. In addition, we resample based on a mean-adaptive Gaussian in each bootstrapping step, which helps to populate out-of-domain samples, especially at the domain boundary.In contrast to many other approaches, InverseFaceNet also regresses color reflectance and illumination. Our main technical contribution is the introduction of a self-supervised bootstrapping step in our training loop, which continuously updates the training corpus to better reflect the distribution of real-world face images. The key idea is to apply the latest version of the inverse face rendering network to real-world images without ground truth, to estimate the corresponding face model parameters, and then to create synthetic face renderings for perturbed, but known, parameter values. In this way, we are able to bootstrap additional synthetic training data that better reflects the real-world distribution of face model parameters, and our network therefore better generalizes to the real-world setting. Our experiments demonstrate that our approach greatly improves the quality of regressed face models for real face images compared to approaches that are trained exclusively on synthetic data.The main contribution of our paper is InverseFaceNeta real-time, deep, single-shot inverse face rendering network that estimates pose, shape, expression, color reflectance and illumination from just a single input image in a single forward pass, and is multiple orders of magnitude faster than previous optimization-based methods estimating similar models. To improve the accuracy of the results, we further propose a loss function that measures model-space distances directly in a modified parameter space. We further propose self-supervised bootstrapping of a synthetic training corpus based on real images without available ground truth to produce labeled training data that follows the real-world parameter distribution. This leads to significantly improved reconstruction results for in-the-wild face photos.

Related Work
Inverse Rendering (of Faces) The goal of inverse rendering is to invert the graphics pipeline, i.e., to recover the geometry, reflectance (albedo) and illumination from images or videos of a scene -or, in our case, a face. Early work on inverse rendering made restrictive assumptions like known scene geometry and calibrated input images  #b45  #b65 . However, recent work has started to relax these assumptions for specific classes of objects such as faces. Deep neural networks have been shown to be able to invert simple graphics pipelines  #b32  #b42 , although these techniques are so far only applicable to low-resolution grayscale images. In contrast, our approach reconstructs full-color facial reflectance and illumination, as well as geometry. Aldrian and Smith [2] use a 3D morphable model for optimization-based inverse rendering. They sequentially solve for geometry, reflectance and illumination, while we jointly regress all dimensions at once. Thies et al.  #b60  recently proposed a real-time inverse rendering approach for faces that estimates a person's identity and expression using a blendshape model with reflectance texture and colored spherical harmonics illumination. Their approach is designed for reenactment and is visually convincing, but relies on nonlinear least-squares optimization, which requires good initialization and a face model calibration step from multiple frames, while our approach estimates a very similar face model in a single shot, from a single in-the-wild image, in a fraction of the time. Inverse rendering has also been applied to face image editing  #b40  #b55 , for example to apply makeup  #b34  #b35 . However, these approaches perform an image-based intrinsic decomposition without an explicit 3D face model, as in our case.Face Models The appearance and geometry of faces are often modeled using 3D morphable models [5] or active appearance models  #b14 . These seminal face models are powerful and expressive, and remain useful for many applications even though more complex and accurate appearance models exist  #b30  #b37 . Recently, a large-scale parametric face model [7] was created from 10,000 facial scans, Booth et al. [6] extend 3D morphable models to "in-the-wild" conditions, and deep appearance models  #b17  extend active appearance models by capturing geometry and appearance of faces more accurately under large unseen variations. We describe the face model we use in Section 4.

3D Face Reconstruction
The literature on reconstructing face geometry, often with appearance, but without any illumination, is much more extensive compared to inverse rendering. We focus on single-view techniques and do not further discuss multi-view or multi-image approaches  #b23  #b29  #b44  #b48  #b57 . Recent techniques approach monocular face reconstruction by fitting active appearance models [1, #b17 , blendshape models [9, #b18  #b19  #b61 , affine face models  #b15  #b16  #b20  #b46  #b51  #b54  #b58  #b62 , mesh geometry  #b26  #b33  #b47  #b48  #b52 , or volumetric geometry  #b24  to input images or videos. Shading-based surface refinement can extract even fine-scale geometric surface detail [11, #b19  #b26  #b47  #b48  #b52 . Many techniques use facial landmark detectors for more robustness to changes in the head pose and expression, and we discuss them in the next section. A range of approaches use RGB-D input [e.g.  #b36  #b59  #b64 , and while they achieve impressive face reconstruction results, they rely on depth data which is typically not available for in-the-wild images or videos.Deep neural networks have recently shown promising results on various face reconstruction tasks. In a paper before its time, Nair et al.  #b42  proposed an analysis-by-synthesis algorithm that iteratively explores the parameter space of a black-box generative model, such as active appearance models (AAM)  #b14 , to learn how to invert it, e.g., to convert a photo of a face into an AAM parameter vector. We are inspired by their approach and incorporate a self-supervised bootstrapping approach into our training process (see Section 7) to make our technique more robust to unseen inputs, in our case real photographs.Richardson et al.  #b46  use iterative error feedback [12] to optimize the shape parameters of a grayscale morphable model from a single input image. Richardson et al.  #b47  build on this to reconstruct detailed depth maps of faces with learned shape-from-shading. Sela et al.  #b52  learn depth and correspondence maps directly using image-to-image translation, and follow this with non-rigid template mesh alignment. Dou et al.  #b16  regress only the identity and expression components of a face. All these approaches are trained entirely on synthetic data [5]. Tran et al.  #b62  train using a photo collection, but their focus lies on estimating morphable model parameters to achieve robust face recognition. In contrast to these approaches, ours not only recovers face geometry and texture, but a more complete inverse rendering model that also comprises color reflectance and illumination, from just a single image without the need for iteration. Jackson et al.  #b24  directly regress a volumetric face representation from a single input image, but this requires a large dataset with matching face images and 3D scans, and does not produce an editable face model, as in our case. Schönborn et al.  #b51  optimize a morphable model using Bayesian inference, which is robust and accurate, but very slow compared to our approach (taking minutes rather than milliseconds). Tewari et al.  #b58  learn a face regressor in a self-supervised fashion based on a CNN-based encoder and a differentiable expert-designed decoder. Our self-supervised bootstrapping approach combines the advantages of synthetic and real training data, which leads to similar quality reconstructions without the need for a hand-crafted differentiable rendering engine.Face Alignment Many techniques in 3D face reconstruction, including ours, draw on facial landmark detectors for robustly identifying the location of landmark keypoints in the photograph of a face, such as the outline of the eyes, nose and lips. These landmarks can provide valuable pose-independent initialization. Chrysos et al. [13] and Jin and Tan  #b27  provide two recent surveys on the many landmark detection approaches that have been proposed in the literature. Perhaps unsurprisingly, deep learning approaches [4, #b68  are again among the best available techniques. However, none of these techniques works perfectly [8, #b56 : facial hair, glasses and poor lighting conditions pose the largest problems. In many cases, these problems can be overcome when looking at video sequences instead of single images  #b43 , but this is a different setting to ours.

Overview
We first detect a set of 66 2D facial landmarks  #b50 , see Figure 2. The landmarks are used to segment the face from the background, and mask out the mouth interior to effectively remove the parts of the image that cannot be explained by our model. The masked face is input to our deep inverse face rendering network (Section 6), which is trained on synthetic facial imagery (Section 5) using a parametric face and image

Input
Ours Garrido13 Garrido16 Figure 9. Qualitative comparison to optimization-based approaches [2,3] on the Thomas dataset [2]. Input Geometry Estimate Contours Figure 10. Qualitative results on several datasets. Left to right: input image, our estimated face model and geometry, and contours (red: input mask, green: ours). Top to bottom: LFW [4], 300-VW [10], CelebA [6], FaceWarehouse [1], Volker [13] and Thomas [2]. Our approach achieves high-quality reconstructions of geometry as well as skin reflectance from just a single input image.

Single-Shot
Inverse Face Rendering Figure 2. Our single-shot inverse face renderer regresses a dense reconstruction of the pose, shape, expression, skin reflectance and incident illumination from a single photograph.formation model (Section 4). Starting from this low-quality corpus, we apply our self-supervised bootstrapping approach that updates the parameter distribution of the training set (Section 7) to bootstrap a training corpus that better approximates the real-world distribution. This leads to higher quality reconstructions (Section 8). Finally, we discuss limitations (Section 8.4) and conclude (Section 9).

The Space of Facial Imagery
We parameterize face images using m = 350 parameters:θ = R,θ [s] ,θ [e] ,θ [r] ,θ [i] ∈ R m .(1)Here, R specifies the global rotation (3 parameters  #b27 . Note that we do not include translation as our network works on consistently segmented input images (see Figure 2 and Section 3).

Affine Face Model
We employ an affine face model to parameterize facial geometry F [g] ∈ R 3V and reflectance F [r] ∈ R 3V , where V is the number of vertices of the underlying manifold template mesh. The geometry vector F [g] stacks the V 3D coordinates that define the mesh's embedding in space. Similarly, the reflectance vector F [r] stacks the RGB per-vertex reflectance values. The space of facial geometry is modeled by the shape θ [s] ∈ R Ns and expression θ [e] ∈ R Ne parameters:F [g] (θ [s] ,θ [e] ) = a [g] + Ns i=1 b [s] i σ [s] i θ [s] i + Ne j=1 b [e] j σ [e] j θ [e] j . (2)The spatial embedding is modeled by a linear combination of orthonormal basis vectors b [s] i and b [e] j , which span the shape and expression space, respectively. a [g] ∈ R 3V is the average geometry of a neutral expression, the σ [s] i are the shape standard deviations and the σ [e] j are the standard deviations of the expression dimensions.Per-vertex reflectance is modeled similarly using a small number of reflectance parameters θ [r] ∈ R Nr :F [r] (θ [r] ) = a [r] + Nr i=1 b [r] i σ [r] i θ [r] i .(3)Here, b [r] i are the reflectance basis vectors, a [r] is the average reflectance and the σ [r] i are the standard deviations. The face model is computed from 200 high-quality 3D scans [5] of Caucasians (100 male and 100 female) using PCA. We use the N s = N r = 128 most significant principal directions to span our face space. The used expression basis is a combination of the Digital Emily model [3] and FaceWarehouse [10] (see Thies et al.  #b60  for details). We use PCA to compress the over-complete blendshapes (76 vectors) to a subspace of N e = 64 dimensions.

Image Formation
We assume the face to be Lambertian, illumination to be distant and smoothly varying, and there is no self-shadowing. We thus represent the incident illumination on the face using second-order spherical harmonics (SH)  #b41  #b45 . Therefore, the irradiance at a surface point with normal n is given byB n | θ [i] = b 2 k=1 θ [i] k H k (n),(4)where H k are the b 2 = 3 2 = 9 SH basis functions, and the θ [i] k are the corresponding illumination coefficients. Since we consider colored illumination, the parameters θ [i] k ∈ R 3 specify RGB colors, leading to 3·9 = 27 parameters in total.We render facial images based on the SH illumination using a full perspective camera model Π : R 3 → R 2 . We render the face using a mask (painted once in a preprocessing step) that ensures that the rendered facial region matches the crops produced by the 66 detected landmark locations (see Figure 2). The global rotation of the face is modeled with three Euler angles using R = Rot xyz (α,β,γ) that successively rotate around the x-axis (up, α), y-axis (right, β), and z-axis (front, γ) of the camera-space coordinate system.

Initial Synthetic Training Corpus
Training our deep inverse face rendering network requires ground-truth training data {I i , θ i } N i=1 in the form of corresponding pairs of image I i and model parameters θ i . However, training on real images is challenging, since the ground-truth parameters cannot easily be obtained for a large dataset. We therefore train our network based on synthetically rendered data, where exact ground-truth labels are available.We sample N = 200,000 parameter vectors θ i and use the model described in Section 4 to generate the corresponding images I i . Data generation can be interpreted as sampling from a probability P (θ) that models the distribution of real-world imagery. However, sampling from this distribution is in general difficult and non-trivial. We therefore assume statistical independence between the components of θ, i.e.,P (θ) = P (R)P (θ [s] )P (θ [e] )P (θ [r] )P (θ [i] ).(5)This enables us to efficiently generate a parameter vector θ by independently sampling each subset of parameters. We uniformly sample the yaw and pitch rotation angles α,β ∼ U(−40°,40°) and the roll angle γ ∼ U(−15°,15°) to reflect common head rotations. We sample shape and reflectance parameters from the Gaussian distributions provided by the parametric PCA face model [5]. Since we already scale with the appropriate standard deviations during face generation (see Equations 2 and 3), we sample both from a standard normal distribution, i.e., θ [s] ,θ [r] ∼ N (0,1). The expression basis is based on artist-created blendshapes that only approximate the real-world distribution of the space of human expressions; this will be addressed by the self-supervised bootstrapping presented in Section 7. We thus uniformly sample the expression parameters using θ [e] ∼ U(−12,12). To prevent closing the mouth beyond anatomical limits, we apply a bias of 4.8 to the distribution of the first parameter 1 . Finally, we sample the illumination parameters usingθ [i] ∼ U(−0.2,0.2), except for the constant coefficient θ [i]1 ∼ U(0.6,1.2) to account for the average image brightness, and set all RGB components to the same value. The self-supervised bootstrapping step presented in Section 7 automatically introduces colored illumination.

InverseFaceNet
Given the training data {I i ,θ i } N i=1 consisting of N images I i and the corresponding ground-truth parameters θ i , we train a deep inverse face rendering network F to invert image formation. In the following, we provide details on our network architecture and the employed loss function.

Network Architecture
We have tested several different networks based on the popular AlexNet  #b31  and ResNet  #b21  architectures, both pre-trained on ImageNet  #b49 . In both cases, we resize the last fully-connected layer to match the dimensionality of our model (350 outputs), and initialize biases with 0, and weights ∼ N (0,0.01). These minimally modified networks provide the baseline we build on. We propose more substantial changes to the training procedure by introducing a novel model-space loss in Section 6.2, which more effectively trains the same network architecture. The color channels of the input images are normalized to the range [−0.5,0.5] before feeding the data to the network. We show a comparison between the results of AlexNet and ResNet-101 in Section 8.1, and thus choose AlexNet for our results.Input Pre-Processing The input to our network is a color image of a masked face with a resolution of 240×240 pixels (see Figure 2). We mask the face to remove any background and the mouth interior, which cannot be explained by our face model. For this, we use detected landmarks  #b50  and resize their bounding box uniformly to fit inside 240×240 pixels, to approximately achieve scale and translation invariance.Training We train all our inverse face rendering networks using the Caffe deep learning framework  #b25  with stochastic gradient descent based on AdaDelta  #b66 . We perform 75K batch iterations with a batch size of 32 for training our baseline approaches. To prevent overfitting, we use an  Figure 3. Our approach updates the initial training corpus (left) based on real-world images without available ground truth (right) using a self-supervised bootstrapping approach. The generated new training corpus (middle) better matches the real-world face distribution.

Model-Space Parameter Loss
We use a weighted norm to define a model-space loss between the predicted parameters θ and ground-truth θ g by taking the statistics of the face model into account:L(θ,θg) = θ−θg 2 A (6) = (θ−θg) A Σ Σ (θ−θg).(7)Here, Σ is a weight matrix that incorporates the standard deviations σ • of the different parameter dimensions:Σ = diag(ωR13,ωsσ [s] ,ωeσ [e] ,ωrσ [r] ,ωi127) ∈ R m×m .(8)The coefficients ω • balance the global importance of the different groups of parameters, and 1 k is a kdimensional vector of ones. We use the same values (ω R ,ω s ,ω e ,ω r ,ω i ) = (400,50,50,100,20) for all our results. Note that we do not scale the rotation and illumination dimensions individually. Intuitively speaking, our model-space loss enforces that the first PCA coefficients (higher variation basis vectors) should match the ground truth more accurately than the later coefficients (lower-variation basis vectors), since the former have a larger contribution to the final 3D geometry and skin reflectance of the reconstructed face in model space (see Equations 2 and 3). As shown in Section 8, this leads to more accurate reconstruction results. The difference to Zhu et al.  #b68  is the computation of the weights, which leads to a statistically meaningful metric.

Self-Supervised Bootstrapping
To evaluate the strength of our self-supervised bootstrapping step in the training loop, we use synthetic validation images, as it is difficult to acquire the ground-truth parameters for real-world images. This section explains in more detail the evaluation shown in Section 7.2 and Figure 4 of the main document, in particular the image sets used for training, bootstrapping and validation. We first generate a set of 50,000 training images with a parameter distribution that has little variation; the mouth, for instance, is not opening much. We then modify the distribution with a bias and more variation in face expression and color to simulate real-world images, and generate two sets of 5,000 images each for bootstrapping and validation. The difference between the image sets is clearly visible in Figure 2.In this evaluation, InverseFaceNet uses a set of 5,000 images without the corresponding parameters for self-Training Figure 2. Images used for training (top) and testing (bottom) in the bootstrapping evaluation. The synthetic examples for testing and bootstrapping are sampled from a wider distribution than the training images. Thus, there is more variation in face shape, expression and color, such as mouth opening and colored illumination. supervised bootstrapping. The initialization, used weights and number of training iterations are explained in the main document. For evaluation, we visualize the face parameters estimated from premature to fully domain-adapted networks, i.e., along the bootstrapping iterations, in the testing phase as shown in Figure 3. In addition, we compute the model-  Figure 3. Comparison of baseline and bootstrapping approaches on a synthetic test corpus with higher parameter variation than in the used training corpus (also synthetic). Top: Reconstructions of an unseen input image after different numbers of bootstrapping iterations. Notice how the reconstructions with bootstrapping gradually converge towards the ground-truth face model (right), e.g., opening the mouth, while the baseline approach does not improve visibly over time. The last two columns visualize the photometric (2× scaled) and geometric errors at the final bootstrapping step. The mean photometric error is 16.74 pixels in the L1-norm distance for the baseline method, and 12.11 pixels after bootstrapping. The Hausdorff distance is 2.56 mm and 2.01 mm for the baseline method and after bootstrapping, respectively. Bottom: Model-space parameter loss for the baseline and bootstrapping approaches. While our domain-adaptive bootstrapping approach continuously decreases the error by adapting the parameter distribution based on a higher variation corpus without available ground truth, the baseline network overfits to the training data and fails to generalize to the unseen data. space parameter loss of the validation image set. With the visual and numeric metrics, the performance of bootstrapped InverseFaceNet is compared against a vanilla AlexNet without bootstrapping. The decrease of the model-space loss via bootstrapping substantiates that the parameter distribution of the training set is automatically adapted to better match the image set used for bootstrapping, i.e., more mouth opening is added to the initial training set. This is in contrast to the regressed face with a closed mouth, and non-decreasing model-space parameter error by the baseline method, which estimates the best possible parameters only within the initial training set. On the basis of this evaluation, we conclude that our self-supervised bootstrapping approach results in better generalization to unseen input images in the real-world scenario. For an evaluation on real-world face images, we refer to the main document.

Algorithm
Our self-supervised parameter bootstrapping is a four-step process (see Algorithm 1). It starts with a deep neural network F initially trained on a synthetic training corpus (see Section 5) for 15K batch iterations. This guarantees a suitable initialization for all weights in the network. Given a set of images from the corpus of real-world images R, we first obtain an estimate of the corresponding model parameters θ r , i.e., θ(I real ) in Equation 9, using the synthetically trained network (step 1). These reconstructed parameters are used to seed the bootstrapping. In step 2, we apply small perturbations to the reconstructed parameters based on the noise distribution N (0,σ 2 ). This generates new data around the seed points In step 3, we generate new synthetic training images I r based on the resampled parameters θ r , i.e., θ(I real )+N (0,σ 2 ). The result is a new synthetic training set R that better reflects the real-world distribution of model parameters. Finally, the network F is fine-tuned for N iter = 7.5K batch iterations on the new training corpus (step 4). In total, we repeat this process for N boot = 8 self-supervised bootstrapping steps. Over the iterations, the data distribution of the training corpus adapts and better reflects the real-world distribution of the provided in-the-wild facial imagery, as illustrated in Figure 3. We also evaluate the parameter loss throughout bootstrapping iterations in Figure 4, and observe a clear reduction with our self-supervised bootstrapping. This leads to higher quality results at test time, as shown in Section 8. The variance σ 2 could be adaptively scaled based on the photometric error of estimates. However, we found empirically that our framework works well with a fixed variance.

Experiments and Results
We evaluate our InverseFaceNet on several publicly available datasets. We validate our design choices regarding network architecture, model-space loss, and self-supervised bootstrapping. We then show quantitative and qualitative results and comparisons on the datasets LFW (Labeled Faces in the Wild)  #b22 , 300-VW (300 Videos in the Wild)  #b53 , CelebA  #b39 , FaceWarehouse [10], Volker  #b63  and Thomas  #b18 . For more results, we refer to our supplemental document and video 2 . Error Measures We compute the photometric error using the RMSE of RGB pixel values (within the mask of the input image) between the input image and a rendering of the reconstructed face model. An error of 0 is a perfect color match, and 255 is the difference between black and white (i.e. while AlexNet has lower photometric error (also on average, see Table 1). AlexNet with MSL and bootstrapping clearly improves the reconstruction of reflectance and geometry, in all error categories.lower is better). The geometric error measures the RMSE in mm between corresponding vertices in our reconstruction and the ground-truth geometry. We quantify the image-space overlap of the estimated face model and the input face image using the intersection over union (IOU) of face masks (e.g. see 'contours' in Figure 5). An IOU of 0% means no overlap, and 100% means perfect overlap (i.e. higher is better). Table 1 evaluates different design choices on a test dataset of 5,914 images (one shown in Figure 5) from CelebA  #b39  using the error measures described earlier (using our implementation of Garrido et al.  #b19  as ground-truth geometry, up to blendshape level).

Evaluation of Design Choices
Network Architecture We first compare the results of the AlexNet  #b31  and ResNet-101  #b21  architectures, both with our model-space loss (see Section 6). Reconstructions using ResNet-101 have smaller geometric errors, but worse photometric error and IOU than AlexNet, which is exemplified by Figure 5. ResNet-101 is significantly deeper than AlexNet, so training takes about 10× longer and testing about 5× longer. We thus use AlexNet for our inverse face rendering network, which only requires 3.9 ms for the forward pass (on an Nvidia Titan Xp). Landmark detection takes 4.5 ms and face morphing 1 ms (on the GPU). In total, our approach requires 9.4 ms. Table 1 shows that our model-space loss improves on baseline AlexNet  #b31  in all error categories, particularly the photometric error and IOU. As our model-space loss does not modify the network architecture, the time for the forward pass remains the same fast 3.9 ms as before.

Importance of Model-Space Loss
Importance of Self-supervised Bootstrapping Our self-supervised bootstrapping (see Section 7) significantly improves the reconstruction quality and produces the lowest errors in all categories, as shown in Table 1. This can also   #b58  in terms of geometry and overlap, and worse in terms of the photometric error on this test set.

Quantitative Evaluation
We compare the geometric accuracy of our approach to state-of-the-art monocular reconstruction techniques in Figure 6. As ground truth, we use the high-quality stereo reconstructions of Valgaerts et al.  #b63 . Compared to Thies et al.  #b60 , our approach obtains similar quality results, but without the need for explicit optimization. Therefore, our approach is two orders of magnitude faster (9.4 ms vs 600 ms) than optimization-based approaches. Note that while Thies et al.  #b60  run in real time for face tracking, it requires significantly longer to estimate all model parameters from an initialization based on the average model. In contrast to the state-of-the-art learning-based methods by Richardson et al.  #b46  #b47 , Jackson et al.  #b24  and Tran et al.  #b62 , ours obtains a reconstruction of all dimensions, including pose, shape, expression, and colored skin reflectance and illumination. In addition, we performed a large quantitative ground-truth comparison on the FaceWarehouse [10] dataset, see Table 2. We show the mean error (in mm) and standard deviation (SD) for 180 meshes (9 different identities, each with 20 different expressions). As can be seen, our bootstrapping approach increases accuracy. Our approach is only slightly worse than the optimization-based approach of Garrido et al.  #b19 , while being orders of magnitude faster. Bootstrapping is on par with the weakly supervised approach of Tewari et al.  #b58 , which is trained on real images and landmarks. We also compare to a baseline network 'MonoFit' that has been directly trained on the monocular fits of Garrido et al.  #b19  on the CelebA  #b39  dataset. Our self-supervised bootstrapping approach obtains higher accuracy results.

Qualitative Evaluation
In the following, we show additional results and comparisons that unfortunately did not fit into the limited space of the main document. Specifically, we compare to the approaches of Richardson et al. [8], Sela et al. [9], Jackson et al. [5], Tran et al. [12], Tewari et al. [11], Garrido et al. [2] and Garrido et al. [3] on a variety of challenging face datasets, including LFW (Labeled Faces in the Wild) [4], 300-VW (300 Videos in the Wild) [10], CelebA [6], FaceWarehouse [1], Volker [13] and Thomas [2]. The results are shown in Figure 4 to Figure 9. We compare to the results of Richardson Figure 4. Qualitative comparison to Richardson et al. [8] on LFW [4]. Note that our reconstruction results are colored, and better fit the face shape and mouth expressions of the input images.as we are interested in comparing the reconstructed parametric face models. As can be seen in Figures 4 and 5, we obtain similar or even higher quality results than these two state-of-the-art approaches. Note that their approaches do not require landmarks for initial cropping, but they are significantly slower due to their iterative regression strategy [8] or the involved non-rigid registration [9], and do not recover color reflectance. In contrast, our approach provides a one-shot estimate of all face model parameters. Jackson et al. [5] recover coarse volumetric reconstructions, and do not reconstruct facial appearance or illumination ( Figure 6). In contrast to Richardson et al. [7,8] and Jackson et al. [5], our approach obtains an estimate of the colored skin reflectance and illumination.The approach of Tran et al. [12] is targeted at face recognition, and thus does not recover the facial expression and illumination (Figure 7). Our results are comparable to Tewari et al. [11], but we avoid the geometric shrinking seen in Figure 8. Notice that their estimated geometry is visibly thinner than the input faces. Our approach also obtains similar quality results (Figure 9) as the optimization-based approaches by Garrido et al. [2,3], while being several orders of magnitude faster. For a detailed discussion, we refer the reader to the main document.

Limitations
We propose a solution to the highly challenging problem of inverse face rendering from a single image. Similar to previous learning-based approaches, ours has a few  Figure 6. Quantitative comparison of geometric accuracy compared to Thies et al.  #b60 , Richardson et al.  #b47 , Jackson et al.  #b24  and Tran et al.  #b62  on Volker  #b63 . The heat maps visualize the pointwise Hausdorff distance (in mm) between the input and the ground-truth. The ground-truth has been obtained by the high-quality binocular reconstruction approach of Valgaerts et al.  #b63 .

Conclusion
We have presented InverseFaceNet -a single-shot inverse face rendering framework. Our key contribution is to overcome the lack of well-annotated image datasets by selfsupervised bootstrapping of a synthetic training corpus that captures the real-world distribution. This enables high-quality face reconstruction from just a single monocular image. Our evaluation shows that our approach compares favorably to the state of the art. InverseFaceNet could be used to quickly and robustly initialize optimization-based reconstruction approaches close to the global minimum. We hope that our approach will stimulate future work in this exciting field.  Figure 1. Our single-shot deep inverse face renderer InverseFaceNet obtains a high-quality geometry, reflectance and illumination estimate from just a single input image. We jointly recover the facial pose, shape, expression, reflectance and incident scene illumination. From left to right: the input photo, our estimated face model, its geometry, and the pointwise Euclidean geometry error compared to Garrido et al. [3].This document provides additional discussion, comparisons and results for our approach. We discuss technical details on how we evaluate the self-supervised bootstrapping approach using a synthetic test set in Section 1. Also, we provide quantitative and qualitative comparisons in Sections 2 and 3, respectively, to further demonstrate the accuracy and effectiveness of our approach. Finally, we demonstrate the robustness of our approach on a wide range of challenging face images in Section 4.

Additional Quantitative Evaluation
In addition to the quantitative evaluation on FaceWarehouse [1] in the main document, we here evaluate and compare our approach on a challenging video sequence (300 frames of Volker [13]). As ground-truth geometry, we use the highquality binocular reconstructions of Valgaerts et al. [13]. Our approach outperforms Tewari et al. [11] on this sequence, and comes close to the optimization-based results of Garrido et al. [3], which is orders of magnitude slower than our approach (2 minutes vs our 9.4 ms). 

Additional Results
Our approach works well even for the challenging images shown in Figure 10 with different head orientations (rows one and two), challenging expressions (rows three to five),

Input Geometry Estimate
Geometry Estimate Ours Jackson17 Figure 6. Qualitative comparison to Jackson et al. [5] on LFW [4]. Our reconstruction results include reflectance and illumination, and better fit the face shape. and variation in skin reflectance (rows four to six). Our approach provides perceptually more plausible reconstructions Input Geometry

Reflectance
Reflectance Geometry Ours Tran17 Figure 7. Qualitative comparison to Tran et al. [12] on images of the CelebA [6] (top 3 rows) and LFW [4] (rest) datasets: our approach reconstructs expressions, while theirs cannot recover this dimension (arrows).due to our novel model-space loss and the self-supervised bootstrapping that automatically adapts the parameter distribution to match the real world. For more results and a detailed discussion, we refer the reader to the main document.

Footnote
2 : -regularizer (aka weight decay) of 0.001. We train with a base learning rate of 0.01.1  The first parameter mainly corresponds to mouth opening and closing.