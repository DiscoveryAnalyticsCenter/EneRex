EFFICIENT LEARNING OF RESTRICTED BOLTZMANN MACHINES USING COVARIANCE ESTIMATES

Abstract
Learning RBMs using standard algorithms such as CD(k) involves gradient descent on the negative log-likelihood. One of the terms in the gradient, which involves expectation w.r.t. the model distribution, is intractable and is obtained through an MCMC estimate. In this work we show that the Hessian of the loglikelihood can be written in terms of covariances of hidden and visible units and hence, all elements of the Hessian can also be estimated using the same MCMC samples with small extra computational costs. Since inverting the Hessian may be computationally expensive, we propose an algorithm that uses inverse of the diagonal approximation of the Hessian, instead. This essentially results in parameterspecific adaptive learning rates for the gradient descent process and improves the efficiency of learning RBMs compared to the standard methods. Specifically we show that using the inverse of diagonal approximation of Hessian in the stochastic DC (difference of convex functions) program approach results in very efficient learning of RBMs.

INTRODUCTION
The Restricted Boltzmann Machine (RBM), an energy based generative model  #b27  #b7  #b9 , is among the basic building blocks of several deep learning models including Deep Boltzmann Machine (DBM) and Deep Belief Networks (DBN)  #b25  #b10  #b20 . It can also be used as a discriminative model with suitable modifications.The traditional method of learning the parameters of an RBM involves minimizing the KL divergence between the data and the model distribution. This is equivalent to the maximum likelihood estimation and is implemented as a gradient ascent on the log-likelihood. However, evaluating the gradient (w.r.t. the parameters of the model) of the log-likelihood is computationally expensive (exponential in minimum of the number of visible/hidden units in the model) since it contains an expectation term w.r.t. the model distribution. Therefore, in the iterative stochastic gradient methods this term is approximated using samples from the model distribution. The samples are obtaining using Markov Chain Monte Carlo (MCMC) methods which are efficient in this regard due to RBM's bipartite connectivity structure. The popular Contrastive Divergence (CD) algorithm uses samples obtained through an MCMC procedure with a specific initialization strategy. However, the resulting estimated gradient may be poor when the RBM model is high dimensional. The poor estimate can make the stochastic gradient descent (SGD) based algorithms such as CD to even diverge in some cases  #b5 .There are two general approaches to make the learning of RBMs more efficient. The first is to design an efficient MCMC method to get good representative samples from the model distribution and thereby reduce the variance of the estimated gradient  #b3  #b29 ). However, advanced MCMC methods are computationally intensive, in general. The second approach is to design better optimization strategies which are robust to the noise in the estimated gradient  #b15  #b4  #b2 . Most approaches to design better optimization algorithms for learning RBMs are second order optimization techniques that either need approximate Hessian inverse or an estimate of the inverse Fisher matrix (The two approaches differ for the RBM since it contains hidden units). The Hessian-Free (H-F) algorithm  #b15  is an iterative procedure which approximately solves a linear system to obtain the curvature through matrix-vector product. In  #b4  H-F algorithm is used to design natural gradient descent for learning Boltzmann machines. A sparse Gaussian graphical model is proposed in  to estimate the inverse Fisher matrix in order to devise factorized natural gradient descent procedure. All these algorithms either need additional computations to solve an auxiliary linear system or are computationally intensive algorithms to directly estimate the inverse Fisher matrix.There have been attempts to exploit the fact that the RBM log-likelihood function is a difference of convex functions by modifying the standard difference of convex programming (DCP) approach to handle the stochasticity  #b30  #b22 . The stochasticdifference of convex functions programming (S-DCP) algorithm  #b30  uses only the first order derivatives of the log-likelihood and solves a series of convex optimization problems using constant step-size gradient descent method for a fixed number of iterations. The Stochastic proximal DC (SPD) algorithm  #b22  uses an additional proximal term along with the DC objective function and solves series of convex optimization problems. Unlike S-DCP, the SPD solves each subproblem to a certain level of accuracy (predefined). In order to achieve the required accuracy level large minibatch is used which significantly increases the computational cost  #b31 . However, the computational cost of S-DCP algorithm can be made identical to that of CD based algorithms with a proper choice of hyperparameters and is shown to perform well compared to other algorithms  #b30 .Motivated by the simplicity and the efficiency of the S-DCP algorithm, in this work, we modify the S-DCP algorithm using the diagonal approximation of the Hessian of the log-likelihood and propose a diagonally scaled S-DCP, denoted as S-DCP-D. Use of a diagonal approximation of Hessian essentially amounts to having an adaptive stepsize which is different for different parameters.We show that the diagonal terms of the Hessian can be expressed in terms of the covariances of the visible and hidden units and can be estimated using the same MCMC samples that are used to get the gradient estimates. Therefore, the additional computational cost incurred is small. Thus, the main contribution of the paper is a well-motivated algorithm (with small additional computational costs) that can automatically adopt the step-size (through the inverse of the diagonal approximation of the Hessian) to improve the efficiency of learning an RBM. Through empirical investigations we show the effectiveness of the proposed algorithm.The rest of the paper is organized as follows. In section 2, we briefly describe the RBM model and the maximum likelihood (ML) learning approach for RBM. We explain the proposed algorithm, S-DCP-D, in section 3. In section 4, we present simulation results on some benchmark datasets to show the efficiency of S-DCP-D. Finally, we conclude the paper in section 5.

BACKGROUND


RESTRICTED BOLTZMANN MACHINES
The Restricted Boltzmann Machine (RBM) is an energy based model with a two layer architecture, in which m visible stochastic units (v) in one layer are connected to n hidden stochastic units (h) in the other layer  #b27  #b7  #b9 . There are no connections from visible to visible and hidden to hidden nodes and the connections between the layers are undirected. An RBM with parameters θ represents a probability distribution (v,h;θ) is the normalizing constant which is called the partition function and E(v, h; θ) is the energy function. The energy function is defined based on the type of units, discrete or continuous. In this work, we consider binary units, i.e., v ∈ {0, 1} m and h ∈ {0, 1} n for which the energy function is defined asp(v, h|θ) = e −E(v,h;θ) /Z θ (1) where, Z θ = v,h e −EE(v, h; θ) = − i,j w ij h i v j − m j=1 b j v j − n i=1 c i h i(2)where, θ = {w ∈ R n×m , b ∈ R m , c ∈ R n } is the set of model parameters. Here, w ij is the weight of the connection between the i th hidden unit and the j th visible unit. The c i and b j denote the bias for the i th hidden unit and the j th visible unit, respectively.

MAXIMUM LIKELIHOOD LEARNING
One of the methods to learn the RBM parameters, θ, is through the maximization of the loglikelihood over the training samples. The log-likelihood, for a given training sample (v), is given by,L(θ|v) = log p(v|θ) = log h p(v, h|θ) g(θ, v) − f (θ) (3)where,g(θ, v) = log h e −E(v,h:θ) f (θ) = log Z θ = log v ′ ,h e −E(v ′ ,h;θ) .(4)The optimal RBM parameters can be found by solving the following optimization problem.θ * = arg max θ L(θ|v) = arg max θ (g(θ, v) − f (θ))(5)The above optimization problem is solved using an iterative gradient ascent procedure:θ t+1 = θ t + η ∇ θ L(θ|v)| θ=θ tThe gradient of g and f are given by  #b9  #b6 ,∇ θ g(θ, v) = − h e −E(v,h:θ) ∇ θ E(v, h; θ) h e −E(v,h:θ) = −E p(h|v;θ) [∇ θ E(v, h; θ)] ∇ θ f (θ) = − v ′ ,h e −E(v ′ ,h;θ) ∇ θ E(v ′ , h; θ) v ′ ,h e −E(v ′ ,h;θ) = −E p(v ′ ,h;θ) [∇ θ E(v ′ , h; θ)](6)where, E q denotes the expectation w.r.t. the distribution q. The expectation under the conditional distribution, p(h|v; θ), for a given v, has a closed form expression and hence, ∇ θ g is easily evaluated analytically. However, expectation under the joint density, p(v, h; θ), is computationally intractable since the number of terms in the expectation summation grows exponentially with (minimum of) the number of hidden units/visible units present in the model. Hence, sampling methods are used to obtain ∇ θ f .The contrastive divergence  #b9 , a popular algorithm to learn RBMs, uses a single sample (obtained after running a Markov chain for K steps) to approximate the expectation as,∇ θ f (θ) = −E p(v,h;θ) [∇ θ E(v, h; θ)] = −E p(v;θ) E p(h|v;θ) [∇ θ E(v, h; θ)] ≈ −E p(h|ṽ (K) ;θ) ∇ θ E(ṽ (K) , h; θ) f ′ (θ,ṽ (K) )(7)Here,ṽ (K) is the sample obtained after K transitions of the Markov chain (defined by the current parameter values θ) initialized with the training sample v. There exist many variations of this CD algorithm in the literature, such as persistent (PCD)  #b28 , fast persistent (FPCD)  #b29 ), population (pop-CD) (Oswin Krause, 2015), average contrastive divergence (ACD)  #b12  and weighted contrastive divergence (WCD)  #b17 . Another popular algorithm, parallel tempering (PT)  #b3 , is also based on MCMC. All these algorithms differ in the way they obtain representative samples from the model distribution for estimating the gradient. The centered gradient (CG)  #b19  algorithm also uses the same principle as that of CD algorithm to obtain the samples; however, while Algorithm 1 S-DCP update for a single training sample vInput: v, θ (t) , η, d, K ′ Initializeθ (0) = θ (t) ,ṽ (0) = v for l = 0 to d − 1 do for k = 0 to K ′ − 1 do sample h (k) i ∼ p(h i |ṽ (k) ,θ (l) ), ∀i sampleṽ (k+1) j ∼ p(v j |h (k) ,θ (l) ), ∀j end for θ (l+1) =θ (l) − η f ′ (θ (l) ,ṽ (K ′ ) ) − ∇g(θ (t) , v) ṽ (0) =ṽ (K ′ ) end for Output: θ (t+1) =θ (d)estimating the gradient it removes the mean of the training data and the mean of the hidden activations from the visible and the hidden variables respectively. This approach has been seen to improve the conditioning of the underlying optimizing problem  #b19 .As mentioned earlier, here we propose S-DCP-D which is a modification of the S-DCP algorithm  #b30 . The S-DCP approach is advantageous since a non-convex problem is solved by iteratively solving a sequence of convex optimization problems.

DIAGONALLY SCALED S-DCP (S-DCP-D)
The DCP  #b32  #b0  is an algorithm useful for solving optimization problems of the form,θ * = arg min θ F (θ) = arg min θ (f (θ) − g(θ))(8)where, both the functions f and g are convex and smooth but F is non-convex. It is an iterative procedure defined by,θ (t+1) = arg min θ f (θ) − θ T ∇g(θ (t) ) .(9)In the RBM setting, F corresponds to the negative log-likelihood function and the functions f, g are as defined in eq. (4).In the S-DCP algorithm, the convex optimization problem given by RHS of eq. (9) is (approximately) solved using a few iterations of gradient descent on f (θ) − θ T ∇g(θ (t) , v) for which the ∇f is estimated using samples obtained though MCMC (as in Contrastive Divergence). Thus, it is a stochastic gradient descent for the (convex) objective function f (θ) − θ T ∇g(θ (t) , v) for a fixed number of iterations (denoted as d). A description of this S-DCP algorithm is given as Algorithm 1. Note that, it is possible to choose the hyperparameters d and K ′ such that the amount of computation required is identical to CD(K) algorithm  #b30 .The S-DCP algorithm can be viewed as two loops. The outer loop is the iteration given by eq. (9). Each iteration here involves a convex optimization which is (approximately) solved by the inner loop of S-DCP through stochastic gradient descent (w.r.t. θ) on the convex function, f (θ) − θ T ∇g(θ (t) ).The proposed S-DCP-D is a scaling of this stochastic gradient descent by using the diagonal elements of the Hessian of this convex function.By using the above equations, the diagonal elements of the Hessian of f can be estimated simply by using the same MCMC samples used for gradient estimates. For a compact notation, the diagonal terms in ∇ 2 θ f can be written asDiag ∇ 2 θ f = −E p(v,h) [∇ θ E(v, h)] ⊙ 1 + E p(v,h) [∇ θ E(v, h)]where ⊙ represents element-wise multiplication, 1 represents vector of all ones and Diag(A) represents the vector consisting of the diagonal elements of matrix A.These estimates are used in obtaining the gradient descent updates (in the inner loop S-DCP) as,θ t+1 l = θ t l − η ∇ θ f (θ) − θ T ∇g(θ (t) ) l [H t + ǫI)] l θ=θ t(11)where [a] l represents l th element of vector a, I is the identity matrix of appropriate dimension, H t is the estimated Hessian at iteration t and ǫ is a small constant (and the term ǫI is added for numerical stability). A detailed description of the proposed algorithm is given as Algorithm 2.The inverse of the diagonal approximation of the Hessian essentially provides parameter-specific learning rates for the gradient ascent process. In case of S-DCP algorithm the objective function for the gradient descent is convex and the diagonal terms of the ∇ 2 f are greater than or equal to zero since f is convex. Therefore, inverse of the diagonal terms of the Hessian added with a small ǫ is numerically stable.Since the estimate of the gradient is noisy, the estimated Hessian is also noisy. Therefore, exponential averaging of the estimated Hessian is used to make the algorithm stable in terms of learning. LetAlgorithm 2 S-DCP-D update for a mini-batch of size N BInput: V = [v (0) , v (1) , . . . , v (NB −1) ], θ (t) , η, d, K ′ , ǫ Initializeθ (0) = θ (t) , V T = V for l = 0 to d − 1 do ∆θ = 0, G f = 0 for i = 0 to N B − 1 dõ v (0) = V T [:, i] → [i th column of V T ] for k = 0 to K ′ − 1 do sample h (k) i ∼ p(h i |ṽ (k) ,θ (l) ), ∀i sampleṽ (k+1) j ∼ p(v j |h (k) ,θ (l) ), ∀j end for ∆θ = ∆θ + f ′ (θ (l) ,ṽ (K ′ ) ) − ∇g(θ (t) , v (i) ) G f = G f +f ′ (θ (l) ,ṽ (K ′ ) ) V T [:, i] =ṽ (K ′ ) end for H f = G f NB ⊙ 1 − G f NB /* ⊙ represents element-wise multiplication */ θ (l+1) s =θ (l) s − η H fs +ǫ ∆θs NB , ∀s /* H fs is the diagonal element corresponding to θ s */ end for Output: θ (t+1) =θ (d)H t denote the ∇ 2 θ f calculated at iteration t as explained earlier. Let H t denote the Hessian that is used at iteration t for updating the weights. We calculate H t asH t = λ H H t−1 + (1 − λ H )H t(12)where λ H is a parameter that decides the memory of the exponential averaging. 

EXPERIMENTS AND DISCUSSIONS
In this section, we give a detailed comparison between the S-DCP-D and other algorithms, namely, centered gradient (CG)  #b16 , S-DCP, CD and PCD algorithms. The CG algorithm is essentially a CD(k) algorithm with additional centering heuristic which improves learning. Further, the objective here is to compare algorithms which have similar computational complexity and hence we do not consider algorithms which are significantly computationally expensive (SPD, H-F, etc).

THE EXPERIMENTAL SET-UP
We consider four benchmark datasets in our analysis, namely, a two-step procedure. In the first step, all the pixels in each row are set to zero or one with equal probability and then the pattern is rotated by 90 degrees with a probability of 0.5 in the second step. We have choose D = 3, for which we get 16 distinct patterns. We refer to MNIST, CalTech and the kannada-MNIST datasets as large datasets. The MNIST , CalTech 101 Silhouettes and the kannada-MNIST datasets have data dimension of 784.For the Bars & Stripes dataset, we consider three RBMs with 4, 8, 16 hidden units and for the large datasets, we consider RBMs with 500 hidden units. We evaluate the algorithms using the performance measures obtained from multiple trials, where each trial fixes the initial configuration of the weights and biases. The biases of visible units and hidden units are initialized to the inverse sigmoid of the training sample mean and zero, respectively. The weights are initialized to samples drawn from a Gaussian distribution with mean zero and standard deviation 0.01. We use 25 trials for the Bars & Stripes dataset and 10 trials for the large datasets. The mini-batch learning procedure is used and the training dataset is shuffled after every epoch. However, for Bars & Stripes dataset full batch training procedure is used. We learn the RBM for a fixed number of epochs and avoid using any stopping criterion. The training is performed for 5000 epochs for Bars & Stripes dataset (corresponding to 5000 gradient updates, due to full batch training) and 200 epochs for the MNIST dataset (corresponding to 60, 000 gradient updates due to the batch size of 200).We compare the performance of the proposed S-DCP-D with centered gradient (CG),S-DCP, CD and PCD. We keep the computational complexity (on each mini-batch) of S-DCP roughly the same as that of CD by choosing K, d and K ′ such that K = dK ′  #b30 . Since previous works stressed on the necessity of using a large K for CD based algorithms to get a sensible generative model  #b2  #b26 , we use K = 24 in CD (with d = 6, K ′ = 4 for S-DCP) for large datasets and K = 4 in CD (with d = 2, K ′ = 2 for S-DCP) for Bars & Stripes dataset. In order to get an unbiased comparison, we did not use momentum and weight decay for any of the algorithms. For the centered gradient algorithm, we use the Algorithm 1 in  #b16  which corresponds to dd b s in their notation. We use CD step size K = 24 and the hyperparameters ν µ and ν λ are set to 0.01. The initial value of µ is set to the mean of the training data and λ is set to 0.5.The learning rate and other hyperparameters for each algorithm is set to obtain the best performance by doing a grid search over a set of values of hyperparameters.

EVALUATION CRITERION
The performance comparison is based on the log-likelihood achieved on the training and test samples. For comparing the speed of learning of different algorithms, the average train log-likelihood is a reasonable measure. The average test log likelihood also indicates how well the learnt model generalizes. We show the maximum (over all trials) of the average train and test log-likelihood. The average test log-likelihood (denoted as ATLL) is evaluated as,AT LL = 1 N N i=1 log p(v (i) test |θ)(13)We evaluate the average train log-likelihood similarly by using the training samples rather than the test samples. For small RBMs the above expression can be evaluated exactly. However for large RBMs, we estimate the ATLL with annealed importance sampling  #b21  with 100 particles and 10000 intermediate distributions according to a linear temperature scale between 0 and 1.The evaluation in terms of the generative ability of the learnt models is carried out by observing the samples that they generate. We randomly initialize the states of the visible units and run the alternating Gibbs Sampler for 5000 steps (for large datasets)/200 steps (for Bars & Stripes dataset) and plot the state of the visible units.Overall, we use three evaluation criteria to show the effectiveness of the proposed S-DCP-D algorithm, specifically, i) speed of convergence ii) generalization (Average Test log-likelihood) and iii) generative ability (quality of the generated samples).

PERFORMANCE COMPARISON
In this section, we present experimental results to illustrate the performance of S-DCP-D in comparison with the other algorithms (CG, S-DCP, CD and PCD). The algorithms are implemented using Python and CUDAMat (A CUDA-based matrix class for Python bindings)  #b18 ) on a system with Intel processor i7 − 7700 (4 CPU cores and processor base frequency 3.60 GHz), NVIDIA Titan X Pascal GPU and 16 GB RAM configuration.In our results we show that the speed of learning, in terms of number of training epochs, exhibited by S-DCP-D is significantly higher compared to the other algorithms. As mentioned earlier, all three algorithms have comparable computational load (per minibatch) and hence comparison in terms of number of epochs would be similar to comparison in terms of actual running time. However, since the computations performed by the different algorithms are not identical, we need to understand difference in computational time per epoch of different algorithms as well. For this, we present below the actual computational time of different algorithms for a fixed number of epochs.The mean and standard deviation(σ) of the utilized system time in seconds, for 5000 epochs of learning for Bars & Stripes dataset and for 200 epochs of learning for large datasets, for each algorithm over 10 trials are shown in the table below. As can be seen from table 1, the computational time for S-DCP is 3% (for large datasets) more compared to that of CG. As mentioned earlier, by taking K = dK ′ we can make the computational time of these two algorithms nearly same. Compared to S-DCP, the time for S-DCP-D is about 8% more for Bars & Stripes and 24% more for MNIST/CalTech. The additional computation for S-DCP-D is calculating diagonal of Hessian and this grows linearly with m, n.In all results presented here we show evolution of ATLL with number of epochs for different algorithms. 2 As would be seen from the results, the S-DCP-D is faster in terms of number of epochs by much more than 25% thus justifying the claim that it results in efficient learning. In addition, on large datasets, the ATLL achieved by S-DCP-D is also larger. Fig. 1 shows the evolution of the mean and maximum ATLL achieved by the RBM with 4 hidden units, learnt for the Bars & Stripes dataset. (Note that here all patterns are used for training and hence there is no distinction between training and test data sets). As can be seen, the S-DCP-D has significantly higher speed of learning compared to S-DCP indicating the effectiveness of the parameter-specific learning rate induced by the diagonal scaling. It is also faster than CG, CD and PCD. This increased speed does not come at the expense of accuracy; the final ATLL of all algorithms is roughly same though S-DCP and CG take more epochs to converge. We observed similar behavior with RBM models having number of hidden units 8 and 16. The mean and maximum average log-likelihood over all the trials on the training and test set for MNIST dataset. Note that the learning rate for each algorithm is set to obtain the best performance.

BARS & STRIPES


LARGE DATASETS
evolution is smoother compared to S-DCP which suggests that the stability of the learning algorithm is improved by the parameter-specific learning rate employed. Further, the ATLL evolution in Fig.  2 indicates that the generalization ability of the model learnt using S-DCP-D is comparable to that learnt by the other algorithms. The maximum ATLL achieved by S-DCP-D is −87.1 which is comparable to the other algorithms. The provided maximum ATLL score for S-DCP matches with that reported in an earlier study in  #b30 . Also, the ATLL achieved by the learnt models are comparable to that of the VAE (Variational Autoencoder) and IWAE (Importance Weighted Autoencoder) models  #b1 . We observe a similar behaviour for the CalTech and kannada-MNIST datasets, as shown in Fig. 3 and 4 respectively. The performance of S-DCP-D is superior to that of S-DCP, CG, CD and PCD algorithms.The samples generated by the models learnt using MNIST dataset are given in Fig. 5. As observed from Fig. 5, the samples generated by S-DCP-D are sharp compared to those produced by CG based model. Also, it can be observed that the samples generated by CG and S-DCP-D are more diverse compared to those produced by S-DCP. We observed a similar behaviour for the CalTech and kannada-MNIST dataset. While subjectively the samples produced by S-DCP-D look better, it is important to note that there exist no objective measures to evaluate a generative model based on the quality of the generated samples.  

CONCLUSIONS
Learning an RBM is difficult due to the noisy estimates of the gradient of the log-likelihood obtained through an MCMC procedure. In this work we proposed an algorithm where we can automatically obtain different adaptive step-sizes for gradient descent for different parameters. This is done by using the inverse of the diagonal approximation of the Hessian. We showed that the Hessian of the log likelihood is given by covariances of the model distribution and hence the Hessian can be estimated using the same MCMC samples that are used for estimating the gradient. Thus, estimating the diagonal of the Hessian has only small additional computational cost.Through extensive simulations, we showed that the S-DCP-D results in a more efficient learning of RBMs compared to S-DCP and CG algorithms. The additional attraction in using the Hessian here is that in S-DCP-D the gradient descent in the inner loop is on a convex objective function. The diagonal scaling also seems to stabilize the learning and the resulting generative model seems to produce better samples as we showed empirically.It is known that learning of RBMs can be more efficient if the learning rate is reduced with iterations using a heuristically devised schedule. But the schedule has to be fixed through cross validation. The proposed approach automatically provides parameter-specific learning rates which makes the learning procedure both stable and efficient. The only hyper parameters of the proposed algorithm is ǫ which does not affect the learning dynamics much and is there only to control numerical underflows.The main attraction of S-DCP-D, in our opinion, is its simplicity compared to other sophisticated second-order optimization techniques which use computationally intensive algorithms to estimate the inverse of the Hessian.For learning an RBM, the centered gradient algorithms are shown to be better compared to CD(k) type algorithm. The reason is conjectured to be the similarity among the second order optimization algorithms and centered gradient method. We feel that the well-motivated and simple second-order algorithm proposed, namely S-DCP-D, can provide the correct platform to further explore this view of centered gradient algorithms.

Footnote
1 : statistically binarized as in(Salakhutdinov & Murray, 2008) 
2 : Since we do not employ any stopping criterion, we cannot give 'time taken to learn' for different algorithms; we can only show how log likelihood evolves with number of training epochs.