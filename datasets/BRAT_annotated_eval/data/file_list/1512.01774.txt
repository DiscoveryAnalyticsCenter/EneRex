Image reconstruction from dense binary pixels

Abstract


I. INTRODUCTION
The pursuit of smaller pixel sizes at ever increasing resolution in digital image sensors is mainly driven by the stringent price and form-factor requirements of sensors and optics in the cellular phone market. Recently, Eric Fossum proposed a novel concept of an image sensor with dense sub-diffraction limit one-bit pixels (jots)  #b0 , which can be considered a digital emulation of silver halide photographic film. This idea has been recently embodied as the EPFL Gigavision camera.We denote by x the radiant exposure at the camera aperture measured over a given time interval. This exposure is subsequently degraded by the optical point spread function denoted by the operator H, producing the exposure of the sensor λ = Hx. The number of photoelectrons e jk generated at pixel j in time frame k follows the Poisson distribution with the rate λj. A binary pixel compares the accumulated charge against a pre-determined threshold qj, outputting a one-bit measurement b jk . Thus the probability of a single binary pixel j to assume an "on" value in frame k is P(b jk = 1) = P(e jk ≥ qj). Our goal is to estimate an intensity field vectorx best predicting x given the measurement matrix B.In  #b1 , a maximum likelihood (ML) approach was proposed. Assuming independent measurements, the negative likelihood function can be expressed as(x; B) = const − kj log P (b jk | qj, λj),(1)In  #b1  this objective is minimized w.r.t x via standard iterative optimization techniques.

II. MAXIMUM LIKELIHOOD WITH SPARSE PRIOR
Since the ML approach assumes no prior, it needs a large amount of binary measurements in order to achieve good reconstruction. Sparsity priors had been shown to give state of the art results in denoising tasks in general, and particularly in low light Poisson noise  #b2 ,  #b3 . In this work, we show that by introducing a similar sparsity spatial prior the number of measurements can be decreased significantly. Assuming the light intensity λ admits a non-linear sparse synthesis model λ = Hρ(Dz), with the dictionary D and an element-wise non-linear transformation ρ such as the non-negativity enforcing function from  #b3 , we may construct the estimator aŝx = ρ(Dẑ), wherê z = arg min z (ρ(Dz); B) + µ z 1.(2)µ should be selected to best represent the tradeoff between the negative log-likelihood and the sparsity prior, in all experiments we selected µ empirically. The likelihood data fitting term is convex with a Lipschitz-continuous gradient (details are omitted due to lack of space), thus problem (2) can be solved using proximal algorithms such as FISTA  #b4 . Figures IV and IV show the significant improvement in image quality when using the sparse prior.  Training these parameters using standard backpropagation on a set of representative inputs allows the network to approximate the output of the underlying iterative algorithm with much lower complexity.

III. FAST APPROXIMATION
Iterative solutions of (2) typically require hundreds of iterations to converge. This results in prohibitive complexity and unpredictable input-dependent latency unacceptable in real-time applications. To overcome this limitation, we follow the approach advocated by  #b5  and  #b6 , in which a small number of ISTA iterations are unrolled into a feed-forward neural network, that subsequently undergoes supervised training on typical inputs. In our case, a single ISTA iteration can be written in the formzt+1 = σ θ zt − Wdiag(ρ (Qzt))H T ∇ λ (ρ(Azt); B) ,(3)where A = Q = D, W = ηD T , θ = µη1 (η is the step size used by ISTA) and σ θ is the two-sided shrinkage function. Each such operation may be viewed as a single layer of the network parametrized by A, Q, W, θ, receiving zt as the input and producing zt+1 as the output. Figure 1 depicts the network architecture, henceforth referred to as MLNet. When initializing the parameters as prescribed by the ISTA iteration and then adapting them by training that minimizes the reconstruction error of the entire network, the number of layers required to achieve comparable output quality on typical inputs is smaller by about two orders of magnitude than the number of corresponding ISTA iterations (see Figure IV). To the best of our knowledge, this is the first time a similar strategy is applied to reconstruction problems with a non-Euclidean data fitting term.IV. RESULTS Figure IV shows reconstruction results of an HDR image using ML with and without the sparse prior. FISTA was used to reconstruct overlapping 8 × 8 patches that were subsequently averaged. An overcomplete dictionary was trained using k-SVD  #b7 . Figure  IV shows reconstruction results of an emulated low-light image. Figure     Bounded reconstruction latency comparison. The plot shows the reconstruction quality for iterative algorithms (ISTA and FISTA) stopped after a given number of iterations, and the proposed MLNet with equivalent number of layers. As a reference, the performance of ML without the sparse prior is shown. Iteration 1 represents the initialization for all algorithms. MLNet produces acceptable output quality and is about two orders of magnitude faster than ISTA and FISTA. The use of sparse prior has a clear advantage over pure ML.