Curved Surface Patches for Rough Terrain Perception

Abstract


A B S T R A C T
Attaining animal-like legged locomotion on rough outdoor terrain with sparse foothold affordances -a primary use-case for legs vs other forms of locomotion -is a largely open problem. New advancements in control and perception have enabled bipeds to walk on flat and uneven indoor environments. But tasks that require reliable contact with unstructured world surfaces, for example walking on natural rocky terrain, need new perception and control algorithms.This thesis introduces 3D perception algorithms for contact tasks such as foot placement in rough terrain environments. We introduce a new method to identify and model potential contact areas between the robot's foot and a surface using a set of bounded curved patches. We present a patch parameterization model and an algorithm to fit and perceptually validate patches to 3D point samples. Having defined the environment representation using the patch model, we introduce a way to assemble patches into a spatial map. This map represents a sparse set of local areas potentially appropriate for contact between the robot and the surface. The process of creating such a map includes sparse seed point sampling, neighborhood searching, as well as patch fitting and validation. Various ways of sampling are introduced including a real time bio-inspired system for finding patches statistically similar to those that humans select while traversing rocky trails. These sparse patch algorithms are integrated with a dense volumetric fusion of range data from a moving depth camera, maintaining a dynamic patch map of relevant contact surfaces around a robot in real time. We integrate and test the algorithms as part of a real-time foothold perception system on a mini-biped robot, performing foot placements on rocks. Human locomotion considering a sparse set of footholds 1 Figure 2 Patch mapping and tracking on the RPBP biped robot . . A dense point cloud input from an RGB-D camera . . . . 10 Figure 5 Different types of error modeling for a 3D point cloud . 13 A paraboloid and a planar patch example . . . . . . . . . 22 Figure 10 Examples of all four different types of boundaries . . . . 28 Figure 11 Rock data acquisition, error ellipsoid estimation for each Input for the paraboloid patch fitting process . . . . . . . 33 Figure 13 Paraboloid patch fitting process . . . . . . . . . . . . . . . 34 Figure 14 Automatic fits for all patch types . . . . . . . . . . . . . . 38 Figure 15 The "side-wall" effect reparameterization . . . . . . . . . 38 Figure 16 Residual, coverage, and curvature patch validation . . . . 42 Figure 17 Bad residual due to an outlier sample point . . . . . . . . 42 Figure 18 Perturbed sample points for each patch type. . . . . . . . 44 Figure 19 Sorted residuals for 1000 random patches. . . . . . . . . . 45 Figure 20 Coverage evaluation for all types of patch models. . . . . 46 Figure 21 Cell intersection cases for all types of boundaries. . . . . 47 Figure 22 Curvature Background removal preprocessing . . . . . . . . . . . . . 59 Figure 27 Hiking saliency filtering . . . . . . . . . . . . . . . . . . . 60 Figure 28 Illustration of the Difference of Normals measure . . . . 61 Figure 29 Illustration of the Difference of Normal-Gravity measure 61 Figure 30 Illustration of the Distance to Fixation Point measure . . 62 Qualitative patch fitting and validation results . . . . . . 75 Figure 40 Patch size histogram comparison . . . . . . . . . . . . . . 76 Figure 41 The Comparison of histograms of salient measures for humanselected and automatically identified patches . . . . . . . 79 Figure 45 Camera pose with respect to the volume frame . . . . . . 82 Figure 46 KinectFusion The gravity vector from the IMU sensor . . . . . . . . . . 85 Figure 49 The frustum of the virtual birds-eye view camera . . . . 86 Figure 50 Bubble camera vs real camera ray casting . . . . . . . . . 87 Figure 51 Patch Mapping and Tracking . . . . . . . . . . . . . . . . 90 Figure 52 Kinematics specifications of the RPBP robot. . . . . . . . 93 Figure 53 Physical hardware of the RPBP robot. . . . . . . . . . . . 94 Figure 54 Software interface for the RPBP robot. . . . . . . . . . . . 96 Trained patches for foot placement . . . . . . . . . . . . . 99 Figure 58 The motion sequences for foot placement on four rocks . 100 

I N T R O D U C T I O N
In 1986, Daniel Whitney in his article "Real Robots Don't Need Jigs"  #b168  highlighted the need for redesigning robots to complete tasks in very unstructured environments, under significant uncertainty. Almost three decades later, robots have achieved high efficiency in well-structured environments like factories and labs, but still are not flexible enough to reliably deal with real-world tasks. Interest in uncertainty goes back to the beginning of robotics  #b88 , but only over the last few years have mobile manipulators (e.g.  #b135  #b103 ) and rough terrain robots (e.g.  #b84  #b125 ) started dealing with it efficiently, both in the environment and in their own state. The Fukushima Daiichi nuclear disaster in 2011 had a profound impact on robotics. Despite rapid advancements in actuation and control, robots were unable to directly replace humans in hazardous tasks, like climbing in a damaged nuclear plant, searching rubble piles after a disaster  #b55 , or operating in humantraversable rough terrain. Legged locomotion in uneven 3D terrain is a key aspect for completing these and similar tasks, because of the primary advantage of legs to efficiently negotiate highly faceted 3D trails with more flexibility and mobility than other forms of locomotion such as wheels or tracks.Recent advancements in control and perception have enabled bipeds to walk on flat  #b18  and uneven indoor terrains  #b99 . Major advances have also been made for outdoor quadrupeds and bipeds in rough terrain where the probability of blindly landing footholds is high  #b125  and uncertainty can be tolerated by low-level feedback control. Online footfall selection has been considered for quadrupeds and hexapods  #b75  #b4  #b72  #b121  #b60 , but still, to the best of our knowl- edge, no physical humanoid has previously been shown to walk autonomously on unmodeled sparse 3D terrain. New advances in both perception and control  #b67  are required; here we attempt to disentangle these two aspects to a degree and focus primarily on perception. The foothold selection problem is particularly interesting for bipeds with non-point feet that make contact with patches of terrain. Perception is one of the key enablers for finding such patches in the environment  #b65  #b5  #b16 . This brings us to our main hypothesis ( Figure 1):Main Hypothesis: Robots operating in many unstructured environments need to perceive sparse areas for potential contact. These can be detected and modeled using curved surface patches, and spatially mapped in realtime.

thesis outline and contributions
Sparsity of footholds for bipedal robots requires i) a model formulation for the local contact surface areas, ii) an online perception algorithm for finding them, iii) techniques for handling uncertainty and reliability, and iv) a method for creating a map of the detected local contact areas around the robot and localizing within it during motion. This thesis presents algorithms to address each of these four requirements. We have also developed and released the Surface Patch Library (SPL)  #b62  which contains the software implementations we used to evaluate the algorithms in our experiments. In Chapter 2 we describe the sensing system we are using for acquiring data from the environment. This includes both a range sensor that produces a set of 3D point clouds over time and an Inertial Measurement Unit (IMU) that gives the corresponding gravity vector. We also discuss uncertainty models for the input 3D sample points associated with the sensor, along with some types of point cloud filtering, including outlier removal and smoothing. We also introduce a way for calibrating the IMU sensor with respect to the range sensor to which it is attached.In Chapter 3 we describe the system for representing the environment. We introduce a set of 10 bounded curved-surface patch types (Figure 8 left,  #b161 ) suitable for modeling local contact regions both in the environment and on a robot. We present minimal geometric parameterizations using the exponential map for spatial pose both in the usual 6DoF case and also for patches with revolute symmetry that have only 5DoF. We then give an algorithm to fit any patch type to 3D point samples of a surface, with quantified uncertainty both in the input points (including nonuniform variance, common in data from range sensors) and in the output patch. We also introduce an algorithm for validating the fitted patch for fit quality and fidelity to the actual data -extrapolations (like hole-filling) which are not directly supported by data are avoided (  #b63 ).In Chapter 4 we define the notion of a volumetric working space around the robot and we describe the patch mapping system. A dynamic map of bounded curved patches fit randomly over an environment surface that has been sampled by a range sensor is developed. The mapping algorithm is divided into four main steps after data acquisition. The first is a data pre-processing step, where both a bilateral filter is applied to the cloud to reduce noise and a sample decimation filter for performance purposes. A bio-inspired saliency filter is also introduced for detecting points in a hiking-task scenario, so only relevant parts of the environment are considered for patch fitting. Recordings of human subjects traversing rough rocky trails were analyzed to give a baseline for target surface properties for foot placement. After filtering, the second step is the selection of seed points, where a random grid-based approach is introduced and applied to the filtered samples. Next is a neighborhood search around these points. Three different approaches for finding local neighborhoods were analyzed, which have different properties near surface discontinuities. The last step is to fit the pose, curvatures, and boundary of patches to the neighborhoods and validate them to quantify fit quality and to ensure that the patch is sufficiently representative of the actual data. We finally highlight the construction of a spatial map of the fitted patches around a robot.In Chapter 5 we present the patch tracking method that completes the whole Patch Mapping and Tracking system. For tracking the camera pose at each frame an adapted version of the Moving Volume KinectFusion  #b96  #b132  algorithm is applied. It is the first time that this camera tracking method is used for a bipedal locomotion application on physical hardware (Kinect Fusion without the moving volume algorithm is used in  #b126 , though in simulation only). We improve the original algorithm for our particular application both by using the gravity vector from the IMU to keep the local map in a pose aligned to gravity, and also by using a virtual camera, which lies above the robot looking down in the direction of gravity, for acquiring a point cloud from a synthetic birds-eye viewpoint during walking. In contrast to the original real camera raycasting method that considers upcoming surfaces only, the advantage of our virtual camera version is that the raycasting considers the environment around and under the robot's feet, even portions that were previously visible but currently occluded by the robot itself. In Chapter 6 we test the patch mapping and tracking system on a mini-biped robot platform developed in our lab called RPBP (Rapid Prototyped Biped). We ran two experiments. In the first one ( Figure 2, right) we test the system integrated on the robot independently of its control, making sure that shaking and vibration while the robot is walking do not hinder the tracking process. In the second one ( Figure 2, left) we first train the robot to place its foot on patches that were manually fitted on four different rocks. Then we let the robot, starting from a fixed position, detect patches in the environment and if any of them matches one of the trained patches it executes the corresponding foot placement motion. These experiments conclude the thesis, whose main contributions are as follows.

Contributions


1.
A new sparse environment surface representation using a set of bounded curved patches suitable for modeling local contact regions both in the environment and on the robot.

2.
Check whether the absolute difference between their boundary parameters are smaller than a threshold d s = 0.015m.

4.
Check whether the angle between their normal vectors (z axis) is smaller than a threshold a s = 20 • .

5.
Check whether the distance between their position t (translation vector) is smaller than a threshold r s = 0.01m.For checking the angle between the normal vector we should consider any possible symmetry. For all patch types except planar and circular paraboloids, we need in addition to compare the angle between the y axes using the same threshold a s , as well as the y axes of the patches rotated 180 • around their z rock 1 rock 2 rock 3 rock 4 Figure 57: The robot at the lookdown pose, in front of four rocks, with the trained patches fit at the contact areas where foot placement will take place.axes. If any of the patch in the map matches with a trained one we execute the corresponding motion sequence. We ran the experiment twenty times for each rock and the robot never failed to match the correct trained patch and successfully run the motion sequence for placing its foot every time. Success was defined as maintaining balance and Figure 58: The motion sequences for foot placement on the four rocks using the trained patches in Figure 57.ending with the foot on the rock. We also tried to place the robot in front of a few other rocks that it was not trained on and it stayed still, not having detected any patch match. An example is visualized in Figure 59 for the first rock, where the robot detects a match with the corresponding trained patch and executes the motion sequence. We also can see that the robot tracks very accurately a matched patch when it is moving. In the last step of the visualization the foot is placed in contact with the patch. Figure 59: RPBP detecting a patch on rock 1 for foot placement and proceeding with the predefined motion ( Figure 58, first row). The TSDF volume outline appears with the robot, the point cloud, the physical and virtual overhead camera frusta, and the patch at each step.

related work
The number of on-line perception systems for bipedal foot placement that are tested on real robots is very limited in the literature. We are not aware of work on a biped that includes perception for significantly curved surfaces like natural rocks. Though it is true that most current bipeds and humanoids have flat feet with limited ability to physically contact curved surfaces, this use case will become more important as more capable feet are developed  #b35 . Some work has been done for the case of flat surfaces. In a series of papers Okada et al.  #b100  #b101  $b102 ] developed a perception system for detecting flat surfaces and having various humanoids stepping on them. In their first paper a 16-DoF mini-humanoid that uses stereo vision data, detects planar segments that appear in various heights in front of the robot and performs a climbing step on them. In their second paper they apply the same plane detection system on the HOAP-1 humanoid robot for detecting floor regions, creating a local map of polygon patches that belong to the floor and step on them. In their third paper they apply their method on an HRP-2 robot for step climbing on horizontal flat surfaces. Gutmann et al.  #b37  #b147  #b38  have their QRIO robot detect and climb up and down on horizontal planar patches (mainly stairs) segmented using a point cloud. Chestnutt et al.  #b19  used 3D laser point cloud data for detecting horizontal obstacles of different heights in the environment and climb on and off, using their own prototype humanoid robot, while more recently in  #b99  HRP-2 was able to detect uneven flat surfaces with some slope and walk efficiently on them using dynamic ZMP-based walking motion. In a series of papers  #b83  #b50  $b82 ] and a thesis  #b49  the Humanoid Robots Lab in University of Freiburg has developed a system based on range sensing using a NAO robot for detecting flat obstacles and either step on or over them, using predefined motion primitives.

I N P U T D ATA
Both perceiving the environment around a robot (exteroceptive perception) and sensing the robot's own internal state (proprioceptive perception) are important aspects for driving planning and control actions in a real world scenario. Various perception sensors can be used for acquiring these important measurements (see  #b30  #b140 ). In this thesis we use both exteroceptive range sensing for detecting upcoming 3D terrain contacts from a distance and proprioceptive inertial measurement unit (IMU) sensing for acquiring the robot's orientation relative to gravity. In the next two sections we summarize the range and IMU sensors that provide the main inputs to our system.

range sensing
3D perception has gained a lot of interest over the last few years  #b133 , mainly because low cost but high quality range sensors are now commonly available. Stereo and structured light systems, time-of-flight cameras, and laser scanners produce clouds of 3D sample points of environment surfaces in real time. Here we focus on organized point cloud data in the form of an image grid acquired from a single point of view. Initially we take such images directly from a depth camera. Then in Chapter 4 a considerable level of indirection is added: the actual depth sensor images are fused (over space and time) into a volumetric model, from which a simulated sensor extracts virtual depth images for patch mapping. In this thesis either the Microsoft Kinect or the Primesense Carmine 1.09 (see Figure 3) have been used for acquiring 3D point clouds, depending on different experimental requirements (mainly involving range limits when the sensor is hand-held or on the mini-biped). Both the Kinect and the Carmine sensor consists of three parts: 1) an infrared (IR) projector, 2) an infrared (IR) camera, and3) an RGB camera. For estimating the 3D point cloud a triangulation method is applied using the IR emitter and detector that are separated by a baseline. As described in  #b13  #b68  #b141  #b69  in detail, given an image pixel with coordinates (u, v) and disparity d from triangulation, the corresponding 3D point (x, y, z) expressed in the camera frame is:z = f x b d (1) x = z f x (u − c x ) (2) y = z f y (v − c y )(3)using:(u, v, d) image pixel coordinates and disparity of the point (in pixels) (x, y, z) 3D point coordinates in camera frame (in physical units, e.g. m)b the baseline between IR camera and projector (in physical units) f x , f y IR camera focal length (in pixels)(c x , c y ) the principal point (in pixels)The origin of camera frame is the center of projection, the z axis points into the scene through the principal point (c x , c y ), the x axis points to the right in the camera image, and the y axis points down in the image. The 3D sample point coordinates (x, y, z) in camera frame can be also expressed as a function of the coordinates of the measurement ray direction vector m = (m x , m y , m z ) through pixel (u, v) and the range r of the data point along that vector as:[x y z] = [m x m y m z ] r(4)From the above equations, the backprojected 2D (u, v) pixel corresponding to an (x, y, z) 3D point can be calculated as: Using either of these two range sensors we receive 30Hz 640 × 480 RGB-D (red, green, blue, depth) data 1 . The structured light method used by the Kinect and Carmine does not work well in full sunlight, so when outdoor data were needed they were taken at twilight. Sunlight operation could be possible with other types of depth camera or stereo vision. Two point cloud examples of rocks along with their RGB images appear in Figure 4.u = xf x z + c x (5) v = yf y z + c y(6)

3D Point Cloud Uncertainty
A big challenge with range sensors is to quantify the uncertainty of the acquired data. The uncertainty could either be due to inaccuracies in the sensor system or due to triangulation errors (i.e. the correspondence problem  #b138 ) and it can be twofold; the data may include outlier points and noisy inliers.

Outlier Points
An outlier point is distant from the others and does not represent the underlying surface from where it was sampled. Ideally such points would be removed from the data. In many cases outliers appear along depth discontinuities due to occlusions, jumps in surfaces, or reflections. Veil points  #b91  #b146 , which are interpolated across a depth discontinuity, usually appear in data acquired by lidars (which we do not use in the thesis).There are various methods in the literature for detecting outliers. One simple approach is to consider as inliers only the points that have a minimum number of neighbors in a fixed distance. A heuristic has been introduced in  #b146  for finding points that belong to borders both in foreground and background and removing those in between as veil points. Other methods, for instance the one introduced in  #b134 , use statistical analysis for removing neighborhood points that are more than a fixed number of standard deviations away from the median. Similarly in  #b171  another statistical method is proposed to identify and remove outliers by checking for big residuals during plane fitting. When dealing with static environments either data fusion over time  #b96  #b132 , or outlier removal using octree raycasting as proposed in  #b12  can also be used.In this thesis we address outliers both in a preprocessing step where a realtime discontinuity-preserving bilateral filter removes some outliers from the data (Section 4.2), and also when Kinect Fusion is used (Chapter 5) for tracking and inherently ignores some outliers when data fusion over time is applied.

Noisy Inlier Points
A noisy inlier point deviates from the ground truth that represents the underlying surface. To express the data noise we use Gaussian modeling with 3 × 3 covariance matrices. Though this is not the only way to represent uncertainty, it does cover common situations 2 . There are various ways to estimate these covariance matrices, depending on the error model assumptions. Some assumptions can be the following ( Figure 5):• Constant Error (Figure 5-c): with constant nonnegative uncertainty k in range, independent of the sample range, and no uncertainty in pointing direction, the covariance matrix for a sample with measurement vector m is:Σ = kmm T(7)• Linear Error ( Figure 5-d): with a nonnegative factor k that scales uncertainty linearly with the sample range, and no uncertainty in pointing direction, the covariance matrix for a sample with range r and measurement vector m is:Σ = krmm T(8)• Quadratic Error (Figure 5-e): with a nonnegative factor k that scales uncertainty quadratically with the sample range, and no uncertainty in pointing direction, the covariance matrix for a sample with range r and measurement vector m is:Σ = kr 2 mm T(9)• Stereo Error (Figure 5-f): in Murray and Little's  #b95  two-parameter error model for stereo disparity uncertainty is represented by two nonnegative parameters σ p and σ m :σ p is the variance of the pointing error of the measurement vectors, represented as the variance in pixels of their intersections with the image plane at z = f x .σ m is the variance in the disparity matching error, also measured in pixels.The covariance matrix for a 3D point in physical units is:Σ = JEJ T(10)where:E =      σ p 0 0 0 σ p 0 0 0 σ m      and J =      b d 0 − bu d 2 0 b d − bv d 2 0 0 − f x b d 2     (11)b is the baseline (in physical units), d the disparity (in pixels), (u, v) the image pixel coordinates, and f x the IR camera focal length (in pixels).(a) (b) (c) (d)(e) (f) Figure 5: Different types of error modeling for a 3D point cloud. (a) Stereo range sensing sampling from a simulated surface (black paraboloid) along measurement rays (green) from a depth camera whose field of view is defined from the viewing frustum (pink); (b) The sampled 3D point data (blue dots) deviated from their original position by adding white Gaussian noise (using the stereo error model in (f)); (c) constant error modeling; (d) linear error modeling; (e) quadric error modeling; (f) stereo error modeling, visualizing the 95% probability error ellipsoids (pointing error exaggerated for illustration) Whether based on stereo or time-of-flight, range data exhibits heteroskedasticity (non-uniform variance) -typically there is much more uncertainty in range than aim  #b110  #b95 , the variance changes with range, and because the measurement rays usually have a single center of projection, the error ellipsoids for the sampled points are not co-oriented: each is elongated in the direction of its own measurement ray (Fig 5).Thus, to estimate range data covariances we apply the two-parameter pointing/disparity stereo error model proposed by Murray and Little in  #b95  (based on earlier work by others such as  #b90 ) to estimate input sample covariances Σ i . The error model parameters we used for the Kinect are σ p = 0.35px, σ m = 0.17px; the former is from  #b74 , the latter was determined experimentally fol-lowing [95] 3 .We use this error model when fitting patches to points sampled from a single sensor viewpoint (i.e. a single captured range image). In Chapter 5 we apply KinectFusion to the range data, which provides an alternate approach to handling inlier noise by averaging over many re-samplings of the same environment surfaces. In some cases we also use either discontinuity-preserving bilateral  #b155  or median filters  #b52  to reduce noise effects:• Median Filter The median filter replaces the central pixel of a fixed size window in the image with the median inside the window. The method can be very efficient  #b116  and effective for reducing noise and removing outliers from the data, while preserving discontinuities.• Bilateral Filter The bilateral filter is similar to the median filter with the difference that central pixel's neighbors are weighted making the filter non-linear  #b106 .

3D Point Cloud Filtering
There are various other filtering methods for the acquired point clouds serving different purposes  #b133 . Some used in this thesis are the following:• Passthrough: The passthrough filter removes points whose specified properties (e.g. x,y,z-coordinates, intensity, etc) are outside of some limits.• Radius Outlier Removal: Removes outliers by checking the number of points in a predefined radius neighborhood.• Decimation: Decimates the points by a given factor, discarding rows and columns of pixels in the image, e.g. a factor of 2 will discard all the even rows and columns.• Lower Resolution: Lowers the resolution by a given factor by block averaging, e.g. a factor of 2 will replace each 2-by-2 submatrix with its average value. It is similar to the median filter, but the latter can be more robust to outliers.• Voxel Grid: The approximate voxel grid filter downsamples the cloud by creating a 3D voxel grid and replacing all the points in each voxel with their centroid. This method leaves the point cloud unorganized. Some fast approximations have been introduced  #b133  to improve the efficiency of this filter.

inertial measurement unit (imu)
The use of proprioceptive Inertial Measurement Units (IMUs) for sensing the direction of gravity is very useful for locomotion. Using a CH Robotics UM69-DoF IMU mounted on the top of our range sensors ( Figure 3), we receive 100Hz IMU data spatiotemporally coregistered with the 30Hz RGB-D data received from the depth sensor. Though an IMU can also sense velocities and accelerations, in this work we use only the gravity direction as input to our algorithms. In this thesis temporal registration of the RGB, depth, and IMU datastreams is based on timestamps, and is approximate because the underlying operating systems used were not hard real-time. Spatial registration of the RGB and depth data is based on manufacturer hardware calibration and image warping implemented in the hardware driver. Spatial registration of the depth and IMU data uses a custom calibration algorithm described next.

IMU Calibration
Calibration is required for calculating the rotation transform that gives the orientation of the UM6 relative to the range sensor. Given a dataset of depth images of a flat horizontal surface that includes a dominant plane (e.g. a flat floor) and the corresponding UM6 orientation data, the gravity vector is calculated for each depth image in the UM6 coordinate frame from the UM6 orientation data. We pair each gravity vector with the corresponding one in the depth camera coordinate frame, which is estimated as the downward facing normal of the dominant plane. For all these pairs of gravity vectors we solve the orthogonal Procrustes problem  #b28  that gives the UM6 to Camera transform ( Figure 6). : IMU calibration instances for 4 different frames, where the gravity vector (yellow) and the ground plane normal (magenta) appear before (left) and after (right) calibration. Before calibration the two vectors have some angle difference between them, but after calibration they are nearly on top of each other.

Perception and Sensing
Much research on locomotion focuses on control or path planning and assumes known terrain models or uses motion capture systems to extract information about the robot and its position with respect to the terrain (e.g.  #b24 ). Other systems, such as  #b20 , use only proprioceptive sensors for driving locomotion actions. For an autonomous real world task, where there is no prior information about the environment, driving actions from high-level-but quantitativeperception using exteroceptive sensors is essential. Tactile sensors are helpful only when actual contact is taking place. For a priori information about the surface, range sensing is required. There are systems that only use color cameras  #b33  and others that use laser scanners  #b99 , stereo  #b150 , or time-of-flight  #b130  cameras to extract depth data. Several other walking robots have used depth or RGB-D sensors, as we do, including stereo vision on QRIO  #b38 , Xtion on NAO  #b83 , and depth camera on HRP-2  #b99  #b126  #b17 . Since sensors are noisy and the uncertainty of the measurements is high, perception using range sensing and IMU is a very challenging task, but it is rapidly advancing  #b133 .

Uncertainty Representation
The importance of representing 3D range data uncertainty has been considered at least since the 80's  #b90 , where 2D  #b34  and 3D  #b40  #b15  normal distributions were used, as well as additive zero mean Gaussian noise modeling  #b31  for 3D stereo measurements. In  #b90  non-Gaussian noise was considered for errors in the non-linear triangulation operation, which are approximated with 3D Gaussian distributions, while later in  #b57  a cylindrical Gaussian distribution centered at the 3D point and oriented along the measurement ray was used for modeling the stereo error uncertainty. In  #b23  uncertainty was modeled in the depth measurement using ellipses  #b42 . Tasdizen and Whitaker  #b151  assumed a Gaussian distribution for representing the depth noise with zero angular error. Gaussian modeling is not the only way to represent 3D point cloud uncertainty. Pauly, Mitra, and Guibas  #b114  considered the point cloud as a result of a stochastic process corrupted by zero-mean additive noise to come up with a likelihood and a confidence map for the data. Closed form variance formulations  #b2  and non-Gaussian distributions  #b105  are also alternative ways to represent the uncertainty of the range data. Recently an uncertainty model for the Kinect sensor has been introduced  #b68  #b141  #b69 , while a mixture of Gaussians has been used in  #b25 .

summary and future work
In this chapter we presented experiments with our patch mapping and tracking system integrated on a real mini-biped for foot placement. Using a simple control system where the walking motion is a human trained combination of sequences, we first train the robot to place its foot on four different patches and then we let the robot create a patch map of its environment, find matches between the patches and the trained ones, and if any exist run the predefined sequence motion. A more advanced dynamic walking system requires balance control, possibly using the Zero Moment Point method  #b58  and appropriate path planning. For our experiments we assume that there are no collisions between the leg and any surface while executing the motion. The problem of generating collision-free motions is well studied (e.g.  #b50 ) and we are not con-sidering it for these experiments, though it would be required in practical applications. Various other directions are possible for experimental validation of the patch mapping system on the robot, for instance a comparison between our perception algorithm with a proprioceptive blind robot that tries to complete the same task  #b160 . It is also necessary to consider different types of foot for curved surfaces contact. In our experiments we used flat feet, but a different design for a better contact, possibly using miltiple toes and/or compliance, may be preferable  #b35 .

E N V I R O N M E N T R E P R E S E N TAT I O N
Some of the most challenging open problems in robotics are those which require reliable contact with unstructured world surfaces when locomoting (Figure 7 right). To enable rough-terrain walking and climbing, a perception system that can spatially model and finely quantify potential 3D contact patches may be needed. Contact is well-studied (e.g.  #b89 ) but, arguably, there is not yet any accepted general system for modeling the shape and pose of potential contact surface patches, including both patches on the robot (e.g. finger tips, foot soles, etc) and also in the surrounding environment. This is especially true when (a) curved, bounded patches with (b) geometrically meaningful minimal parameterizations and (c) quantified uncertainty are desired (Figure 7    Why curved patches? Our interest is legged locomotion on large rocks. Flat areas can be rare in such natural environments. More broadly, contact surfaces in man-made environments are also often curved-railings, doorknobs, steering wheels, knobs, etc. Though curved surfaces can be approximated by sets of smaller planar patches  #b158 , the job can often be done with fewer and larger curved patches. Curved surface geometry is more complex, but it may still be an advantageous trade-off to reason about fewer and larger patches. For example, a spherical robot foot stepping into a divot on a rock might be modeled as the interaction between just one spherical and one elliptic paraboloid patch (on foot and rock, respectively). If the surfaces were approximated using collections of smaller planar patches the interaction could require combinatorial reasoning about many possible contacting pairs.By "geometrically meaningful minimal parameterizations" we mean that each patch is defined by the fewest possible parameters, and that these have direct geometric interpretations-rotations, translations, curvatures, lengths, and angles. Geometric (vs. algebraic) parameterizations also support reasoning  #b22  about possible actions with patches, and allow some representation of spatial uncertainty with geometric error ellipsoids. Minimality is desirable because redundant (non-minimal) parameterizations can slow the numerical optimizations used in surface fitting  #b36  and must be handled specially in uncertainty modeling  #b110 .It is often important to get both a best estimate of patch parameters and a quantification of the uncertainty therein. We develop full uncertainty quantifications based on Gaussian modeling with covariance matrices as were described in Chapter 2, by propagating the input 3D point cloud uncertainty  #b92  #b154  to the output patch. Though in this thesis we use dense volumetric depth map fusion for mapping (Chapter 4), we also intend our models to be usable in sparse Kalman-type SLAM (Simultaneous Localization and Mapping  #b143  #b142  #b26 ) algorithms that maintain a dynamic local patch map, Figure 7, of contact patch features around a robot. Such a map could potentially include both environment surfaces and contact pads on the robot itself, which may themselves be potentially uncertain due to kinematic error. We first give the details of the patch models for representing the environment in local contact areas, followed by an algorithm to fit and validate a patch to noisy point cloud data from common types of range sensor. This fitting is the main step in using patches to represent surface shape and pose. We also demonstrate the algorithms in experiments with simulated and real range data. More experiments are presented in Chapters 4 and 6 in practical contexts including humans walking on rocky natural terrain and a biped robot walking near and stepping on rocks.

patch modeling
In  #b161 , we introduced a general-purpose set of ten curved and flat patch types ( Figure 8, Table 1) suitable for both natural and man-made surfaces and balancing expressiveness with compactness of representation. Eight come from the general second-order polynomial approximation to a smooth surface at a given point-the principal quadric-which is always a paraboloid, possibly degenerated to a plane  #b118 . We add two non-paraboloid types to better model common man-made spherical and cylindrical surfaces, and we pair each surface type with a specific boundary curve to capture useful symmetries and asymmetries. Each patch is parametrized using extrinsic and intrinsic parameters (see parametric surfaces in  #b94 ) for its shape and spatial pose. Concave variants shown inset.

Extrinsic and Intrinsic Surface Parameters
An instance of a patch will be a vector of real parameters which define both its shape (curvature and boundary) and its 3D rigid-body pose. We call the former intrinsic and the latter extrinsic parameters  #b144 . We must consider different issues to achieve minimal parametrization for each, and the distinction also enables the option to model shape (intrinsic) and pose (extrinsic) uncertainty separately. Minimal intrinsic parametrization for the proposed patches will be given by (a) one parameter for each variable curvature, and (b) a minimal parametrization of the boundary curve. However, minimal extrinsic parametrization depends on the continuous symmetry class of the patch. For example, a patch with two different curvatures ( Figure 9 left) has no continuous symmetry: its rigid   But a planar patch with a circular boundary (Figure 9 right) has a continuous rotation symmetry and only five extrinsic DoF. Remarkably, it has been shown that there are exactly seven continuous symmetry classes in 3D  #b144 : revolute, prismatic, planar, spherical, cylindrical, helical, and general (the first six correspond to the lower kinematic pairs; the last represents no continuous symmetry). Since we only consider patches with boundaries, we need only the general (no continuous symmetry, 6 DoF pose) and revolute (one continuous rotation symmetry, 5 DoF pose) classes-continuous translation symmetries are not possible for bounded patches.

Pose Representation with the Exponential Map
We require two extrinsic parameterizations: one with six parameters for asymmetric patches and one with five parameters for patches with revolute symmetry. It is well known that, because the Lie-manifold of the special orthogonal group SO(3) (the rotation subgroup of SE (3)) is non-Euclidean, there is no singularity-free minimal parametrization of SE  #b2 . For the general 6-DoF case we thus select a minimal parametrization with singularities that are easiest to handle for our application. One of the core computations will be patch fitting by iterative optimization, and for this Grassia showed in  #b36  that a useful pose representation is 1[r T t T ] T ∈ R 6 with (r, t) ∈ R 3 × R 3(12)where t is a translation and r is an orientation vector giving an element of SO (3) via an exponential map. Grassia observed that in this parametrization singularities are avoidable by a fast dynamic reparameterization, reviewed below. We use Rodrigues' rotation formula for the exponential map R(r) :R 3 → SO(3) ⊂ R 3×3 (Grassia used quaternions): R(r) = I + [r] × α + [r] 2 × β (13) θ r , α sin θ θ , β 1 − cos θ θ 2 r = r x r y r z , [r] × 0 −r z r y r z 0 −r x −r y r x 0 .Despite division by θ = r , (13) converges to I as θ → 0. For numerical stability we use the series expansion approximations α ≈ 1 − θ 2 /6 and β ≈ 1/2 − θ 2 /24 for small θ (e.g. for θ 4 machine precision). As promised, the (r, t) representation has a direct geometric interpretation: t is just a translation, and (wlog for θ = 0) θ gives the right-hand-rule rotation angle about the spatial axis defined by the unit vector r/θ. While exponential map approaches are not new  #b14  #b107 , matrices in se(3) ⊂ R 4×4 , the Lie algebra of SE(3), are typically used instead of (r, t). Though elegant, the former do not satisfy our goals of minimal parametrization and direct geometric interpretation.  $b2  Using the fact that counterclockwise rotation by θ is equivalent to clockwise rotation by 2π − θ, Grassia's reparametrization converts any r into a canonical 3 one with r π:θ θ mod 2π, r            0 if θ = 0 rθ /θ if 0 < θ π r(θ − 2π)/θ otherwise.(14)r represents the same rotation as r, but stays away from the singularity surfaces where θ is a multiple of 2π.Algebraically, (r, t) corresponds to an element  R(r) t 0 T 1   of SE(3), a 4 × 4 homogeneous rigid body transform, and can thus define the pose of a local coordinate frame L (and a patch therein) relative to a world frame W: R(r) is a basis for L and t is its origin. The transformation of a point q l in L to q w in W, and the reverse, are familiar functions X f,r :R 3 × R 3 × R 3 → R 3 q w = X f (q l , r, t) R(r)q l + t (15) q l = X r (q w , r, t) R(−r)(q w −t) = R(r) T (q w −t)(16)where (16) makes use of the inverse transform(r, t) −1 (−r, −R(−r)t) = (−r, −R(r) T t).(17)Equations (12-16) constitute our 6 DoF pose parametrization. For the 5 DoF case, observe that only one of the three basis vectors of L need be specified; rotation symmetry allows the others to make any mutually orthogonal triple.Only two DoF are required, equivalent to specifying a point on the unit sphere. We do this by re-using (12-16) with r z fixed at 0:(r xy , t) ∈ R 2 × R 3 corresp. ([r T xy 0] T , t) ∈ R 3 × R 3 .(18)The geometric interpretation of (r xy , t) is the same as for (r, t), except that r xy is constrained to the xy plane. In some contexts we may want to calculate an r xy ∈ R 2 that induces a local coordinate frame with the sameẑ l as a given r ∈ R 3 . For any given canonical r, a canonical r xy always exists that satisfiesR([r T xy 0] T )ẑ = R(r)ẑ with [xŷẑ] I 3×3 .(19)r xy can be calculated asr xy (r) =  x T y T        r if θ xy ≈ π (ẑ ×ẑ l )/α xy otherwise(20)z l R(r)ẑ, θ xy atan2( ẑ ×ẑ l ,ẑ Tẑ l ), α xy sin θ xy θ xyAs in Brockett's product of exponentials  #b14 , 6 DoF poses can be composed to make any kinematic chain. Let(r n , t n ) φ n , . . . , (r 1 , t 1 ) φ 1 with φ i ∈ {+1, −1}(21)be the poses (equiv. transforms) in the chain from end to base in order from right to left. Then the pose (r c , t c ) of a patch attached to the end of the chain relative to the base is(r c , t c ) = (r(R n · · · R 1 ), (X n • · · · •X 1 )(0))(22)R j R(φ j r j ), X j (q)      X f (q, r j , t j ) if φ j = +1 X r (q, r j , t j ) if φ j = −1substituting r xy (r c ) for 5 DoF patches, and using the log map r(R) : SO(3) → R 3 corresponding to the inverse of (13). We give an algorithm to compute r(R) in Appendix A.2.We will need the partial derivatives of (16)∂q l ∂q w = R T , ∂q l ∂r = ∂R T ∂r (q w −t), ∂q l ∂t = −R T , R R(r)(23)the Jacobian of (13)-including its use as part of ∂q l /∂r in (23)-and the Jacobians of  #b20  and (22):∂R ∂r , ∂r xy ∂r , ∂(r c , t c ) ∂(r 1 , t 1 ), . . . , (r n , t n ) .The latter three are given in Appendix A.1.

Patch Models
We now present the details of ten surface patch models ( Figure 8, Table 1) based on seven curved surface types. Five of these partition the paraboloids, including the important degenerate case of a plane; the other two add true spherical and circular cylinder patches, non-paraboloids that are common in man-made environments and on robots. For non-planar surfaces we select one specific parametrized boundary shape which trims the surface into a local patch. For planes we allow a choice of four boundary shapes. The next two sections give the details of the paraboloid and non-paraboloid patch models. This particular system is not the only possible taxonomy; it reflects our design choices in an attempt to balance expressiveness vs minimality.

Paraboloids
The best-fit degree-two local polynomial approximation to any smooth surface S ⊂ R 3 at a given point t ∈ S, called the principal quadric, is always a paraboloida quadric of one sheet with a central point of symmetry about which the surface has two independent curvatures κ x , κ y in orthogonal directions (Figure 9 left). These are the principal curvatures of S at t, and t is the symmetry point. Defininĝx l andŷ l as unit vectors in the directions of the principal curvatures in the tangent plane to S at t, the surface normal to S at t isẑ l x l ×ŷ l . If S is considered to be embedded in a world coordinate frame W, then t ∈ R 3 is the origin andR [x lŷlẑl ]is a basis for the principal coordinate frame (all standard terms) of S at t, which we also call local frame L 4 . Using the log map, the transform(r, t) (r(R), t)takes points from L to W, enabling a short derivation of equations for a general paraboloid parametrized by k [κ x κ y ] T , r, and t. Starting in L where the paraboloid is in standard position, with p li : R 3 ×R 2 → R and p le :R 2 ×R 2 → R 3 , 0 = p li (q l , k) q T l diag([k T 0] T )q l − 2q T lẑ (24) q l = p le (u, k) [xŷ]u + 1 2 u T diag(k)uẑ(25)are the implicit and explicit forms for the surface equation, respectively, with q l ∈ R 3 a point on the patch in L and u ∈ R 2 parameters of the explicit form.Moving to q w ∈ R 3 in world frame W is accomplished by composing  #b24  #b25  with  #b15  #b16 , yielding0 = p wi (q w , k, r, t) p li (X r (q w , r, t), k) (26) q w = p we (u, k, r, t) X f (p le (u, k), r, t)(27)p wi : R 3 ×R 2 ×R 3 ×R 3 → R, p we : R 2 ×R 2 ×R 3 ×R 3 → R 3 .Note that in this formulation u is always the projection of q l onto the local frame xy plane:u Π xy q l = Π xy X r (q w , r, t), Π xy [xŷ] T .(28)In the general case κ x = κ y , giving 7 or 8 DoF paraboloids-6 pose DoF plus up to two curvatures (boundary parameterizations will add DoF). Six DoF pose is required because κ x = κ y implies no continuous rotation symmetries, only discrete symmetries about t. It is standard to separate three surface types where κ x = κ y ( Figure 8): elliptic paraboloids have two nonzero curvatures with equal signs, hyperbolic paraboloids have two nonzero curvatures with opposite signs, and cylindric paraboloids have one nonzero curvature. In all casesẑ l is the outward pointing surface normal and positive/negative curvatures correspond to concave/convex directions on the patch, respectively 5 . Boundaries t d x d x d y d y (a) (b) (c) (d) t t t d y d y d x d x d c d c d c d c0 e(u, d e ) u T diag([1/d 2 x 1/d 2 y ])u − 1.(29)For cylindric paraboloid patches, replace the ellipse boundary with an axis Figure 10 (a)). In the xy plane of L the vertices arealigned rectangle with half-widths d r = [d x d y ] T (v 1 d r , v 2 [−d x d y ] T , v 3 −v 1 , v 4 −v 2(30)in counterclockwise order, and the bounding condition can be stated as, with q :R 2 × R 2 × R 2 × R 2 × R 2 → R, 0 q(u, v 1 , v 2 , v 3 , v 4 ) (31) max(l(u, v 1 , v 2 ), l(u, v 2 , v 3 ), l(u, v 3 , v 4 ), l(u, v 4 , v 1 ))where l : R 2 × R 2 × R 2 → R is the implicit form for a 2D line given two points on the line; u is on or to the left of the directed line throughv i towards v j iff 0 l(u, v i , v j ) (u − v i ) T [v j − v i ] ⊥ ,   x y   ⊥   y −x   .(32)For the special case κ x = κ y we identify two more surface types ( Figure 8): circular paraboloids have both curvatures equal and non-zero, and planes have both curvatures zero. Both of these have continuous rotation symmetry about z l , so we use the 5-DoF pose parametrization (r xy , t), provided that the patch boundary also has the same continuous rotation symmetry. The latter holds for circular boundaries, which we use for circular paraboloids ( Figure 10 (d)). Let κ be the surface curvature and d c the bounding circle radius; circular paraboloids are then defined by  #b24  #b25  #b26  #b27  #b28  #b29  with k = [κ κ] T , r = [r T xy 0] T , d e = [d c d c ] T ,and with the dimensions of the function domains correspondingly reduced.For the important case of paraboloids degenerated to planes we give a choice of four boundary types: ellipses, circles, rectangles, or general convex quadrilaterals (developed next). For all except circles, the planar patch loses its continuous rotation symmetry and requires full 6-DoF pose parametrization; the patch is defined by  #b24  #b25  #b26  #b27  with k = 0 (and correspondingly reduced function domains) and either  #b29  or  #b31 . Planar patches with circular boundary are the same as circular paraboloids but with k = 0.For convex quadrilateral boundaries, keep t at the intersection of the diagonals v 1 v 3 and v 2 v 4 ( Figure 10 v i d i [cos φ i sin φ i ] T (33) φ 1 γ, φ 2 π − γ, φ 3 π + γ, φ 4 −γ 0 < γ < π/2.Then the quadrilateral is defined by (31) using vertices (33) parametrized byd q [d 1 d 2 d 3 d 4 γ] T .Convexity is ensured by construction, and only five parameters are needed even though a general planar quadrilateral has 8 DoF-the remaining three (a rotation about the plane normal and two in-plane translations) are contributed by the extrinsic pose.

Spheres and Circular Cylinders
Spheres and circular cylinders are common on robots and in man-made environments. Though still quadrics, neither is a paraboloid, suggesting two additional patch types ( Figure 8). (We do not model complete spheres or cylinders, only bounded patches of hemispheres and half-cylinders.) Again starting in local frame L, the implicit and explicit equations of an upright hemisphere with apex at the origin and curvature κ (hence possibly infinite radius |1/κ|) are 6 , with s li : R 3 × R → R and s le :R 2 × R → R 3 , 0 = s li (q l , κ) κq T l q l − 2q T lẑ , 0 κq T lẑ 1 (34) q l = s le (u, κ) [xŷ]u+(ẑ/κ) 1− 1−κ 2 u T u .(35)Composing these with  #b15  #b16  gives the world frame forms s wi :R 3 × R × R 2 × R 3 → R, s we : R 2 × R × R 2 × R 3 → R 3 0 = s wi (q w , κ, r xy , t) s li (X r (q w , [r T xy 0] T , t), κ) (36) 0 = s we (u, κ, r xy , t) X f (s le (u, κ), [r T xy 0] T , t).(37)Circular half-cylinder surfaces are similar but (a) have no dependence on x l and (b) require 6 DoF pose:0 = c li (q l , κ) q T l Kq l − 2q T lẑ , 0 κq T lẑ 1 (38) q l = c le (u, κ) [xŷ]u+(ẑ/κ) 1− 1−κ 2 u T Yu (39) K diag([0 κ κ] T ), Y [0 1] T [0 1] 0 = c wi (q w , κ, r, t) c li (X r (q w , r, t), κ) (40) 0 = c we (u, κ, r, t) X f (c le (u, κ), r, t).(41)

Boundaries
To maintain revolute symmetry we use circular boundary for spherical patches:u must satisfy  #b29  with d e = [d c d c ] T and |κ|d c 1. For circular cylinder patches we use rectangular boundary: u must satisfy  #b30  #b31  with |κ|d y 1.

patch fitting
Having defined the patch models, it is natural to consider recovering contact surface areas in the environment by fitting bounded curved patches to noisy point samples with quantified uncertainty both in the inputs (the points) and the outputs (the patch parameters), which is not a trivial problem 7 ( Figure 11). Though linear least squares (LLS) can fit a quadric surface to points  #b22 , and its extension to linear χ 2 maximum likelihood fits data corrupted by white noise, the problem appears to become non-linear when the points are heteroskedastic (i.e. have nonuniform variance). Also, we want to fit bounded paraboloids, spheres, and circular cylinders, not just unconstrained quadrics. In  #b161  we give a non-linear fitting algorithm which handles these issues. It is based on a variation of Levenberg-Marquardt iteration that fits a bounded curved patch to a set of 3D sample points. The algorithm minimizes a sum-ofsquares residual by optimizing the patch implicit and explicit parameters. The residual for an individual sample point is computed by scaling the value of the implicit form by the inverse of a first-order estimate of its standard deviation, which is derived in turn from a covariance matrix modeling the sensor uncertainty for the point.Next we describe the patch fitting algorithm. Elliptic, hyperbolic, circular, and cylindrical paraboloid as well as planar patches are fitted automatically depending on the detected curvatures of the underlying surface. The non-paraboloids (cylindrical and spherical patches) are fitted only if requested. Also, similarly, for planar paraboloids, the type selection for the boundary curve is only partially automatic -rectangles and convex quads are only used if requested.

Patch Fitting Algorithm
The inputs are ( Figure 12):• N sample points q i ∈ R 3 with covariance matrices Σ i ∈ R 3×3 (positive semi-definite)• the general surface type to fit 8 s ∈ {parab, plane, sphere, ccyl}• the boundary type b ∈ {ellipse, circle, aarect, cquad} if s = plane 9• a boundary containment probability Γ ∈ (0, 1]The outputs are:• the fitted patch type (s, b)• parameters p ∈ R p , where p is the DoF of the patch type ( Table 1)• covariance matrix Σ ∈ R p×pThe algorithm proceeds in 2 stages (9 total steps), which include heuristics for avoiding local minima when solving the non-linear system. The first three steps fit an unbounded surface; the rest are largely concerned with fitting the boundaries, which can include final resolution of the patch center and orientation (in steps 6 and 9) where the bounding shape breaks symmetries of the underlying surface. An illustration of the whole fitting process for a simulated paraboloid can be shown in Figure 13. 

Stage I: Fit an Unbounded Surface
Step 1: Plane fitting Input: r xy , t, Σ r xy ,t ∈ R 5×5 (from the WLM fitting) Output: Σ r xy ,t ∈ R 5×5 Let r = [r xy 0],q = avg(qi ) = 1 N N i=1 q i t =q −ẑ T (q − t)ẑ =q − (ẑ T q)ẑ + (ẑ T t)ẑ ẑ = R(r)ẑThe propagated covariance is: Σ r xy ,t = JΣJ T ∈ R 5×5(76)0 5×5 0 5×5 Σ r xy ,t    ∈ R 11×11(78)Σẑ = JΣ r J T , with J = ∂R ∂rẑ ∈ R 3×3 Σq = 1 N 2 N i=1 Σ i , with Σ i point's q i covariance matrixStep 2: Surface Fitting If Equation (45) is enabled for the side-wall effect, then the input Σ is replaced by JΣJ T , where J = I 11×11 with J(6 : 8, 6) =n p .

The side-wall effect problem and flipping patch normals towards viewpoint
When the neighborhood points don't have a central symmetry then they may be unevenly distributed in the patch if left unconstrained. We call this the sidewall effect (Fig. 15). To handle this, in  #b63  we introduced a constrained fitting where the center of the patch t ∈ R 3 must lie on the line through the centroid t p of the neighborhood parallel to the normaln p to an initial fit plane. This is implemented as a reparameterization during the WLM incorporated in Step 2 t = t p + an p (45) Figure 14: Automatic fits (red) for both paraboloid and non-paraboloid (lower right) patch types with requested elliptic boundary in simulated noisy range samples, using Kinect projection and error models.where a ∈ R is the new patch parameter replacing t. Note that this constrained fitting affects the error propagation (see Appendix A.3).The "outward" facing direction of the patch surface normal, which is the same as the local frame basis vectorẑ , is ambiguous globally in the point cloud. But considering that the the data were acquired from a single point of view v p then the following equation should be satisfied: Figure 15: The reparameterization in Eq. (45) keeps the fitted paraboloid centered on the data (right). This prevents the "side-wall" effect (left) and helps ensure good coverage, but can compromise the Euclidean residual.z · (v p − t) > 0(46)If not we flip the patch by rotating it's local frame basis π aroundx and flipping its curvatures.

Weighted Levenberg-Marquardt
Levenberg-Marquardt (LM) is a standard iterative non-linear optimization  #b124 . It can find a parameter assignment p opt ∈ R p that locally minimizes the sum-ofsquares residual r of a differentiable objective function f : R d × R p → R applied to a dataset q i ∈ R d , 1 i N, starting from an initial estimate p 0 . That is, it findsp opt = argmin p near p 0 r, r N i=1 e 2 i , e i f(q i , p).(47)Implementations typically take as inputs functions f and ∂f/∂p, the data q i , and p 0 , and return both p opt and a covariance matrix Σ ∈ R p×p representing its uncertainty. A well known extension is to replace e i with E i e i /σ i where σ i > 0 are constant standard deviations modeling uncertainty in e i . The residual is then called χ 2 , and p opt locally maximizes the likelihood of the "observations" e i .For our use, f is always the implicit form of a surface in world frame, i.e. p wi , s wi , or c wi  #b26  #b36  #b40 . The σ i are not constant, but can be estimated with with first order error propagation as 11σ i = var(f(q i , p)) v f (i, p) (48) v f (i, p) ∂f ∂q (q i , p) Σ i ∂f ∂q (q i , p) T .We define weighted LM (WLM) to combine σ i and f into a meta-objective function F : [1 . . . n] × R p → R:F(i, p) f(q i , p)/σ i = f(q i , p)/ v f (i, p).(49)Both F and its gradient ∂F/∂p are implied given q i , Σ i , f, ∂f/∂p, ∂f/∂q, and∂ 2 f/∂p∂q (which is d × p): ∂F ∂p (i,p) = ∂f ∂p (q i ,p) σ i − e i ∂f ∂q (q i ,p)Σ i ∂ 2 f ∂p∂q (q i ,p) σ i v f (i,p) .(50)Given q i , Σ i , f, ∂f/∂p, ∂f/∂q, and ∂ 2 f/∂p∂q, WLM synthesizes F and ∂F/∂p by  #b49  #b50  and then applies LM. This is simplified further by the common implicit form of the world-frame surfaces  #b26  #b36  #b40 , which are all variants off l (q l , k 3 ) q T l Kq l − 2q T lẑ (51) f w (q w , p s ) = f l (X r (q w , r, t), k 3 ) (52) p s [k T 3 r T t T ] T , k 3 [κ x κ y κ z ] T , K diag(k 3 )where some components of k 3 , and for (36) the last component of r, are held at zero. The required derivatives of (52) are given by the chain rule from (23) and derivatives of (51) (using R R(r)):∂f w ∂q w = ∂f l ∂q l ∂q l ∂q w , ∂f l ∂q l = 2(q T l K−ẑ T ), ∂q l ∂q w = R T (53) ∂f w ∂p s = ∂f w ∂k ∂f w ∂r ∂f w ∂t , ∂f w ∂k = q T l diag(q l )(54)∂f w ∂r = ∂f l ∂q l ∂q l ∂r , ∂q l ∂r = ∂R T ∂r (q w − t) ∂f w ∂t = ∂f l ∂q l ∂q l ∂t , ∂q l ∂t = −R T ∂ 2 f w ∂p s ∂q w = ∂ ∂p s ∂f w ∂q w T = ∂ ∂k 3 ∂f w ∂q w T ∂ ∂r ∂f w ∂q w T ∂ ∂t ∂f w ∂q w T (55) ∂ ∂k 3 ∂f w ∂q w T = 2R diag(q l ), ∂ ∂t ∂f w ∂q w T = −2RKR T ∂ ∂r ∂f w ∂q w T = 2 ∂R ∂r (Kq l −ẑ) + 2RK ∂R T ∂r (q w − t)

Experimental Results
We tested the fitting algorithm both in real data from a Kinect viewing a rock and in simulation ( Figure 11). For this initial experiment we implemented a simple interactive segmenter to manually select neighborhoods to fit patches in the 3D point cloud of the rock. In Chapter 4 we present algorithms for automatic neighborhood finding. We used the two-parameter pointing/disparity stereo error model proposed by Murray and Little in  #b95  to estimate input sample covariances Σ i for all experiments (as described in Chapter 2).The results show that the algorithm can produce reasonable curved-surface patch models for local parts of non-flat environment surfaces. Average times for our Matlab implementation are ∼20ms to fit n ≈ 50 sample points on a commodity workstation, while in a C + + implementation we reached ∼0.6ms to fit n ≈ 50 sample points. SVD computations within LM are quadratic in n, though runtime also depends on the LM convergence rate.Note: In the rest of this thesis we will consider fitting only paraboloid patches, unless otherwise indicated. Paraboloids are complete in that they form an approximation system for local regions on any surface with zero,one, or two nonzero local principal curvatures.

patch validation
After fitting a patch to a set of point cloud data it is important to evaluate it, because it may fit the data but still not faithfully represent the surface. In  #b63  we introduced three measures based on the residual, coverage, and curvature. Residual and curvature evaluate the surface shape, while coverage evaluates the boundary of the patch. (see Figure 16).

Residual Evaluation
The patch residual measures the deviation between the sample points and the (unbounded) patch surface. The residual can be bad either due to outliers (see Figure 17) or due to local minima in the WLM process. We use the root-meansquare error (RMSE) Euclidean residual ρ between the sample points q i and their corresponding closest points p i on the patch 12 (which must be calculated for each q i )ρ = RMSE({q}, {p}) = N i=1 q i − p i 2 N .(56)Whereas the patch fitting algorithm uses an algebraic residual for speed, ρ is a Euclidean residual and gives its result in the same physical units as the input data (e.g. meters)  #b122 , enabling it to be compared to a meaningful threshold. However, calculating the p i for each q i can be computationally expensive. We use a technique based on Lagrange multipliers  #b27 .When κ x ≈ κ y ≈ 0 the paraboloid surface was fitted as a plane, so p i = (I −ẑẑ T )q i , i.e. p i is the projection of q i onto the xy plane of L. Otherwise p i is characterized as: minp i satisfying (51) q i − p i .(57)Define a Lagrange function Λ asΛ(p i , λ) = (q i − p i ) T (q i − p i ) + λ(p T i Kp i − 2p T iẑ ).(58)with Lagrange gradient constraints∇Λ(p i , λ) = 0 T ⇔ ∂Λ/p i = [0 0 0] and ∂Λ/λ = 0.(59)Expand the first gradient constraint from (59)−2q T i +2p T i +λ(2p T i K−2ẑ T ) = [0 0 0] −q i +p i +λ(Kp i −ẑ) = [0 0 0] T (I + λK)p i = q i + λẑ p i = (I+λK) −1 (q i +λẑ)(60)and substitute 13 for p i in the second gradient constraint, which is the same as  #b51 . This leads to a fifth degree polynomial in λ, for which there is at least one real solution because imaginary solutions come in pairs. To solve the polynomial, we can either compute the eigenvalues of the companion matrix or we use Newton iteration. For Newton's method an initial root guess for λ (and thus for p i ) is required. We pick as p i the point projected from q i along the z -axis (local frame). Newton's method appears to be ∼ 50 times faster than Eigendecomposition in tests with around ∼ 1000 sample points per patch.Finally, backsubstitute 14 the real solution(s) in  #b60  and find the minimum as in  #b57 .

Residual Approximations
The problem of the Euclidean residual estimation is well studied, both for exact and approximate solutions; for instance Taubin's first and second order approximations  #b152  #b153 , the 3L algorithm  #b10 , the constrained minimization method  #b0 , and MinMax  #b43  have been proposed.We implemented three approximations. The simplest approximation is to consider the vertical distance in the local zˆ -axis of the patch. The other two are  $b13  The inverse of the diagonal matrix in (60) is evaluated symbolically and then denominators are cleared after substitution in (51), avoiding any issue of non-invertability or division by zero. 14 Division by zero can occur during this backsubstitution when q i is on a symmetry plane or axis, but alternate forms can be used in those cases. 

Residual Thresholds
To determine the residual threshold T r such that any patch with ρ > T r will be dropped, we sorted all residuals ( Figure 19) for a sampling of 1000 random patches (r = 0.1m, k-d tree neighborhoods), 100 on each of 10 rock datasets  #b63 . The value T r = 0.01m was selected to include approximately 95% of the patches. In general T r can be set in an application dependent way. Furthermore, the choice of RMSE residual is not essential. For example, an alternate residualρ alt = max q i − p i(61)could be used to check if any small surface bumps protrude more than a desired amount from the surface.individual patch residual patches sorted by increasing residual 0.005 Figure 19: Sorted residuals for 1000 random patches (see text), approximately 95% of which are below 0.01.

Coverage Evaluation
A different evaluation is needed to take into account the patch boundary. A patch may fit the data but still not faithfully represent the neighborhood, either because too many sample points are outside the patch boundary or there is too much area inside the boundary that is not supported by data points. (Unlike  #b73 , we opt not to speculatively fill holes in the data.) To detect these cases we generate an axis-aligned grid of fixed pitch w c on the xy plane of the patch local frame L. We generate only the required number of rows and columns in this grid to fit the projection of the patch boundary.Define I c and O c to be the number of data points whose xy projections are both inside a given cell c and respectively inside or outside the projected patch boundary. Define A i to be the area of the geometric intersection of the cell and the projected patch boundary, which will be detailed below. The cell is considered bad iffI c < A i w 2 c T i or O c > (1 − A i w 2 c )T o .(62)for thresholds T i and T o . Here we fix these thresholds relative to the expected number of samples N e in a given cell if all samples were in-bounds and evenly distributed:T i = ζ i N e , T o = ζ o N e , N e k/N p , N p A p w 2 c ,(63)where k is the number of sample points in the neighborhood and A p is the area of the patch approximated as the area inside the projected boundary. The patch fails coverage evaluation iff there are more than T p bad cells. After some experiments in fitting paraboloid patches with neighborhood radius r = 0.1m, we set w c = 0.01m, ζ i = 0.8, ζ o = 0.2, and T p = 0.3N p . Figure 20 illustrates patches that pass and fail coverage evaluation.

Intersection Area for Ellipse and Circle Boundaries
For an ellipse boundary with radii a, b, or for the degenerate case of a circle boundary with radius r = a = b, we compute the intersection area with a secant approximation since the exact computation involves a relatively expensive inverse trig function. Wlog we describe only the top right quadrant (Fig. 21,  left); the other three are symmetric. Let p 0...3 be the four corners of a grid cell in counter-clockwise order starting from the lower left. The algorithm for computing the intersection area is:1. If p 2 is inside the ellipse then A i = w 2 c 2. else if p 0 is not inside the ellipse then A i = 0 3. else if p 1 is inside the ellipse then if p 3 is inside the ellipse then A i = A 1 else A i = A 2 4. else if p 3 is inside the ellipse then A i = A 3 5. else A i = A 4 . A 1 = (x b − x 0 )w c + (x c − x b )(Y(x b ) − y 0 )+ ((x c − x b )(y 0 + w c − Y(x b )))/2(64)A 2 = (x c − x 0 )(Y(x c ) − y 0 )+ (x c − x 0 )(Y(x 0 ) − Y(x c ))/2(65)A 3 = (y c − y 0 )(X(y c ) − x 0 )+ (y c − y 0 )(X(y 0 ) − X(y c ))/2(66)A 4 = (X(y 0 ) − x 0 )(Y(x 0 ) − y 0 )/2 (67) X(y) a 1 − y 2 /b 2 , Y(x) b 1 − x 2 /a 2 x b X(y 0 + w c ), x c x 0 + w c , y c y 0 + w c [x 0 , y 0 ] T p 0Intersection Area for Axis-Aligned Rectangle BoundaryAs above we consider only the top right quadrant (Fig 21, middle). Let the rectangle half-lengths be a, b, and define [x 0 , y 0 ] T p 0 . The exact intersection area can be computed as follows:1. If p 2 is inside the rect then A i = w 2 c 2. else if p 0 is not inside the rect then A i = 0 3. else if p 1 is inside the rect then A i = A 5 = w c (b − y 0 ) 4. else if p 3 is inside the rect then A i = A 6 = w c (a − x 0 ) 5. else A i = A 7 = (a − x 0 )(b − y 0 ).

Intersection Area for Convex Quadrilateral Boundary
To handle the case of a general convex quadrilateral (Fig. 21, right), we use the fact that the intersection between a convex quad and a rectangle is always convex:1. Find the set of grid cell corner points that are inside the quad and viceversa.

Curvature Evaluation
Residual and coverage evaluation may still not be enough. There may be cases where both residual and coverage checking passes, but the bounded patch does not represent the data correctly. This may happen either when the point cloud data form a very curved surface or when the the LM non-linear fitting gets stuck in local minima as appears in Figure 22 in yellow. A patch fails curvature evaluation iff its minimum curvature is smaller than a threshold κ min,t or its maximum curvature is bigger than a threshold κ max,t . We set this threshold experimentally to κ min,t = −1.5max(d) and κ max,t = 1.5max(d), where d is the patch boundary vector. More fitting and validation experimental results are presented in Section 4.7.

Modeling
Modeling the environment and detecting potential contact surface areas around a robot using exteroceptive sensing is a common task, but still very challenging, especially for locomotion in uncertain environments. The approach explored here contrasts with the traditional study of range image segmentation, which also has a significant history  #b47 , where a partition of non-overlapping but potentially irregularly bounded regions is generated producing a dense labeling of the whole image. In image segmentation, some work has been done with curved surfaces  #b123 , but the main focus still appears to be on planes  #b56  #b165  #b38  #b110 . Many prior systems typically use dense approaches in that they attempt to model all of the terrain in view. Some are grid based  #b3 , like those on Ambler  #b75 , Dante II  #b4 , and Mars Exploration Rovers (MER)  #b84  using laser scanners or stereo cameras to build an elevation map, find obstacles, and quantify traversability. Usually these don't model detailed 3D contact features, though some attempt to recover surface models  #b121 . A few other works do take a sparse approach but are restricted to planar patches, ranging from large flats in man-made environments  #b165  #b38  #b110  down to small "patchlets"  #b95 .We proposed to only map a sparse set of patches. Also, our patch-based approach can homogeneously model contact surfaces both in the environment and on the robot itself, whereas most prior work considers modeling environment surfaces exclusively. Irregularly bounded regions, which may be very large or non-convex, can present a challenge for higher-level contact planning algorithms which still need to search for specific contact areas within each region. One aim of using regularly bounded regions of approximately the same size as relevant contact features on the robot is to trade potentially complex continuous searches within patches for a discrete search across patches. Fewer and larger paraboloid patches can give a comparable fidelity of representation as the many small planar patches needed to cover a curved environment surface  #b158 . Of course other parts of the robot may also make contact in unintended ways. The patch model could help plan intentional contacts while other data structures are simultaneously used for general collision prediction  #b145  #b117  #b17  #b50 .One challenge in modeling is dealing with missing data. In  #b72  texture synthesis was presented to deal with the problem of occluded terrain by filling in the missing portions. Our approach avoids representing such missing areas where uncertainty is high. As we described in Chapter 5 we instead integrate multiple range scans taken from different perspectives (as the robot moves) to fill in missing areas with new observations using a volumetric fusion approach  #b96  #b132 .

Fitting
One of our main results  #b161  is an algorithm to fit curved, bounded patches to noisy point samples. Fitting planes is well studied  #b162 , including uncertainty  #b61  and fitting heteroskedastic range data  #b110 . For curved surfaces quadrics are a natural option; Petitjean  #b118  surveyed quadric fitting, but there were few results that (a) quantified uncertainty, (b) recovered geometric parameterizations, and (c) fit bounded patches. In  #b22 , Dai, Newman, and Cao describe recovery of paraboloid geometric parameters 15 by linear least squares, without considering uncertainty. In  #b163  Wang, Houkes, Jia, Zhenga, and Li studied quadric extraction in the context of range image segmentation, including quantified uncertainty in the algebraic (not geometric) patch parameters, but not on the input points, while in  #b166  superquadrics are fit using Levenberg-Marquardt considering variance in the range data. Our fitting algorithm quantifies both input and output uncertainty and recovers geometric parameters of bounded patches.

PAT C H M A P P I N G
Having introduced in Chapter 3 a new surface model for contact patches between a robot and local areas in the environment and algorithms to fit and validate patches to 3D point cloud data, algorithms to find potentially useful patches and spatially map them relative to the robot are now presented. Patches are sparsely fit using the following five-stage approach 1 ( Figure 23):Stage I: Acquire Input Data from a Depth Camera and IMU (Section 4.1).

Stage II: Preprocess the Input Point Cloud (Section 4.2).


Stage III: Select Seed Points on the Surface (Section 4.3).


Stage IV: Find Neighborhoods of Seed Points (Section 4.4).


Stage V: Fit & Validate Curved Bounded Patches to the Neighborhoods (Section 4.5).
These functions dovetail with the patch tracking algorithms in Chapter 5 to maintain a spatially and temporally coherent map (Section 4.6 of up to hundereds of nearby patches as the robot moves through the environment. After acquiring RGB-D and IMU data from the sensors (Stage I), preprocessing (Stage II), like background removal, decimation, or saliency filtering, can be applied depending on the application. Seed points (Stage III), and neighborhoods (Stage IV) are found, and finally patches are fit and validated to the neighborhoods (Stage V). The neighborhood size r is set to a fixed value derived from the size of the intended contact surface on the robot 2,3 . Using this algorithm a spatial patch map is defined in Section 4.6.  Before describing the details of patch mapping we first introduce the notion of local volumetric workspace (or simply the volume), which will be extensively used from now on.

Local Volumetric Workspace
When a robot moves in the environment, it constantly acquires new 3D point clouds and IMU data frames, typically at about 30Hz for the former and 100Hz or more for the latter. Keeping all this information (even after fusion) to remove redundancies creates a huge amount of data over time, affecting both the performance of any downstream algorithm applied to them and memory requirements 4 . Moreover, in many tasks, such as a biped robot locomoting on a rough terrain, the robot only needs to know an area around it for local 3D contact planning. Thus, it is natural to consider only the (potentially fused) data in a moving volume around the robot. Though there are several potentially useful definitions for such a volume, here we define it as a cube with a volume coordinate frame at a top corner ( Figure 24). The y axis of the volume frame points down (and may be aligned to the gravity vector derived from the IMU data) and the x and z axes point along the cube edges forming a right-handed frame. We use the cubic volume model and this definition of the volume frame so that our local volumetric workspace is the same as the TSDF 5 volume in moving volume KinectFusion [132] that we will use in Chapter 5. At any time t the volume is fully described by: 1) its size V s (a constant), and 2) its pose relative to the camera with the following 4 × 4 rigid body transformationC t =   R t t t 0 1  (68)where R t is the rotation matrix and t t the translation vector that transforms from the camera to the volume frame at time t. The volume pose relative to the environment may change as the robot moves around using one of the following policies:1. fv (fixed volume): The volume remains fixed in the physical world.2. fc (fixed camera): Holds the camera pose fixed relative to the volume by applying 3D rigid transformations to the volume pose when the camera has moved beyond a distance c d or angle c a threshold. Note that the thresholds can be specified as infinite, resulting in volume rotations or translations only, respectively.3. fd (fix down then forward): Rotates the volume first to keep the volume frame y-axis direction parallel to a specified down vector (which may be 5 Truncated Signed Distance Function e.g. the gravity vector from the IMU), then holding the volume frame yaxis fixed, rotate the volume about it to align the z-axis as close as possible to a specified forward vector (e.g. the camera's z-axis vector). The volume is also automatically translated to keep the camera at the same point in volume frame.4. ff (fix forward then down): Does the same transformations as fd but in the opposite order. In both cases the camera location remains fixed in the volume but the volume orientation is driven from specified down and forward vectors.In Chapter 5 we review how KinectFusion can track the camera pose C t relative to the volume.

input data acquisition
The first stage of the algorithm is about acquiring the input data in each frame. Chapter 2 describes in details this process that can be wrapped up in the following two steps. Step 1: Receive image Z from the depth camera and absolute orientation quaternionq from the IMU. The depth camera may either be a physical sensor like Kinect or Carmine described in Chapter 2, returning 640 × 480 images, or a virtual camera in the context of KinectFusion (see Chapter 5) which typically has a lower resolution, e.g. 200 × 200.In the later case the virtual camera may also have a different pose in the volume than the physical camera.Step 2: Convert Z to an organized 6 point cloud C in camera frame andq to a unit gravity vectorĝ pointing down in camera frame.

point cloud preprocessing
Various types of preprocessing and filtering on the point cloud input may be applied depending on the task and the application requirements. Some are related to the quality of the input data and some to performance. In Section 2.1 we introduced some of these general filters, but apart from these we may have task-specific ones. In this Section we introduce some preprocessing filters we developed for the rough terrain hiking task. Note that we do not apply filtering that is not close to real-time performance (i.e. 30Hz). Also, when filters remove points, we actually replace them with NaN 7 values to maintain the organization of the point cloud with 1:1 correspondence to an M × N depth image, which is important for some later steps, like an optimized algorithm for finding neighborhoods (Section 4.4).

Stage II: Preprocess the Input Point Cloud


Hiking Saliency Filter
In  #b64  we introduced a real-time bio-inspired system for automatically finding and fitting salient patches for bipedal hiking in rough terrain. A key aspect of the proposed approach is that we do not just fit as many patches as possible, but instead attempt to balance patch quality with sufficient sampling of the appropriate parts of upcoming terrain.The term saliency has been used in computer graphics (e.g.  #b98  #b78  #b80  #b164 ) to describe parts of surfaces that seem perceptually important to humans. Often these are locations of curvature extrema. Such a definition may also be relevant here, as humans do sometimes step on e.g. the peak of a rock. However, this seems relatively uncommon. We thus introduce three new measures of saliency that relate to patches that humans commonly select for stepping and can be quickly applied in a point cloud to find good neighborhoods for fitting patches: Difference of Normals (DoN), Difference of Normal-Gravity (DoNG), and Distance to Fixation Point (DtFP). These measures involve aspects of patch orientation and location. The approach is bio-inspired both in that one of these relates to a known biomechanical property-humans tend to fixate about two steps ahead in rough terrain  #b86 -and also because we used observations of the patches humans were observed to select as a baseline for setting parameters of the measures. 

Difference of Normals (DoN)
The difference of normals operator was introduced in  #b76  as the angle between the normals of fine scale vs coarse scale neighborhoods of a point (Figures 28  and 29)  $b8  . This value relates to the irregularity of the surface around the point, and also to the local uniqueness of the point (following the same idea as the difference of Gaussians operator in 2D images). We conjectured that points with low DoN may be salient for the purpose of footfall selection. The coarse scale neighborhoods we use are of radius r = 10cm and the fine scale are r/2 (for this and the next measure square neighborhoods are actually used to enable fast normal computation with integral images, see the algorithm below). 

Difference of Normal-Gravity (DoNG)
The angle between the r-neighborhood normal vector of each point and the reverse of the gravity vector −ĝ (from the IMU,ĝ points down) gives a measure of the slope of that area ( Figure 29). For fairly obvious reasons, points with low DoNG can be considered more salient for footfall selection. 

Distance to Fixation Point (DtFP)
Various biomechanical studies on vision for human locomotion (e.g.  #b86  #b87  #b85 ) find that humans fixate approximately two steps ahead when locomoting in rough terrain. We thus estimate a spatial fixation point near the ground approximately two steps ahead of the current position ( Figure 30). We define points with smaller Euclidean distance from the fixation point to have higher saliency. We now present the algorithm for calculating these three measures. They can be calculated quickly for all points and so are useful to identify good seed points before fitting.

DtFP saliency
Parameters l d = 1m, l f = 1.2m are the distances down and forward from the camera to the estimated fixation point (l d is the approximate height at which we held the camera; l f is an approximation of two human step lengths  #b87 , minus the approximate distance from the body to the camera as we held it); parameter R = 0.7m can be adjusted to capture the ground area to be sampled for upcoming steps. 

DoN and DoNG saliency
Parameter r = 10cm is the patch neighborhood radius, which can be adjusted to match foot contact geometry; f is the focal length of the depth camera in pixels; φ d = 15 • and φ g = 35 • are DoN and DoNG angle thresholds estimated from human-selected patches (Section 4.7.2).6.3 Compute surface normals N, N s corresponding to D using integral images  #b46 . The normal N(i) uses window size 2rf/Z(i) where Z(i) is the z coordinate (depth) of point i in camera frame, and N s (i) uses window size rf/Z(i).

6.4
Remove from H all points i for which N(i) T N s (i) < cos(φ d ).

6.5
Remove from H all points i for which −N(i) Tĝ < cos(φ g ).The same integral image algorithm used for fast normal estimation can also produce "surface variation" values  #b115  which are often related to local curvature, but this relation depends on the input and is not guaranteed. We thus defer considering patch curvature for task-specific saliency until after patch fitting, which does give estimates of the true principal curvatures (see Sec. 4.5).

seed selection
The selection of seed points around which patches will be fit is an important step in the algorithm. We use uniformly random seed selection in H relative to a coarse grid imposed on the xz (horizontal) plane in volume frame. We split the volume frame xz-plane into V g × V g grid cells ( Figure 31). We typically use V g = 8. The reason for splitting the space into grid cells is to sample the whole space more uniformly with seed points. Using a random number generator only for selecting uniformly random points will not achieve the same effect since the density of the point cloud depends on the distance from the camera. We next randomly pick up to n g points from each cell for a total of n s seed points. We experimented with a non-maximum suppression algorithm  #b119  instead of random subsampling, using a weighted average of the DoN and DoNG angles. However the results were not clearly preferable. Depending on real-time constraints we may use only a subset of the seeds. We thus order the cells with respect to their distance from the projected camera position onto the volume frame xz plane and we use the seeds in order of increasing distance until a time limit is reached. As an option to limit the number of patches per cell, we can also ignore any new seed points for a cell that already has n g patches fitted to seeds within it. Note that if the volume moves in the physical space following one of the moving volume policies introduced above the cloud remains in the same position and the seed points need to be remapped to new cells in the volume. This remapping may move some prior seeds or patches out of the volume -they will be removed from the map as described in Chapter 5. It may also remap more than n g patches into a cell 9 ; the extra patches can be culled if desired. Figure 31 (right) illustrates n s = 31 seeds with the volume divided into an 8 × 8 grid (V g = 8) and one seed point per grid (n g = 1) was requested. The seed selection proceeds as follows.

Stage III: Select Seed Points on the Surface
Let n g be the max number of seed points per grid cell.Step 7: Split the volume frame xz-plane into V g × V g grid cells.Step 8: Project each point in cloud H onto the xz-plane and find the cell it falls in by transforming the points from camera frame to volume frame and then setting their z coordinate to 0.Step 9: Project the camera location on the xz-plane and order the cells in increasing distance of their center to the projected camera point.Step 10: For each grid cell in order of increasing distance from the camera, randomly select new seed points from H until at most n g seeds are associated to the cell.

neighborhood searching
Having an ordered list of seed points, the next step is neighborhood searching in the original point cloud C for each of them. Many methods have been introduced for finding local neighborhoods of 3D points, including approximations. Two concepts of a neighborhood are: (a) k nearest neighbors, i.e. the k closest points to a seed; (b) all neighbors within distance r from the seed, for some distance metric. For fitting uniformly bounded patches we use the latter; the number of points k in the recovered neighborhood thus varies depending on r and the specifics of the distance metric and the search algorithm ( Figure 32). We later uniformly subsample within each neighborhood if necessary to limit the total number of points used to fit each patch. For general point clouds spatial decompositions like k-dimensional (k-d) trees  #b8  are commonly used, as well as triangle mesh structures for representing 3D sample points of surfaces. For organized point clouds back-projection on the image plane has been used for a more efficient neighborhood extraction  #b133 . We next present the two structures and the three methods that we have tested.

Triangle Mesh
The triangle mesh structure can be constructed quickly since the input data is in the form of a grid. The basic algorithm is to locally connect (x, y) grid neighbors with triangle edges using only the presence or absence of valid depth data, but not the actual z values. We connect neighboring valid points in the same row and column and close triangles by adding diagonals (Figure 33, left). A well known problem ( Figure 34) is that depth discontinuities, i.e. jumps, between (x, y) neighbors could be bridged. To address this we use Canny edge detection on the z values  #b169 . The resulting edge points are used to limit triangle construction, creating gaps in the mesh at jumps. However, Canny edge detection does not guarantee continuous edges. To help with this, we also remove both the triangles with sides longer than a threshold T es = 5cm and those whose ratio of the longest side to shortest side (aspect ratio) is more than a threshold T ar = 5.Mesh building, Canny edge detection, and removal of long triangles are all O(N). The cost for finding k nearest neighbors (with breadth first search) is O(k).

Neighborhood Searching Using the Triangle Mesh
First define chain distance as the weighted edge path length between vertices in the mesh, with the weight between two vertices that share an edge equal to their Euclidean distance in 3D. To find neighbors within distance r from a seed point we apply a breadth-first search from the seed, pruning it when the chain distance exceeds r. In that way we reduce the chances that the extracted neighbors cross discontinuities in the point cloud, even if they are spatially close (Figure 33, right).  

K-D Tree
One of the most common data structures for spatial points is the k-d tree  #b8 . Whereas the triangle mesh approach 10 depends on the grid organization of the data, k-d trees can be constructed from any point cloud. However, k-d trees do not directly encode information about depth discontinuities. The cost for building a k-d tree is O(N log 2 N) when using an O(N log N) sorting algorithm for computing medians, or O(N log N) with a linear medianfinding algorithm  #b8 . The cost for finding k nearest neighbors is O(k log N).

Neighborhood Searching Using the K-D Tree
We search for neighbors within Euclidean distance r of the seed using the classic method introduced in  #b8 . The extracted neighborhood may span surface discontinuities ( Figure 32).

Image Plane Back-Projection
This method has been used in PCL  #b133 , when the point cloud is organized, i.e. comes from a single projection point, which is the case in our system. This method is faster than k-d trees and no extra data structure is required. Given the 3D neighborhood-sphere around the seed point and the camera parameters, we can simply backproject it as a circle in the image plane centered at the seed's pixel. The bounding square of pixels that the circle covers can be easily extracted. For each one of these O(r 2 ) 11 pixels, the Euclidean distance of the corresponding 3D points (if any) to the seed point is checked to see if it is contained to the r-sphere.The backprojection method has the same results as the k-d tree one, but its time and space complexity are improved in the common case by taking advantage of the fact that the point cloud is organized. The sphere backprojection to a circle, as well as the bounding box of the circle in the image plane can be computed in constant time given the camera model, the seed point, and the neighborhood size r. The Euclidean distance checking is linear in the number of checked pixels so the total cost to find an r-neighborhood is O(r 2 ). The neighborhood finding algorithm proceeds as follows.

Stage IV: Find r-Neighborhoods of Seed Points
Parameter n f = 50 is the maximum neighborhood size for patch fitting, which can also be adjusted depending on patch size.Step 11: Use an organized search to find a neighborhood with at most n f points from C randomly distributed within an r ball of each seed S(i). In  #b63  we studied the three different neighborhood methods described above. Here we use the image plane backprojection method.  $b11  The circle radius in pixels is proportional to the original sphere radius r in meters (the constant of proportionality depends on both the focal length and the distance of the sphere center to the camera center of projection).

patch modeling and fitting
Since we have a set of point cloud neighborhoods around each seed, we can proceed in patch fitting and validation as been described in detail in Chapter 3 (Figures 35,36), with the difference that during curvature validation we can also aply a fourth post-processing saliency measure for the hiking task 12 .

Stage V: Patch Fitting
Stage IV: 0.05m-Neighborhoods Searching  

Minimum and Maximum Principal Curvature
The smaller of the two principal curvatures κ min min(κ x , κ y ) at a point is the inverse of the radius of the smallest osculating circle tangent to the surface there; similarly the largest osculating circle has radius 1/κ max . The signs of the principal curvatures also indicate whether the surface is concave (both positive), convex (both negative), or saddle (opposite signs) at that point. These values can be used in a few different ways depending on the shape of the robot foot. For example, for a flat footed robot (or to a rough approximation, for a human wearing a hiking boot), concave regions with more than slightly positive κ max could be considered less salient, because the foot can't fully fit there. A robot with spherical feet might prefer areas that are not too convex (as the foot would only make contact at a tangent point) but also not too concave to fit.

Stage V: Fit & Validate Curved Bounded Patches to the Neighborhoods
Patch fitting, curvature saliency, and post-processing. Parameter κ min = −13.6m −1 , κ max = 19.7m −1 are the min and max principal curvatures estimated from human selected patches (Section 4.7.2); d max = 0.01m is the maximum RMS Euclidean patch residual.Step 12: Fit a patch P(i) to each neighborhood as described in Chapter 3.Step 13: Discard patches with min principal curvature less than κ min or max principal curvature greater than κ max (curvature saliency). This step could be adjusted depending on the application.Step 14: Compute Euclidean patch residual (Section 3.3.1)  #b153  #b63  and discard patches where this is greater than d max .Step 15: Apply the patch coverage algorithm (Section 3.3.2)  #b63  to discard patches with areas not sufficiently supported by data.

termination criteria
Various termination criteria may be applied while adding patches to the map for each new data frame, for example: wall-clock time, total number of patches, or task-specific criteria. Another approach is to specify a desired fraction ν of the total sampled surface area S that should probabilistically be covered by patches. Note that ν can be both less than 1, to sample sparsely, or more than 1, to oversample. For instance with r-ball neighborhood search and ellipse-bounded paraboloid patch fitting we can estimate the expected number of patches for this criteria as ν S πr 2 .Or, as we do in the experiments below, we can fit patches until the sum of their areas reaches or exceeds νS. In practice it is nontrivial both to calculate the total sampled surface area S and the area of any individual patch. To approximate S we compute the triangle mesh (Sec In practice on commodity hardware (one 2.50GHz core, 8GB RAM) the bilateral filter and downsampling (stage I) run in ∼20ms total. Normal computation, DtFP, DoN, and DoNG saliency in Stage II take ∼35ms combined, dominated by ∼30ms for integral image computation using 640 × 480 input images from a hardware depth camera downsampled to 320 × 240 (the main reason for downsampling is that the required integral images take ∼150ms at 640 × 480  #b46 ). Neighborhood finding in Stage IV takes ∼0.03ms per seed, and patch fitting and validation in Stage V are ∼0.8ms total per neighborhood with n f = 50. The total time elapsed per frame when using 640 × 480 input images is 20 + 35 + 0.83n p ms, where n p is the number of patches actually added. n p can range from 0 in the case that the map is already full (or there are no new seed points) up to n g V 2 g . In practice we additionally limit the total time spent per frame to e.g. 100ms, allowing up to around 50 patches to be added per frame in this configuration.

homogeneous patch map
Salient patches from the algorithm proposed in this chapter could form the basis for a homogeneous patch map; a dynamically maintained local spatial map of curved surface patches suitable for contact both on and around the robot. Figure 37 illustrates the idea, including both environment surfaces and contact pads on the robot itself (potentially uncertain due to kinematic error). Patches on the robot are not fully developed in this thesis since they would not be found and fitted by 3D exteroception, but would come from the robot model and proprioception.The homogeneous aptch map could provide a sparse "summary" of relevant contact surfaces for higher-level reasoning. As contacts are made the map could be further refined. Exteroception can detect upcoming terrain patches from a distance, but with relatively high uncertainty. Kinematic proprioception could sense the pose of contact patches on the robot itself-e.g. heel, toe, foot solepotentially with relatively low uncertainty. When a contact is made between a robot and environment patch, the latter could be re-measured exproprioceptively through kinematics and touch, possibly with reduced uncertainty compared to prior exteroception.  Figure 37: Concept of the homogeneous patch map: a sparse set of patches that locally approximate both environment surfaces (green) and key contact surfaces on a robot (brown). All are spatially mapped with quantified uncertainty (blue Gaussians) relative to a body-centered reference frame.All of the classic elements of SLAM  #b143  would apply to such a map: propagation of spatial uncertainty through kinematic chains, associating different observations of the same surface patch, and optimal data fusion. Fusion by Kalman update is supported by the patch covariance matrices. First-order propagation of uncertainty through a chain of transforms with 6×6 covariances S j is facilitated by the chain Jacobian J c given in Appendix A.1: Σ c = J c SJ T c , S diag(S n , . . . , S 1 ).(70)

experimental results
We run the patch mapping and tracking on the recording of rocky trails (Section 4.7) and we show some qualitative results in Figure 51. 

Triangle Mesh vs K-D Tree Neighborhood Searching
The parameters were: neighborhood radius r = 0.1m, residual threshold T r = 0.01m, coverage cell size w c = 0.01m, coverage threshold factors ζ i = 0.8, ζ o = 0.2, and T p = 0.3A p /w 2 c (all motivated above). We let the algorithm run for each dataset until the sum of the patch areas equaled or exceeded 90% of the sampled surface area, both approximated as described in Section 4.5.Qualitatively, as depicted in Figure 39 and 40, the algorithm appears to give a reasonable representation of non-smooth environment surfaces. Quantitatively, we measured the following statistics ( Table 2): the total number of patches before evaluation, the number of valid patches passing both residual and coverage  Table 2 k-d tree neighborhoods terrain mesh neighborhoods Figure 39: A subset of patches fit to the fake rock dataset corresponding to the neighborhoods in Figure 32. The black patches failed coverage evaluation.evaluation, the number of dropped patches for each test, the average Euclidean residual (of the valid patches), and the total surface area for each dataset.There are generally more patches dropped due to residual for k-d tree neighborhoods, possibly because the triangle mesh neighborhoods avoid discontinuities which may not be fit well by a paraboloid. We see the opposite effect for patches dropped due to coverage: more patches are generally dropped due to insufficient coverage when using triangle mesh neighborhoods. The k-d tree neighborhoods may distribute samples more evenly, particularly near discontinuities.Another interesting result is that more patches are required to reach 90% of the surface area when using triangle mesh neighborhoods. In Figure 40 we see that the distribution of patch areas created using mesh neighborhoods is skewed more to the low side than those created using k-d tree neighborhoods. This can again be explained by the fact that k-d tree neighborhoods will span discontinuities but remain roughly circular, whereas triangle mesh neighborhoods may be less circular when the seed point is near an edge.

Human Subject Data for Hiking Saliency Thresholds
For setting the saliency thresholds φ d,g and κ min,max used in Section 4.2.1, patches that human subjects use when locomoting on rocky trails were analyzed. Research on human locomotion shows that visual information is crucial when walking on uneven terrain  #b45  #b112  #b113  #b48  #b128  #b86  #b85  #b87 ), but so far only a few works (e.g.  #b79 ) have specifically applied this to perception for bipedal robots.

Method
The trail sections were located in the the Middlesex Fells in Melrose, MA and were 9, 4, and 10.5 meters long. All included rocks and other types of solid surfaces normally encountered outdoors. We put strips of colored tape on the ground to mark nominal routes and to help establish visual correspondence among multiple video and RGB-D 13 recordings. The tape strips are intended to give subjects a rough idea of which route to pick but not the exact spots to place their feet. We collected 30Hz 640 × 480 RGB-D recordings of all trails with spatiotemporally coregistered 14 100Hz IMU data using a handheld Kinect camera with a CH Robotics UM6 9-DoF IMU (3-axis accelerometers, 3-axis gyroscopes, and 3-axis magnetometers) attached, including a Kalman filter to estimate absolute geo-referenced orientation ( Figure 41). The structured light method used by the Kinect does not work well in full sunlight so we took this data at twilight. Sunlight operation could be possible with other types of depth camera or stereo vision. The camera was held facing ∼ 45 • forward and down and ∼ 1m above  $b13  The color data was used only for visual correspondence. 14 Though calibration methods have been developed (Section 2.2.1), here spatial coregistration of IMU and depth data was based on the construction of the sensor apparatus. As mentioned in Chapter 2 spatial registration of the depth and RGB data used built-in calibration in the Kinect sensor. Temporal registration of all three datastreams was approximate.the ground by a human operator who walked at normal pace along each trail section. The data were saved in lossless PCLZF format  #b133 . We also took video recordings of the feet of five healthy human volunteers walking on these trails. For each trail participants were asked to walk at normal pace twice in each direction, following the nominal marked route (60 recordings). We visually matched all footsteps (total 867) in these recordings to corresponding (pixel, frame) pairs in the RGB-D+IMU data, and we fit patches (algorithm steps 11 and 12) at these locations ( Figure 42). 

Results and Threshold Estimation
We took statistics 15 of properties of the human selected patches including the max and min curvatures, the difference angle between the two-level normals (DoN) and the difference angle (DoNG) between the full patch normal and the upward pointing vector −ĝ from the IMU (Fig. 44 top and rows labeled "man" in Table 3). Thresholds φ d,g and κ min,max for the saliency algorithm were set to the corresponding averages from the human selected patches plus (minus for κ min ) 3σ, where is σ is the standard deviation. We ran the full algorithm on the same data frames as the human-selected patches and collected statistics on the same patch properties ( Figure 44 bottom and rows labeled "auto" in Table 3). The results are similar to the humanselected patches. In Figure 43 a set of 100 fitted patches using the humanderived saliency thresholds are illustrated. Notice that: 1) there are no patches fitted further than 0.7m from the Fixation Point, 2) there are no patches in areas with big slope, and 3) there are no patches with big curvature. In a way this is by construction, 16 but it does help establish that the algorithm can work as intended. In total 82052 patches were fit across 832 data frames, meaning (since n s = 100) that about 1.4% of patches were dropped due to the curvature,  Table 3 residual, and coverage checks (algorithm Steps 18, #b19  #b20 . This relatively low number indicates that the saliency checks performed prior to fitting (DoN, DoNG, and DtFP) have an additional benefit in that they help reduce time wasted fitting bad patches. In the experiments in Section 4.7.1 where patches were fit purely at random either 3% (for triangle mesh-based neighborhoods) or 10% (for K-D tree neighborhoods) of patches were dropped due to residual alone  #b63 . We observed humans walking on rocky trails and we took statistics of these four properties of the selected patches. From these we calculated four thresholds (one per property). A patch would be salient only if the values of its properties are in the corresponding interval of the average human-produced value plus-minus three times the standard deviation. We then ran the full automated algorithm on the same data frames and collected statistics on the same properties ( Figure 44). The results are statistically similar to the human-selected patches.

PAT C H T R A C K I N G
In Chapter 4 we introduced a method to create a map of patches in the environment around the robot. Along with the map most locomotion applications will require patch tracking, where patches are found and added to the map online, tracked as the robot moves and new frames are acquired, and then dropped when they are left behind. This will complete the patch mapping and tracking system for creating and maintaining a dynamic patch map around a robot. For solving the patch tracking problem, what is really needed to be tracked is the pose C t of the range sensor with respect to the volume frame at every frame t ( Figure 45). Camera tracking is well-studied including in the context of Simultaneous Localization and Mapping (SLAM)  #b26 . Various methods have been introduced depending on particular applications. One challenge for a walking robot is the potential for shaking or jerky camera motion during walking. C t Figure 45: Camera pose C t with respect to the volume frame.In this Chapter we introduce a method for real-time camera tracking, using the Moving Volume KinectFusion system introduced by Roth and Vona in  #b132  and implemented on a GPU 1 , which extends the original KinectFusion system developed by Newcombe, Izadi, et al in  #b96  #b54 . We review this system in Section 5.1 and we introduce some adjustments that were required for our walking robot system. We then describe the patch mapping and tracking algorithm in Section 5.3, along with some experimental results on real rock data in Section 5.4. We cover related work in Section 5.5 and discuss the future directions in Section 5.6.

review of moving volume kinectfusion
The original KinectFusion system for real-time 3D camera tracking and dense environment mapping was introduced in  #b96  #b54 . To briefly describe this system that was used for achieving very accurate and fast 3D mapping, we have to extend the notion of the volume which was introduced in Section 4 to a Truncated Signed Distance Formula (TSDF) volume  #b21 . The TSDF volume is divided into small voxels, each one representing a portion of the physical world using two numbers: i) the distance from a physical surface (positive if it is in front, zero if it crosses, and negative if it is behind the closest surface), and ii) a confidence weight that represents the reliability of the data. Ray casting  #b108  or marching cubes  #b81  methods can then produce a point cloud that represents the surface represented in the TSDF Volume. Two main processes alternate as new depth images are acquired (the KinectFusion system does not use the IMU, though that would be a possible extension):1. Camera Tracking: the camera is tracked using the Generalized Iterative Closest Point algorithm (GICP)  #b139 , giving the camera-to-volume transformation C t at any frame t.2. Data Fusion: the distance and confidence values are updated in all TSDF voxels observed in the newly acquired depth image.This system was implemented on a GPU achieving real-time performance as well as impressive camera tracking results which can handle shaking during locomotion. The data integration process fills holes in the cloud ( Figure 46) and also provides outlier rejection. The disadvantage of the original system is that original point cloud point cloud using KinectFusion Figure 46: Left: the TSDF volume on a rocky trail. Only the portion raycast from the camera viewport is visible. Right: the original point cloud and the point cloud using KinectFusion data integration; note that the data with Kinect-Fusion are smoother than the original and that small holes are filled. The big empty spaces in the cloud are due to missing input data from the range sensor at the particular spots during all the previous frames.the TSDF volume was fixed in the physical space. For a robot moving in the environment, a volume that moves with it and keeps only the information around it for local locomotion purposes is required. These features were introduced in Moving Volume KinectFusion  #b132 , where the TSDF volume is not fixed in the environment, but using the moving volume policies we introduced in Section 4, it can move with the robot, by remapping (translating and rotating) the volume when needed. The remapping leaves the camera and the cloud fixed relative to the physical world, but moves the TSDF volume by applying a rigid transform ( Figure 47). Note that as explained in  #b132  this is not a typical SLAM system  #b26 , but more a 3D Visual Odometry  #b136  one, since loop-closure is not handled.  This system is ideal for our purposes, except that 1) we require a task-specific way to set the inputs to the moving volume policies, and 2) a point cloud around and under the robot's feet will be required, and not only where the real camera is facing. To handle these two requirements we modified the original system as described below.

adaptations to moving volume kinectfusion
The TSDF Volume moving policies fd and ff as introduced in  #b132  and briefly described in Section 4 require a down vector for keeping the volume's y-axis aligned to when the remapping takes place. This down vector may be defined in various ways depending on the application. In our purpose we would like the volume to be aligned with the gravity vector since we assume that the robot is locomoting in a standing-like pose. For this purpose the first adaption in the original moving volume KinectFusion algorithm is to consider the gravity vector coming from the IMU as the down vector and not the volume's y-axis which was used in  #b132  (Figure 48). The second adaption has to do with the raycast point cloud recovered from the TSDF volume, which by default is performed from the real camera viewpoint. As we mentioned above this method does not produce points near the feet of the robot if the camera is not looking in that direction. As long as the TSDF volume voxels have already captured some surface information from previous frames, we could raycast from a virtual birds-eye view ( Figure 49).To define a virtual birds-eye view camera, we first let its reference frame to be axis-aligned with the TSDF volume frame but with its z-axis pointing down (along the volume frame y-axis). The center of projection of the virtual camera is at a fixed offset distance b o above the location of the real camera. The width and height (in pixels) of the virtual camera are set as fixed resolution b r = 200px. The result of using the virtual camera above the robot instead of the real one appears in Figure 50, where the point cloud covers the surrounding area around and under the robot (since the physical camera is carried in the robot's head). In that way patches can be fit under and around the feet even when the real camera is not facing in that direction. Note that, as we mentioned in Section 4.2.1, humans are performing perception in very similar ways, by fixating two steps ahead when locomoting in rough terrain, while considering step contact areas close to their feet, visually acquired before they reach them. 

patch mapping and tracking algorithm
We now present the full patch mapping and tracking algorithm using the mapping system introduced in Chapter 4 and the camera tracking and data fusion methods above. The inputs are described in Table 3, while the output is a set of patches in the volume reference frame.

Stage I: System Initialization
Step 1: Initialize the camera pose to the middle of the volume looking down at an angle corresponding to the viewpoint of the robot when standing in a default pose.Step 2: Initialize the selection grid on the volume frame xz plane.Runs for every new frame t (up to the 30Hz input rate of the depth camera):

Stage II: Data Acquisition
Step 2: Acquire a new frame of RGB-D and IMU data.

Stage III: Patch Tracking
Step 3: Get the new camera pose C t with respect to the Volume frame.Step 4: Update the TSDF Volume voxels with the fused data.

Input
Symbol Initial camera pose in the volume frame.C 0The TSDF Volume and cubic voxel size. V s , X sThe TSDF volume moving policy along with its (angle and position) thresholds, if any.{fv, fc, fd, ff}, c d , c aThe virtual camera frame offset from the real camera and resolution. The distance for culling patches behind the heading camera vector (z-axis).

d cp
The patch fitting options as described in Chapter 3.r, s, b, Γ , V g , n f , d max , ρ, w c , ζ i , ζ o , T pThe saliency options as described in Section 4.2.1.l d , l f , R, φ d,g , κ min,max Table 4 Step 5: Remap (i.e. translate and/or rotate) the volume if it is required according to the moving policies, applying a rigid body transform.Step 6: If the TSDF Volume was remapped:• Update the position of each patch relative to the volume frame using the same rigid body transform.• Remove the patches that have moved outside the volume. Optionally remove patches that are further than d cp behind the camera (and thus behind the robot, with the assumption that the camera is forward-facing).• Update the association of existing patches to grid cells in the volume frame xz-plane.

Stage IV: Patch Mapping
Step 8: Create a point cloud by raycasting in the TSDF from the birdseye view camera.Step 9: Find, fit, and validate salient patches using the patch map method described in Chapter 4. Note that if either the clock time limit t m exceeds or the maximum number of patches n s were fitted in the map, we proceed with the next frame.If the moving volume KinectFusion looses track, the whole map gets reset and the system is initialized again and proceeds fro the beginning.

A P P L I C AT I O N T O B I P E D L O C O M O T I O N
Bipedal locomotion is one of the most challenging tasks in robotics. Compared to quadrupeds and hexapods which usually have small point-like feet, bipeds usually have larger feet to support torques for balance 1 One challenge for bipedal locomotion in rough terrain is how to find potentially good footfall locations that can accommodate the feet. In this thesis we proposed a novel patch mapping and tracking system that provides potential good areas for contact between the robot and a rough environment. in this chapter we experimentally test our perception hypothesis with experiments on a real biped that steps on rocks. Our lab has developed a mini 12-DoF biped robot (Section 6.1), with a depth camera and an IMU attached, for applying our perception algorithm as part of a real-time foot selection system. The focus of the experiments is perception and we thus use a very simple control system, where the robot uses predefined leg motion primitives (as in  #b82 ) driven from the type of patch that is selected for contact. We run two experiments. In the first (Section 6.2) we let the robot walk open-loop on a flat area and create a spatial patch map. In the second (Section 6.3) we train the robot to place its foot on four different types of patches on rocks. We then place it in front of the same rocks again and let it create a patch map and find whether a match between the trained patches and those in the map exist. If so we let it run the corresponding trained motion sequence and place its foot on the rock.

rpbp robot and software interface
For the locomotion experiments we use the Rapid Prototyped Biped (RPBP), which is a 12-DoF mini biped robot developed in our lab. We briefly describe the design specifications of this platform as well as the software modules for connecting the patch mapping and tracking algorithms with the control system.

Rapid Prototyped Biped Robot (RPBP) Platform
RPBP (Figures 52, 53) is a 3D printed mini-biped robot. It is 47cm tall and it weights around 1.7kg. It has two 6-DoF legs kinematically similar to the DARwIn-OP humanoid  #b39 . We use Robotics Dynamixel MX-28 actuators with high resolution magnetic rotation sensors and PID control. We also use the short-range Carmine 1.09 depth camera with a mounted CH Robotics UM6 IMU sensor as described in Chapter 2. The robot does not have an on-board CPU. We use off-board power and a 3-channel communication tether between the robot and an external computer that includes:   

Software Interface
The software interface for patch mapping and tracking and robot control system has been developed in C++, using the PCL  #b133  library. It is divided into two big subsystems: perception and control ( Figure 54). The perception system includes three libraries: i) imucam, ii) rxkinfu, and iii) SPL (Surface Patch Library)  #b62 . The imucam library, developed in our lab, builds on PCL and implements an RGB-D+IMU frame grabber for the Carmine 1.09 camera and the UM6 IMU sensor, providing 30fps depth and 100fps IMU data (Chapter 2). The rxkinfu library implements the modified moving volume Kinect Fusion system (Chapter 5) providing a real-time dense 3D mapping and tracking system. It was developed in our lab based on the kinfu code from PCL. As input it gets the frames coming from imucam. Finally SPL implements the patch mapping system (Chapter 4) where salient patches are fit to the environment and tracked using rxkinfu.The perception system provides a set of patches to the control system, which is divided into two parts: i) a URDF (Unified Robot Description Format)  #b131  model of the robot and the dxrobot library, also developed in our lab, for Robotis Dynamixel-based communication, and ii) the RPBP walk control library. The latter includes a patch library, i.e. patches in fixed positions relative to the robot and a library of corresponding predefined motions for each patch. The walk control system is responsible for finding matches between the patches from the perception system and those in the library and executing the corresponding motion sequence.

rock patch tracking
For the first experiment we let the robot walk on a flat table using a predefined motion sequence. The table includes four rocks that do not come in contact with the robot during locomotion. When the robot is moving a map of patches is created. We split the whole environment into an 8 × 8 grid and we let the map contain one seed point per cell. The purpose of this experiment is to understand whether the shaking and the vibrations affect the patch mapping and tracking process. For this we visually check particular patches ( Figure 55) while the robot is moving, making sure that they are tracked correctly during the run. A more precise evaluation would be to quantitatively measure the camera drifts in a way similar to how the original moving volume Kinect Fusion system was validated  #b96  #b132 .

foot placement on rock patches
In the second experiment we test the ability of the robot to use the real-time patch mapping system integrated in a foot placement application. Our apparatus ( Figure 56) includes a table with 4 solid rocks in fixed positions. The robot is always attached to a safety belay, but this does not affect its motion significantly, i.e. it does not hold it upright during the run. We developed a simple control system where the robot executes a set of predefined motions, we manually trained by creating a library of patches and a motion sequence for each.

Foot Placement Training
We let the lookdown robot pose, as appears in Figure 57, be the starting point for training on each rock. We place the robot in front of each of the four rocks in a defined position and we let the rxkinfu system provide us with a point cloud of the environment. For each rock we manually select a neighborhood where we   would like the robot to place its foot and we fit a patch to it. The four patches we trained the robot to recognize, appear in Figure 57. For each one of them we train the robot to place its foot with a corresponding motion sequence as shown in Figure 58. For the training we used the the BRBrain library  #b159 , which is more convenient for that purpose. In a similar way we could train the robot to place its feet on various other positions, but this goes beyond the intention of the experiment which focuses on perception, not control.

Foot Placement Test
The foot placement experiment proceeds as follows. We place the robot in the lookdown pose in front of each rock, roughly in the same position as it was trained in. We then let the perception system to create a patch map as it was described in Chapter 4, but using a different method for seed selection. Here we consider all the points within 5cm from the center of each trained patch 2 .In this way we map patches close to what the robot is trained to step on. We then perform a patch matching. We compare every patch in the map with ev- ery trained patch. The similarity comparison between two patches proceeds as follows:1. Check whether the patches are of the same type (elliptic/hyperbolic/cylindric paraboloid or flat).

3.
Check whether the absolute difference between the curvatures are smaller than a threshold k s = 5m −1 .

C O N C L U S I O N S A N D F U T U R E W O R K
In real world applications articulated robots need to come in contact with unstructured environments either for locomotion or manipulation. In this thesis we introduced a novel perception system for modeling the contact areas between a robot and a rough surface and demonstrated its use in bipedal locomotion. Our system creates in real-time a map of bio-inspired salient bounded curved patches that fit to the environment in locations of potential footfall contact and tracks them when the robot is moving using an RGB-D depth camera and an IMU sensor.We envision our method to be part of a bigger system where not only foot placement, but also other types of contact (for instance dexterous manipulation) is driven using similar patches. In this thesis we developed experiments which prove that the robot can find contact patches for footfall placement, but as explained in Chapter 6 high level path planning along with a more advanced control system is required for dynamic walking using these patches. Furthermore, it is also interesting to understand how the patch uncertainty can play a role not only for data fusion while the robot is moving, but also for foot placement decisions. For instance a highly uncertain patch may not be considered for foot placement, or the motor compliance may be adapted with respect to the level of patch uncertainty. Last but not least, a mapping and tracking method (e.g.  #b149  #b66 ) that does not use a GPU device (which is not always available in all autonomous robots) instead of the Kinect Fusion system may need to be used.

Part III


A P P E N D I X


A E N V I R O N M E N T R E P R E S E N TAT I O N


a.1 jacobians
We calculate the Jacobian of the exponential map (13)  The Jacobian of (20) is, with θ xy , α xy , andẑ l from (20) [r T n t T n . . . r T 1 t T 1 ] T ∈ R 6n → [r T c t T c ] T ∈ R 6 , its Jacobian J c is 6×6n where 6×6 block j from right to left is, with φ j from (21), R j , X j from (22), ∂r ∂R from Appendix A.2,∂[r T c t T c ] T ∂[r T j t T j ] T =   ∂r c ∂r j 0 ∂t c ∂r j ∂t c ∂t j  (74)1 (72) remains finite as θ → 0. Small angle approximations for α and β were given in Section 3.1.2; their derivatives can be approximated as ∂α ∂r ≈ (θ 2 /30 − 1/3)r T and ∂β ∂r ≈ (θ 2 /180 − 1/12)r T . 2 For small θ xy , γ xy ≈ 2/(3 − θ 2 xy /2). ∂r c ∂r j = ∂r c ∂R c R l ∂R j ∂r j R r  #b75  if φ j = +1 if φ j = −1 ∂t c ∂r j = R l ∂R j ∂r j t r ∂t c ∂r j = R l ∂R j ∂r j (t r − t j ) ∂t c ∂t j = R l ∂t c ∂t j = −R l R j R l R n · · · R j+1 , R r R j−1 · · · R 1 , R c R n · · · R 1 t r (X j−1 • · · · •X 1 )(0).

a.2 the logarithmic map
Due to numerical issues with other equations we found in the literature, we developed the following numerically stable algorithm to calculate the log map r(R), R R xx R xy R xz R yx R yy R yz R zx R zy R zz and its 3 ×   During the fitting process the covariance matrix of the patch parameters Σ is calculated by first order error propagation  #b92  using the Gaussian uncertainty model as follows. In each step the input covariance matrix Σ will either come from the WLM fitting or from the previous step.

If (s = sphere)
Input: r xy = r xy , k, t, Σ k,r xy ,t ∈ R 6×6 (from the WLM fitting) Output: Σ k,r xy ,t ∈ R 6×6 Σ k,r xy ,t = JΣ k,r xy ,t J T ∈ R 6×6 , since r xy = r xy  #b79  whereJ =    0 1×2 0 3×3 0 1×3 I 2×2 0 2×1 0 2×3 0 3×2 0 5×5 I 3×3    ∈ R 6×6(80)

If (s = circ cyl)
Input: r = r(R ) = r([x ŷ ẑ ]) (log map),ẑ = R(r)ẑ,x R(r)x,ŷ = z ×x = [x ] T xẑ = [ẑ ] xx , Σ k,r,t ∈ R 6×6 Output: Σ k,r ,t ∈ R 7×7 Σ r xy ,t = JΣJ T ∈ R 7×7  #b81  where J =     0 1×3 0 1×3 1 0 1×3 ∂r ∂ẑ ∂r ∂x 0 3×1 0 3×3 0 3×3 0 3×3 0 3×1 I 3×3     ∈ R 7×10(82)0 4×3 0 4×3 Σ k,t    ∈ R 10×10(83)Σẑ = JΣ r J T , with J = ∂R ∂rẑ ∈ R 3×3 Σx = JΣ r J T , with J = ∂R ∂rx ∈ R 3×3Step 3: Curvature Discrimination (if s = parab) If max(|κ x |, |κ y |) < k (s = plane) Input: r xy = r xy (r), Σ r xy ,t ∈ R 8×8 Output: Σ r xy ,t ∈ R 5×5 Σ r xy ,t = JΣ k,r,t J T ∈ R 5×5  #b84  whereJ = 0 2×2 ∂r xy ∂r 0 3×3 0 3×2 0 3×3 0 3×3 ∈ R 5×8(85)Else if min(|κ x |, |κ y |) < k (s = cyl parab) If |κ y | > k Input: κ = κ y , Σ r xy ,t ∈ R 8×8 Output: Σ k,r,t ∈ R 7×7 Σ k,r,t = JΣ k,r,t J T ∈ R 7×7whereJ =    [0 1] 0 1×3 0 1×3 0 3×2 I 3×3 0 3×3 0 3×2 0 3×3 I 3×3    ∈ R 7×8(87)Else swap axes Input: κ = κ x , r = r(R(r)[ŷ −xẑ]) (log map), Σ k,r,t ∈ R 8×8 Output: Σ k,r ,t ∈ R 7×7 Σ k,r,t = JΣ k,r,t J T ∈ R 7×7 Else if |κ x − κ y | < k (s = circ parab) Input: κ = κ x +κ y 2 , r xy = r xy (r), Σ k,r,t ∈ R 8×8 Output: Σ k,r xy ,t ∈ R 6×6 Σ k,r,t = JΣ k,r,t J T ∈ R 6×6  #b90  where J =   [1/2 1/2] 0 1×3 0 1×3 0 3×2 ∂r xy ∂r 0 3×3 0 3×2 0 3×3 I 3×3    ∈ R 6×8(91)Else (s = ell parab or hyp parab) No change in k, r, and t Stage II: Fit the BoundaryStep 5: Initialize Bounding Parameters Input: m = xȳ v x v y v xy T , Σ k,r,t ∈ R (n k +n r +3) 2Output: Σ m,k,r,t ∈ R (5+n k +n r +3) 2

Let
m =        x y v x v y v xy         =        x T X r (q i , r, t)y T X r (q i , r, t) (x T X r (q i , r, t)) 2 (ŷ T X r (q i , r, t)) 2 (x T X r (q i , r, t))(ŷ T X r (q i , r, t))       (92)where q i X r (q i , r, t) = R(−r)(q i − t) = (R(r)) T (q i − t)Σ m,k,r,t = JΣ q 1 ...q N ,m,k,r,t J T R (5+n k +n r +3) 2 0 n k ×3 . . . 0 n k ×3 I n k ×n k 0 n k ×n r 0 n k ×3 0 n r ×3 . . . 0 n r ×3 0 n r ×n k I n r ×n r 0 n r ×3 0 3×3 . . . 0 3×3 0 3×n k 0 3×n r I 3×3       ∈ R (5+n k +n r +3)×(5N+n k +n r +3) Step 6: Cylindrical Paraboloid and Circular Cylinder Boundary Fitting Input: t = R(r)(xx) + t, d r = λ[ v x −x 2 √ v y ] T , Σ m,k,r,t ∈ R 12×12 Output: Σ d r ,k,r,t ∈ R 9×9 Σ d r ,k,r,t = JΣ m,k,r,t J T ∈ R 12×12  #b95  where∂m ∂q i = 1 N        x T y T 2(x T q i )x T 2(ŷ T q i )ŷ T (x T q i )ŷ T +x T (ŷ T q i )         ∂q i ∂q i ∈ R 5×3 ,(94)J =       ∂d r ∂m 0 2×1 0 2×3 0 2×3 0 1×9 1 0 1×3 0 1×3 0 3×5 0 3×1 I 3×3 0 3×3 ∂t ∂m 0 3×1 ∂t ∂r ∂t ∂t       ∈ R 9×12 (96) ∂d r ∂m = λ −x(v x −x 2 ) − 1 2 0 1 2 (v x −x 2 ) − 1 2 ) 0 0 0 0 0 1 2 v − 1 2 y 0 ∂t ∂m = R(r)x 0 3×1 , ∂t ∂r = ∂R ∂r (xx), ∂t ∂t = I 3×3Step 7: Circular Paraboloid and Sphere Boundary Fitting Input: d c = λ max( √ v x , √ v y ), Σ m,k,r xy ,t ∈ R 11×11 Output: Σ d c ,k,r xy ,t ∈ R 9×9 Σ d c ,k,r xy ,t = JΣ m,k,r xy ,t J T ∈ R 11×11  #b97  whereJ =       ∂d c ∂m 0 0 1×2 0 1×3 0 1×5 1 0 1×2 0 1×3 0 2×5 0 2×1 I 2×2 0 2×3 0 3×5 0 3×1 0 3×2 I 3×3       ∈ R 7×11(98)If v Step 8: Elliptic and Hyperbolic Boundary Fitting Input: d e = λ[ √ v x √ v y ] T , Σ m,k,r,t ∈ R 11×11 Output: Σ d e ,k,r,t ∈ R 13×13 Σ d e ,k,r,t = JΣ m,k,r,t J T ∈ R 13×13  #b99  where 1. If b=circle Input: d c =max(l + , l − ), Σ l,ρ,r xy ,t ∈ R 10×10 Output: Σ d c ,r xy ,t ∈ R 6×6 Σ d c ,r xy ,t = JΣ l,ρ,r xy ,t J T R 6×6J =       ∂d e ∂m 0 2×2 0 2×3 0 2×3 0 2×5 I 2×2 0 2×3 0 2×3 0 3×5 0 3×2 I 3×3 0 3×3 0 3×5 0 3×2 0 3×3 I 3×3       ∈ R 13×10(100)where    The distance δ of a point p = (p x , p y , p z ) ∈ R 3 from a paraboloid whose implicit (local) form is f(x, y, z) = k x x 2 + k y y 2 − 2z, using the first order Taubin's  #b153  approximation is:J =    ∂d c ∂l 0 1×3 0 1×2 0 1×3 0 2×2 0 2×3 I 2×2 0 2×3 0 3×2 0 3×3 0 3×2 I 3×3    ∈ R 6×10(104)δ = F 0 (p) F 2 (p)(107)F 0 (p) = k x x 2 + k y y 2 − 2z F 2 (p) = 4(k 2 x + k 2 y + 1)Using the second order Taubin approximation the distance δ is computed in three steps:1. Taylor series coefficient computation F 0,0,0 (p) = k x x 2 + k y y 2 − 2z F 1,0,0 (p) = 2k x p x , F 0,1,0 (p) = 2k y p y , F 0,0,1 (p) = −2 F 2 (p)δ 2 + F 1 (p)δ + F 0 (p) = 0(108)where δ is the approximate min Euclidean distance.

Footnote
1 : One exception are bipeds that are constrained by a boom, which often have small feet[156].
2 : The trained patches are stored with poses defined relative to the robot
3 : For the Bumblebee2 camera σ p = 0.05px and σ m = 0.1px (from the specifications document)
4 : .2 point cloud preprocessing
5 : To reduce ambiguity wlog choose |κ x | < |κ y |, though some ambiguity is unavoidable due to bilateral symmetries.
6 : In the limit as κ → 0 (34-41) all reduce to planes.
7 : We have found very few prior reports on the particular fitting problem including boundaries and quantified uncertainty.
8 : It was also used in[53] as the norm of the normals difference.
10 : At least the relative fast version given above.
11 : This assumes the Σ i 's are positive-definite, which is a common requirement for covariance matrices. A heuristic to allow semi-definite Σ i is to clamp small v f (i, p) to a minimum positive limit.
12 : We are not aware of a method for finding true principal curvatures, short of fitting patches, that is as fast as we would like on the raw point cloud during pre-processing.
9 : A patch can be considered in a cell if its seed is in the cell.
15 : Min, max, median (med), average (avg), and standard deviation (std). 
17 : In the style of a pyramid.
16 :  All values for "auto" are defined to fall within the corresponding "man" average plus (minus for κ min ) 3σ.