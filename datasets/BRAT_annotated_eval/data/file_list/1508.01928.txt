A VARIATIONAL APPROACH TO THE CONSISTENCY OF SPECTRAL CLUSTERING

Abstract
This paper establishes the consistency of spectral approaches to data clustering. We consider clustering of point clouds obtained as samples of a ground-truth measure. A graph representing the point cloud is obtained by assigning weights to edges based on the distance between the points they connect. We investigate the spectral convergence of both unnormalized and normalized graph Laplacians towards the appropriate operators in the continuum domain. We obtain sharp conditions on how the connectivity radius can be scaled with respect to the number of sample points for the spectral convergence to hold. We also show that the discrete clusters obtained via spectral clustering converge towards a continuum partition of the ground truth measure. Such continuum partition minimizes a functional describing the continuum analogue of the graph-based spectral partitioning. Our approach, based on variational convergence, is general and flexible.

Introduction
Clustering is one of the basic problems of statistics and machine learning: having a collection of n data points and a measure of their pairwise similarity the task is to partition the data into k meaningful groups. There is a variety of criteria for the quality of partitioning and a plethora of clustering algorithms, overviewed in  #b14  #b34  #b50  #b51 . Among most widely used are centroid based (for example the k-means algorithm), agglomeration based (or hierarchical) and graph based ones. Many graph partitioning approaches are based on dividing the graph representing the data into clusters of balanced sizes which have as few as possible edges between them  #b3  #b4  #b24  #b35  #b39  #b40  #b49 . Spectral clustering is a relaxation of minimizing graph cuts, which in any of its variants,  #b29  #b35  #b47 , consists of two steps. The first step is the embedding step where data points are mapped to a euclidean space by using the spectrum of a graph Laplacian. In the second step, the actual clustering is obtained by applying a clustering algorithm like k-means to the transformed points.The input of a spectral clustering algorithm is a weight matrix W which captures the similarity relation between the data points. Typically, the choice of edge weights depends on the distance between the data points and a parameter ε which determines the length scale over which points are connected. We assume that the data set is a random sample of an underlying ground-truth measure. We investigate the convergence of spectral clustering as the number of available data points goes to infinity.For any given clustering procedure, a natural and important question is whether the procedure is consistent. That is, if it is true that as more data is collected, the partitioning of the data into groups obtained converges to some meaningful partitioning in the limit. Despite the abundance of clustering procedures in the literature, not many results establish their consistency in the nonparametric setting, where the data is assumed to be obtained from a unknown general distribution. Consistency of k-means clustering was established by Pollard  #b31 . Consistency of k-means clustering for paths with regularization was recently studied by Thorpe, Theil and Cade  #b44 , using a similar viewpoint to those of this paper. Consistency for a class of single linkage clustering algorithms was shown by Hartigan  #b22 . Arias-Castro and Pelletier have proved the consistency of maximum variance unfolding  #b2 . Pointwise estimates between graph Laplacians and the continuum operators were studied by Belkin and Niyogi  #b8 , Coifman and Lafon  #b11 , Giné and Koltchinskii  #b18 , Hein, Audibert and von Luxburg  #b23 , and Singer  #b37 . Spectral convergence was studied in the works of Ting, Huang, and Jordan  #b45 , Belkin and Niyogi  #b7  on the convergence of Laplacian eigenmaps, von Luxburg, Belkin and Bousquet on graph Laplacians, and of Singer and Wu  #b38  on connection graph Laplacian. The convergence of the eigenvalues and eigenvectors these works obtain is of great relevance to machine learning. However obtaining practical and rigorous rates at which the connectivity length scale ε n → 0 as n → ∞ remained an open problem. Also relevant to point cloud analysis are studies of Laplacians on discretized manifolds by Burago, Ivanov and Kurylev  #b10  who obtain precise error estimates for eigenvalues and eigenvectors.Recently the authors in  #b15 , and together with Laurent, von Brecht and Bresson in  #b17 , introduced a framework for showing the consistency of clustering algorithms based on minimizing an objective functional on graphs. In  #b17  they applied the technique to Cheeger and Ratio cuts. Here the framework of  #b15  #b17  is used to prove new results on consistency of spectral clustering, which establish the (almost) optimal rate at which the connectivity radius ε can be taken to 0 as n → ∞. We prove the convergence of the spectrum of the graph Laplacian towards the spectrum of a corresponding continuum operator. An important element of our work is that we establish the convergence of the discrete clusters obtained via spectral clustering to their continuum counterparts. That is, as the number of data points n → ∞ the discrete clusters (obtained via spectral clustering) are show to converge towards continuum objects (measures), which themselves are obtained via a clustering procedure in the continuum setting (performed on the ground truth measure). That is, the discrete clusters are shown to converge to continuum clusters obtained via spectral clustering procedure with full information (ground truth measure) available. We obtain results for unnormalized (Theorem 1.2), and normalized (Theorems 1.5 and 1.7) graph Laplacians. The bridge connecting the spectrum of the graph Laplacian and the spectrum of a limiting operator in the continuum is built by using the notion of variational convergence known as Γ-convergence. The setting of Γ-convergence, combined with techniques of optimal transportation, provides an effective viewpoint to address a range of consistency and stability problems based on minimizing objective functionals on a random sample of a measure.1.1. Description of spectral clustering. Let V = {x 1 , . . . , x n } be a set of vertices and let W ∈ R n×n be a symmetric matrix with non-negative entries. We define D ∈ R n×n , the degree matrix of the weighted graph (V, W ), to be the diagonal matrix with D ii = j W i,j for every i. Also, we define L, the unnormalized graph Laplacian matrix of the weighted graph (V, W ), to be (1.1) L := D − W.We also consider the matrices N sym and N rw given byN sym := D −1/2 LD −1/2 , N rw := D −1 L,both of which we refer to as normalized graph Laplacians. The superscript sym indicates the fact that N sym is symmetric, whereas the superscript rw indicates the fact that N rw is connected to the transition probabilities of a random walk that can be defined on the graph. Each of the matrices L, N sym , N rw is used in a version of spectral clustering. The so called unnormalized spectral clustering uses the spectrum of the unnormalized graph Laplacian to embed the point cloud into a lower dimensional space, typically a method like k-means on the embedded points then provides the desired clusters (see  #b47 ). This is Algorithm 1 below.

Algorithm 1 Unnormalized spectral clustering
Input: Number of clusters k and similarity matrix W . -Construct the unnormalized graph Laplacian L.-Compute the eigenvectors u 1 , . . . , u k of L associated to the k smallest (nonzero) eigenvalues of L. -Define the matrix U ∈ R k×n , where the i-th row of U is the vector u i . -For i = 1, . . . , n, let y i ∈ R k be the i-th column of U .-Use the k-means algorithm to partition the set of points {y 1 , . . . , y n } into k groups, that we denote by G 1 , . . . , G k . Output: Clusters G 1 , . . . , G k .In the same spirit, the normalized graph Laplacians are used. An algorithm for normalized spectral clustering using N sym was introduced in  #b29  (see Algorithm 2), and an algorithm using N rw was introduced in  #b35  (see Algorithm 3).Algorithm 2 Normalized spectral clustering as defined in  #b29  Input: Number of clusters k and similarity matrix W .-Construct the normalized graph Laplacian N sym . -Compute the eigenvectors u 1 , . . . , u k of N sym associated to the k smallest (nonzero) eigenvalues of N sym . -Define the matrix U ∈ R k×n , where the i-th row of U is the vector u i . -Construct the matrix V by normalizing the columns of U so that the columns of V have all euclidean norm equal to one. -For i = 1, . . . , n, let y i ∈ R k be the i-th column of V .-Use the k-means algorithm to partition the set of points {y 1 , . . . , y n } into k groups that we denote by G 1 , . . . , G k . Output: Clusters G 1 , . . . , G k .Algorithm 3 Normalized spectral clustering as defined in  #b35  Same as Algorithm 1 but using the normalized graph Laplacian N rw instead of L.Spectral properties of graph Laplacians have connections to balanced graph cuts. For example, the spectrum of N rw is shown to be connected to the Ncut problem, whereas the spectrum of L is connected to RatioCut (see  #b47 ). A probabilistic interpretation of the spectrum of N rw may be found in  #b28 . In addition, connections between normalized graph Laplacians, data parametrization and dimensionality reduction via diffusion maps are developed in  #b25 .We now present some facts about the matrices L, N sym and N rw , all of which may be found in  #b47 . First of all L is a positive semidefinite symmetric matrix. In fact for every vector u ∈ R n(1.2) Lu, u = 1 2 i,j W i,j (u i − u j ) 2 ,where on the left hand side we are using the usual inner product in R n . The smallest eigenvalue of L is equal to zero, and its multiplicity is equal to the number of connected components of the weighted graph. The matrix N sym is symmetric and positive semidefinite as well. Moreover, for every u ∈ R n(1.3) N sym u, u = 1 2 i,j W i,j u i D ii − u j D jj 2 .In addition, 0 is an eigenvalue of N sym , with multiplicity equal to the number of connected components of the weighted graph. The vector D 1/2 1 (where 1 is the vector with all entries equal to one) is an eigenvector of N sym with eigenvalue 0. The two forms of normalized graph Laplacians are closely related due to the correspondence between the spectruma of N sym and N rw . In fact, it is straightforward to show that(1.4) N rw u = λu if and only if N sym w = λw, where w = D 1/2 u.That is, N sym and N rw have the same eigenvalues, and there is a simple relation between their corresponding eigenvectors.1.2. Spectral clustering of point clouds. Let V = {x 1 , . . . , x n } be a point cloud in R d .To give a weighted graph structure to the set V , we consider a kernel η, that is, we consider η : R d → [0, ∞) a radially symmetric, radially decreasing function decaying to zero sufficiently fast. The kernel is appropriately rescaled to take into account data density. In particular, let η ε depend on the length scale ε where we take η ε : R d → R to be defined byη ε (z) := 1 ε d η z ε .In this way we impose that significant weight is given to edges connecting points up to distance ε. We consider the similarity matrix W ε defined by(1.5) W ε i,j = η ε (x i − x j ). We denote by L n,ε the unnormalized graph Laplacian (1.1) of the weighted graph (V, W ε ), that is(1.6) L n,ε = D ε − W ε where D ε is the diagonal matrix with D ε i,i = j W ε i,j .We define the Dirichlet energy on the graph of a function u : V → R to be(1.7) i,j W ε i,j (u(x i ) − u(x j )) 2 .The fact that η is a symmetric function guarantees that W is symmetric and thus all the facts presented in Subsection 1.1 apply. In particular, (1.2) can be stated as: for every function u :V → R (1.8) L n,ε u, u = 1 2 i,j W ε i,j (u(x i ) − u(x j )) 2 ,where on the left hand side we have identified the function u with the vector (u(x 1 ), . . . , u(x n )) in R n and where ·, · denotes the usual inner product in R n . The symmetric normalized graph Laplacian N sym n,ε is given by N sym n,ε := D −1/2 L n,ε D −1/2 . Since the kernel η is assumed radially symmetric, it can be defined as η(x) := η(|x|) for allx ∈ R d , where η : [0, ∞) → [0, ∞)is the radial profile. We assume the following properties on η:(K1) η(0) > 0 and η is continuous at 0. (K2) η is non-increasing. (K3) The integral ∞ 0 η(r) r d+1 dr is finite.Remark 1.1. We remark that the last assumption on η is equivalent to imposing that the surface tension(1.9) σ η := R d η(h)|h 1 | 2 dhis finite, where h 1 represents the first component of h. The second condition implies that more relevance is given to the interactions between points that are close to each other. We notice that the class of acceptable kernels is quite broad and includes both Gaussian kernels and discontinuous kernels like one defined by a function η of the form η = 1 for t ≤ 1 and η = 0 for t > 1.We focus on point clouds that are obtained as independent samples from a given distribution ν. Specifically, consider an open, bounded, and connected set D ⊂ R d with Lipschitz boundary (i.e. locally the graph of a Lipschitz function) and consider a probability measure ν supported on D.We assume ν has a continuous density ρ, which is bounded above and below by positive constants on D. We assume that the points x 1 , . . . , x n (i.i.d. random points) are chosen according to the distribution ν. We consider the graph with nodes V = {x 1 , . . . , x n } and edge weights W ε i,j i,j defined in (1.5). For an appropriate scaling of ε := ε n with respect to n, we study the limiting behavior of the eigenvalues and eigenvectors of the graph Laplacians as n → ∞. We now describe the continuum problems which characterize the limit.1.3. Description of spectral clustering in the continuum setting: the unnormalized case. Let domain D, "ground-truth" measure ν with density ρ be as above. The object that characterizes the limit of the graph Laplacians L n,εn as n → ∞ is the differential operator:(1.10) L : u → − 1 ρ div(ρ 2 ∇u).We consider the pairs λ ∈ R and u ∈ H 1 (D) (the Sobolev space of L 2 (D) functions with distributional derivative ∇u in L 2 (D, R d )), with u not identically equal to zero, such thatLu = λu, in D, ∂u ∂n = 0, on ∂D. (1.11)A function u as above is said to be an eigenfunction of L with corresponding eigenvalue λ ∈ R. In Subsection 2.4 we discuss the precise definition of a solution of (1.11) and present some facts about it. In particular L is a positive semidefinite self-adjoint operator with respect to the inner product ·, · L 2 (D,ν) and has a discrete spectrum that can be arranged as an increasing sequence converging to infinity0 = λ 1 ≤ λ 2 ≤ . . . ,where each eigenvalue is repeated according to (finite) multiplicity. Furthermore, there exists a orthonormal basis of L 2 (D) (with respect to the inner product ·, · L 2 (D,ν) ) consisting of eigenfunctions u i of L. Given a mapping Φ : D −→ R k by Φ ♯ ν we denote the push forward of the measure ν, namely the measure for which Φ ♯ ν(A) = ν(Φ −1 (A)), for any Borel set A. The continuum spectral clustering analogous to the discrete one of Algorithm 1 is as follows. Let u 1 , . . . , u k : D → R be the orthonormal set of eigenfunctions corresponding to eigenvalues λ 1 , . . . , λ k . Consider the measure µ = (u 1 , . . . , u k ) ♯ ν. LetG i ⊂ R k be the clusters obtained by k-means clustering of µ. Then G i = (u 1 , . . . , u k ) −1 (G i ) for i = 1, . . . , k define the spectral clustering of ν.

1.4.
Description of spectral clustering in the continuum setting: the normalized cases. The object that characterizes the limit of the symmetric normalized graph Laplacians N sym n,εn as n → ∞ is the differential operatorN sym : u → − 1 ρ 3/2 div ρ 2 ∇ u √ ρ .We consider the space(1.12) H 1 √ ρ (D) := u ∈ L 2 (D) : u √ ρ ∈ H 1 (D) .The spectrum of N sym is the set of pairs τ ∈ R and u ∈ H 1 √ ρ (D), where u is not identically equal to zero, such thatN sym (u) = τ u, in D ∂(u/ √ ρ) ∂n = 0 on ∂D. (1.13)The sense in which (1.13) holds is made precise in Subsection 2.4. The spectrum of the operator N sym has similar properties to those of the spectrum of L. We let0 = τ 1 ≤ τ 2 ≤ . . . ,denote the eigenvalues of N sym , repeated according to multiplicity.The continuum spectral clustering analogous to the discrete one of Algorithm 2 is as follows. Let u 1 , . . . , u k : D → R be the orthonormal set of eigenfunctions (with respect to the inner product ·, · L 2 (D,ν) ) corresponding to eigenvalues τ 1 , . . . , τ k . Normalize them by(ũ 1 (x), . . . ,ũ k (x)) = (u 1 (x), . . . , u k (x)) (u 1 (x), . . . , u k (x)) for all x ∈ D.Consider the measureμ = (ũ 1 , . . . ,ũ k ) ♯ ν. LetG i ⊂ R k be the clusters obtained by k-means clustering ofμ. Then G i = (ũ 1 , . . . ,ũ k ) −1 (G i ) for i = 1, . . . , k define the spectral clustering o ν.Finally, the operator that describes the limit of the graph Laplacians N rw n,εn = D εn −1 L n,εn is described by the operator N rw :N rw (u) = − 1 ρ 2 div(ρ 2 ∇u).As discussed in Subsection 2.4, the eigenvalues of N rw are equal to the eigenvalues of N sym . The continuum clustering, which is analogous to the discrete one of Algorithm 3, is as in Subsection 1.3, where eigenfunctions of N rw are used.1.5. Passage from discrete to continuum. We are interested in showing that as n → ∞ eigenvalues of discrete graph Laplacians and the associated eigenvectors converge towards eigenvalues and eigenfunctions of corresponding differential operators. The issue that arises is how to compare functions on discrete and continuum setting. Typically this is achieved by introducing an interpolation operator that takes discretely defined functions to continuum ones and a restriction operator which restricts the continuum function to the discrete setting. For this setting to work some smoothness of functions considered is required. Furthermore the choice of the interpolation operator and its properties adds an intermediate step that needs to be understood. We choose a different route and introduce a way to compare the functions between settings directly. This approach is quite general and does not require any regularity assumptions. We use the T L p -topologies introduced in  #b15  and in particular in this paper we focus in the T L 2 -topology that we now recall. Denote by ν n the empirical measure associated to the n data points, that is (1.14)ν n := 1 n n i=1 δ xi .For a given function u ∈ L 2 (D, ν), the question is how to compare u with a function v ∈ L 2 (D, ν n ) (a function defined on the set V ). More generally, one can consider the problem of how to compare functions in L 2 (D, µ) with those in L 2 (D, θ) for arbitrary probability measures µ, θ on D. We define the set of objects that includes both the functions in discrete setting and those in continuum setting as follows:T L 2 (D) := {(µ, f ) : µ ∈ P(D), f ∈ L 2 (D, µ)},where P(D) denotes the set of Borel probability measures on D. For (µ, f ) and (θ, g) in T L 2 we define the distanced T L 2 ((µ, f ), (θ, g)) = inf π∈Γ(µ,θ) D×D |x − y| 2 + |f (x) − g(y)| 2 dπ(x, y) 1 2 ,where Γ(µ, θ) is the set of all couplings (or transportation plans) between µ and θ, that is, the set of all Borel probability measures on D × D for which the marginal on the first variable is µ and the marginal on the second variable is θ. It was proved in  #b15  that d T L 2 is indeed a metric on T L 2 . As remarked in  #b15 , one of the nice features of the convergence in T L 2 is that it simultaneously generalizes the weak convergence of probability measures and the convergence in L 2 of functions. It also provides us with a way to compare functions which are supported in sets as different as point clouds and continuous domains. In Subsection 2.1 we present more details about this metric. For a given µ ∈ P(D) we denote by L 2 (µ) the space of L 2 -functions with respect to the measure µ. Also, for f, g ∈ L 2 (µ) we write f, g µ := D f gdµ andf 2 µ = f, f µ .Finally, if the measure µ has a density ρ, that is, if dµ = ρdx, we may write f, g ρ and f ρ instead of f, g µ and f µ .1.6. Convergence of eigenvalues, eigenvectors, and of spectral clustering: the unnormalized case. Here we present one of the main results of this paper. We state the conditions on ε n for the spectrum of the unnormalized graph Laplacian L n,εn , given in (1.6), to converge to the spectrum of L, given by (1.10) and for the spectral clustering of Algorithm 1 to converge to the clustering of Subsection 1.3. Let λ 1 ≤ λ 2 ≤ · · · be the eigenvalues of L and u 1 , u 2 , . . . the corresponding orthonormal eigenfunctions, as in Subsection 1.3. We recall that orthogonality is considered with respect to the inner product in L 2 (ν).To state the results it is convenient to introduce 0 = λ 1 < λ 2 < · · · , the sequence of distinct eigenvalues of L. For a given k ∈ N, we denote by s(k) the multiplicity of the eigenvalue λ k and we letk ∈ N be such that λ k = λk +1 = · · · = λk +s(k) . Also, we denote by Proj k : L 2 (ν) → L 2 (ν) the projection (with respect to the inner product ·, · ν ) onto the eigenspace of L associated to the eigenvalue λ k . For all large enough n, we denote by Proj (n) k : L 2 (ν n ) → L 2 (ν n ) the projection (with respect to the inner product ·, · νn ) onto the space generated by all the eigenvectors of L n,εn associated to the eigenvalues λ  k+s(k) . Here, as in the rest of the paper, we identify R n with the space L 2 (D, ν n ). 1 ε n = 0 if d = 2, lim n→∞ (log n) 1/d n 1/d 1 ε n = 0 if d ≥ 3.(1.16)Assume the kernel η satisfies conditions (K1)-(K3). Then, with probability one, all of the following statements hold true:1. Convergence of Eigenvalues: For every k ∈ N (1.17) lim n→∞ 2λ (n) k nε 2 n = σ η λ k ,where σ η is defined in (1.9). 2. For every k ∈ N, every sequence {u n k } n∈N with u n k an eigenvector of L n,εn associated to the eigenvalue λ (n) k and with u n k νn = 1 is pre-compact in T L 2 . Additionally, whenever u n k T L 2 −→ u k along a subsequence as n → ∞, then u k ν = 1 and u k is an eigenfunction of L associated to λ k .

Convergence of Eigenprojections: For all k ∈ N and for arbitrary sequence
v n ∈ L 2 (ν n ), if v n T L 2 −→ v as n → ∞ along some subsequence. Then along that subsequence Proj (n) k (v n ) T L 2 −→ Proj k (v), as n → ∞.

4.
Consistency of Spectral Clustering. Let G n 1 , . . . G n k be the clusters obtained in Algorithm 1. Let ν n i = ν n G n i (the restriction of ν n to G n i ) for i = 1, . . . , k. Then (ν n 1 , . . . , ν n k ) is precompact with respect to weak convergence of measures and furthermore if (ν n 1 , . . . , ν n k ) converges along a subsequence to (ν 1 , . . . , ν k ) then (ν 1 , . . . , ν k ) = (ν G1 , . . . , ν G k ) where G 1 , . . . , G k is a spectral clustering of ν, described in Subsection 1.3. Remark 1.3. We remark that although the choice of the T L 2 -topology used in the previous theorem may seem unusual at first sight, it actually reduces to a more common notion of convergence (like the one used in  #b48  which we described below) in the presence of regularity assumptions on the density ρ and the domain D. In fact, assume for simplicity that D has smooth boundary and that ρ is a smooth function. Consider {u n k } n∈N where u n k is an eigenvector of L n,εn associated to the eigenvalue λ (n) k and satisfying u n k νn = 1. The second statement in Theorem 1.2 says that up to subsequence, u n k T L 2 −→ u k , where u k is an eigenfunction of L associated to λ k . From the regularity theory of elliptic PDEs it follows that u k is smooth up to the boundary. In particular, it makes sense to define a functionũ n k on the point cloud, by simply taking the restriction of u k to the points {x 1 , . . . , x n }. It is straightforward to check thatũ n k T L 2 −→ u k due to the smoothness of u k . In turn, u n k T L 2 −→ u k , implies that d T L 2 ((ν n ,ũ n k − u n k ), (ν, 0)) → 0. From this and Proposition 2.1, we conclude that u n k −ũ n k νn → 0. This is precisely the mode of convergence used in  #b48 .The proof of Theorem 1.2 relies on the study of the limiting behavior of the following rescaled form of the Dirichlet energy (1.7) on the graph:(1.18) G n,ε (u) := 1 ε 2 n 2 i,j W i,j (u(x i ) − u(x j )) 2 .The type of limit which is relevant for the problem, is the one given by variational convergence known as the Γ-convergence. The notion of Γ-convergence is recalled in Subsection 2.2. This notion of convergence is particularly suitable in order to study the convergence of minimizers of objective functionals on graphs as n → ∞, as it is discussed in  #b17 .The relevant continuum energy is the weighted Dirichlet energy G : L 2 (D) → [0, ∞]: (1.19) G(u) := D |∇u(x)| 2 ρ 2 (x)dx if u ∈ H 1 (D) ∞ if u ∈ L 2 (D) \ H 1 (D} n∈N with u n ∈ L 2 (ν n ) for which sup n∈N u n νn < ∞, sup n∈N G n,εn (u n ) < ∞, is precompact in T L 2 .The fact that the weight in the limiting functional G is ρ 2 (and not ρ) essentially follows from the fact that the graph Dirichlet energy defined in (1.18) is a double sum. This is the same weight that shows up in the study of the continuum limit of the graph total variation in  #b15 . Theorem 1.4 is analogous to Theorems 1.1 and 1.2 in  #b15  combined. 1.7. Convergence of eigenvalues, eigenvectors, and of spectral clustering: the normalized case. We also study the limit of the spectra of N sym n,εn , the symmetric normalized graph Laplacian which we recall is given by, N sym n,εn := D −1/2 L n,εn D −1/2 . For a function u : V → R, (1.3) can be written as (1.20) N sym n,εn u, u =1 2 i,j W i,j u(x i ) √ D ii − u(x j ) D jj 2 .We denote by0 = τ (n) 1 ≤ · · · ≤ τ (n)n the eigenvalues of N sym n,εn repeated according to multiplicity. Their limit is described by differential operatorN sym : u → − 1 ρ 3/2 div ρ 2 ∇ u √ ρ . Let 0 = τ 1 ≤ τ 2 ≤ . . . ,denote the eigenvalues of N sym , repeated according to multiplicity. We write 0 = τ 1 < τ 2 < . . . , to denote the distinct eigenvalues of L. For a given k ∈ N, we denote by s(k) the multiplicity of the eigenvalueτ k and we letk ∈ N be such thatτ k = τk +1 = · · · = τk +s(k) . We define Proj k and Proj(n) kanalogously to the way we defined them in the paragraph preceding Theorem 1.2. The following is analogous to Theorem 1.2.Theorem 1.5 (Convergence of the spectra of the normalized graph Laplacians). Consider the same setting as in Theorem 1.2 and the same assumptions on η and on {ε n } n∈N . Then, with probability one, all of the following statements hold1. Convergence of Eigenvalues: For every k ∈ N lim n→∞ 2τ (n) k ε 2 n = σ η β η τ k ,where β η is given by(1.21) β η := R d η(h)dh,and where σ η is given by (1.9). 2. For every k ∈ N, every sequence {u n k } n∈N with u n k being an eigenvector of N sym n,εn associated to the eigenvalue τ (n) k and with u n k νn = 1 is pre-compact in T L 2 . Additionally, whenever u n k T L 2 −→ u k along a subsequence as n → ∞, then u k ν = 1 and u k is an eigenfunction of N associated to τ k .

Convergence of Eigenprojections: For all k ∈ N and for arbitrary
v n ∈ L 2 (ν n ), if v n T L 2 −→ v along a subsequence as n → ∞ then, Proj (n) k (u n ) T L 2 −→ Proj k (u), as n → ∞, along the subsequence. 4. Consistency of Spectral Clustering. Assume ρ ∈ C 1 (D). Let G n 1 , .. . G n k be the clusters obtained in Algorithm 2. Let ν n i = ν n G n i for i = 1, . . . , k. Then (ν n 1 , . . . , ν n k ) is precompact with respect to weak convergence of measures and furthermore if (ν n 1 , . . . , ν n k ) converges along a subsequence to (ν 1 , . . . , ν k ) then (ν 1 , . . . , ν k ) = (ν G1 , . . . , ν G k ) where G 1 , . . . , G k is a spectral clustering of ν, described in Subsection 1.4.The proof of the previous theorem is completely analogous to the one of Theorem 1.2 once one has proved the variational convergence of the relevant energies. Indeed, consider G n,ε :L 2 (ν n ) → R defined by (1.22) G n,ε (u) := 1 nε 2 i,j W i,j u(x i ) √ D ii − u(x j ) D jj 2 and G : L 2 (D) → [0, ∞] by (1.23) G(u) :=      D ∇ u √ ρ 2 ρ 2 (x)dx if u ∈ H 1 √ ρ (D), ∞ if u ∈ L 2 (D) \ H 1 √ ρ (D), where H 1 √ ρ (D) is defined in (1.12).The following holds. 2 -metric. That is, every sequence {u n } n∈N with u n ∈ L 2 (ν n ) for which sup n∈N u n νn < ∞, sup n∈N G n,εn (u n ) < ∞, is precompact in T L 2 .Finally, we consider the limit of the spectrum of N rw n,εn , where N rw n,εn = D −1 L n,εn . Consider the operator N rw given byN rw (u) = − 1 ρ 2 div(ρ 2 ∇u). As discussed in Subsection 2.4, the eigenvalues of N rw are equal to the eigenvalues of N sym . Thus from (1.4) and from Theorem 1.5, it follows that after appropriate rescaling, the eigenvalues of N rw n,εn converge to the eigenvalues of N rw . Moreover, using again (1.4) and Theorem 1.5, we have the following convergence of eigenvectors. and with u n k νn = 1 is pre-compact in T L 2 . Additionally, all its cluster points are eigenfunctions of N rw with eigenvalue τ k . Finally the clusters obtained by Algorithm 3 converge to clusters obtained by spectral clustering corresponding to N rw described at the end of Subsection 1.3.1.8. Stability of k-means clustering. One of the final elements of the proof of the consistency results of spectral clustering (statement 4. in Theorems 1.2 and 1.5) requires new results on stability of k-means clustering with respect to perturbations of the measure being clustered. These results extend the result of Pollard  #b31  who proved the consistency of k-means clustering. It is important to extend such results because in our setting, at the discrete level, the point set used as input for the k-means algorithm is not a sample from a given distribution and thus one can not apply the results in  #b31  directly.Given k ∈ N and given a measure µ on R N with finite second moments, let F µ,k :R N ×k → [0, ∞) be defined by (1.24) F µ,k (z 1 , . . . , z k ) := d(x, {z 1 , . . . , z k }) 2 dµ(x)where z i ∈ R N for i = 1, . . . , k. For brevity we write z both for (z 1 , . . . , z k ) and {z 1 , . . . , z k } where the object considered should be clear from the context. The problem of k-means clustering is to minimize F µ,k over R N ×k . In Subsection 2.3 we show the existence of minimizers of the functional (1.24). The main result is the following.Theorem 1.8 (Stability of k-means clustering). Let k ≥ 1. Let µ be a Borel probability measure on R N with finite second moments and whose support has at least k points. Assume {µ m } m∈N is a sequence of probability measures on R N with finite second moments which converges in the Wasserstein distance (see (2.1)) to µ. Then,lim m→∞ min z F µm,k (z) = min z F µ,k (z).Moreover, if z m is a minimizer of F µm,k for all m, then the set {z m , m ∈ N} is precompact in R N ×k and all of its accumulation points are minimizers of F µ,k .We present the proof of the previous Theorem in Subsection 2.3.The clusters corresponding to z minimizing the F µ,k are the Voronoi cells:G i = {x ∈ R N : d(x, z) = d(x, z i )}.We prove in Lemma 2.10 that the measure of the boundaries of clusters is zero, that is we show that if i = j then µ(G i ∩ G j ) = 0. In other words it is irrelevant to which cluster are the points on the boundary assigned to and because of this, we are allowed to define the clusters to be either open or closed sets. We furthermore note that the associated measures, µ Gi are mutually orthogonal and satisfy i µ Gi = µ.A consequence of Theorem 1.8 is that as the cluster centers converge so do the measures representing the clusters. The corollary follows from Theorem 1.8, since the convergence of centers of Voronoi cells, along with the fact that the boundaries of cells change continuously with respect to cell centers implies that the measures converge in Levy-Prokhorov metric, which characterizes the weak convergence of measures.1.9. Discussion. Theorems 1.2, 1.5, and 1.7 establish the consistency of spectral clustering. An important difference between our work and the available consistency results is that we provide an explicit range of rates at which ε n (the length scale used to construct the graph) is allowed to converge to 0 as n → ∞. In  #b48  the parameter ε is not allowed to depend on n. As a result, the functional obtained in the limit is a non-local (i.e. integral, rather than differential) operator. Operators with very different spectral properties are obtained in the limit depending on whether one uses a normalized or unnormalized graph Laplacian. In particular, it is argued that normalized spectral clustering is more advantageous than the unnormalized clustering, because in the normalized case the spectrum of the limiting operator is better behaved and the spectral consistency in the unnormalized case is only guaranteed in restrictive settings. We remark that our results show that when the parameter ε n decays to zero such difference between the normalized and the unnormalized settings disappears and the limiting operators in both cases have a discrete spectrum.When constructing the graph it is advantageous, from the point of view of computational complexity, to have fewer edges (that is to take ε small). However below some threshold the graph thus constructed does not contain enough information to accurately recover the geometry of the underlying ground-truth distribution. How large ε should be taken depends on n, the number of data points available. As number of data points increases ε converges to zero. We remark that for d ≥ 3, the results of Theorems 1.2, 1.5, and 1.7 are (almost) optimal in the sense of scaling. Namely, we show that if the kernel η used to construct the graph is compactly supported, then convergence holds if ε n ≫ log(n) 1/d n 1/d , while if ε n ≪ log(n) 1/d n 1/d the convergence does not hold. This follows from the results on the connectivity of random geometric graphs in  #b20  #b19  #b30  which show that with high probability for large n the graph thus obtained is disconnected.Finally, we remark that our results are essentially independent of the kernel used to construct the weights. For example, when the points are sampled from the uniform distribution on a domain D, our results show that the spectra of the graph Laplacians converge to the spectrum of the Laplacian on the domain D, regardless of the kernel used.1.10. Outline of the approach. Theorem 1.2 is based on the variational convergence of the energies G n,εn towards σ η G, together with the corresponding compactness result (Theorem 1.4). In order to show Theorem 1.4, we first introduce the functional G εn :L 2 (D, ρ) → [0, ∞) given by, (1.25) G εn (u) := 1 ε 2 n D D η εn (x − y)|u(x) − u(y)| 2 ρ(x)ρ(y)dxdy,which serves as an intermediate object between the functionals G n,εn and G. It is important to observe that the argument of G n,εn is a function u n supported on the data points, whereas the argument of G εn is a L 2 (D, ρ) function; in particular a function defined on D. The functional G εn is a non-local functional, where the term non-local refers to the fact that differences of a given function on a ε n -neighborhood are averaged, which contrasts the local approach of averaging derivatives of the given function. Non-local functionals have been of interest in the last decades due to their wide range of applications which includes phase transitions, image processing and PDEs. From a statistical point of view, for a fixed function u : D → R, G εn (u) is nothing but the expectation of G n,εn (u). On the other hand, the functional G εn is relevant for our purposes because not only it approximates G defined in (1.19) in a pointwise sense, but it also approximates it in a variational sense (as the parameter ε n goes to zero). More precisely the following holds. {G ε k } k∈N (defined in (1.25)) Γ-converges with respect to the L 2 (D, ρ)-metric to σ η G, where σ η is defined in (1.9) and G is defined in (1.19). Moreover, the functionals {G ε k } k∈N satisfy the compactness property, with respect to the L 2 (D, ρ)- metric. That is, every sequence {u k } n∈N with u k ∈ L 2 (D, ρ) for which sup k∈N u k L 2 (D,ρ) < ∞, sup k∈N G ε k (u n ) < ∞, is precompact in L 2 (D, ρ). Finally, for every u ∈ L 2 (D, ρ) (1.26) lim n→∞ G ε k (u) = σ η G(u).Proof. When ρ is constant, the proof may be found in the Appendix of  #b0  in case D is a convex set, and in  #b32  for a general domain D satisfying the assumptions in the statement. In case ρ is not constant the results are obtained in a straightforward way by adapting the arguments presented in  #b32  just as it is done in Section 4 in  #b15  when studying the variational limit of the non-local functionalT V ε (u) := 1 ε n D D η ε (x − y)|u(x) − u(y)|ρ(x)ρ(y)dxdy,which is the L 1 analogue of G ε .As observed earlier, the argument of G n,εn is a function u n supported on the data points, while the argument of G εn is an L 2 (D) function. For a function u n defined on the set V = {x 1 , . . . , x n }, the idea is to associate an L 2 (D) functionũ n which approximates u n in the T L 2 -sense and is such that G εn (ũ n ) is comparable to G n,εn (u n ). The purpose of doing this is to use Proposition 1.10. We construct the approximating functionũ n by using transportation maps (i.e. measure preserving maps) between the measure ν and ν n . More precisely, we setũ n = u n • T n where T n is a transportation map between ν and ν n which moves mass as little as possible. The estimates on how far the mass needs to be moved were known in the literature when ρ is constant and when the domain D is the unit cube (0, 1) d (see  #b26  #b41  #b42  #b43  for d = 2 and  #b36  for d ≥ 3). In  #b16  these estimates are extended to general domains D and densities ρ satisfying (1.15). Indeed, the following is proved. Proposition 1.11. Let D ⊆ R d be a bounded,(denoted T n♯ ν = ν n ) such that T n − Id ∞ ≤ C    ln(n) 3/4 n 1/2 , if d = 2, ln(n) 1/d n 1/d , if d ≥ 3,where C depends only on α, D, and the constants m, M .From the previous result, Chebyshev's inequality and Borel-Cantelli lemma one obtains the following rate of convergence of the ∞-transportation distance between the empirical measures ν n and the measure ν (see  #b16  for details associated to Proposition 1.11). Proposition 1.12. Let D be an open, connected and bounded subset of R d which has Lipschitz boundary. Let ν be a probability measure on D with density ρ satisfying (1.15). Let x 1 , . . . , x n , . . . be a sequence of independent samples from ν and let ν n be the associated empirical measures (1.14). Then, there is a constant C > 0 such that with probability one, there exists a sequence of transportation maps {T n } n∈N from ν to ν n (T n♯ ν = ν n ) and such that:if d = 2 then lim sup n→∞ n 1/2 Id − T n ∞ (log n) 3/4 ≤ C (1.27) and if d ≥ 3 then lim sup n→∞ n 1/d Id − T n ∞ (log n) 1/d ≤ C. (1.28)As shown in Section 3, Proposition 1.10 and Proposition 1.12 are at the backbone of Theorem 1.4. Schematically,G ε Γ −→ σ η G in L 2 + Proposition 1.12 =⇒ G n,εn Γ −→ σ η G in T L 2 .We note that the statement G ε Γ −→ σ η G is a purely analytic, purely deterministic fact. Proposition 1.12, on the other hand contains all the probabilistic estimates needed to establish all the results on this paper. Such estimates in particular provide the constraints on the parameter ε n in Theorem 1.4. It is worth observing that Proposition 1.12 is a statement that only involves the underlying measure ν and the empirical measure ν n , and that in particular it does not involve estimates on the difference between the functional G εn (u) and the functional G n,εn (u) for u belonging to a small (in the sense of V C-dimension) class of functions. In other words our estimates are related to the domains where the functions are defined (discrete/continuous) and not to the actual values of functions defined on those domains.With Theorem 1.4 at hand, the proof of Theorem 1.2 now relies on some spectral properties of the operator L and analogous properties of L n,εn . As shown in Section 2.4, the space L 2 (D, ρ) has a countable orthonormal basis (with respect to the inner product ·, · ρ ) formed with eigenfunctions of L. Additionally, the different eigenvalues of L can be organized as an increasing sequence of positive numbers converging to infinity. Each of the eigenvalues has finite multiplicity. Moreover, the eigenvalues of L have a variational characterization, as they can be written as the minimum value of optimization problems over successive subspaces of L 2 (D, ρ). This is the content of the Courant-Fisher mini-max principle which states that for every k(1.29) λ k = sup S∈Σ k−1 min u ρ =1 , u∈S ⊥ G(u),where we recall 0 = λ 1 ≤ λ 2 ≤ . . . , denote the eigenvalues of L repeated according to multiplicity, Σ k−1 denotes the set of (k − 1)-dimensional subspaces of L 2 (D, ρ), and where S ⊥ represents the orthogonal complement of S with respect to the inner product ·, · ρ . Moreover, the supremum in (1.29) is attained by the span of the first (k − 1) eigenfunctions of L. In Subsection 2.4 we review the previously mentioned spectral properties of L. Likewise, we can write the eigenvalues of L n,εn as(1.30) λ (n) k = nε 2 n 2 sup S∈Σ (n) k−1 min u νn =1 , u∈S ⊥ G n,εn (u),where Σ (n) k−1 denotes the set of (k − 1)-dimensional subspaces of R n , and where S ⊥ represents the orthogonal complement of S with respect to the inner product ·, · νn in R n . Moreover, as in the continuum setting, the supremum in (1.30) is attained by the span of the first (k − 1) eigenvectors of L n,εn . Theorem 1.4 allows us to exploit expressions (1.30) and (1.29) and in fact in Section 3 we show how 1.30 and 1.29 together with Theorem 1.4 imply Theorem 1.2, thus establishing the spectral convergence in the unnormalized case.In the normalized case, the same approach used in the unnormalized case can be taken. In fact, the proof of Theorem 1.5 follows from the proof of Theorem 1.2 by mutatis mutandis after Theorem 1.6 has been proved.The paper is organized as follows. Section 2 contains the notation and the background we need in the rest of the paper. In particular in Subsection 2.1 we review some facts about the T L 2 space, in Subsection 2.2 we review the definition of Γ-convergence, in Subsection 2.3 we present some results on stability of k-means clustering, and in Subsection 2.4 some facts about the spectrum of the operators L, N sym and N rw . In Section 3 we prove Theorem 1.4 and Theorem 1.2. Finally, in Section 4 we prove Theorem 1.6, Theorem 1.5 and Corollary 1.7. (2.1) d 2 (µ,μ) := min D×D |x − y| 2 dπ(x, y) 1/2 : π ∈ Γ(µ,μ) ,where Γ(µ,μ) is the set of all couplings between µ andμ, that is, the set of all Borel probability measures on D × D for which the marginal on the first variable is µ and the marginal on the second variable isμ. The elements π ∈ Γ(µ,μ) are also referred as transportation plans between µ and µ. The existence of minimizers, which justifies the definition above, is straightforward to show, see  #b46 . It is known that the convergence in Wasserstein metric is equivalent to weak convergence of probability measures and uniform integrability of second moments.In the remainder, unless otherwise stated, we assume that D is a bounded set. In that setting, we have P(D) = P 2 (D) and uniform integrability of second moments is immediate. In particular, convergence in the Wasserstein metric is equivalent to weak convergence of measures. For details see for instance  #b46 ,  #b1  and the references within. In particular, µ n ⇀ µ (to be read µ n converges weakly to µ) if and only if there is a sequence of transportation plans between µ n and µ, {π n } n∈N , for which:(2.2) lim n→∞ D×D |x − y| 2 dπ n (x, y) = 0.Actually, note that if D is bounded, (2.2) is equivalent to lim n→∞ D×D |x − y|dπ n (x, y) = 0. We say that a sequence of transportation plans, {π n } n∈N (with π n ∈ Γ(µ, µ n )), is stagnating if it satisfies (2.2). Given a Borel map T : D → D and µ ∈ P(D), the push-forward of µ by T , denoted by T ♯ µ ∈ P(D) is given by:T ♯ µ(A) := µ T −1 (A) , A ∈ B(D). For any bounded Borel function ϕ : D → R the following change of variables in the integral holds:(2.3) D ϕ(x) d(T ♯ µ)(x) = D ϕ(T (x)) dµ(x).We say that a Borel map T : D → D is a transportation map between the measures µ ∈ P(D) andμ ∈ P(D) ifμ = T ♯ µ. In this case, we associate a transportation plan π T ∈ Γ(µ,μ) to T by:(2.4) π T := (Id ×T ) ♯ µ, where (Id ×T ) : D → D × D is given by (Id ×T )(x) = (x, T (x)).It is well known that when the measure µ ∈ P 2 (D) is absolutely continuous with respect to the Lebesgue measure, the problem on the right hand side of (2.1) is equivalent to:(2.5) min D |x − T (x)| 2 dµ(x) 1/2 : T ♯ µ =μ .In fact, the problem (2.1) has a unique solution which is induced (via (2.4)) by a transportation map T solving (2.5) (see  #b46 ). In particular, boundedness of D implies that when µ has a density, then µ n ⇀ µ as n → ∞ is equivalent to the existence of a sequence {T n } n∈N of transportation maps, (T n♯ µ = µ n ) such that:(2.6) D |x − T n (x)| 2 dµ(x) → 0, as n → ∞.We say that a sequence of transportation maps {T n } n∈N is stagnating if it satisfies (2.6).We now introduce the space of objects that allows to simultaneously consider the discrete and continuum setting. LetT L 2 (D) := {(µ, f ) : µ ∈ P 2 (D), f ∈ L 2 (µ)}, where L 2 (µ) denotes the space of L 2 functions with respect to measure µ. For (µ, f ), (ν, g) in T L 2 define (2.7) d T L 2 ((µ, f ), (ν, g)) = inf π∈Γ(µ,ν) D×D |x − y| 2 + |f (x) − g(y)| 2 dπ(x, y) 1 2 .The set T L 2 and d T L 2 were introduced in  #b15 , where it was also proved that d T L 2 is a metric. Note that if we delete the second term on the right hand side of (2.7) we recover the Wasserstein distance between the measures µ and ν. The idea of introducing the second term on the right hand side of (2.7) is to make it possible to compare functions in spaces as different as point clouds and continuous domains. We have the following characterization of convergence in T L 2 . See  #b15 [Propositions 3.3 and 3.12] for its proof. that is(I × f n ) ♯ µ n d2 −→ (I × f ) ♯ µ as n → ∞.3. µ n ⇀ µ and for every stagnating sequence of transportation plans {π n } n∈N (with π n ∈ Γ(µ, µ n ))(2.8) D×D |f (x) − f n (y)| 2 dπ n (x, y) → 0, as n → ∞.4. µ n ⇀ µ and there exists a stagnating sequence of transportation plans {π n } n∈N (with π n ∈ Γ(µ, µ n )) for which (2.8) holds. Moreover, if the measure µ is absolutely continuous with respect to the Lebesgue measure, the following are equivalent to the previous statements:4. µ n ⇀ µ and there exists a stagnating sequence of transportation maps {T n } n∈N (with T n♯ µ = µ n ) such that:  Proof. From Proposition 2.1 follows that (µ n , χ An )(2.9) D |f (x) − f n (T n (x))| 2 dµ(x) → 0, as n → ∞.T L 2 −→ (µ, χ A ) if and only ifμ n × δ {1} + (µ n −μ n ) × δ {0} d2 −→μ × δ {1} + (µ −μ) × δ {0}, as n → ∞. Since convergence in Wasserstein distance implies weak convergence, we deduce thatμ n × δ {1} + (µ n −μ n ) × δ {0} ⇀μ × δ {1} + (µ −μ) × δ {0} , and in particular we conclude that µ n ⇀μ, as n → ∞.Conversely, the weak convergenceμ n ⇀μ, together with the fact that µ n d2 −→ µ (which in particular implies that µ n ⇀ µ), imply thatµ n × δ {1} + (µ n −μ n ) × δ {0} ⇀μ × δ {1} + (µ −μ) × δ {0} .In order to conclude that the above convergence also holds in the Wasserstein sense, we simply note that this follows from the the uniform integrability of the second moments of {μ n } n∈N , which in turn follows from The equality in the previous expression follows from the fact that µ n d2 −→ µ.The following proposition states that inner products are continuous with respect to the T L 2convergence. For this purpose, consider a stagnating sequence of transportation plans {π n } n∈N with π n ∈ Γ(µ, µ n ).We can write u n µn = D×D |u n (y)| 2 dπ n (x, y)1/2 and u µ = D×D |u(x)| 2 dπ n (x, y) 1/2 . Hence, (2.13) | u n µn − u µ | ≤ D×D |u n (y) − u(x)| 2 dπ n (x, y) 1/2 → 0 , as n → ∞.In proving the convergence of k-means clustering (statement 4. in Theorems 1.2 and 1.5)) we also need the following result on T L 2 convergence of a composition of functions.Lemma 2.7 (Continuity of composition in T L 2 ). Let {µ n } n∈N and µ be a collection of Borel probability measures on R d with finite second moments. Let F n ∈ L 2 (µ n , R d : R k ) for all n ∈ N and F ∈ L 2 (µ, R d : R k ). Consider the measuresμ n := F n♯ µ n for all n ∈ N andμ := F ♯ µ. Finally, let f n ∈ L 2 (μ n , R k : R) for all n ∈ N andf ∈ L 2 (μ, R k : R). If(µ n , F n ) T L 2 −→ (µ, F ) as n → ∞, and (μ n ,f n ) T L 2 −→ (μ,f ) as n → ∞.Then,(µ n ,f n • F n ) T L 2 −→ (µ,f • F n ) as n → ∞.Proof. First of all note that the fact that F n ∈ L 2 (µ n , R d : R k ) and F ∈ L 2 (µ, R d : R k ) guarantees thatμ n andμ are probability measures on R k with finite second moments. On the other hand, (µ n , F n ) T L 2 −→ (µ, F ) as n → ∞ implies the existence of a stagnating sequence of transportation maps {π n } n∈N with π n ∈ Γ(µ, µ n ) such that (2.14) limn→∞ R d ×R d |F (x) − F n (y)| 2 dπ n (x, y) = 0.We consider the measuresπ n := (F × F n ) ♯ π n for all n ∈ N. It is straightforward to check that π n ∈ Γ(μ,μ n ) for all n ∈ N and by the definition ofπ n thatlim n→∞ R k ×R k |x −ỹ| 2 dπ n (x,ỹ) = lim n→∞ R d ×R d |F (x) − F n (y)| 2 dπ n (x, y) = 0In other words, {π n } n∈N is a stagnating sequence of transportation maps withπ n ∈ Γ(μ,μ n ). But again by the definition ofπ n we deducelim n→∞ R d ×R d |f (F (x)) −f n (F n (y))| 2 dπ n (x, y) = lim n→∞ R k ×R k |f (x) −f n (ỹ)| 2 dπ n (x,ỹ) = 0Using again Proposition 2.1 we obtain the desired result.2.2. Γ-convergence. We recall the notion of Γ-convergence in general setting. F n (x) ≤ F (x)The notion of Γ-convergence is particularly useful when combined with an appropriate notion of compactness. See  #b9  #b12 . Definition 2.9. We say that the sequence of nonnegative random functionals {F n } n∈N satisfies the compactness property if for almost every ω ∈ Ω, it is true that every bounded (with respect to d X )sequence {x n } n∈N in X for which sup n∈N F n (x) < ∞,is precompact in X.Now that we have defined the T L 2 -space, and we have defined the notion of Γ-convergence, we can rephrase the content of Theorem 1.4 in the following way. Under the conditions on the domain D, the density ρ and the parameter ε n in Theorem 1.4, with probability one, all of the following statements hold:(1) Liminf inequality: For all u ∈ L 2 (ν), and all sequences {u n } n∈N with u n ∈ L 2 (ν n ) and with u n T L 2 −→ u it is true that lim inf n→∞ G n,εn (u n ) ≥ σ η G(u).(2) Limsup inequality: For all u ∈ L 2 (ν), there exists a sequence {u n } n∈N with u n ∈ L 2 (ν n ) and with u nT L 2 −→ u for which lim sup n→∞ G n,εn (u n ) ≤ σ η G(u).(3) Compactness: Every sequence {u n } n∈N with u n ∈ L 2 (ν n ), satisfyingsup n∈N F n (x) < ∞,is precompact in T L 2 , that is, every subsequence of {u n } n∈N has a further subsequence, which converges in the T L 2 -sense to an element of L 2 (D).In a similar fashion we can rephrase the content of Theorem 1.6.

Stability of k-means clustering.
Here we prove some basic facts about the functional F µ,k defined in (1.24) and about Theorem 1.8. Our first observation is that there exist minimizers of F µ,k . We note that F µ,k is a continuous function which is non-negative and the existence of minimizers can be obtained from a straightforward application of the direct method of the calculus of variations as we now illustrate. If the support of µ has k or fewer points, then including these points in z provides a minimizer for which F (z) = 0. On the other hand, if the support of µ has more than k points then to show that a minimizer exists it is enough to obtain pre-compactness of a minimizing sequence due to the continuity of F µ,k . Let {z m } m∈N be a minimizing sequence of F µ,k .By considering a subsequence we can assume that for any i = 1, . . . , k, z m i either converges to some z i ∈ R N or diverges to ±∞. Also without the loss of generality we can assume that for some 1 ≤ l ≤ k + 1, the sequence {z m i } m∈N converges for i < l and diverges for i ≥ l. Our goal is to show that l = k + 1. Assume for the sake of contradiction that l ≤ k. First note that if l = 1 (when no subsequence converges) then F µ,k (z m ) → ∞ as m → ∞, which is impossible. So we can assume that z m 1 converges to z 1 as m → ∞. It is straightforward to show using the finiteness of the second moment of µ, that(2.15) F µ,k (z m ) → F µ,l−1 ({z 1 , . . . , z l−1 }), as m → ∞.However unless l = k+1, adding k−(l−1) points from supp(µ)\{z 1 , . . . , z l−1 } to {z 1 , . . . , z l−1 } would result on a value of F µ,k that is strictly below F µ,l−1 ({z 1 , . . . , z l−1 }) and from (2.15), this would contradict the assumption that {z m } m∈N is a minimizing sequence. We conclude that {z m } m∈N converges up to subsequence. We now turn to comparing the properties of F µ,k for different measures µ, the ultimate goal is to prove Theorem 1.8. Let µ and ν be Borel probability measures on R N with finite second moments and let π ∈ Γ(µ, ν) be the optimal transportation plan realizing the Wasserstein distance between µ and ν, that is, assumed 2 2 (µ, ν) = R N ×R N |x − y| 2 dπ(x, y).Then|F µ,k (z) − F ν,k (z)| = R N d(x, z) 2 dµ(x) − R N d(y, z) 2 dν(y) = R N ×R N d(x, z) 2 − d(y, z) 2 dπ(x, y) ≤ R N ×R N d(x, z) 2 − d(y, z) 2 dπ(x, y) ≤ R N ×R N (|x − y| + d(y, z)) 2 − d(y, z) 2 dπ(x, y) ≤ d 2 2 (µ, ν) + 2d 2 (µ, ν) F ν,k (z),where the last inequality is obtained after expanding the integrand and using Cauchy-Schwartz inequality. By symmetry, we conclude that(2.16) |F µ,k (z) − F ν,k (z)| ≤ d 2 (µ, ν) 2 min F µ,k (z), F ν,k (z) + d 2 (µ, ν) .We also need the following Lemma. = x ∈ R k : |x − z i | = d(x, z) . Then, µ(V i ∩ V j ) = 0, ∀i = j.Proof. We start by recalling that if k = 1 then the minimizer z 1 of F µ,1 is the centroid of µ, that is z 1 = xdµ(x). We now consider k ≥ 2. Since the support of µ has at least k points, the points z 1 , . . . , z k are distinct. Assume that µ(V i ∩ V j ) > 0 for some i = j. Note that the set V i ∩ V j is contained in the plane P ij with normal vector z i − z j , and containing the point 1 2 z i + 1 2 z j . Let µ i = µ Vi and θ i = µ−µ i . Letẑ = (z 1 , . . . , z i−1 , z i+1 , . . . , z n ). Note that F µ,k (z) = F µ i ,1 (z i )+F θi,k−1 (ẑ). Consequently z i minimizes F µ i ,1 and by remark above, z i is the centroid of µ i , that isz i = 1 µ i (R k ) R k xdµ i (x). Now, let µ i = µ Vi\Vj and θ i = µ − µ i . Analogously to above F µ,k (z) = F µ i ,1 (z i ) + F θ i ,k−1 (ẑ).Hence z i minimizes F µ i ,1 and thus is the centroid of µ i , i.e.,z i = 1 µ i (R k ) R k xdµ i (x). But note that µ(V i ∩ V j ) > 0 implies that 1 µ i (R k ) R k x, z j − z i dµ i (x) > 1 µ i (R k ) R k x, z j − z i dµ i (x).This, contradicts the fact that the centroids of µ i and µ i are both equal to z i .The proof of Theorem 1.8 is now a direct consequence of (2.16) and Lemma 2.10.Proof of Theorem 1.8. Let a k = F µ,k (z k ) and a k m = F µm,k (z k m ), where z k is a minimizer of F µ,k and where z k m is a minimizer of F µm,k . Note that since the support of µ has at least k points, a l > a k for all l < k. From (2.16) it follows that a k m → a k as m → ∞ for any k ∈ N. To show that {z k m , m ∈ N} is precompact it is enough to show that all coordinates are uniformly bounded. If this is not the case then there exists 1 ≤ l ≤ k such that coordinates 1 to l converge, while those between l + 1 and k diverge to ±∞. Arguing as in the proof of the existence of minimizers at the beginning of this Section, and using (2.16), one obtains that F µm,k (z k m ) converges to F µ,l ({z 1 , . . . , z l }) for some z 1 , . . . , z l ∈ R N . If l < k, then this would imply that a l ≤ F µ,l ({z 1 , . . . , z l }) = lim m→∞ a k m = a k , which would contradict the fact that a l > a k . Thus concluding that along a subsequence z k m → z k for some z k . To show that z k minimizes F µ,k simply observe that from (2.16) and the continuity of F µ,k it follows thatF µ,k (z k ) = lim m→∞ F µ,k (z k m ) = lim n→∞ F µm,k (z k m ) = lim m→∞ a k m = a k ,which implies that indeed z k minimizes F µ,k . Finally, the last part of the Theorem on convergence of clusters, follows from the fact that µ m converge weakly to µ, that their second moments are uniformly bounded, and that the boundaries of Voronoi cells change continuously when the centers are perturbed.2.4. The Spectra of L, N sym and N rw . The purpose of this section is to present some facts about the spectra of the operators L, N sym , and N rw . These facts are standard (see  #b13 , or Chapter 8 in  #b5 ). We present them for the convenience of the reader. where we recall L is formally defined as L(u) = − 1 ρ div(ρ 2 ∇u). We say that u ∈ H 1 (D) is a weak solution of (2.17) if(2.18) D ∇u · ∇vρ 2 (x)dx = D vwρ(x)dx, ∀v ∈ H 1 (D).Remark 2.11. Note that if u is a solution of (2.17) in the classical sense, then integration by parts shows that u is a weak solution of (2.17).A necessary condition for (2.17) to have a solution in the weak sense, is that w belongs to the space In addition, a is continuous and symmetric. Therefore by Lax-Milgram theorem  #b13 [Sec. 6.2] for any w ∈ U there exists a unique solution u ∈ V to (2.17). From (2.18) and the assumption (1.15) on ρ, it follows thatU := w ∈ L 2 (D) : D wρ(x)dx = 0 .(2.20) D |∇u| 2 ρ 2 (x)dx ≤ C D |w| 2 ρ(x)dx,for a constant C. We can then define the inverse L −1 : U → V of L, by letting L −1 : w → u, where u is the unique solution of (2.17). From (2.20), it follows that L −1 is a continuous linear function. Rellich-Kondrachov theorem (see Theorem 11.10 in  #b27 ) implies that L −1 is compact.We say that λ ∈ R is an eigenvalue of the operator L, if there exists a nontrivial u ∈ H 1 (D) which is a weak solution of (1.11). That is if(2.21) a(u, v) = D ∇u · ∇vρ 2 (x)dx = λ D uvρ(x)dx = λ u, v ρ , ∀v ∈ H 1 (D).Such function u is called an eigenfunction.Remark 2.12. We remark that λ 1 = 0 is an eigenvalue of L and that the function u 1 identically equal to one is an eigenfunction associated to λ 1 . Given that D is connected, it follows that the eigenspace associated to λ 1 = 0 is the space of constant functions on D. We also remark that U is by definition the orthogonal complement (with respect to the inner product ·, · ρ ) of Span {u 1 }.Using the definition of L, the definition of weak solutions to (2.17) it follows that (2.22) u is an eigenfunction of L with eigenvalue λ = 0 iff L −1 (u) = 1 λ u.In other words the non-constant eigenfunctions of L are the eigenfunctions of L −1 , and the nonzero eigenvalues of L are the reciprocals of the eigenvalues of L. Thus, by understanding the structure of the spectrum of L −1 , one can obtain properties of the spectrum of L.Proposition 2.13. The operator L −1 : V → V is a selfadjoint, positive semidefinite (with respect to the inner product a(·, ·)) and compact. The eigenvalues of L −1 can be arranged as a decreasing sequence of positive numbers, k . Proof. In order to show that L −1 : V → V is self-adjoint with respect to a(·, ·), take v 1 , v 2 ∈ V and let u i = L −1 v i for i = 1, 2. We claim thatλ −1 2 ≥ λ −1 3 ≥ . . .a(L −1 v 1 , v 2 ) = v 1 , v 2 ρ .In fact, from the definition of L −1 it follows thata(L −1 v 1 , v 2 ) = a(u 1 , v 2 ) = D ∇u 1 · ∇v 2 ρ 2 (x)dx = D v 1 v 2 ρ(x)dx = v 1 , v 2 ρFrom the previous identity, it immediately follows that L −1 is self-adjoint and positive semidefinite with respect to the inner product a(·, ·). The compactness of L −1 follows from Rellich-Kondrachov theorem (see Theorem 11.10 in  #b27 ). The statements about the spectrum of L −1 are a direct consequence of Riesz-Schauder theorem and Hilbert-Schmidt theorem (see  #b33 ).For k ≥ 2, let v k be eigenfunctions as in the previous proposition and define u k by (2.23) u k := λ k v k .We claim that {u k } k≥2 is an orthonormal base of U with respect to ·, · ρ . In fact, it follows from the definition of L −1 and(2.28) thatδ kl = a(v k , v l ) = λ k v k , v l ρ = √ λ k √ λ l u k , u l , ρ ,where δ kl = 1 if k = l and δ kl = 0 if k = l. Hence u k , u l , ρ = δ kl . In other words {u k } k≥2 is an orthonormal set. Completeness follows from the completeness in Proposition 2.13 and density of H 1 (D) in L 2 (D). By setting u 1 ≡ 1 and by noticing that L 2 (D) = Span {u 1 } ⊕ U, we conclude that {u k } k∈N is a orthonormal base for L 2 (D) with inner product ·, · ρ . The next proposition is a direct consequence of the previous discussion and (2.22). Proposition 2.14. L has a countable family of eigenvalues {λ k } k∈N which can be written as an increasing sequence of nonnegative numbers which tends to infinity as k goes to infinity, that is,0 = λ 1 < λ 2 ≤ · · · ≤ λ k ≤ . . .Each eigenvalue, is repeated according to (finite) multiplicity. Moreover, there exists {u k } k∈N an orthonormal basis (with respect to ·, · ρ ) of L 2 (D), such that for every k ∈ N, u k is an eigenfunction of L associated to λ k .Finally we present the Courant-Fisher maxmini principle. Proposition 2.15. Consider an orthonormal base {u k } k∈N for L 2 (D) with respect to the inner product ·, · ρ , where for each k ∈ N, u k is an eigenfunction of L with eigenvalue λ k . Then, for every k ∈ N (2.24) λ k = minu ρ =1 , u∈S * ⊥ G(u),where S * = Span {u 1 , . . . , u k−1 } and where S * denotes the orthogonal complement of S * with respect to the inner product ·, · ρ . Additionally,(2.25) λ k = sup S∈Σ k−1 min u ρ =1 , u∈S ⊥ G(u),where Σ k−1 denotes the set of (k − 1)-dimensional subspaces of L 2 (D), and where S ⊥ represents the orthogonal complement of S with respect to the inner product ·, · ρ .The proof (of a similar statement) can be found in Chapter 8.3 in  #b5 .Remark 2.16. If the density ρ is smooth, then the eigenfunctions of L are smooth inside D.We now turn to the spectrum of N sym . We say that τ ∈ R is an eigenvalue of the operator N sym , if there exists a nontrivial u ∈ H 1 √ ρ (D) which solves (1.13). That is if(2.26) D ∇ u √ ρ · ∇ v √ ρ ρ 2 (x)dx = τ D uvρ(x)dx, ∀v ∈ H 1 √ ρ (D).The function u is then called an eigenfunction of N sym with eigenvalue τ .Remark 2.17. We remark that τ 1 = 0 is an eigenvalue of N sym and that the function u 1 equal tou 1 (x) = ρ(x) √ ρ ρis an eigenfunction of N sym , with eigenvalue τ 1 = 0. Given that D is connected, it actually follows that τ 1 = 0 has multiplicity one and thus the eigenspace associated to τ 1 = 0 is the space of multiples of √ ρ.Following the same ideas used when considering the spectrum of L, we can establish the following analogous results. Proposition 2.18. N sym has a countable family of eigenvalues {τ k } k∈N which can be written as an increasing sequence of nonnegative numbers which tends to infinity as k goes to infinity, that is,0 = τ 1 ≤ τ 2 ≤ · · · ≤ τ k ≤ . . .Each eigenvalue, is repeated according to (finite) multiplicity. Moreover, there exists {u k } k∈N an orthonormal basis (with respect to ·, · ρ ) of L 2 (D), such that for every k ∈ N, u k is an eigenfunction of N sym associated to τ k . Proposition 2.19. Consider a orthonormal base {u k } k∈N for L 2 (D) with respect to the inner product ·, · ρ , where for each k ∈ N, u k is an eigenfunction of N sym with eigenvalue τ k . Then, for every k ∈ N(2.27) τ k = min u ρ =1 , u∈S * ⊥ G(u),where S * = Span {u 1 , . . . , u k−1 }. Additionally,τ k = sup S∈Σm−1 min u ρ=1 , u∈S ⊥ G(u),where Σ m−1 denotes the set of (m − 1)-dimensional subspaces of L 2 (D), and where S ⊥ represents the orthogonal complement of S with respect to the inner product ·, · ρ .Finally, we consider the spectrum of N rw . We say that τ ∈ R is an eigenvalue of the operator N rw , if there exists a nontrivial u ∈ H 1 (D) for which(2.28) D ∇u · ∇vρ 2 (x)dx = τ D uvρ 2 (x)dx , ∀v ∈ H 1 (D).The function u is then called an eigenfunction of N rw with eigenvalue τ . From the definition, it follows that τ is an eigenvalue of N rw with eigenfunction u if and only if τ is an eigenvalue of N sym with eigenvector w := √ ρu. This is analogous to (1.4) in the discrete case.

Convergence of the spectra of unnormalized graph Laplacians
We start by establishing Theorem 1.4.Proof of Theorem 1.4. As done in Section 5 in  #b15  and due to the assumptions (K1) − (K3) on η, we can reduce the problem to that of showing the result for the kernel η defined byη(t) := 1, if t ∈ [0, 1], 0, if t > 1.We use the sequence of transportation maps {T n } n∈N from Proposition 1.12. Let ω ∈ Ω be such that (1.27) and (1.28) hold in cases d = 2 and d ≥ 3 respectively. By Proposition 1.12 the complement in Ω of such ω's is contained in a set of probability zero. The key idea in the proof is that the estimates of Proposition 1.12 imply that the transportation happens on a length scale which is small compared to ε n . By taking a kernel with slightly smaller radius than ε n we can then obtain a lower bound, and by taking a slightly larger radius a matching upper bound on the functional G n,εn .Liminf inequality: Assume that u n T L 1 −→ u as n → ∞. Since T n♯ ν = ν n , using the change of variables (2.3) it follows that (3.1) G n,εn (u n ) = 1 ε 2 n D×D η εn (T n (x) − T n (y)) (u n • T n (x) − u n • T n (y)) 2 ρ(x)ρ(y)dxdy.Note that for (x, y) ∈ D × D(3.2) |T n (x) − T n (y)| > ε n ⇒ |x − y| > ε n − 2 Id − T n ∞ .Thanks to the assumptions on {ε n } n∈N ((1.27) and (1.28) in cases d = 2 and d ≥ 3 respectively), for large enough n ∈ N:(3.3)ε n := ε n − 2 Id − T n ∞ > 0.By (3.2), and our choice of kernel η, for large enough n and for every (x, y) ∈ D × D, we obtainη |x − y| ε n ≤ η |T n (x) − T n (y)| ε n .We now considerũ n = u n • T n . Thanks to the previous inequality and (3.1), for large enough nG n,εn (u n ) ≥ 1 ε d+1 n D×D η |x − y| ε n (ũ n (x) −ũ n (y)) 2 ρ(x)ρ(y)dxdy = ε n ε n d+2Gε n (ũ n ).Note thatε n εn → 1 as n → ∞ and that u nT L 1 −→ u by definition impliesũ n L 1 (D,ρ)−→ u as n → ∞. We deduce from the liminf inequality of Proposition 1.10 that lim inf n→∞ Gε n (ũ n ) ≥ σ η G(u) and hence:lim inf n→∞ G n,εn (u n ) ≥ σ η G(u).Limsup inequality: By using a diagonal argument it is enough to establish the limsup inequality for a dense subset of L 2 (D) and in particular we consider the set of Lipschitz continuous functions u : D → R. That is, we want to show that if u : D → R is a Lipschitz continuous function, then there exists a sequence of functions {u n } n∈N , where u n ∈ L 2 (ν n ) andu n T L 2 −→ u as n → ∞, lim sup n→∞ G n,εn (u n ) ≤ σ η G(u).We define u n to be the restriction of u to the first n data points x 1 , . . . , x n . We note that this operation is well defined due to the fact that u is in particular continuous. It is straightforward to show that given that u is Lipschitz we have u n T L 2 −→ u. Now, considerε n := ε n + 2 Id − T n ∞ and letũ n = u • T n . The choice of kernel η implies that for every (x, y)∈ D × D η |T n (x) − T n (y)| ε n ≤ η |x − y| ε n .It follows that for all n ∈ N 1 ε d+2n D×D η |T n (x) − T n (y)| ε n (ũ n (x) −ũ n (y)) 2 ρ(x)ρ(y)dxdy ≤ 1 ε 2 n D×D ηε n (x − y) (ũ n (x) −ũ n (y)) 2 ρ(x)ρ(y)dxdy. (3.4)Now let A n and B n be given byA n := 1 ε 2 n D×D ηε n (x − y)(u(x) − u(y)) 2 ρ(x)ρ(y)dxdy B n := 1 ε 2 n D×D ηε n (x − y)(ũ n (x) −ũ n (y)) 2 ρ(x)ρ(y)dxdy.Then,A n − B n 2 ≤ 1 ε 2 n D×D ηε n (x − y) (u(x) −ũ n (x) +ũ n (y) − u(y)) 2 ρ(x)ρ(y)dxdy ≤ 4 ε 2 n D×D ηε n (x − y)(u(x) −ũ n (x)) 2 ρ(x)ρ(y)dxdy ≤ 4C Lip(u) 2 ρ 2 L ∞ (D) Id − T n 2 ∞ ε 2 n ,(3.5)where the first inequality follows using Minkowski's inequality, and where C = R d η(h)dh. The last term of the previous expression goes to 0 as n → ∞, yieldinglim n→∞ | A n − B n | = 0.On the other hand, by (1.26) it follows that A n is bounded on n and in particular it follows that(3.6) lim n→∞ |A n − B n | = 0.We conclude that lim sup n→∞ G n,εn (u n ) = lim supn→∞ 1 ε d+2 n D×D η |T n (x) − T n (y)| ε n (ũ n (x) −ũ n (y)) 2 ρ(x)ρ(y)dxdy ≤ lim sup n→∞ 1 ε 2 n D×D ηε n (x − y)(ũ n (x) −ũ n (y)) 2 ρ(x)ρ(y)dxdy = lim sup n→∞ Gε n (u) = σ η G(u),where the first equality is obtained from the fact that εñ εn → 1 as n → ∞, the first inequality is obtained from (3.4), the second equality is obtained from (3.6) and the last equality is obtained from (1.26).Compactness: Finally, to see that the compactness statement holds suppose that {u n } n∈N is a sequence with u n ∈ L 2 (ν n ) and such that sup n∈N u n L 2 (νn) < ∞, sup n∈N G n,εn (u n ) < ∞.Note that in particular sup n∈N u n • T n L 2 (ν) < ∞. We want to show thatsup n∈N G εn (u n • T n ) < ∞To see this, note that for large enough n, we can setε n := ε n − 2 Id − T n ∞ as in (3.3). Thus, for large enough n:1 ε d+2 n D×D η |z − y| ε n (u n • T n (z) − u n • T n (y)) 2 ρ(z)ρ(y)dzdy ≤ 1 ε d+2 n D×D η |T n (z) − T n (y)| ε n (u n • T n (z) − u n • T n (y)) 2 ρ(z)ρ(y)dzdy = G n,εn (u n ). Thus sup n∈N 1 ε d+2 n D×D η |z − y| ε n (u n • T n (z) − u n • T n (y)) 2 ρ(z)ρ(y)dzdy < ∞.Finally noting thatε n εn → 1 as n → ∞ we deduce that: supn∈N G εn (u n • T n ) < ∞.By Proposition 1.10 we conclude that {u n • T n } n∈N is relatively compact in L 2 (ν) and hence {u n } n∈N is relatively compact in T L 2 .Now we prove Theorem 1.2.

Convergence of Eigenvalues.
First of all note that because L n,εn is self-adjoint with respect to the Euclidean inner product in R n , in particular it is also self-adjoint with respect to the inner product ·, · νn and furthermore, it is positive semi-definite. In particular, we can use the Courant-Fisher maxmini principle to write the eigenvalues 0 = λ G n,εn (u n ) = 2 nε 2 n L n,εn u n , u n νn Therefore, 2λ(n) k nε 2 n = sup S∈Σ (n) k−1 min u νn =1 , u∈S ⊥ G n,εn (u).Let us first prove the first statement from Theorem 1.2. The proof is by induction on k. For k = 1, we know that λ (n) 1 = 0 for every n. Also, λ 1 = 0, so trivially (1) is true when k = 1. Now, suppose that (1) is true for i = 1, . . . , k − 1. We want to prove that the result holds for k.Step 1:In this first step we prove that σ η λ k ≤ lim inf n→∞ Thus, for large enough n, the space generated by u n 1 , . . . , u n k−1 is k − 1 dimensional. We can use the Gram-Schmidt orthogonalization process, to obtain an orthonormal base ũ n 1 , . . . ,ũ n k−1 . That is, we defineũ n 1 := u n 1 / u n 1 νn , and recursivelyṽ n i := u n i − and in that case (3.9) follows trivially. Let us now assume that lim inf n→∞ min u νn =1, u∈Sn ⊥ G n,εn (u) < ∞. Working on a subsequence that we do not relabel, we can assume without the loss of generality that the liminf is actually a limit, that is,lim n→∞ min u νn =1, u∈Sn ⊥ G n,εn (u) = lim inf n→∞ min u νn =1, u∈Sn ⊥ G n,εn (u) < ∞.Consider now a sequence {v n } n∈N with v n νn = 1 and v n ∈ S n ⊥ such thatlim n→∞ G n,εn (v n ) = lim n→∞ min u νn =1, u∈Sn ⊥ G n,εn (u) < ∞.Using the compactness from Theorem 1.4 and working on a subsequence that we do not relabel, we may assume that(3.10) v n T L 2 −→ v, as n → ∞,for some v ∈ L 2 (D). From Proposition 2.6, v ρ = lim n→∞ v n νn = 1 and v, u i ρ = lim n→∞ v n ,ũ n i νn = 0 for every i = 1, . . . , k − 1. In particular, v ρ = 1 and v ∈ S ⊥ . Moreover, given that v n T L 2 −→ v, it follows from the liminf inequality of Theorem 1.4 thatmin u ρ=1, u∈S ⊥ σ η G(u) ≤ σ η G(v) ≤ lim inf n→∞ G n,εn (v n ) = lim n→∞ min u =1, u∈Sn ⊥ G n,εn (u) ≤ lim inf n→∞ sup S∈Σ (n) k−1 min u =1, u∈S ⊥ G n,εn (u) = lim inf n→∞ 2λ (n) k nε 2 n .Thus showing (3.9) in all cases. Finally, since S ∈ Σ k−1 was arbitrary, taking the supremum over all S ∈ Σ k−1 and using the Courant-Fisher maxmini principle we deduce thatσ η λ k ≤ lim inf n→∞ 2λ (n) k nε 2 n .Step 2: Now we prove that lim sup n→∞ 2λ (n) k nε 2 n ≤ λ k . Consider u n 1 , . . . , u n k−1 an orthonormal set (with respect to ·, · νn ) with u n i an eigenvector of L n,εn associated to λ (n) i (this is possible because L n,εn is self-adjoint with respect to ·, · νn ). Consider then S * n := Span u n 1 , . . . , u n k−1 . We have:2λ (n) k nε 2 n = sup S∈Σ (n) k−1 min u νn =1, u∈S ⊥ G n,εn (u) = min u νn =1, u∈S * n ⊥ G n,εn (u).Working along a subsequence that we do not relabel, we can assume without the loss of generality that lim sup n→∞ Thanks to this, we can use the compactness from Theorem 1.4 to conclude that for every i = 1, . . . , k − 1 (working with a subsequence that we do not relabel) :u n i T L 2 −→ u i , as n → ∞,for some u i ∈ L 2 (D). From Proposition 2.6, u i , u j ρ = lim n→∞ u n i , u n j νn = 0 for i = j and u i ρ = lim n→∞ u n i νn = 1 for every i. Take S := Span {u 1 , . . . , u k−1 }, note that in particular S ∈ Σ k−1 . Also, take v ∈ S ⊥ with v ρ = 1 and such that:(3.12) σ η G(v) = min u ρ =1, u∈S ⊥ σ η G(u) ≤ σ η λ k .The last inequality in the previous expression holds thanks to the Courant-Fisher maxmini principle.By the limsup inequality from Theorem 1.4, we can find {v n } n∈N with v n T L 2 −→ v as n → ∞ and such that lim sup n→∞ G n,εn (v n ) ≤ σ η G(v). Letṽ n be given bỹv n := v n − k−1 i=1 v n , u n i νn u n i .Note thatṽ n ∈ S * n ⊥ . Also note that from Proposition 2.6, we deduce that v n , u n i νn → 0 as n → ∞ for all i = 1, . . . , k − 1 and thusṽ n T L 2 −→ v as n → ∞. Moreover,G n,εn (ṽ n ) = 2 nε 2 n L n,εnṽn ,ṽ n = 2 nε 2 n L n,εn v n , v n − 2 nε 2 n k−1 i=1 v n , u n i νn L n,εn v n , u n i νn − 2 nε 2 n k−1 i=1 v n , u n i νn L n,εn u n i ,ṽ n νn = G n,εn (v n ) − k−1 i=1 2λ (n) i nε 2 n v n , u n i 2 νn − 2 nε 2 n k−1 i=1 λ (n) i v n , u n i νn u n i ,ṽ n νn = G n,εn (v n ) − k−1 i=1 2λ (n) i nε 2 n v n , u n i 2 νn ≤ G n,εn (v n ).(3.13) Therefore,(3.14) lim sup n→∞ G n,εn (ṽ n ) ≤ lim sup n→∞ G n,εn (v n ) ≤ σ η G(v).Sinceṽ n T L 2 −→ v and v ρ = 1, once again from Proposition 2.6 we obtain lim n→∞ ṽ n νn = 1. In particular we can setũ n :=ṽ n ṽn νn and use (3.14) together with (3.12) to conclude that:lim n→∞ 2λ (n) k nε 2 n = lim n→∞ min u νn =1, u∈S * n ⊥ G n,εn (u) ≤ lim sup n→∞ G n,εn (ũ n ) = lim sup n→∞ G n,εn (ṽ n ) ≤ σ η G(v) ≤ σ η λ k ,which implies the desired result.

3.2.
Convergence of Eigenprojections. We prove the second and third part of Theorem 1.2. We recall that the numbersλ 1 <λ 2 < . . . denote the distinct eigenvalues of L n,εn . For a given k ∈ N, we recall that s(k) is the multiplicity of the eigenvalueλ k and thatk ∈ N is such that λ k = λk +1 = · · · = λk +s(k) .We let E k be the subspace of L 2 (D) of eigenfunctions of L associated to λ k , and for large n we let E (n) k be the subspace of R n generated by all the eigenvectors of L n,εn corresponding to all eigenvalues listed in λ (n) k+1 , . . . , λ (n) k+s(k) . We remark that by the convergence of the eigenvalues proved in Subsection 3.1 we have(3.15) lim n→∞ dim(E (n) k ) = dim(E k ) = s(k).We prove simultaneously the second and third statement of Theorem 1.2. The proof is by induction on k. Base Case: Let k = 1. Suppose that u n T L 2 −→ u. We need to show that Proj(n) 1 (u n ) T L 2 −→ Proj 1 (u). Now, note that since the domain D is connected, the multiplicity of λ 1 is equal to one. In particular, Proj 1 (u) is the function which is identically equal tou, 1 ρ = D udν(x).On the other hand, thanks to (3.15), it follows that for all large enopugh n, we have dim(E (n) 1 ) = 1 (note that in particular this means that assymptotcally the graphs are connected regardless of what kernel η is being used). Therefore, for large enough n, Proj (n) 1 (u n ) is the function which is identically equal to u n , 1 νn . Proposition 2.6 implies that lim n→∞ u n , 1 νn = u, 1 ρ and thus Proj (n) 1 (u n ) T L 2 −→ Proj 1 (u) as desired. The second statement of Theorem 1.2 is trivial in this case since for large enough n, the only two eigenvectors of L n,εn with eigenvalue λ (n) 1 = 0 and with · νn -norm equal to one is the function which is identically equal to one or the function that is identically equal to −1.InductiveStep: Now, suppose that the second and third statements of Theorem 1.2 are true for 1, . . . , k − 1. We want to prove the result for k. Let j ∈ k + 1, . . . ,k + s(k) . We start by proving the second statement of the theorem. Consider u n j n∈N as in the statement. From (3.7) it follows that G n,εn (u n j ) = Since the norms of the u n j are equal to one, the compactness statement from Theorem 1.4, implies that u n j n∈N is pre-compact. We have to prove now that every cluster point of u n j n∈N is an eigenfunction of L with eigenvalue λ j . So without the loss of generality let us assume that u n j T L 2 −→ u j for some u j . Our goal is to show that u j is an eigenfunction of L with eigenvalue λ j .By the induction hypothesis, we have Proj  i (u n j ) = 0 for every n ∈ N and for every i = 1, . . . , k − 1, we conclude that Proj i (u j ) = 0 for all i = 1, . . . , k − 1. A straightforward computation as in the proof of Proposition 2.14 shows that:(3.16) G(u j ) = ∞ i=k λ i Proj i (u j ) 2 ρ ≥ λ k ∞ i=k Proj i (u j ) 2 ρ = λ k u j 2 ρ .In addition, since u n j νn = 1 for all n, we deduce from Proposition 2.6 that u j ρ = 1. Thus,G(u j ) ≥ λ k .On the other hand, the liminf inequality from Theorem 1.4 implies that:σ η λ k = σ η λ j = lim n→∞ 2λ (n) j nε 2 n = lim n→∞ G n,εn (u j n ) ≥ σ η G(u j ) ≥ σ η λ k .Therefore, G(u j ) = λ k and from (3.16) we conclude that Proj i (u j ) ρ = 0 for all i = k. Thus, u j is an eigenfunction of L with corresponding eigenvalue λ j (= λ k ).Now we prove the third statement from Theorem 1.2. Suppose that u n T L 2 −→ u. We want toshow that Proj(n) k (u n ) T L 2 −→ Proj k (u). To achieve this we prove that for a given sequence of natural numbers there exists a further subsequence for which the convergence holds. We do not relabel subsequences to avoid cumbersome notation. From (3.15) it follows that for large enough n, dim(E (n) k ) = s(k). Hence, for large enough n, we can consider u n 1 , . . . , u n s(k)an orthonormal basis (with respect to the inner product ·, · νn ) for E (n) k , where u n j is an eigenvector of L n,εn with corresponding eigenvalue λ (n) k+j . Now, by the first part of the proof, for every j = 1, . . . , s(k), the sequence u n j n∈N is pre-compact in T L 2 . Therefore, passing to a subsequence that we do not relabel we can assume that for every j = 1, . . . , s(k) we have:(3.17) u n j T L 2 −→ u j , as n → ∞for some u j ∈ L 2 (D). From (2.6), the u j satisfy u j ρ = 1 for every j and u i , u j ρ = 0 for i = j. In other words, u 1 , . . . , u s(k) is an orthonormal set in L 2 (D) (with respect to ·, · ρ ). Furthermore, u j ∈ E k for all j by the first part of the proof. In other words, u 1 , . . . , u s(k) is an orthonormal basis for E k and in particularProj k (u) = s(k) j=1 u, u j ρ u j .On the other hand, for large enough n, we haveProj (n) k (u n ) = s(k) j=1u n , u n j νn u n j .Finally, the fact that u n T L 2 −→ u and (3.17) combined with Proposition 2.6 imply that Proj k , respectively. Consider the functional F µn,k . Let z n be its minimizer, and letG n 1 , . . .G n k be corresponding clusters. The clusters G 1 , . . . , G k of Algorithm 1 are defined by G i = (u n 1 , . . . , u n k ) −1 (G i ). By Theorem 1.8 the sequence z n is precompact. By Corollary 1.9 the sequence of measures µ i n = µ n Gn i is precompact for all i = 1, . . . , k. Consider a subsequence along which µ i n converges for every i = 1, . . . , k, and denote the limit by µ i . Since z i n = − ydµ i n (y) it follows that z i n converge as n → ∞, along the same subsequence. By statement 2. of Theorem 1.2 along a further subsequence (ν n , u n i ) converge to (ν, u i ) in T L 2 sense for all i = 1, . . . , k as n → ∞. Furthermore from the definition of T L 2 convergence follows that measures µ n converge in the Wasserstein sense to µ := (u 1 , . . . u k ) ♯ ν. Combined with convergence of µ i n to µ i implies, via Lemma 2.5, that (µ n , χGn i ) converge in T L 2 topology to (µ, χG i ). Consequently, by Lemma 2.7, (ν n , χGn i •(u n 1 , . . . , u n k )) converge to (ν, χG i • (u 1 , . . . , u k )) in T L 2 topology. Noting that χ G n i = χGn i • (u n 1 , . . . , u n k ) and χ Gi = χG i • (u 1 , . . . , u k ) implies that ν n G n i converges weakly to ν Gi as desired.

Convergence of the spectra of normalized graph Laplacians
We start by proving Theorem 1.6. Recall that for given u n ∈ L 2 (ν n )G n,εn (u n ) = 1 nε 2 n i,j W i,j u n (x i ) √ D ii − u n (x j ) D jj 2 , where W ij = η εn (x i − x j ) and D ii = n k=1 η εn (x i − x k ).With a slight abuse of notation we set D(x i ) := D ii .For u n ∈ L 2 (ν n ), defineū n ∈ L 2 (ν n ) by(4.1)ū n (x i ) := u n (x i ) D(x i )/n , i ∈ {1, . . . , n} .From the definition of G n,εn and G n,εn , it follows that G n,εn (u n ) = G n,εn (ū n ). Similarly, for every u ∈ L 2 (D) it is true that G(u) = G( u √ ρ ). To prove Theorem 1.6 we use the following lemma. Proof. We prove that u nT L 2 −→ u impliesū n T L 2 −→ u √ βη ρ; the converse implication is obtained similarly.Let {T n } n∈N be the transportation maps from Proposition 1.12, which we know exist with probability one. Using the change of variables (2.3) we obtain D(X i ) n = D η εn (x i − T n (y))ρ(y)dy. η α (t) := η(t), if t > 2α η(0), if t ≤ 2α, where we recall that η is the radial profile of the kernel η. We let η α and η α be the isotropic kernels whose radial profiles are η α and η α respectively. Note that thanks to assumption (K2) on η, we have η α ≤ η ≤ η α . Setε n := ε n − 2 Id − T n ∞ α , ε n := ε n + 2 Id − T n ∞ α .Note that thanks to the assumptions on ε n and the properties of the maps T n , for large enough n, ε n > 0,ε n εn → 1 andε n εn → 1 as n → ∞. In addition, from assumption (K2) on η and the definitions of η α , η α ,ε n andε n , it is straightforward to check that for large enough n and for Lebesgue almost every x, y ∈ D, η T n (x) − T n (y) ε n ≥ η α x − ŷ ε n , and η T n (x) − T n (y) ε n ≤ η α x − ỹ ε n .From these inequalities, we conclude that for large enough n and Lebesgue almost every x ∈ D Given that D is assumed to be a bounded open set with Lipschitz boundary, it is straightforward to check that exists a ball B(0, θ), a cone C with nonempty interior and a family of rotations {R x } x∈D with the property that for every x ∈ D it is true that x + R x (B(0, θ) ∩ C) ⊆ D. For large enough n ( so that 1 >ε n > 0 ), and for almost every x ∈ D we have: where in the first inequality we used assumption (1.15) on ρ, and we used the change of variables h = x−ŷ εn to deduce the first equality; to obtain the last equality we used the fact that η α is radially symmetric. From the previous chain of inequalities and from (4.4) we conclude that for large enough n and for almost every x ∈ D we have D η εn (T n (x) − T n (y))ρ(y)dy ≥ b > 0 for some positive constant b. Form the previous inequality we obtain the desired L ∞ -control on the terms  For this purpose, we use the continuity of ρ to deduce that for every x ∈ D, where β α = R d η α (h)dh. From (4.4), we deduce that for large enough n, and for almost every x ∈ D, β η ρ(x) − D η εn (T n (x) − T n (y))ρ(y)dy ≤β η ρ(x) − ε n ε n d D η α εn (x − y)ρ(y)dy ≤ ε n ε n d β η ρ(x) − β α ρ(x) + β α ρ(x) − D η α εn (x − y)ρ(y)dy+ 1 − ε n ε n d β η ρ(x).Analogously, from (4.5), for almost every x ∈ D, D η εn (T n (x) − T n (y))ρ(y)dy − β η ρ(x) ≤ ε n ε n d β α ρ(x) − β η ρ(x) + D η α εn (x − y)ρ(y)dy − β α ρ(x)+ ε n ε n d − 1 β η ρ(x).From these previous inequalities, (4.7) and (4.8) we conclude that for almost every x ∈ D, lim sup n→∞ β η ρ(x) − D η εn (T n (x) − T n (y))ρ(y)dy ≤ ρ(x)(β α − β α ).Finally, given that α was arbitrary we can take α → 0 in the previous expression to deduce that the left hand side of the previous expression is actually equal to zero. This establishes (4.6) and thus the desired result.The proof of Theorem 1.6 is now straightforward.Proof of Theorem 1.6. Liminf inequality: Let u ∈ L 2 (D) and suppose that {u n } n∈N , u n ∈ L 2 (ν n ), v n ) ≤ σ η G u β η ρ = σ η β η G(u).Let us consider the function u n ∈ L 2 (ν n ) given by u n (x i ) := v n (x i ) D(x i )/n for i = 1, . . . , n. Compactness: Suppose that {u n } n∈N , u n ∈ L 2 (ν n ), is such that sup n∈N u n νn < ∞, sup n∈N G n,εn (u n ) < ∞.From the discussion at the beginning of the section, we deduce that sup n∈N G n,εn (ū n ) < ∞ . Also, from the proof of Lemma 4.1, the terms 1 √ D•Tn/n are uniformly bounded in L ∞ . This implies that sup n∈N ū n L 2 (νn) < ∞ as well. Hence, we can apply the compactness property from Theorem 1.4 to conclude that {ū n } n∈N is precompact in T L 2 . Using Lemma 4.1, this implies that {u n } n∈N is precompact in T L 2 as well.Proof of Theorem 1.5. Using Theorem 1.6, similar arguments to the ones used in the proof of Theorem 1.2 can be used to establish statements 1., 2., and 3. of Theorem 1.5. The proof of statement 4. (consistency of spectral clustering) of Theorem 1.5 is analogous to the proof of the statement 4. of Theorem 1.2 which is given in Subsection 3.3. The reason that the normalization step does not create new difficulties is the following: since the eigenvectors u n := (u n 1 , . . . , u n k ) of N sym n,εn converge in T L 2 to eigenfunctions u = (u n 1 , . . . , u n k ) of N sym along a subsequence, it can be shown that the normalized vectors u n / u n converge to u/ u in T L 2 provided that the set of x ∈ D for which u = 0 is of ν-measure zero.In fact, assuming that ν({x ∈ D : u(x) = 0}) = 0 let us show the T L 2 convergence. From the assumption on the set of zeroes of u follows that lim H→0 + ν({ u(x) < H}) = 0. Let U H = {(x, y) ∈ D × D : u(x) < H}. Given n ∈ N let π n ∈ Π(ν n , ν) be such that |x − y| 2 + u n (x) − u(y) 2 dπ n (x, y) ≤ 2d 2 T L 2 (u n , u).Then for any H > 0 d T L 2 u n u n , u u ≤ |x − y| 2 dπ n (x, y) + UH 2 2 dπ n (x, y) + D×D\UH || u n (y) u(x) ± u n (y) u n (y) − u(x) u n (y)|| 2 u(x) 2 u n (y) 2 dπ n (x, y) The right hand side can be made arbitrarily small by first picking H small enough and then n large enough along the subsequence where u n converges to u. The convergence of normalized eigenvector k-tupples follows.To show that ν({x ∈ D : u(x) = 0}) = 0 it suffices to show that the set of x ∈ D for which u 1 (x) = 0 has zero Lebesgue measure. To show this, we need the extra technical condition that ρ ∈ C 1 (D). Because of it and the fact that ρ is bounded away from zero, it follows from the regularity theory of elliptic PDEs, that the function w 1 := u1 √ ρ is of class C 1,α (D) (for α ∈ (0, 1)) and is a solution of − div(ρ 2 ∇w 1 ) − τ 1 ρ 2 w 1 = 0, ∀x ∈ D. By the implicit function theorem, it follows that N (w 1 ) \ S(w 1 ) can be covered by at most countable d−1 dimensional manifolds and hence it follows that the Lebesgue measure of N (w 1 )\S(w 1 ) is equal to zero. On the other hand, it follows from the results in  #b21 , that S(w 1 ) is (d − 2)-rectifiable, which in particular implies that the Lebesgue measure of S(w 1 ) is equal to zero. Since u −1 1 ({0}) = N (w 1 ), we conclude that the set in which u 1 is equal to zero has zero Lebesgue measure.Proof of Corollary 1.7. Given a sequence {u n k } n∈N , as in the statement of the corollary, we define w n k := D 1/2 u n k .