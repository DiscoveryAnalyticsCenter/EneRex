Tensor Factorization for Computational Phenotyping

Abstract
Tensor factorization models o er an e ective approach to convert massive electronic health records into meaningful clinical concepts (phenotypes) for data analysis. ese models need a large amount of diverse samples to avoid population bias. An open challenge is how to derive phenotypes jointly across multiple hospitals, in which direct patient-level data sharing is not possible (e.g., due to institutional policies). In this paper, we developed a novel solution to enable federated tensor factorization for computational phenotyping without sharing patient-level data. We developed secure data harmonization and federated computation procedures based on alternating direction method of multipliers (ADMM). Using this method, the multiple hospitals iteratively update tensors and transfer secure summarized information to a central server, and the server aggregates the information to generate phenotypes. We demonstrated with real medical datasets that our method resembles the centralized training model (based on combined datasets) in terms of accuracy and phenotypes discovery while respecting privacy.

INTRODUCTION
Electronic health records (EHRs) become one of most important sources of information about patients, which provide insight into diagnoses  #b18  and prognoses  #b10 , as well as assist in the development of cost-e ective treatment and management programs  #b0  #b11 . X. Jiang and H. Yu are co-corresponding authors. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD'17, Halifax, Nova Scotia, Canada © 2017 Copyright held by the owner/author(s). 123-4567-24-567/08/06. . . $15.00 DOI:  $b10 .475/123 4 But meaningful use of EHRs is also accompanied with many challenges, for example, diversity of populations, heterogeneous of information, and data sparseness. e large degree of missing and erroneous records also complicates the interpretation and analysis of EHRs. Furthermore, clinical scientists are not used to the complex and high-dimensional EHR data  #b7  #b20 . Instead, they are more accustomed to reasoning based on accurate and concise clinical concepts (or phenotypes) such as diseases and disease subtypes. Useful phenotypes should capture multiple aspects of the patients (e.g., diagnosis, medication and lab results) and be both sensitive and speci c to the target patient population. Although some phenotypes can be easily concluded based on EHR data, a wide range of clinically important ones such as disease subtypes are not obtainable in a straightforward manner. e transformation from EHR data into useful phenotypes, or phenotyping is a fundamental challenge to learn from EHR data. Current approaches for translating EHR data into useful phenotypes are typically slow, manually intensive and limited in scope  #b3  #b4 . Overcoming several disadvantages of the previous methods, tensor factorization methods have shown great potential in discovering meaningful phenotypes from complicated and heterogeneous health records  #b12  #b13  #b27 .Nevertheless, phenotypes developed from one hospital are often limited due to a small sample size and inherent population bias. Ideally, we would like to compute phenotypes on a large population with data combined from multiple hospitals. However, this will require healthcare data sharing and exchange, which are often impeded by policies due to the privacy concerns. For example, PCORnet data privacy guidance does not allow record-level research participant information sharing and it recommends a minimum count threshold (e.g.,  #b9  for aggregate data sharing  #b24 . e same threshold is used in Informatics for Integrating Biology & the Bedsides (I2B2)  #b23 , a famous system developed by National Center for Biomedical Computing based at Partners HealthCare. e real-world challenges motivate the development of a federated phenotyping method to learn phenotypes across multiple hospitals with mitigated privacy risks.In the federated method, the hospitals perform most of computations, and a semi-trusted server supports the hospital by aggregating results from hospitals. e hospitals demand a certain form of summarized patient information (not patient-level data) anyhow for updating tensor. A challenge of the federated tensor factorization is that the summarized information can disclose the patientlevel data. For example, an objective function of tensor factorization is ||X − O|| 2 where X is a tensor to be estimated using an observed tensor O. Because the objective function is not linearly separable over hospitals, tensor factorization for each hospital inevitably demands the others patient-level data.us, hospitals should share summarized information that does not disclose the patient-level data but instead contains accurate phenotypes from the patient-level data.However, sharing the summarized information raises another challenge when the data are distributed in many hospitals as a relatively small size, or when the data are unevenly distributed. Because of sampling error, noise in the summarized information can increase with small patient populations. Accuracy then can be decreased or unstable. erefore, we need to ensure the robustness of summary information even with small sized or unevenly distributed samples.In this paper, we develop federated Tensor factorization for privacy preserving computational phenotyping (T ), a new federated framework for tensor factorization over horizontally partitioned data (i.e., data are partitioned based on rows or patients). Our major contributions are the following:i) Accurate and fast federated method: T is as accurate as centralized training model (based on combined datasets). e accuracy of T is robust on the patient size or distribution. T is fast compared to the centralized training model thanks to federated computation.ii) Rigorous privacy and security analysis: T preserves the privacy of patient data by transferring summarized information. We prove that the summarized information does not disclose the patient data.iii) Phenotype discovery from real datasets: Phenotypes that T discovers without sharing the patient-level data are the same phenotypes based on the combined data. T even discovers some phenotypes that individual hospital cannot discover due to biased and limited population.

RELATED WORKS
Many privacy preserving data mining algorithms aim at constructing a global model only from aggregated statistics locally generated by participating institutions on their own data, without seeing others' data at a ne-grained level  #b21  #b26 . More rigorous privacy criteria like di erential privacy  #b9 , which introduces noises, have been applied for several classi cation models through parameter or objective perturbations  #b5 . However, this is not desirable for computational phenotyping applications because noise can lead to "ghost" phenotypes, which do not exist in the original databases and might mislead healthcare providers with severe consequences. In this work, we will consider privacy protection like in the former privacy preserving data mining methods to compute phenotypes by only exchanging summary statistics, calculated by local participants.Tensor factorization emerged as a promising solution for computational phenotyping thanks to its interpretability and exibility. In the medical context, tensor factorization has been adapted to enforce sparsity constraints  #b12 , model interactions among groups of the same modality  #b13 , and absorbing prior medical knowledge via customized regularization terms  #b27 . Our goal is to develop a federated tensor factorization framework to compute phenotypes in a privacy-preserving way. is is di erent from distributed tensor factorization models  #b6  #b15  and grid tensor factorization models  #b8 . e la er assumes data spread across di erent but interconnected computer systems, in which the communication cost is negligible and data/computation can be arbitrarily reallocated to improve parallelization e ciency. In contrast, our T framework deals with data stored in separate sources (hospital at di erent locations) and requires the ability to go through policy barriers using accepted practices that respect privacy.

PRELIMINARIES
We rst describe some preliminaries of tensor factorization, and summarize the notations and symbols in Table 1.De nition 3.1. Outer product of N vectors a (1) • · · ·•a (N ) makes N -order rank-one tensor X.

De nition 3.2. Kronecker product of two vectors
a ∈ R I a ×1 and b ∈ R I b ×1 is a ⊗ b =        a 1 b . . . a I a b        ∈ R I a I b ×1 .

De nition 3.3. Kharti-Rao product of two matrices
A ∈ R I A ×R and B ∈ R I B ×R is A ⊙ B = [a 1 ⊗ b 1 · · · a R ⊗ b R ] ∈ R I A I B ×R .De nition 3.4. Matricization is to reshape the tensor into a matrix by unfolding elements of the tensor. Mode-n matricization of tensor O is denoted as O (n) .Tensor factorization is a dimensionality reduction approach that represents the original tensor as a lower dimensional latent matrix. e CANDECOMP/PARAFAC (CP)  #b2  model is the most common tensor factorization, which approximates the original tensor O as X, a linear combination of R rank-one tensors that are made from outer product of N vectors. at is, CP tensor factorization is represented asO ≈ X = R r =1 A (1) (:, r ) • · · · • A (N ) (:, r ),where A (n) (:, r ) refers to the r th column of A (n) . Here, A (n) is the nth factor matrix. R is referred as the rank of the X. e columns from factor matrices represent latent concepts that describe the data as lower dimensions.Tensor factorization for phenotyping is to compute a factorized tensor X that contains latent medical concepts from data (or observed tensor O). X consists of the R most prevalent phenotypes. e nth factor matrix, A (n) de nes the elements from the mode n to comprise the phenotypes. at is, r th phenotype consists of r th column of factor matrices  #b12 . e objective function of the tensor factorization with regularization terms for pairwise distinct constraints  #b27  is formulated  Ψ =||X − O|| 2 F + N n=1 λ 2 ||I − A (n)T A (n) || 2 F .(1)It is rewri en with respect to mode-n matricization  #b0  . is is our decomposition goal in the rest of this paper. Solving the problem (1) while preserving privacy is technically challenging because the tensor residual term X − O inherently contains other hospitals' patient data that involve sensitive information.min A (n) Ψ = ||A (n) Π (n)T − O (n) || 2 F + λ 2 ||I − A (n)T A (n) || 2 F . (2) where Π (n) = A (N ) ⊙ . . . ⊙ A (n+1) ⊙ A (n−1) ⊙ . . . ⊙ A (

FEDERATED TENSOR FACTORIZATION
We rst provide a general overview of the T and then formulate the problem with iterative updating rules for optimization.

Overview
T is a federated tensor factorization for horizontally partitioned patient data. We assume the data are horizontally partitioned along patient mode, that is, hospitals have their own patient data on the same medical features (Figs. 1, 2). Let us assume that there are K hospitals and a central server, where the server distributes most decomposition computation to hospitals and aggregates intermediate results from them. We assume Honest-but-Curious adversary model, in which the server and hospitals are curious on data of others but do not maliciously manipulate intermediate results  #b17 . A local observed tensor O k is the local patient data in hospital k (Fig. 2); a local factorized tensor X k is the factorized tensor generated by local observed tensor in hospital k, X k has N modes for the set of patient and medical features (eg. medication, diagnosis). In this case, N = 3 because we have modes for patient, medication, and diagnosis. e horizontally partitioned patient mode of each X k is generated from distinct set of patients whose size is I 1k . For simplicity, rst mode (n = 1) always denotes patient mode. On the other hand, N − 1 medical features modes that hospitals share of each X k is generated from the same set of N − 1 medical features whose size is I n , (n = 2, . . . , N ). For example, diagnosis and medication can be the feature modes. e size of X k isI 1k × I 2 × · · · × I N , ∀k.We assume that factor matrix on feature modes of the local factorized tensor X k is the same for all the hospitals. By assuming that, all hospitals are enforced to share the same phenotypes. Also, the objective function Ψ in Eq. (1) can be linearly separable on hospitals; consequently hospitals can update their local factorized tensor indirectly using other hospitals' patient data while respecting privacy. e local factorized tensor X k is computed as following steps: rst, in patient mode, hospital k (k = 1, . . . , K) computes local factor matrix independently (step 1) in Fig. 1. For feature modes, hospital k computes the local factor matrices (step 2) and send them together with the Lagrangian multipliers to the server (step 3). e server then generates harmonized factor matrix (global factor matrix) by combining all the local factor matrices with Lagrangian multipliers (step 4). A er receiving the global factor matrix (step 5), hospital k updates the Lagrangian multipliers (step 6). Hospitals and the server repeat the procedures until the local factor matrices are converged. During the procedures, the global factor matrices can retain phenotypes from local factor matrices without directly using the local patient data.

Formulation
We rst formulate separable objective function on hospitals for federated tensor factorization. e objective function for tensor factorization, Ψ in Eq. (1) is reformulated with respect to the local factorized tensor.X k is decomposed into factor matrices Ak ∈ R I 1k ×R (patient mode) and A (n) k ∈ R I n ×R , n ≥ 2 (feature modes). We assume that the local factor matrices of feature modes A (n) k from all hospitals are equal to the global factor matrix (Fig. 2), i.e.,A (n) = A (n) 1 = A (n) 2 = . . . = A (n) K , n ≥ 2.(3)is assumption is reasonable because all hospitals aim to have the same phenotypes and share them with others. By assuming Eq. (3), the horizontal concatenation of the local factor matrices of patient mode A  #b0  k forms the (global) factor matrix A (1) (Fig. 2):A (1) =         A(1)1 ;. . .A (1) K         .(4)Accordingly, we represent the global factorized tensor X in Eq. (1) with respect to the local factorized tensor X k (Fig. 2) asX =        X 1 ; . . . X K        =         r A (1) 1 (:, r ) • A (2) (:, r ) • · · · • A (N ) (:, r ); . . . r A (1) K (:, r ) • A (2) (:, r ) • · · · • A (N ) (:, r )         ,and we can make the objective function Ψ linearly separable on k as ||X − O|| 2F = K k=1 ||X k − O k || 2F . e optimization problem for tensor factorization is reformulated with respect to local tensors:min X k Ψ = K k=1 ||X k − O k || 2 F + N n=2 λ 2 ||I − B (n)T A (n) || 2 F s.t .A (n) = A (n) k n ≥ 2, ∀k B (n) = A (n) n ≥ 2.(5)Here, the non-convex second term||I − A (n)T A (n) || 2 F in Eq. (1) is replaced to a convex term ||I − B (n)T A (n) || 2 F using B (n) such that A (n) = B (n) .We assume that the pairwise constraint is only applied to the feature modes. is assumption is reasonable because phenotypes are de ned as only combination of medical features in feature modes.Augmented Lagrangian function L for the new optimization problem (5) isL = Ψ+ K k=1 N n=2 (A (n) − A (n) k ) T H (n) k + ω 2 ||A (n) − A (n) k || 2 F + N n=2 (B (n) − A (n) ) T Y (n) + µ 2 ||B (n) − A (n) || 2 F where H (n)k and Y (n) are the Lagrangian multipliers. e penalty terms that are multiplied by parameter ω and µ help L to improve the convergence property (i.e., method of multiplier)  #b22  during federated optimization in Section 4.3.

Federated optimization
e optimization problem (5) is then solved via consensus alternating direction method of multipliers (ADMM)  #b1 , which decomposes the original problem into sub-problems using auxiliary variables and ensures convergence to a stationary point even with nonconvex problem  #b14 . Our problem is decomposed to sub-problems for hospitals with respect to the local factor matrices. Individual components of the local factor matrices are iteratively updated while other local factor matrices are xed. Once all hospitals update the local factor matrices, server updates the global factor matrix and send it back to every hospital. Hospitals and server repeat this procedure until the local factor matrices converge before maximum iteration.

Patient mode.
Because the factor matrix for patient mode does not need to be shared, each hospital updates the local factor matrix without sharing the intermediate results.e local matricized residual tensor on patient mode is(X k − O k ) (1) = A (1) k Π (1)T − O (1)k ∈ R I 1k ×(I 2 ···I N ) .

Horizontal concatenation of the local matricized residual tensors
A (1) k Π (1)T − O (1)k from K hospitals becomes the global matricized residual tensor A (1) Π (1)T − O (1) . To compute A (1)k , we separate the rst term in Ψ in Eq. (1) to each hospital as||A (1) Π (1)T − O (1) || 2 F = K k=1 ||A (1) k Π (1)T − O (1)k || 2 F .(6)By se ing derivatives of Ψ with respect to A  #b0  k to zero, a closed form solution for updating A(1) k is A (1) k = {O (1)k Π (1) }{Π (1)T Π (1) } −1 .(7)4.3.2 Feature modes. Hospitals update the local factor matrices using the global factor matrix, and server makes the global factor matrix by aggregating the intermediate local factor matrices from hospitals in turn. Update the local factor matrices: e local matricized residual tensor on feature modes is(X k − O k ) (n) = A (n) Π (n)T k − O (n)k ∈ R I n ×(I N ···I n+1 I n−1 ···I 1k ) where Π (n) k = A (N ) k ⊙ . . . ⊙ A (n+1) k ⊙ A (n−1) k ⊙ . . . ⊙ A (1) k , n ≥ 2. Contrast to patient mode, vertical concatenation of the local matricized residual tensors A (n) Π (n)T k − O (n)k becomes the global matricized residual tensor A (n) Π (n)T − O (n) .e rst term in Ψ becomes||A (n) Π (n)T − O (n) || 2 F = K k=1 ||A (n) k Π (n)T k − O (n)k || 2 F (8) with A (n) = A (n) k . e closed form solution for A (n) k is A (n) k = {O (n)k Π (n) k + ωA (n) + H (n) k }{Π (n)T k Π (n) k + ωI} −1 . (9)is closed form solution updates the local factor matrices using the both local observed tensor O (n)k and global factor matrix A (n) .at is, each hospital uses both their patient data and the common phenotypes from others to update their local phenotypes. Now, hospitals send the local information A (n) k and H (n) k to server for following updates on the global factor matrix. Update the global factor matrix: Server updates the global factor matrix based on the local information. e objective function ismin A (n) λ 2 ||I − B (n)T A (n) || 2 F + µ 2 ||A (n) − B (n) − Y (n) /µ|| 2 F + ω 2 K k=1 ||A (n) − A (n) k + H (n) k /ω|| 2 Fthat also uses the pairwise constraint. A (n) is updated to be similar with A (n) k in the third term. at is, the global phenotypes are made to be similar with all other hospitals' phenotypes. By derivatives of this function with respect to A (n) , we derive the following closed form solution: k ∀k (Eq. 7). 6: for n = 2, . . . , N do //Update feature modes 7: Hospitals set and send A (n) k ∀k (Eq. 9). 8: Server sets and sends A (n) (Eq. 10). 9: Server sets B (n) and Y (n) (Eq.11, 12). 10: Hospitals set and send H (n) k ∀k (Eq. 13). 11: end for 12: until Converged Now, server sends the global information A (n) to hospitals for the next iteration. Server updates B (n) byA (n) ={(µ + Kω)I + λB (n) B (n)T } −1 ·{λB (n) + µB (n) + Y (n) + ω k A (n) k − k H (n) k }.B (n) = A (n) + 1 µ Y (n) .(11)Update Lagrangian multipliers: Finally, server updates Lagrangian multipliers asY (n) = Y (n) + µ(B (n) − A (n) ).(12)Hospitals also updates local Lagrangian multipliers asH (n) k = H (n) k + ω(A (n) − A (n) k )(13)to adjust the gap between local and global factor matrices. e entire procedures of updating the tensors are summarized in Algorithm 1.

Convergence proof
We prove that our federated tensor factorization (5) converges. Due to limited space, detailed proof of inequality (17) and (22) can be found in our technical report  #b16  or  #b1 . For each n = 2, · · · , N , let us denote r t = x t − z t ,(14)for vectorized local factor matrices, global factor matrix, Lagrangian multipliers, and residual at iteration t, respectively. en L is rewri en asL(x, z, ) = f (x) + (z) + T (x − z) + (ω/2)||x − z|| 2 (15) where f (x) = K k=1 ||A (n) k Π (n)T k − O (n)k || 2 and (z) = λ 2 ||I − B (n)T A (n) || 2 +(B (n) −A (n) ) T Y (n) + µ 2 ||B (n) −A (n) || 2 .Let (x * , z * , * ) be a saddle point, and de neV t = (1/ω)|| t − * || 2 + ω||z t − z * || 2 .(16)V t decreases in each iteration (proof in  #b16 ) becauseV t +1 ≤ V t − ω||r t +1 || 2 − ω||z t +1 − z t || 2 .(17)Adding the inequality (17) through t = 0 to ∞ gives ω ∞ t =0 ||r t +1 || 2 + ||z t +1 − z t || 2 ≤ V 0 ,(18)which implies r t → 0 and z t → z t +1 as t → ∞. Now, we de ne p t = f (x t ) + (z t ) and show p t converges. Because (x * , z * , * ) is a saddle point,L(x * , z * , * ) ≤ L(x t +1 , z t +1 , * ).(19)at is, using x * = z * at the saddle point,p * ≤ p t +1 + * T r t +1 + (ω/2)||r t +1 || 2 ,(20)which implies that upper bound of p * − p t +1 isp * − p t +1 ≤ * T r t +1 .(21)Lower bound of p * − p t +1 (proof in  #b16 ) isp * − p t +1 ≥ ( t +1 ) T r t +1 + ω(z t +1 − z t ) T (r t +1 + (z t +1 − z * )).(22)e upper and lower bounds go to zero because r t → 0 and z t → z t +1 as t → ∞, i.e., lim t →∞ p t = p * . us, the objective function Ψ of our federated optimization converges.

Privacy analysis
In our Honest-but-Curious adversary scenario, privacy of patient data is preserved because patient-level data are not disclosed to the both server and hospitals. e server and hospitals cannot access to unintended ne-grained local information.e local data are only accessible to the corresponding hospital. e server also cannot indirectly learn patient data from the local factor matrices. After receiving A (n) k , the server might try to do reverse-engineering through Eq. (9). However, server cannot access to Π  k is leaked, reverse-engineering cannot still restore patient-level data. at is, the matricized unknown observed tensor (patient data) has an equation in form of A (n) k = O (n)k Π k after removing all the known values in Eq. (9) for simplicity. e size of the unknown information in O (n)k is I n ×(I 1k · · · I n−1 I n+1 · · · I N ), and the size of Π (n) k and A (n) k is (I 1k · · · I n−1 I n+1 · · · I N )×r and I n ×r , respectively. Element-wise computation generates only I n ·r equations for the unknown I 1 · · · I N values. Server cannot recover the unknown values from the I n ·r equations that is less than the number of unknown values (r is always selected as I n · r ≪ I 1 · · · I N ).Hospitals also cannot learn other hospitals' data from the global factor matrix. If hospital k ′ knows all the information of Eq. (10) for global factor matrix by any chance, hospital k ′ cannot restore other hospitals' local factor matrix A  k has still insu cient information to recover the data as shown in the case of server.

Secure alignment of feature modes
In Section 4.2, we rst assume that hospitals have the same element set for each feature in feature modes, but in practice, hospitals may have di erent elements. For example, Hospital 1 and Hospital 2 have set of diagnosis: Y 1 and Y 2 (Fig. 3), but each index of Y 1 and Y 2 refers to a di erent element. In this case, before concatenating the local tensors, the index of feature modes should refer to the same element among hospitals. us, we introduce a secure alignment method for feature modes by which hospitals do not reveal the elements they have and get an integrated and sorted view on the elements of feature modes. is secure alignment enables hospitals to have only the position of its elements without knowing other hospitals' elements as like Y 1 and Y 2 are aligned to make the index from two sets refer to the same element (Fig. 3). For each feature mode, hospital k assigns integer values to the set of elements Y k (eg. ICD9 codes). Hospitals use polynomial properties of set intersection  #b17 :L 4.1. A polynomial function of that represents set of ele- ments Y k = { ik } I i =1 at hospital k is f k ( ) = ( − 1k )( − 2k ) · · · ( − I k ) = I i =0 a i i . A ik is an element of Y k ( ik ∈ Y k ) if and only if f k ( ik ) = 0. L 4.2. A polynomial function that represents intersection of Y k and Y k ′ (Y k ∩ Y k ′ ) is f k * r + f k ′ * s where r, s are polynomial functions with cd(r, s) = 1. Given f k * r + f k ′ * s, one cannot learn individual elements on Y 1 and Y 2 other than elements in Y 1 ∩ Y 2 .Hospitals express Y k as a polynomial function (or in short polynomial) f k by Lemma 4.1. To prevent the factorization of the polynomial, hospital k multiplies a term r = ( −α) to f k (=f k * r ), where α is a random prime number that is selected with overwhelming probability that the α does not represent any element from Y k . For simplicity, f k * r is denoted as f k . Because computing the polynomials with large |Y k | can cause computational overhead, hospitals compute the polynomials' modulus (denoted as %) by a random prime number P (P > k ) instead of the polynomial itself, i.e., hospitals compute f k %P = I i =0 (a i %P) i %P by equivalence of modulus operation and use it instead of f k .en server receives f k %P from hospitals. To nd a pairwise intersection between hospital k and k ′ , server computes a pairwise sum of polynomials as[f k %P + f k ′ %P] %P = [f k + f k ′ ]%P, which refers to the polynomial for intersection between hospital k and k ′ . Server repeats this procedure for every pair of k and k ′ (k ′ k), and send the K − 1 polynomials to each hospital. Hospital k then checks whether its element k ∈ Y k is in the pairwise intersection of hospital k and other hospitalk ′ , that is, if [f k ( k )+ f k ′ ( k )]%P = 0, then k ∈ Y k ∩ Y k ′ by Lemma 4.2.By combining all the pairwise intersection with K − 1 hospitals, hospital k checks whether the element k is in the intersection of all the K hospitals. For example, combining the pairwise intersection of f 1 ( 1 ) + f 2 ( 1 ) = 0 (i.e., 1 ∈ Y 1 ∩Y 2 ) andf 1 ( 1 ) + f 3 ( 1 ) 0 (i.e., 1 Y 1 ∩ Y 3 ) gives 1 ∈ Y 1 ∩ Y 2 ∩ Y c 3 .A er obtaining 2 K −1 intersections with Y k , hospital k sends the size of 2 K −1 intersections to server. Server collects the size of intersection from all the K hospitals and obtain the size of all the 2 K − 1 combinations of intersections (the number of combinations is two cases, whether in or out, for every Y k except one case when Y c 1 ∩ . . . ∩Y c K ). Finally, hospitals receive the size of 2 K − 1 intersection, and align their elements according to the size information. Hospitals have the same order of these intersections such as (Fig.  3). e elements within the intersections are sorted. us, all hospitals have the same size and order of elements for every feature mode.Y 1 ∩ Y 2 , Y 1 ∩ Y c 2 , Y c 1 ∩ Y 2

EXPERIMENTS
We evaluate T by measuring computational performance (accuracy and time) and deriving meaningful phenotypes. We compare T with two baselines: i) Central model: Ordinary tensor factorization method for phenotyping. Regardless of privacy problem that concerns data sharing, this model runs on a central server where all the patient data are combined  #b27 . ii) Local model: We devise an intuitive local model, by which hospitals run the central model at their sides and send the nal factor matrices of feature modes to server. Server averages the factor matrices and sends the averaged factor matrices back to hospitals without iterative updating like T . Because each column in factor matrices can represent di erent phenotypes over hospitals, before averaging the matrices, server sorts the columns of each hospital's factor matrix so that all hospitals have the same phenotypes at each column. For all feature modes n, server rst chooses a pivot hospital k p and computes cosine similarity between every pair of r p th and r th column from factor matrix of hospital k p and other hospitals k as similarity = cos(A (n) k p (:, r p ), A (n) k (:, r )) where ∀k k p , ∀r r p . Server then nds the most similar combination of r p and r for all pairs. Finding the best combination that matches multiple items (columns) to multiple items can be solved in polynomial time by Hungarian method  #b19 . Finally, server changes the order of columns in A 

Accuracy and Time
We use a large publicly available dataset MIMIC-III containing deidenti ed health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012  #b25 . MIMIC-III includes information such as demographics, laboratory test results, procedures, medications, caregiver notes, imaging reports, and mortality. We construct a 3-order tensor with patients, laboratory test results, and medication. e tensor value is the number of cooccurrences of abnormal lab results and medication from the same patient within speci c time window. We generate four datasets as se ing the time window as 3 hours, 6 hours, 1 day, or 7 days, and have the number of nonzero values of around 15 million (M), 25M, 40M, and 50M, respectively. e size for 7-day-window tensor (MIMIC-III 50M) is 38,035 patients by 3,229 medications by 304 lab results. Because duplicated co-occurrence can be counted with large time window, we set the maximum value of count as three, which is a median of 1-day-window tensor (MIMIC-III 40M). e count value larger than three is truncated to three.  Figure 4: RMSE and total time over the number of nonzeros (Fig. 4a, 4b). e rst, second, and third stacked bars in Fig. 4b  refer to central model, T , and local model, respectively. RMSE of T , central model, and local model over iteration (Fig.  4c, 4d).  Figure 5: RMSE over the number of hospitals (Fig. 5a) and skewness (Fig. 5b). Total time over the number of hospitals (Fig. 5c) and skewness (Fig. 5d). e former and latter stacked bars in Fig. 5c, 5d refer to T and local model, respectively.We evaluate accuracy and time of T compared to two baseline models by varying the number of nonzero values, hospitals, and skewness (for unevenly distributed patients). We measure accuracy using root mean square error (RMSE) between the factorized tensor and the observed tensor. We also measure time elapsed by adding time for computation, communication, and alignment. Because T and local model distribute computation of local tensors to hospitals, we consider the computation time on the local tensors as the largest computation time on one hospital. e communication time is measured as the total number of communicated bytes between server and hospitals divided by data transfer rate of 15 MB/sec. e communication time for central model is time for transferring the local patient data to server. We repeat the evaluation ten times and average them. We run the models until it converges before maximum iteration 100. e rank is set to ten. λ is set to 10 −2 a er trying 10 −3 , 10 −2 , 10 −1 , 1, and 10.

Number of nonzeros.
We use the four MIMIC-III datasets that have 15M, 25M, 40M, and 50M nonzero values. We assume that MIMIC-III datasets are distributed in three hospitals, on which the patients are randomly distributed as the same size. As a result, T has low RMSE as much as central model and resembles central model for all the four datasets (Fig. 4a). For MIMIC-III 15M, RMSE from central model converges to 1.4404. Similarly, the RMSE from T starts to stable at around 50 iterations and converges to 1.4409 ( Fig. 4c). Both of the RMSE from central model and T are significantly smaller than that of local model (1.5957). MIMIC-III 50M dataset also shows similar convergence. RMSE from T starts to stable at around 60 iterations and converges to 1.8482, which is also similar to RMSE from central model, 1.8479 (Fig. 4d). MIMIC-III 25M shows the RMSE of 1.4955 from T , 1.4947 from central model, 1.6867 from local model. MIMIC-III 40M shows the RMSE of 1.7913 from T , 1.7903 from central model, 2.0037 from local model. Convergence results on MIMIC-III 25M and 40M can be found in our technical report  #b16 .In addition, total time elapsed for T (and local model) is much faster (half less) than central model in all datasets (Fig. 4b). T reduces computation time by distributing updating procedures to decentralized hospitals; consequently, T reduces total time elapsed although sacri cing communication and alignment time. 

Number of hospitals.
Using MIMIC-III 15M dataset, we partition the patients evenly into one to ve hospitals. RMSE with one hospital refers to RMSE of central model. We observed that RMSE of T is stable as the number of hospitals increases, and is similar to RMSE of central model 1.4404, whereas RMSE of local model increase (Fig. 5a) with large variance. at is, compared to local model, in which local factorized tensors are diverged, T is robust on the nely split data. It means that phenotypes from T are accurate and not biased even with many small sized patient data.Total time of T and local model are signi cantly faster than that of central model. As the number of hospitals increases and the patient data are spread more, the total time of T and local model decrease (Fig. 5c). Speci cally, computation time for T and local model decrease because more hospitals distribute the computation, and communication time for T slightly increases, whereas communication time for local model is negligibly short. Alignment time is negligible for both T and local model.

Skewness.
We partition the patients in MIMIC-III 15M unevenly in three hospitals. One hospital takes 1/3 (evenly distributed), 0.5, 0.7, and 0.9 of patients, and the other two hospitals take the remaining patients evenly. Note that elements in feature mode are still overlapped enough among hospitals. We observed that RMSE of T is stable although patients are distributed unevenly, whereas RMSE of local model is higher than that of T with large variance. Factorized tensor of local model can be inaccurate because the local factorized tensor from a small sized hospital can be biased and far di erent from others' results. However, the hospital can bene t from T by overcoming this bias and producing a generalized results.Total time of T and local model increase (Fig. 5d) as the skewness increases. Time for computation increases because computational overhead occurs on one hospital with large data, and time for communication and alignment does not increase signi cantly.

Phenotype discovery
We use de-identi ed EHRs dataset from University of California, San Diego (UCSD) Medical Center with 8,022 patients by 748 medications by 299 diagnoses. Speci cally, it is from two hospitals that have 4,703 patients (UCSD1) and 3,319 patients (UCSD2). We construct a 3-order tensor with patient, medication, and diagnosis mode with around 1.6 million of non-zero elements. e value of tensor is the number of co-occurrences of medication and diagnosis event from the same patient at the same visit.We discover phenotypes from T and compare them with phenotypes from central model and individual central model of two hospitals in UCSD (i.e., run central model independently at UCSD1 and UCSD2). λ is set 1 to derive more distinct phenotypes than those from MIMIC-III. A domain expert summarizes the factorized tensor into clinically meaningful phenotypes. e phenotypes consist of set of diagnoses and its corresponding medications. Due to limited space, medication factors in phenotypes are omi ed and can be found in our project website  #b16 .As a result, T discovers unbiased and hidden phenotypes compared to the phenotypes from two individual central models (UCSD1, UCSD2). e phenotypes from T contain top-ranked phenotypes from UCSD1 and UCSD2, and are similar to phenotypes from combined central model, UCSD1+UCSD2 (Table 2). e phenotypes from T consist of top ve phenotypes from UCSD1 and top four from UCSD2. e phenotypes from T are also the same with phenotypes from central model except gastrointestinal complaints and neurogenic bladder. Without our federated model, the two individual hospitals could derive biased phenotypes that are only ed to the local data. It means that T can e ectively resemble central model without cost for privacy.In addition, T discovers a new phenotype, sickle cell/chronic pain crisis, that is contained in neither of UCSD1 and UCSD2. is phenotype consists of diagnoses related to sickle cell diseases or chronic pain crisis and corresponding medications (Table 3). Based on physician's judgement, this phenotype is clinically meaningful in that sickle cell disease usually accompanies chronic pain such as constipation, back/neck pain, headache, (pruritic disorder, insomnia, and wheezing. sickle cell/chronic pain crisis is not dominant in individual hospital but is dominant in overall perspective. Note that RMSE of T is low as much as RMSE of central model while reducing total time (Table 4), and RMSEs of two individual UCSD datasets are lower than others because those two use separated small local datasets. Also, note that communication time of the central model is due to transferring the data.

CONCLUSIONS
is paper presents T , a federated tensor factorization for computational phenotyping without sharing patient-level data. We developed secure data harmonization and privacy-preserving computation procedures based on ADMM, and analyzed that T ensure the con dentiality of patient-level data. Experimental results on data from MIMIC-III and UCSD medical center demonstrated that our framework resembles the central model very well. T is also accurate even with small or skewly distributed patient data, and fast compared to the central model. We also showed that T discovers phenotypes as the central model with combined patient data does, which are unbiased or not discovered (hidden) phenotypes from each hospital. As a result, T can help derive useful phenotypes from EHR data to overcome policy barriers due to the privacy concerns. We plan to apply it to much larger scale datasets in the future and facilitate the discovery of novel and important "phenotypes" to support clinical research and precision medicine.