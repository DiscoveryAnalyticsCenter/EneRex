SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction

Abstract
In online social networks people often express attitudes towards others, which forms massive sentiment links among users. Predicting the sign of sentiment links is a fundamental task in many areas such as personal advertising and public opinion analysis. Previous works mainly focus on textual sentiment classification, however, text information can only disclose the "tip of the iceberg" about users' true opinions, of which the most are unobserved but implied by other sources of information such as social relation and users' profile. To address this problem, in this paper we investigate how to predict possibly existing sentiment links in the presence of heterogeneous information. First, due to the lack of explicit sentiment links in mainstream social networks, we establish a labeled heterogeneous sentiment dataset which consists of users' sentiment relation, social relation and profile knowledge by entity-level sentiment extraction method. Then we propose a novel and flexible end-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework to extract users' latent representations from heterogeneous networks and predict the sign of unobserved sentiment links. SHINE utilizes multiple deep autoencoders to map each user into a low-dimension feature space while preserving the network structure. We demonstrate the superiority of SHINE over state-of-the-art baselines on link prediction and node recommendation in two real-world datasets. The experimental results also prove the efficacy of SHINE in cold start scenario.

INTRODUCTION
The past decade has witnessed the proliferation of online social networks such as Facebook, Twitter and Weibo. In these social network sites, people often share feelings and express attitudes towards others, e.g., friends, movie stars or politicians, which forms sentiment links among these users. Different from explicit social links indicating friend or follow relationship, sentiment links are implied by the semantic content posted by users, and involve different types: positive sentiment links express like, trust or support attitudes, while negative sentiment links signify dislike or disapproval of others. For example, a tweet saying "Vote Trump!" shows a positive sentiment link from the poster to Donald Trump, and "Trump is mad..." indicates the opposite case.For a given sentiment link, we define its sign to be positive or negative depending on whether its related content expresses a positive or negative attitude from the generator of the link to the recipient  #b13 , and all such sentiment links form a new network topology called sentiment network. Previous work  #b5  #b10  #b14  mainly focuses on sentiment classification based on the concrete content posted by users. However, they cannot detect the existence of sentiment links without any prior content information, which greatly limits the number of possible sentiment links that could be found. For example, if a user does not post any word concerning Trump, it is impossible for traditional sentiment classifiers to extract the user's attitude towards him because "one cannot make bricks without straw". Therefore, a fundamental question is, can we predict the sign of a given sentiment link without observing its related content? The solution to this problem will benefit a great many online services such as personalized advertising, new friends recommendation, public opinion analysis, opinion polls, etc.Despite the great importance, there is little prior work concerning predicting the sign of sentiment links among users in social networks. The challenges are two-fold. On the one hand, lack of explicit sentiment labels makes it difficult to determine the polarity of existing and potential sentiment links. On the other hand, the complexity of sentiment generation and the sparsity of sentiment links make it hard for algorithms to achieve desirable performance. Recently, several studies  #b11  #b13  #b30  #b34  propose methods to solve the problem of predicting signed links. However, they rely heavily on manually designed features and cannot work well in real-world scenarios. Another promising approach called network embedding  #b7  #b16  #b22  #b25 , which automatically learns features of users in network, seems plausible to solve the task. However, they can only apply to networks with positive-weighted (i.e., unsigned) and singletype (i.e., homogeneous) edges, which limits their power in the task of practical sentiment link prediction. Based on the above facts, in this paper we investigate the problem of predicting sentiment links in absence of sentiment related content in online social networks. Our work is two-step. First, considering the lack of labeled data, we establish a labeled sentiment dataset from Weibo, one of the most popular social network sites in China. We leverage state-of-the-art entity-level sentiment extraction method to calculate the sentiment of the poster towards the celebrity in each tweet. Besides, to handle the sparsity problem, we collect two additional types of side information: social relationship among users and profile knowledge of users and celebrities. Our choices are enlightened by  #b26  and  #b33 , respectively, in which  #b26  demonstrates that the structural information of social networks can greatly affect users' preference towards online items, and  #b33  proves that information from knowledge base could boost the performance of recommendation. The heterogeneous information networks are illustrated in Fig. 1.To explore more possible sentiment links from the network, in the second step, we propose a novel end-to-end framework termed as Signed Heterogeneous Information Network Embedding (SHINE). Greatly different from existing network embedding approaches, SHINE is able to learn user representation and predict sentiment from signed heterogeneous networks. Specifically, SHINE adopts multiple deep autoencoders  #b19 , a type of deep-learning-based embedding technique, to extract users' highly nonlinear representations from the sentiment network, social network and profile network, respectively. The learned three types of user representations are subsequently fused together by specific aggregation function for further sentiment prediction. In addition to the adaptability to signed heterogeneous networks, the superiority of SHINE also lies in its end-to-end prediction technology and high flexibility of adding or removing modules of side information (i.e., social relationship and profile knowledge), which is discuss in Section 5.We conduct extensive experiments on two real-world datasets. The results show that SHINE achieves substantial gains compared with baselines. Specifically, SHINE outperforms other strong baselines by 8.8% to 16.8% in the task of link prediction on Accuracy, and by 17.2% to 219.4% in the task of node recommendation on Recall@100 for positive nodes. The results also prove that SHINE is able to utilize the side information efficiently, and maintains a decent performance in cold start scenario.

RELATED WORK 2.1 Signed Link Prediction
Our problem of predicting positive and negative sentiment links connects to a large body of work on signed social networks, including trust propagation  #b8 , spectral analysis  #b12 , and social media mining  #b21 . For the link prediction problem in signed graphs, Leskovec. et al.  #b13  adopt signed triads as features for prediction based on structural balance theory. Ye et al.  #b30  utilize transfer learning to leverage edge sign information from source network and improve prediction accuracy in target network. Tang et al. design NeLP framework  #b20  which exploits positive links in social media to predict negative links. The difference between the above work and ours is that we construct a labeled dataset by entity-level sentiment extraction method, as there is no explicit signed links in mainstream online social networks. Besides, we use state-of-the-art deep learning approach to learn the representation of links.

Network Embedding
There is a long history of work on network embedding. Earlier works such as IsoMap  #b23  and Laplacian Eigenmap [1] first construct the affinity graph of data using the feature vectors and then embed the affinity graph into a low-dimension space. Recently, DeepWalk  #b16  deploys random walk to learning representations of social network. LINE  #b22  proposes objective functions that preserve both local and global network structures for network embedding. Node2vec  #b7  designs a biased random walk procedure to learn a mapping of nodes that maximizes the likelihood of preserving network neighborhoods of nodes. SDNE  #b25  uses autoencoder to capture first-order and second-order network structures and learn user representation. However, these methods can only address unsigned and homogeneous networks. Additionally, several studies focus on representation learning in the scenario of heterogeneous network  #b2  #b31 , attributed network  #b9 , or signed network  #b28  #b32 . However, these methods are specialized in only one particular type of networks, which is not applicable to the problem of sentiment prediction in real-world signed and heterogeneous networks.

DATASET ESTABLISHMENT
In this section we introduce the process of collecting data from online social networks, and discuss the details of how to extract sentiment towards celebrities from tweets.

Data Collection
3.1.1 Weibo Tweets. We select Weibo 1 as the online social networks studied in this work. Weibo is one of the most popular social network sites in China which is akin to a hybrid of Facebook and Twitter. We collected 2.99 billion tweets on Weibo from August 14, 2009 to May 23, 2014 as raw dataset. To filter out useful data which contains sentiment towards celebrities, we first apply Jieba 2 , the most popular Chinese text segmentation tool, to tag the part of speech (POS) of each word for each tweet. Then we select those tweets containing words with POS tagging as "person name" which exist in our established celebrity database (detailed in Section 3.1.4). After getting the set of candidate tweets, for each tweet we calculate its sentiment value (-1 to +1) towards the mentioned celebrities, and select those tweets with high absolute sentiment values. The final dataset consists of a set of triples (a, b, s), where a is the user who posts the tweet, b is the certain celebrity mentioned in the tweet, and s ∈ {+1, −1} is the sentiment polarity of user a towards user b. The method of calculating sentiment values is detailed in Section 3.2.

Social
Relation. In addition to the sentiment dataset, we also collect the social relation among users from Weibo. The dataset of social relation consists of tuples (a, b), where a is the follower and b is the followee. 3.1.4 Profile of Celebrities. We use Microsoft Satori 3 knowledge base to extract profile of celebrities. First, we traverse the knowledge base and select terms with object type as "person". Then we filter out popular celebrities with high edit frequency in knowledge base and high appearance frequency in Weibo tweets. For each of these "hot" celebrities, we extract 9 attributes as his profile information: place of birth, date of birth, ethnicity, nationality, specialization, gender, height, weight, and astrological sign. Values of these attributes are discretized so that every celebrity's attribute values can be expressed as one-hot vectors. Furthermore, we remove celebrities with ambiguous names as well as other noises.

Sentiment Extraction
To extract users' sentiment towards celebrities in tweets, we first generate a sentiment lexicon consisting of words and their sentiment orientation (SO) scores. To achieve this, we manually construct a emoticon-sentiment mapping file and map each tweet to positive or negative class according to the label of emoticon appeared in the tweet. For example, "I love Kobe! [kiss]" is mapped to positive class if the key-value pair ([kiss], positive) exists in the emoticonsentiment mapping file. Note that the class of emoticon cannot be directly regarded as the sentiment towards celebrities since we found a large number of mismatch cases, e.g., "Miss you Taylor Swift [cry][cry]". Afterwards, for each word (segmented by Jieba) with occurrence frequency from 2,000 to 10,000,000 in the raw tweets datasets, similar to  #b1 , we calculate its SO score asSO(word) = PMI (word, pos) − PMI (word, neд),(1)where PMI is the point-wise mutual information  #b24  defined as PMI (x, y) = log After getting the lexicon, we use SentiCircle  #b18  to calculate sentiment towards celebrities in each tweet. Given a piece of tweet as well as the mentioned celebrity, we represent the contextual semantics of the celebrity as a polar coordinate space, where the celebrity is situated in the origin and other terms in the tweet are scattered around. Specifically, for celebrity term c, the coordinate of term t i is (r i , θ i ), where r i is the inverse of distance between c and t i in syntax dependence graph generated by LTP  #b3 , and θ i = SO(t i ) · π . The overall sentiment towards the celebrity c is, therefore, approximated as the geometric center of all terms c i . We take the projection of the geometric center on y-axis as final sentiment value towards the celebrity.To validate the effectiveness of sentiment extraction, we randomly select 1,000 tweets (500 positive and 500 negative tagged by our method) in Weibo sentiment dataset, and manually label each one of them. The result shows that the precision is 95.2% for positive class and 91.0% for negative class, which we believe is accurate enough for subsequent experiments. The basic statistics of Weibo sentiment datasets is presented in Table 1.

PROBLEM FORMULATION
In this section we formulate the problem of predicting sentiment links in heterogeneous information networks. For better illustration, we split the original heterogeneous network into the following three single-type networks:Sentiment network. The directed sentiment network is denoted as G s = (V , S), where V = {1, ..., |V |} represents the set of users (either ordinary users or celebrities) and S = {s i j | i ∈ V , j ∈ V } represents sentiment links among users. Each s i j can take the value of +1, −1 or 0, representing that user i holds a positive, negative, or unobserved sentiment towards user j, respectively.Social network. The directed social network is denoted asG r = (V , R), where R = {r i j | i ∈ V , j ∈ V }represents social links among users. Each r i j can take the value of 1 or 0, representing that user i follows user j or not in the social network.Profile network. We denote A = {A 1 , ..., A | A | } the set of user's attributes, and a kl ∈ A k the l-th possible value of attribute A k . We take the union of all possible values of attributes and renumber them as U = A k = {a j | j = 1, ..., k |A k |}. Then the undirected bipartite profile network can be denoted as G p = (V , U , P), where P = {p i j | i ∈ V , a j ∈ U } represents profile links between users and attribute values. Each p i j can take the value of 1 or 0, representing that user i possesses attribute value j or not. The three networks are illustrated in Fig. 2. Sentiment links prediction. We define the problem of predicting sentiment links in heterogeneous information networks as follows: Given the sentiment network G s , social network G r and profile network G p , we aim to predict the sentiment of unobserved links between users in G s .

SIGNED HETEROGENEOUS INFORMATION NETWORK EMBEDDING
In this section we introduce the proposed SHINE model. We first show the whole framework of SHINE. Then we present the details of the SHINE model, including how to extract user representation jointly from the three networks as well as the learning algorithm. At last we give some discussions on the model.

Framework
In this paper we propose an end-to-end SHINE model to predict sentiment links. The framework of SHINE is shown in Fig. 3. In general, the whole framework consists of three major components: sentiment extraction and heterogeneous networks construction (the left part), user representation extraction (the middle part), as well as representation aggregation and sentiment prediction (the right part). For each tweet mentioning a specific celebrity, we first calculate the associated sentiment (discussed in Section 3.1), and represent the user and the celebrity in this sentiment link by using their neighborhood information from the three constructed networks (introduced in Section 4). We then design three distinct autoencoders to extract short and dense embeddings from original sparse neighborhood-based representation respectively, and aggregate these three kinds of embeddings into final heterogeneous embedding. The predicted sentiment can thus be calculated by applying specific similarity measurement function (e.g., inner product or logistic regression) to the two heterogeneous embeddings, and the whole model can be trained based on the predicted sentiment and the target (i.e., the ground truth obtained in sentiment extraction step). In the following subsections we will introduce SHINE model in detail.

Sentiment Network Embedding
Given the sentiment graph G s = (V , S), for each user i ∈ V , we define its sentiment adjacency vectorx i = {s i j | j ∈ V } ∪ {s ji | j ∈ V }.Note that x i fully contains the global incoming and outgoing sentiment information of user i. However, it is impractical to take x i directly as the sentiment representation of user i, as the adjacency vector is too long and sparse for further processing. Recently, a lot of network embedding models  #b7  #b16  #b22  #b25  are proposed, which aim to learn low-dimension representations of vertices while preserving the network structure. Among those models, deep autoencoder is proved to be one of state-of-the-art solutions, as it is able to capture highly nonlinear network structure by using deep models  #b25 .In general, autoencoder  #b19  is an unsupervised neural network model of codings aiming to learn a representation of a set of data. Autoencoder consists of two parts, the encoder and the decoder, which contains multiple nonlinear functions (layers) for mapping the input data to representation space and reconstructing original input from representation, respectively. In our SHINE model, we propose to use autoencoders for efficiently user representation learning. Fig. 4 illustrates the autoencoder for sentiment network embedding. As shown in Fig. 4, the sentiment autoencoder maps each user to a low-dimension latent representation space and recover original information from latent representation by using multiple fully-connected layers. Given the input x i , the hidden representations for each layer arex k i = σ W k s x k −1 i + b k s , k = 1, 2, ..., K s ,(2)where W k s and b k s are weight and bias parameters of layer k in the sentiment autoencoder, respectively, σ (·) is the nonlinear activation function, K s is the number of layers of sentiment autoencoder, andx 0 i = x i . For simplicity, we denote x ′ i = x K s i the reconstruction of x i .The basic goal of the autoencoder is to minimize the reconstruction loss between input and output representations. Similar to  #b25 , in SHINE model the reconstruction loss term of sentiment autoencoder is defined as  Fig. 4: Illustration of a 6-layer autoencoder for sentiment network embedding.L s = i ∈V (x i − x ′ i ) ⊙ l i 2 2 ,(3)where ⊙ denotes the Hadamard product, and l i = (l i,1 , l i,2 , ..., l i,2 |V | ) is the sentiment reconstruction weight vector in whichl i, j = α > 1, i f s i j = ±1; 1, i f s i j = 0.(4)The meaning of the above loss term lies in that we impose more penalty to the reconstruction error of the non-zero elements than that of zero elements in input x i , as a non-zero s i j carries more explicit sentiment information than an implicit zero s i j . Note that the sentiment embedding of user i can be obtained from the layer K s /2 in the sentiment autoencoder, and we denote x i = x K s /2 i the sentiment embedding of user i for simplicity.

Social Network Embedding
Similar to previous sentiment network embedding, we apply autoencoder to extract user representation from the social network. Given the social network G r = (V , R), for each user i ∈ V , we define its social adjacency vector y i = {r i j | j ∈ V } ∪ {r ji | j ∈ V }, which fully contains the structural information of user i in the social network. The hidden representations of each layer in the social autoencoder arey k i = σ W k r y k −1 i + b k r , k = 1, 2, ..., K r ,(5)where the meaning of notations are similar to those in Eq. (2). We also denote y ′ i = y K r i the reconstruction of y i . Similarly, the reconstruction loss term of social autoencoder is L r = i ∈V (y i − y ′ i ) ⊙ m i 2 2 ,(6)

Profile Network Embedding
The profile network G p = (V , U , P) is an undirected bipartite graph which consists of two disjoint sets of users and attribute values. For each user i ∈ V , its profile adjacency vector is defined as z i = {p i j | j ∈ U }. User i's hidden representations of each layer in the profile autoencoder arez k i = σ W k p z k −1 i + b k p , k = 1, 2, ..., K p ,(7)where the meaning of notations are similar to those in Eq.  #b1 . We also use the notation z ′ i to denote the reconstruction of z i . Therefore, the reconstruction loss term of profile autoencoder isL p = i ∈V (z i − z ′ i ) ⊙ n i 2 2 ,(8)where n i is the profile reconstruction weight vector defined similarly to m i in the previous subsection. The profile embedding of user i is denoted as z i = zK p /2 i .

Representation Aggregation and Sentiment Prediction
Once we obtain the sentiment embedding x i , social embedding y i , and profile embedding z i of user i, we can aggregate these embeddings into final heterogeneous embedding e i by specific aggregation function д(·, ·, ·). We list some of the available aggregation functions as follows:• Summation  #b33 , i.e., e i = x i + y i + z i ; • Max pooling  #b27 , i.e., e i = element-wise-max(x i , y i , z i ); • Concatenation [23], i.e., e i = ⟨ x i , y i , z i ⟩.Finally, given two users i and j as well as their heterogeneous embedding e i and e j , the predicted sentiments i j can be calculated ass i j = f (i, j), where f (·, ·) is specific similarity measurement function. For example:• Inner product  #b2  #b4 , i.e.,s i j = e T i e j + b, where b is a trainable bias parameter; • Euclidean distance  #b25 , i.e.,s i j = −∥e i − e j ∥ 2 + b, where b is a trainable bias parameter; • Logistic regression  #b16 , i.e.,s i j = W T ⟨e i , e j ⟩ +b, where W and b are trainable weights and bias parameters.We will study the choices of f and д in the experimental part.

Optimization
The complete objective function of SHINE model is as follows:L = i ∈V (x i − x ′ i ) ⊙ l i 2 2 + λ 1 i ∈V (y i − y ′ i ) ⊙ m i 2 2 + λ 2 i ∈V (z i − z ′ i ) ⊙ n i 2 2 + λ 3 s i j =±1 f (e i , e j ) − s i j 2 + λ 4 L r eд ,(9)where λ 1 , λ 2 , λ 3 and λ 4 are balancing parameters. The first three terms in Eq. (9) are the reconstruction loss terms of sentiment autoencoder, social autoencoder, and profile autoencoder, respectively. The fourth term in Eq. (9) is the supervised loss term for penalizing the divergence between predicted sentiment and ground truth. The last term in Eq. (9) is the regularization term that prevents over-fitting, i.e.,L r eд = K s k =1 W k s 2 2 + K r k =1 W k r 2 2 + K p k =1 W k p 2 2 + f 2 2 ,(10)where W k s , W k r , W k p are the weight parameters of layer k in the sentiment autoencoder, social autoencoder, and profile autoencoder, respectively, and ∥ f ∥ 2 2 is the regularization penalty for similarity measurement function f (·, ·) (if appropriate).We employ the AdaGrad  #b6  algorithm to minimize the objective functions in Eq.  #b8 . In each iteration, we randomly select a batch of sentiment links from training dataset and compute the gradient of the objective function with respect to each trainable parameter respectively. Then we update each trainable parameter according to the AdaGrad algorithm till convergence.

Discussions
5.7.1 Asymmetry. Many real-world networks are directed, which implies that for two nodes i and j in the network, edges (i, j) and (j, i) may coexist and their values are not necessarily identical. A few recent studies have focused on this asymmetry issue  #b15  #b35 . In this work, whether the basic SHINE model can characterize asymmetry depends on the choice of similarity measurement function f . Specifically, SHINE is capable of dealing with the direction of a link if and only if f (i, j) f (j, i) (e.g., logistic regression). However (and fortunately), even if we choose a symmetric function (e.g., inner product or Euclidean distance) as f , we can still easily extend the basic SHINE model to asymmetry-aware version by setting two distinct sets of autoencoders to extract representation of source node and target node respectively. From this point of view, in basic SHINE model the parameters of autoencoders are actually shared for source node and target node to alleviate over-fitting, and we can choose to explicitly distinguish the two sets of autoencoders for asymmetry reasons.

Cold start problem.
A practical issue for network embedding is how to learn representations for newly arrived node, which is the cold start problem. Almost all existing models cannot work well in cold start scenario because they only use the information from the target network (e.g., sentiment network in this paper), which is not applicable for the newly arrived node who has little interaction with the existing target network. However, SHINE is free of the cold start problem, as it makes full use of side information and incorporate it naturally into the target network when learning user representations. We will further study the performance of SHINE in cold start scenario in the experiment part.

Flexibility.
It is worth noticing that SHINE is also a framework with high flexibility. For any other new available side information of users (e.g., users' browsing history), we can easily design a new parallel processing component and "plug" it in the original SHINE framework to assist learning representation. Contrarily, we can also "pull out" social autoencoder or profile autoencoder from SHINE framework if such side information is unavailable. Besides, the flexibility of SHINE also lies in that one can choose different aggregation functions д and similarity measurement functions f , as discussed in Section 5.5.

EXPERIMENTS
In this section, we evaluate the performance of our proposed SHINE on real-world datasets. We first introduce the datasets, baselines, and parameter settings for experiments, then present the experimental results of SHINE and baselines.

Datasets
To comprehensively demonstrate the effectiveness of SHINE framework, we use the following two datasets for experiments:• Weibo-STC: Our proposed Weibo Sentiment Towards Celebrities dataset consists of three heterogeneous networks with 12,814 users, 126,380 tweets, 71,268 social links and 37,689 profile values, of which the detail is presented in Section 3. • Wiki-RfA: Wikipedia Requests for Adminship  #b29  is a signed network with 10,835 nodes and 159,388 edges, corresponding to votes cast by Wikipedia uses in election for promoting individuals to the role of administrator. A signed link indicates a positive or negative vote by one user on the promotion of another. Note that Wiki-RfA does not contain any side information of nodes, therefore, this dataset is used to validate the efficacy of the basic sentiment autoencoder in SHINE.

Baselines
We use the following five methods as baselines, in which the first three are network embedding methods, FxG is a signed link prediction approach, and LIBFM is a generic classification model. Note that the first three methods are not directly applicable to signed heterogeneous networks, so we use them to learn user representations from positive and negative part of each network respectively, and concatenate them to form the final embeddings. For FxG on Weibo-STC dataset, we only use the sentiment network as input because the FxG model cannot utilize the side information of nodes.• LINE: Large-scale Information Network Embedding  #b22  defines loss functions to preserve the first-order and secondorder proximity and learns representations of vertices. • Node2vec: Node2vec [8] designs a biased random walk procedure to learn a mapping of nodes that maximizes the likelihood of preserving network neighborhoods of nodes. • SDNE: Structural Deep Network Embedding  #b25  is a semisupervised network embedding model using autoencoder to capture local and global structure of target networks. • FxG: Fairness and Goodness  #b11  predicts the weights of edges in weighted signed networks by introducing two measures of node behavior: goodness (i.e., how much the node is liked by other nodes) and fairness (i.e., how fair the node is in rating other nodes' likeability). • LIBFM: LIBFM  #b17  is a state-of-the-art feature based factorization model. In this paper, we use the concatenated one-hot vectors of users in three networks as input to feed LIBFM.

Parameter Setttings
We design a 4-layer autoencoder in SHINE for each network, in which the hidden layer is with 1,000 units and the embedding layer is with 100 units. Deeper architectures cannot further improve the performance but incur heavier computational overhead according to our experimental results. We choose concatenation as the aggregation function д and inner product as the similarity measurement function f . Besides, we set the reconstruction weight of non-zero elements α = 10, the balancing parameters λ 1 = 1, λ 2 = 1, λ 3 = 20, and λ 4 = 0.01 for SHINE. We will study the sensitivity of these parameters in Section 6.6. For LINE, we concatenate the first-order and second-order representations to form the final 100-dimension embeddings for each node, and the total number of samples is 100 million. For node2vec, the number of embedding dimension is set as 100. For SDNE, the reconstruction weight of non-zero elements is 10 and the weight of first-order term is 0.05. For LIBFM, the dimensionality of the factorization machine is set as {1, 1, 0} and we use SGD method for training with learning rate of 0.5 and 200 iterations. Other parameters in these baselines are set as default.In the following subsections, we conduct experiments on two tasks: link prediction and node recommendation.

Link Prediction
In link prediction setting, our task is to predict the sign of an unobserved link between two given nodes. As the existing links in the original network are known and can serve as the ground truth, we randomly hide 20% of links in the sentiment network and select a balanced test set (i.e., the number of positive links is the same as negative links) out of them, while use the remaining network to train SHINE as well as all baselines. We use Accuracy and Micro-F1 as the evaluation metrics in link prediction task. For a more finegrained analysis, we compare the performance while varying the percentage of training set from 10% to 100%. The result is presented in Fig. 5, from which we have the following observations:• Fig. 5 shows that our methods SHINE achieves significant improvements in Accuracy and Micro-F1 over the baselines in both datasets. Specifically, in Weibo-STC, SHINE outperforms LINE, node2vec, and SDNE by 13.8%, 16.2%, and 8.78% respectively on Accuracy, and achieves 15.5%, 17.6%, 9.71% gains respectively on Micro-F1. • Among the three state-of-the-art network embedding methods, SDNE performs best while LINE and node2vec show relatively poor performance. Note that SDNE also uses autoencoder to learning the embedding of nodes, which proves the superiority of autoencoder in extracting highly nonlinear representations of networks from a side. • FxG performs much better in Wiki-RfA than in Weibo-STC. This is probably due to the following two reasons: 1) Unlike other methods, FxG cannot utilize the side information in Weibo-STC dataset. 2) Weibo-STC is sparser than Wiki-RfA, which is unfavorable to the computing of goodness and fairness of nodes in FxG model. • Although LIBFM is not specially designed for network-structured data, it still achieves fine performance compared with other network embedding methods. However, during experiments we find that LIBFM is unstable and prone to parameters tuning. This can also be validated by the fluctuating curves of LIBFM in Fig. 5c and Fig. 5d.To compare the performance of SHINE and baselines in cold start scenario, we construct a test set of newly arrived users for Weibo-STC, in which the associated ordinary user of each sentiment link dose not appear in the training set. We report Accuracy and Micro-F1 for all users and new users in Table 2. From the results in Table 2 it is evident that SHINE can still maintain a decent performance in the cold start scenario, as it fully exploits the information from social network and profile network to compensate for the lack of sentiment links. By comparison, the performance of other baselines degrades significantly in cold start scenario. Specifically, the Accuracy decreases by 2.46% for SHINE and by 11.58%, 11.28%, 15.14%, 17.90%, 14.57% respectively for LINE, node2vec, SDNE, FxG and LIBFM, which proves that SHINE are more capable of effectively transferring knowledge among heterogeneous information networks, especially in cold start scenario.

Node Recommendation
In addition to link prediction, we also conduct experiments on node recommendation, in which for each user we aim to recommend a set of users who have not been explicitly expressed attitude to but may be liked by the user. The performance of node recommendation can reveal the quality of learned representations as well. Specifically, for each user, we calculate his sentiment score toward all other users, and select K users with largest sentiment score for recommendation. For completeness, we recommend not only the nodes that a user may like but also the nodes that he may dislike. Therefore, we use positive and negative Precision@K and Recall@K respectively for evaluation in corresponding experimental scenarios. The results are shown in Fig. 6, which provides us the following observations:• The curve of SHINE is almost consistently above the curves of baselines, which proves that SHINE can better learn the representations of heterogeneous networks and perform recommendation than baselines.       • Negative precision is low than positive precision while negative recall is higher than positive recall for most methods. This is because negative links are far fewer than positive links in both datasets, which makes it easier to cover more negative links in the recommendation set. • In general, the results of precision and recall on Weibo-STC is better than Wiki-RfA, which is in accordance with the results in link prediction. The reason lies in that Weibo-STC provides more side information which can greatly improve the quality of learned user representations.

Parameters Sensitivity
SHINE involves a number of hyper-parameters. In this subsection we examine how the different choices of parameters affect the Accuracy of SHINE on Weibo-STC dataset. Except for the parameter being tested, all other parameters are set as default.Similarity measurement function f and aggregation function д. We first investigate how the similarity measurement function f and aggregation function д affect the performance by testing on all combinations of f and д, and present the results in Table 3. It is clear that the combination of inner product and concatenation achieves the best Accuracy, while max pooling performs worst, which is probably due to the reason that concatenation preserves more information out of the three types of embeddings than summation and max pooling during embedding aggregation. It should also be noted that there is no absolute advantage of all the three f functions according to the results in Table 3.Dimension of embedding layer and reconstruction weight of non-zero elements α. We also show how the dimension of embedding layer in the three autoencoders of SHINE and the hyperparameter α affect the performance in Fig. 7a. We have the following two observations: 1) The performance is initially improved with the increase of dimension, because more bits in embedding layer can encode more useful information. However, the performance drops when the dimension further increases, as too large number of dimensions may introduce noises which mislead the subsequent prediction. 2) α controls the reconstruction weight of non-zero elements in autoencoders. When α is too small (e.g., α = 1), SHINE will reconstruct the zero and non-zero elements without much discrimination, which deteriorates the performance because nonzero elements are more informative than zero ones. However, the performance will decrease if α gets too large (e.g., α = 30), because large α will lead SHINE to totally ignore the dissimilarity (i.e., zero elements) among users.Balancing parameters λ 1 , λ 2 , and λ 3 . λ 1 , λ 2 , and λ 3 balance the loss terms of the objective function in Eq. (9). We treat λ 1 and λ 2 as binary parameters and vary the value of λ 3 to study the performance of SHINE. Note that whether λ 1 or λ 2 equals 1 indicates that whether we use the additional social information or profile information in link prediction. Therefore, the study of λ 1 and λ 2 can also be seen as to validate the effectiveness of social network embedding module and profile network embedding module. The result is presented in Fig. 7b, from which we can conclude that: 1) The curve of λ 1 = 1, λ 2 = 0 and λ 1 = 0, λ 2 = 1 are both above the curve of λ 1 = 0, λ 2 = 0, which demonstrates the significant gain by incorporating the social information and profile information (especially the latter) into the sentiment network. Moreover, combining both additional information can further improve the performance. 2) Increasing the value of λ 3 can greatly boost the accuracy, as SHINE will concentrate more on the prediction error rather than the reconstruction error. However, similar to other hyper-parameters, too large λ 3 is not satisfactory since it breaks the trade-off among loss terms in objective function. (b) λ 1 , λ 2 , and λ 3 Fig. 7: Parameter sensitivity w.r.t. the dimension of embedding layers, α, λ 1 , λ 2 , and λ 3 .

CONCLUSIONS
In this paper we study the problem of predicting sentiment links in absence of sentiment related content in online social networks. We first establish a labeled, heterogeneous, and entity-level sentiment dataset from Weibo due to the lack of explicit sentiment links. To efficiently learn from these heterogeneous networks, we propose Signed Heterogeneous Information Network Embedding (SHINE), a deep-learning-based network embedding framework to extract users' highly nonlinear representations while preserving the structure of original networks. We conduct extensive experiments to evaluate the performance of SHINE. Experimental results prove the competitiveness of SHINE against several strong baselines and demonstrate the effectiveness of usage of social relation and profile information, especially in cold start scenario.

Footnote
1 : http://weibo.com 2 https://github.com/fxsjy/jieba