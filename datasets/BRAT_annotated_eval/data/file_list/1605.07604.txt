Posterior Dispersion Indices

Abstract
Probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its performance. Evaluation drives the cycle, as we revise our model based on how it performs. This requires a metric. Traditionally, predictive accuracy prevails. Yet, predictive accuracy does not tell the whole story. We propose to evaluate a model through posterior dispersion. The idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure. We propose a family of posterior dispersion indices (PDIs) that capture this idea. A PDI identifies rich patterns of model mismatch in three real data examples: voting preferences, supermarket shopping, and population genetics.

Introduction
Probabilistic modeling is a flexible approach to analyzing structured data. Three steps define the approach. First, specify a model; this captures our structural assumptions about the data. Then, we infer the hidden structure; this means computing (or approximating) the posterior. Last, we evaluate the model; this helps build better models down the road.How do we evaluate models? Decades of reflection have led to deep and varied forays into model checking, comparison, and criticism  #b8 . But a common theme permeates all approaches to model evaluation: the desire to generalize well.In machine learning, we traditionally employ two complementary tools: predictive accuracy and cross-validation. Predictive accuracy is the target evaluation metric. Cross-validation captures a notion of generalization and justifies holding out data. This simple combination has fueled the development of myriad probabilistic models  #b1  #b22 .Does predictive accuracy tell the whole story? Predictive accuracy at the datapoint level offers a way to evaluate each observation in the dataset. In this way, pointwise predictive accuracy indicates datapoints that do not match the model well. Yet it does not tell us how the mismatch occurs.Main idea. We propose to evaluate probabilistic models through the idea of posterior dispersion, analyzing how each datapoint fares in relation to posterior uncertainty around the hidden structure. To capture this, we propose a family of posterior dispersion indices (PDIs). These are per-datapoint quantities, each a variance to mean ratio of the model likelihood with respect to the posterior. A PDI highlights datapoints that exhibit the most uncertainty under the posterior.Consider a model p(x, θ ) and the likelihood of a datapoint p(x n | θ ). It depends on some hidden structure θ that we seek to uncover. Since θ is random, we can view the likelihood as a random variable and ask: how does the likelihood of x n vary with respect to the posterior p(θ | x)?To answer this, we appeal to various forms of dispersion, such as the variance of the likelihood under the posterior. We propose a family of calibrated dispersion criteria of form PDI  .Here is a mental picture. Consider a study of human body temperatures. The posterior represents uncertainty around temperature. Imagine a high measurement T fever . Its likelihood varies rapidly across plausible values for T . This datapoint is well modeled, but is sensitive to the posterior. Now imagine a zero measurement. This datapoint is poorly modeled, but consistently so: the thermometer is broken. A PDI highlights the first datapoint as a more important type of mismatch than the second. Section 3 presents an empirical study of model mismatch in three real-world examples: voting preferences, supermarket shopping, and population genetics. In each of these cases, a PDI provides insight into the model and offers concrete directions for improvement.Related work. This paper relates to a constellation of ideas from statistical model criticism. PDI bears similarity to ANOVA, which is a frequentist approach to evaluating explanatory variables in linear regression  #b6 .  #b11  cemented the idea of studying predictive accuracy of probabilistic models at the data level;  #b31  and  #b0  give up-to-date overviews of these ideas. Recent forays into model criticism, such as  #b10 ,  #b32  and  #b23 , explore the relationship between cross-validation and information criteria, such as widely applicable information criterion (WAIC)  #b33  #b29 . WAIC offers an intuitive connection to cross validation  #b30  #b34 ; we draw inspiration from it in this paper too. In the machine learning community,  #b13  and  #b5  developed effective kernel-based methods for independence and goodness-of-fit tests. Recently,  #b20  visualized smooth regions of data space that the model fails to explain. In contrast, we focus directly on the datapoints, which can live in high-dimensional spaces that are difficult to visualize.

Posterior Dispersion Indices
A posterior dispersion index (PDI) highlights datapoints that exhibit the most uncertainty with respect to the hidden structure of a model. Here is the road map for this section. A small case study illustrates how a PDI gives more insight beyond predictive accuracy. Definitions, theory, and another small analysis give further insight; a straightforward algorithm leads into the empirical study.2.1 A motivating case study: 44% outliers?  #b14  considers the number of days each U.S. president stayed in office. Figure 1 plots the data. One-term presidents stay in office for around 1460 days; two-term presidents approximately double that. Yet many presidents deviate from this "two bump" trend. 1 W a sh in g to n A d a m s J e ff e rs o n M a d is o n M o n ro e A d a m s J a c k so n V a n B u re n H a rr is o n T y le r P o lk T a y lo r F il m o re P ie rc e B u c h a n a n L in c o ln J o h n so n G ra n t H a y e s G a rfi e ld A rt h u r C le v e la n d H a rr is o n C le v e la n d A reasonable model for such data is a mixture of negative binomial distributions. 2 Consider the "second" parameterization of the negative binomial with mean µ and variance µ + µ 2 /φ. Posit gamma priors on the (non-negative) latent variables. Set the prior on µ to match the mean and variance of the data  #b26 . Choose an uninformative prior on φ. Three mixtures make sense: two for the typical trends and one for the rest.The complete model isp(π) = Dirichlet(π ; α = (1, 1, 1)) p(µ) = 3 k=1Gam(µ k ; mean and variance matched to that of data)p(φ) = 3 k=1 Gam(φ k ; a = 1, β = 0.01) p(x n | π, µ, φ) = 3 k=1 π k NB2(x n ; µ k , φ k ).Fitting this model gives posterior mean estimates µ = (1461, 2896, 1578) with corresponding φ = (470, 509, 1.3). The first two clusters describe the two typical term durations, while the third (highlighted in red) is a dispersed negative binomial that attempts to describe the rest of the data.We compute a PDI (defined in Section 2.2) and the posterior predictive density for each president p(x n | x). Figure 2  this is because they are close to the peaked negative binomial cluster at 1460 but not close enough to have good predictive accuracy. They are datapoints whose likelihoods are rapidly changing with respect to the posterior, like the fever measurement in the introduction.This case study suggests that predictive probability does not tell the entire story. Datapoints can exhibit low predictive accuracy in different ways. The following two sections define and explain how a PDI captures this effect.−0.1 −0.05 Dipsersion index PDI −11 −9 −7 Log density log p(x n | x) H a rr is o n R o o se v e lt M c K in le y A rt h u r G a rfi e ld R o o se v e ltT ru m a n W a sh in g to n R e a g a n 

Definitions
Let x = {x n } N 1 be a dataset with N observations. A probabilistic model has two parts. The first is the likelihood, p(x n | θ ). It relates an observation x n to hidden patterns described by a set latent random variables θ . If the observations are independent and identically distributed, the likelihood of the dataset factorizes as p(x | θ ) = n p(x n | θ ).The second is the prior density, p(θ ). It captures the structure we expect from the hidden patterns. Combining the likelihood and the prior gives the joint density p(x, θ ) = p(x | θ )p(θ ). Conditioning the joint on the observed data gives the posterior density, p(θ | x).Treat the likelihood of each datapoint as a function of θ . To evaluate the model, we analyze how each datapoint fares in relation to the posterior density. Consider these expectations and variances with respect to the posterior,µ(n) = θ |x [p(x n | θ )] µ log (n) = θ |x [log p(x n | θ )], σ 2 (n) = θ |x [p(x n | θ )] σ 2 log (n) = θ |x [log p(x n | θ )].(1)Each includes the likelihood in a slightly different fashion. The first expectation is a familiar object: µ(n) is the posterior predictive distribution.A PDI is a ratio of these variances to expectations. Taking the ratio calibrates this quantity for each datapoint. Recall the mental picture from the introduction. The variance of the likelihood under the posterior highlights potential model mismatch; dividing by the mean calibrates this spread to its predictive accuracy.Related ratios also appear in classical statistics under a variety of forms, such as indices of dispersion  #b15 , coefficients of variation  #b18 , or the Fano factor  #b7 . They all quantify dispersion of samples from a random process.In this paper, we propose and study the widely applicable posterior dispersion index (WAPDI),WAPDI(n) = σ 2 log (n) log µ(n) .Its form and name comes from the widely applicable information criterionWAIC = − 1 N n log µ(n) + σ 2 log (n).WAIC measures generalization error; it asymptotically equates to leave-one-one cross validation  #b33  #b34 . WAPDI has two advantages; both are practically motivated. First, we hope the reader is computing some estimate of generalization error.  #b10  recommends WAIC, since it is easy to compute and designed for common machine learning models  #b33 . Computing WAIC gives WAPDI for free. Second, the variance is a second-order moment calculation; using the log likelihood gives numerical stability to the computation. (More on computation in Section 2.4.)WAPDI compares the variance of the log likelihood to the log posterior predictive. This gives insight into how the likelihood of a datapoint fares under the posterior distribution of the hidden patterns. We now analyze this in more detail.

Intuition: not all predictive probabilities are created equal
The posterior predictive density is an expectation, θ |x [p(x new | θ )] = p(x new | θ )p(θ | x) dθ .Expectations are integrals: areas under a curve. Different likelihood and posterior combinations can lead to similar integrals.A toy model illustrates this. Consider a gamma likelihood with fixed shape, and place a gamma prior on the rate. The model isp(β) = Gam(β ; a 0 = 1, b 0 = 1), p(x | β) = N n=1Gam(x n ; a = 5, β),which gives the posterior p(β | x) = Gam(β ; a = a 0 + 2N , b = b 0 + n x n ).Now simulate a dataset of size N = 10 with β = 1; the data have mean a /β = 5. Now consider an outlier at 15. We can find another x value with essentially the same predictive accuracylog p(x 1 = 0.727 | x) = −5.633433, log p(x 2 = 15 | x) = −5.633428.Yet their WAPDI values differ by an order of magnitudeWAPDI( x 1 = 0.727) = −0.067, WAPDI( x 2 = 15) = −0.229.In this case, WAPDI highlights x 2 = 15 as a more severe outlier than x 1 = 0.727, even though they have the same predictive accuracy. What does that mean? Figure 3 depicts the difference.The following lemma explains how WAPDI measures this effect.Lemma 1 If log p(x n | θ ) is at least twice differentiable and the posterior p(θ | x) has finite first and second moments, then a first-order Taylor expansion givesWAPDI(n) ≈ log p (x n | θ |x [θ ]) 2 θ |x [θ ] log θ |x [p(x n | θ )].(2) 0 0.5 1 1.5 WAPDI reports the ratio of this rate-of-change to the area under the curve. In this special example, only the numerator matters, since the denominator is the same for both datapoints. (2)  For most interesting models, we do not expect such a coincidence. However, in practice, we find WAPDI to be close to zero for datapoints that match the model well.p(x 1 | x) p(x 2 | x) ≈ β Density posterior p(β|x) p(x 1 = 0.727 | β) p(x 2 = 15 | β)

Corollary 2 Equation


Computation
Calculating WAPDI is straightforward. The only requirement are samples from the posterior. This is precisely the output of an Markov chain Monte Carlo (MCMC) sampling algorithm. (We used the no-U-turn sampler  #b16  for the analyses above.) Other inference procedures, such as variational inference, give an analytic approximation to the posterior  #b17  #b2 . Drawing samples from an approximate posterior also works.Equipped with S samples from the posterior, Monte Carlo integration  #b27  gives unbiased estimates of the quantities in Equation (1). The variance of these estimates decreases as ( 1 /S); we assume S is sufficiently large to cover the posterior . We default to S = 1000 in our experiments. Algorithm 1 summarizes these steps. Estimate µ(n), µ log (n), σ 2 (n), σ 2 log (n) from samples {θ } S 1 . Store these estimates.Return the desired ratio. end

Experimental Study
We now explore three real data examples using modern machine learning models: voting preferences, supermarket shopping, and population genetics.

Voting preferences: a hierarchical logistic regression model
In 1988, CBS conducted a U.S. nation-wide survey of voting preferences. People indicated their preference towards the Democratic or Republican presidential candidate. Each individual also declared their gender, age, race, education level, and the state they live in; 11 566 individuals participated.  #b9  study this data through a hierarchical logistic regression model. They begin by modeling gender, race, and state; the state variable has a hierarchical prior. This model is easy to fit using automatic differentiation variational inference (ADVI) within Stan  #b19  #b4 . (Model and inference details in Appendix B.)    Tables 1 and 2 show the individuals with the lowest predictive accuracy and WAPDI. The nationwide trend predicts that females (F) who identify as black (B) have a strong preference to vote democratic (D); predictive accuracy identifies the few individuals who defy this trend. However, there is not much to do with this information; the model identifies a nation-wide trend that correctly describes most female black voters. In contrast, WAPDI points to parts of the dataset that the model fails to describe; these are datapoints that we might try to explain better with a revised model. VOTE R R R R R R R R R R SEX F F F F F F F F F F RACE B B B B

Supermarket shopping: a hierarchical Poisson factorization model
Market research firm IRi hosts an anonymized dataset of customer shopping behavior at U.S. supermarkets  #b3 . The dataset tracks 136 584 "checkout" sessions; each session contains a basket of purchased items. An inventory of 7 903 items range across categories such as carbonated beverages, toiletries, and yogurt.What items do customers tend to purchase together? To study this, consider a hierarchical Poisson factorization (HPF) model  #b12 . HPF models the quantities of items purchased in each session with a Poisson likelihood; its rate is an inner product between a session's preferences θ s and the item attributes β. Hierarchical priors on θ and β simultaneously promote sparsity, while accounting for variation in session size and item popularity. Some sessions contain only a few items; others are large purchases. (Model and inference details in Appendix C.)A 20-dimensional HPF model discovers intuitive trends. A few stand out. Snack-craving customers like to buy Doritos tortilla chips along with Lay's potato chips. Morning birds typically pair Cheerios cereal with 2% skim milk. Yoplait fans tend to purchase many different flavors at the same time. Tables 3 and 4 show the top five items in two of these twenty trends.

Item Description Category
Brand A: 2% skim milk milk rfg skim/lowfat Cheerios: cereal cold cereal Diet Coke: soda carbonated beverages Brand B: 2% skim milk milk rfg skim/lowfat Brand C: 2% skim milk milk rfg skim/lowfat  Sessions where a customer purchases many items from different categories have low predictive accuracy. This makes sense as these customers do not exhibit a trend; mathematically, there is no combination of item attributes β that explain buying items from disparate categories. For example, the session with the lowest predictive accuracy contains 117 items ranging from coffee to hot dogs. WAPDI highlights an entirely different aspect of the HPF model. Sessions with low WAPDI contain similar items but exhibit many purchases of a single item. Table 5 shows an example of a session where a customer purchased 14 blackberry Yoplait yogurts, but only a few of the other flavors.

Item Description Quantity
Yoplait: blackberry flavor 14 Yoplait: strawberry flavor 2 Yoplait: raspberry flavor 2 Yoplait: peach flavor 1 Yoplait: cherry flavor 1 Yoplait: mango flavor 1 This indicates that the Poisson likelihood assumption may not be flexible enough to model customer purchasing behavior. Perhaps a negative binomial likelihood could model this kind of spiked activity better. Another option might be to keep the Poisson likelihood but increase the hierarchy of the probabilistic model; this approach may identify item attributes that explain such purchases. In either case, WAPDI identifies a valuable aspect of the data that the HPF struggles to capture: sessions with spiked activity. This is a concrete direction for model revision.

Population genetics: a mixed membership model
Do all people who live nearby have similar genomes? Not necessarily. Population genetics considers how individuals exhibit ancestral patterns of mutations. Begin with N individuals and L locations on the genome. For each location, report whether each individual reveals a mutation. This gives an (N × L) dataset x where x nl ∈ 0, 1, 2, 3. (We assume two specific forms of mutation; 3 encodes a missing observation.)Mixed membership models offer a way to study this  #b24 . Assume K ancestral populations φ; these are the mutation probabilities of each location. Each individual mixes these populations with weights θ ; these are the mixing proportions. Place a beta prior on the mutation probabilities and a Dirichlet prior on the mixing proportions.We study a dataset of N = 324 individuals from four geographic locations and focus on L = 13 928 locations on the genome. Figure  WAPDI reveals three interesting patterns of mismatch here. First, individuals with low WAPDI have many missing observations; the bottom 10% of WAPDI have 1 344 missing values, in contrast to 563 for the lowest 10% of predictive scores. We may consider directly modeling these missing observations.Second, ASW has two individuals with low WAPDI; their mutation patterns are outliers within the group. While the average individual reveals 272 mutations away from the median genome, these individuals show 410 and 383 mutations. This points to potential mishaps while gathering or pre-processing the data.Last, MEX exhibits good predictive accuracy, yet low WAPDI. Based on predictive accuracy, we may happily accept these patterns. Yet WAPDI highlights a serious issue with the inferred populations. The blue and red populations are almost twice as correlated (0.58) as the other possible combinations (0.24 and 0.2). In other words, the blue and red populations represent similar patterns of mutations at the same locations. These populations, as they stand, are not necessarily interpretable. Revising the model to penalize correlation may be a direction worth pursuing.

Discussion
A posterior dispersion index (PDI) identifies informative forms of model mismatch beyond predictive accuracy. By highlighting which datapoints exhibit the most uncertainty under the posterior, a PDI offers a new perspective into evaluating probabilistic models. Here, we show how the widely applicable posterior dispersion index (WAPDI) reveals concrete directions for model improvement across a range of models and applications.There are several exciting research directions going forward. One is to extend the notion of a PDI to non-exchangeable data. Another is to leverage the bootstrap to extend this idea beyond probabilistic models. Finally, theoretical connections to cross-validation await discovery, while ideas from importance sampling could reduce the variance of PDI computations.

B Hierarchical logistic regression model
Hierarchical logistic regression models classification tasks in an intuitive way. We study three variants of  #b9  

C Hierarchical Poisson factorization model
Hierarchical Poisson factorization models a matrix of counts as a low-dimensional inner product.  #b12  present the model in detail along with an efficient variational inference algorithm for inference. We summarize the model below.Model. Consider a U × I dataset, with non-negative integer elements x ui . It helps to think of u as an index over "users" and i as an index over "items".The likelihood for each measurements isp(x ui ) = Poisson(x ui ; θ u β i )where θ is a U × K matrix of non-negative real-valued latent variables; it represents "user preferences". Similarly, β is a K × I matrix of non-negative real-valued latent variables; it represents "item attributes".The priors for these variables are p(θ uk ) = Gam(θ uk ; a, ξ u )p(β ki ) = Gam(β ki ; c, η i )where ξ is a U vector of non-negative real-valued latent variables; it represents "user activity". Similarly, η is a I vector of non-negative real-valued latent variables; it represents "item popularity".The prior for these hierarchical latent variable are 

Inference.
We used the coordinate ascent variational inference algorithm provided by the authors of  #b12 .

D Population genetics data and model
Population genetics studies ancestral trends of genomic mutations. Consider N individuals and L locations on the genome. For each location, we measure whether each individual reveals a mutation. This gives an (N × L) dataset x where x nl ∈ 0, 1, 2, 3. (We assume two specific forms of mutation; 3 encodes a missing observation.)Model.  #b24  propose a probabilistic model for this kind of data. Represent K ancestral populations with a latent variable φ. This is a (K × L) matrix of mutation probabilities. Place a beta prior for each probability. Each individual mixes these populations. Denote this with another latent variable θ . This is a (N × K) matrix of mixture proportions. Place a Dirichlet prior for each individual. The likelihood of each mutation is a K-mixture of categorical distributions.

Data.
We study a subset of the Hapmap 3 dataset from  #b21 . This includes N = 324 individuals and L = 13 928 locations on the genome. Four geographic regions are represented: 49 ASW, 112 CEU, 50 MEX, and 113 YRI.  #b21  expect this data to exhibit at most K = 3 ancestral populations; the full Hapmap 3 dataset exhibits K = 6 populations.In more detail, this data studies single-nucleotide polymorphisms (SNPs). The data is preprocessed from the raw genetic observations such that:• non-SNPs are removed (i.e. genes with more than one location changed),• SNPs with low entropy compared to the dominant mutation at each location are removed,• SNPs that are too close to each other on the genome are removed.Inference. We use the fastSTRUCTURE software suite  #b25  to perform variational inference. We use the default parameters and only specify K = 3.

Footnote
1 : Hayden (2005) submits that 44% of presidents may be outliers.
2 : A Poisson likelihood is too underdispersed.