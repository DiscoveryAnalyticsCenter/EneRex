Beyond Disagreement-based Agnostic Active Learning

Abstract
We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithms for this problem are disagreement-based active learning, which has a high label requirement, and margin-based active learning, which only applies to fairly restricted settings. A major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on two novel contributions -a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and a novel confidence-rated predictor.

Introduction
In this paper, we study active learning of classifiers in an agnostic setting, where no assumptions are made on the true function that generates the labels. The learner has access to a large pool of unlabelled examples, and can interactively request labels for a small subset of these; the goal is to learn an accurate classifier in a pre-specified class with as few label queries as possible. Specifically, we are given a hypothesis class H and a target ǫ, and our aim is to find a binary classifier in H whose error is at most ǫ more than that of the best classifier in H, while minimizing the number of requested labels.There has been a large body of previous work on active learning; see the surveys by  #b9 Set10] for overviews. The main challenge in active learning is ensuring consistency in the agnostic setting while still maintaining low label complexity. In particular, a very natural approach to active learning is to view it as a generalization of binary search  #b16  #b8 Now11]. While this strategy has been extended to several different noise models [Kää06, Now11, NJC13], it is generally inconsistent in the agnostic case  #b10 .The primary algorithm for agnostic active learning is called disagreement-based active learning. The main idea is as follows. A set V k of possible risk minimizers is maintained with time, and the label of an example x is queried if there exist two hypotheses h 1 and h 2 in V k such that h 1 (x) = h 2 (x). This algorithm is consistent in the agnostic setting [CAL94, BBL09, DHM07, Han07, BDL09, Han09, BHLZ10, Kol10]; however, due to the conservative label query policy, its label requirement is high. A line of work due to  #b2  #b5  #b0  have provided algorithms that achieve better label complexity for linear classification on the uniform distribution over the unit sphere as well as log-concave distributions; however, their algorithms are limited to these specific cases, and it is unclear how to apply them more generally.Thus, a major challenge in the agnostic active learning literature has been to find a general active learning strategy that applies to any hypothesis class and data distribution, is consistent in the agnostic case, and has a better label requirement than disagreement based active learning. This has been mentioned as an open problem by several works, such as  #b1  #b9  #b5 .In this paper, we provide such an algorithm. Our solution is based on two key contributions, which may be of independent interest. The first is a general connection between confidence-rated predictors and active learning. A confidence-rated predictor is one that is allowed to abstain from prediction on occasion, and as a result, can guarantee a target prediction error. Given a confidence-rated predictor with guaranteed error, we show how to use it to construct an active label query algorithm consistent in the agnostic setting. Our second key contribution is a novel confidence-rated predictor with guaranteed error that applies to any general classification problem. We show that our predictor is optimal in the realizable case, in the sense that it has the lowest abstention rate out of all predictors that guarantee a certain error. Moreover, we show how to extend our predictor to the agnostic setting.Combining the label query algorithm with our novel confidence-rated predictor, we get a general active learning algorithm consistent in the agnostic setting. We provide a characterization of the label complexity of our algorithm, and show that this is better than disagreement-based active learning in general. Finally, we show that for linear classification with respect to the uniform distribution and log-concave distributions, our bounds reduce to those of  #b2  #b5 .

Algorithm


The Setting
We study active learning for binary classification. Examples belong to an instance space X , and their labels lie in a label space Y = {−1, 1}; labelled examples are drawn from an underlying data distribution D on X × Y. We use D X to denote the marginal on D on X , and D Y |X to denote the conditional distribution on Y |X = x induced by D. Our algorithm has access to examples through two oracles -an example oracle U which returns an unlabelled example x ∈ X drawn from D X and a labelling oracle O which returns the label y of an input x ∈ X drawn from D Y |X .Given a hypothesis class H of VC dimension d, the error of any h ∈ H with respect to a data distribution Π over X × Y is defined as err Π (h) = P (x,y)∼Π (h(x) = y). We define: h * (Π) = argmin h∈H err Π (h), ν * (Π) = err Π (h * (Π)). For a set S, we abuse notation and use S to also denote the uniform distribution over the elements of S. We define P Π (·) := P (x,y)∼Π (·), E Π (·) := E (x,y)∼Π (·).Given access to examples from a data distribution D through an example oracle U and a labeling oracle O, we aim to provide a classifierĥ ∈ H such that with probability ≥ 1 − δ, err D (ĥ) ≤ ν * (D) + ǫ, for some target values of ǫ and δ; this is achieved in an adaptive manner by making as few queries to the labelling oracle O as possible. When ν * (D) = 0, we are said to be in the realizable case; in the more general agnostic case, we make no assumptions on the labels, and thus ν * (D) can be positive.Previous approaches to agnostic active learning have frequently used the notion of disagreements. The disagreement between two hypotheses h 1 and h 2 with respect to a data distribution Π is the fraction of examples according to Π to which h 1 and h 2 assign different labels; formally: ρ Π (h 1 , h 2 ) = P (x,y)∼Π (h 1 (x) = h 2 (x)). Observe that a data distribution Π induces a pseudo-metric ρ Π on the elements of H; this is called the disagreement metric. For any r and any h ∈ H, define B Π (h, r) to be the disagreement ball of radius r around h with respect to the data distribution Π. Formally:B Π (h, r) = {h ′ ∈ H : ρ Π (h, h ′ ) ≤ r}.For notational simplicity, we assume that the hypothesis space is "dense" with repsect to the data distribution D, in the sense that ∀r > 0, sup h∈BD(h * (D),r) ρ D (h, h * (D)) = r. Our analysis will still apply without the denseness assumption, but will be significantly more messy. Finally, given a set of hypotheses V ⊆ H, the disagreement region of V is the set of all examples x such that there exist two hypotheses h 1 , h 2 ∈ V for which h 1 (x) = h 2 (x). This paper establishes a connection between active learning and confidence-rated predictors with guaranteed error. A confidence-rated predictor is a prediction algorithm that is occasionally allowed to abstain from classification. We will consider such predictors in the transductive setting. Given a set V of candidate hypotheses, an error guarantee η, and a set U of unlabelled examples, a confidence-rated predictor P either assigns a label or abstains from prediction on each unlabelled x ∈ U . The labels are assigned with the guarantee that the expected disagreement 1 between the label assigned by P and any h ∈ V is ≤ η.Specifically, for all h ∈ V, P x∼U (h(x) = P (x), P (x) = 0) ≤ η(1)This ensures that if some h * ∈ V is the true risk minimizer, then, the labels predicted by P on U do not differ very much from those predicted by h * . The performance of a confidence-rated predictor which has a guarantee such as in Equation (1) is measured by its coverage, or the probability of non-abstention P x∼U (P (x) = 0); higher coverage implies better performance.

Main Algorithm
Our active learning algorithm proceeds in epochs, where the goal of epoch k is to achieve excess generalization error ǫ k = ǫ2 k0−k+1 , by querying a fresh batch of labels. The algorithm maintains a candidate set V k that is guaranteed to contain the true risk minimizer.The critical decision at each epoch is how to select a subset of unlabelled examples whose labels should be queried. We make this decision using a confidence-rated predictor P . At epoch k, we run P with candidate hypothesis set V = V k and error guarantee η = ǫ k /64. Whenever P abstains, we query the label of the example. The number of labels m k queried is adjusted so that it is enough to achieve excess generalization error ǫ k+1 .An outline is described in Algorithm 1; we next discuss each individual component in detail.Algorithm 1 Active Learning Algorithm: Outline 1: Inputs: Example oracle U, Labelling oracle O, hypothesis class H of VC dimension d, confidence-rated predictor P , target excess error ǫ and target confidence δ. 2: Set k 0 = ⌈log 1/ǫ⌉. Initialize candidate set V 1 = H. 3: for k = 1, 2, ..k 0 do 4:Set ǫ k = ǫ2 k0−k+1 , δ k = δ 2(k0−k+1) 2 .

5:
Define the set V j as follows:V j = h ∈ V : err Sj (h) ≤ err Sj (ĥ j ) +ǫ 2 + σ(n j ,δ j ) + σ(n j ,δ j )ρ Sj (h,ĥ j )Where σ(n, δ) := 8 n (2d ln 2en d + ln 24 δ ).

6:
if sup h∈Vj (σ(n j ,δ j ) + σ(n j ,δ j )ρ Sj (h,ĥ j )) ≤ǫ 6 then 7:j 0 = j, break 8: return V j0 .

2Õ (·) hides logarithmic factors
Lemma 4. Suppose we run Algorithm 2 with inputs hypothesis set V , example distribution ∆, labelling oracle O, target excess errorǫ and target confidenceδ. Let∆ be the joint distribution on X × Y induced by ∆ and D Y |X . Then there exists an eventẼ, P(Ẽ) ≥ 1 −δ, such that onẼ, (1) Algorithm 2 halts and (2) the set V j0 has the following properties:(2.1) If for h ∈ H, err∆(h) − err∆(h * (∆)) ≤ǫ/2, then h ∈ V j0 .(2.2) On the other hand, if h ∈ V j0 , then err∆(h) − err∆(h * (∆)) ≤ǫ.When eventẼ happens, we say Algorithm 2 succeeds.Lemma 5. Suppose we run Algorithm 2 with inputs hypothesis set V , example distribution ∆, labelling oracle O, target excess errorǫ and target confidenceδ. There exists some absolute constant c 1 > 0, such that on the event that Algorithm 2 succeeds, n j0 ≤ c 1 ((d ln 1ǫ + ln 1 δ ) ν * (∆)+ǫ ǫ 2). Thus the total number of labelsqueried is j0 j=1 n j ≤ 2n j0 ≤ 2c 1 ((d ln 1 ǫ + ln 1 δ ) ν * (∆)+ǫ ǫ 2 ).A naive approach (see Algorithm 4 in the Appendix) which uses an additive VC bound gives a sample complexity of O((d ln(1/ǫ) + ln(1/δ))ǫ −2 ); Algorithm 2 gives a better sample complexity.The following lemma is a consequence of our label query procedure in the non-realizable case.Lemma 6. Suppose we run Algorithm 1 in the non-realizable case with inputs example oracle U, labelling oracle O, hypothesis class H, confidence-rated predictor P , target excess error ǫ and target confidence δ. Then with probability 1 − δ, for all k = 1, 2, . . . , k 0 + 1, and for all h ∈ V k , err D (h) ≤ err D (h * (D)) + ǫ k . In particular, theĥ returned at the end of the algorithm satisfies err D (ĥ) ≤ err D (h * (D)) + ǫ. Draw n j = 2 j i.i.d examples from ∆; query their labels from O to get a labelled dataset S j . Denotẽ δ j :=δ/(j(j + 1)).

4:
Train an ERM classifierĥ j ∈ V over S j .

Confidence-Rated Predictor
Our active learning algorithm uses a confidence-rated predictor with guaranteed error to make its label query decisions. In this section, we provide a novel confidence-rated predictor with guaranteed error. This predictor has optimal coverage in the realizable case, and may be of independent interest. The predictor P receives as input a set V ⊆ H of hypotheses (which is likely to contain the true risk minimizer), an error guarantee η, and a set of U of unlabelled examples. We consider a soft prediction algorithm; so, for each example in U , the predictor P outputs three probabilities that add up to 1 -the probability of predicting 1, −1 and 0. This output is subject to the constraint that the expected disagreement 3 between the ±1 labels 3 where the expectation is taken over the random choices made by P assigned by P and those assigned by any h ∈ V is at most η, and the goal is to maximize the coverage, or the expected fraction of non-abstentions.Our key insight is that this problem can be written as a linear program, which is described in Algorithm 3. There are three variables, ξ i , ζ i and γ i , for each unlabelled z i ∈ U ; there are the probabilities with which we predict 1, −1 and 0 on z i respectively. Constraint (2) ensures that the expected disagreement between the label predicted and any h ∈ V is no more than η, while the LP objective maximizes the coverage under these constraints. Observe that the LP is always feasible. Although the LP has infinitely many constraints, the number of constraints in Equation (2) distinguishable by U k is at most (em/d) d , where d is the VC dimension of the hypothesis class H.Algorithm 3 Confidence-rated Predictor 1: Inputs: hypothesis set V , unlabelled data U = {z 1 , . . . , z m }, error bound η. 2: Solve the linear program:min m i=1 γ i subject to: ∀i, ξ i + ζ i + γ i = 1 ∀h ∈ V, i:h(zi)=1 ζ i + i:h(zi)=−1 ξ i ≤ ηm (2) ∀i, ξ i , ζ i , γ i ≥ 0 3:For each z i ∈ U , output probabilities for predicting 1, −1 and 0: ξ i , ζ i , and γ i .The performance of a confidence-rated predictor is measured by its error and coverage. The error of a confidence-rated predictor is the probability with which it predicts the wrong label on an example, while the coverage is its probability of non-abstention. We can show the following guarantee on the performance of the predictor in Algorithm 3.Theorem 1. In the realizable case, if the hypothesis set V is the version space with respect to a training set, then P x∼U (P (x) = h * (x), P (x) = 0) ≤ η. In the non-realizable case, if the hypothesis set V is an (1 − α)confidence set for the true risk minimizer h * , then, w.p ≥ 1 − α, P x∼U (P (x) = y, P (x) = 0) ≤ P x∼U (h * (x) = y) + η.In the realizable case, we can also show that our confidence rated predictor has optimal coverage. Observe that we cannot directly show optimality in the non-realizable case, as the performance depends on the exact choice of the (1 − α)-confidence set.Theorem 2. In the realizable case, suppose that the hypothesis set V is the version space with respect to a training set. If P ′ is any confidence rated predictor with error guarantee η, and if P is the predictor in Algorithm 3, then, the coverage of P is at least much as the coverage of P ′ .

Performance Guarantees
An essential property of any active learning algorithm is consistency -that it converges to the true risk minimizer given enough labelled examples. We observe that our algorithm is consistent provided we use any confidence-rated predictor P with guaranteed error as a subroutine. The consistency of our algorithm is a consequence of Lemmas 3 and 6 and is shown in Theorem 3.Theorem 3 (Consistency). Suppose we run Algorithm 1 with inputs example oracle U, labelling oracle O, hypothesis class H, confidence-rated predictor P , target excess error ǫ and target confidence δ. Then with probability 1 − δ, the classifierĥ returned by Algorithm 1 satisfies err D (ĥ) − err D (h * (D)) ≤ ǫ.We now establish a label complexity bound for our algorithm; however, this label complexity bound applies only if we use the predictor described in Algorithm 3 as a subroutine.For any hypothesis set V , data distribution D, and η, define Φ D (V, η) to be the minimum abstention probability of a confidence-rated predictor which guarantees that the disagreement between its predicted labels and any h ∈ V under D X is at most η.Formally, Φ D (V, η) = min{E D γ(x) : E D [I(h(x) = +1)ζ(x) + I(h(x) = −1)ξ(x)] ≤ η for all h ∈ V, γ(x) + ξ(x) + ζ(x) ≡ 1, γ(x), ξ(x), ζ(x) ≥ 0}. Define φ(r, η) := Φ D (B D (h * , r), η). The label complexity of our active learning algorithm can be stated as follows.Theorem 4 (Label Complexity). Suppose we run Algorithm 1 with inputs example oracle U, labelling oracle O, hypothesis class H, confidence-rated predictor P of Algorithm 3, target excess error ǫ and target confidence δ. Then there exist constants c 3 , c 4 > 0 such that with probability 1 − δ:(1) In the realizable case, the total number of labels queried by Algorithm 1 is at most:c 3 ⌈log 1 ǫ ⌉ k=1 (d ln φ(ǫ k , ǫ k /256) ǫ k + ln( ⌈log(1/ǫ)⌉ − k + 1 δ )) φ(ǫ k , ǫ k /256) ǫ k(2) In the agnostic case, the total number of labels queried by Algorithm 1 is at most:c 4 ⌈log 1 ǫ ⌉ k=1 (d ln φ(2ν * (D) + ǫ k , ǫ k /256) ǫ k + ln( ⌈log(1/ǫ)⌉ − k + 1 δ )) φ(2ν * (D) + ǫ k , ǫ k /256) ǫ k (1 + ν * (D) ǫ k )Comparison. The label complexity of disagreement-based active learning is characterized in terms of the disagreement coefficient. Given a radius r, the disagreement coefficent θ(r) is defined as:θ(r) = sup r ′ ≥r P(DIS(B D (h * , r ′ ))) r ′ ,where for any V ⊆ H, DIS(V ) is the disagreement region of V . As P(DIS(B D (h * , r))) = φ(r, 0) [EYW10], in our notation, θ(r) = sup r ′ ≥r φ(r ′ ,0) r ′ . In the realizable case, the label complexity of disagreement-based active learning isÕ(θ(ǫ) · ln(1/ǫ) · (d ln θ(ǫ) + ln ln(1/ǫ))) [Han13] 4 . Our label complexity bound may be simplified to:O ln 1 ǫ · sup k≤⌈log(1/ǫ)⌉ φ(ǫ k , ǫ k /256) ǫ k · d ln sup k≤⌈log(1/ǫ)⌉ φ(ǫ k , ǫ k /256) ǫ k + ln ln 1 ǫ , which is essentially the bound of [Han13] with θ(ǫ) replaced by sup k≤⌈log(1/ǫ)⌉ φ(ǫ k ,ǫ k /256) ǫ k .As enforcing a lower error guarantee requires more abstention, φ(r, η) is a decreasing function of η; as a result,sup k≤⌈log(1/ǫ)⌉ φ(ǫ k , ǫ k /256) ǫ k ≤ θ(ǫ),and our label complexity is better.In the agnostic case,  #b11  provides a label complexity bound ofÕ(θ(2ν* (D) + ǫ) · (d ν * (D) 2 ǫ 2ln(1/ǫ) + d ln 2 (1/ǫ))) for disagreement-based active-learning. In contrast, by Proposition 1 our label complexity is at most:Õ sup k≤⌈log(1/ǫ)⌉ φ(2ν * (D) + ǫ k , ǫ k /256) 2ν * (D) + ǫ k · d ν * (D) 2 ǫ 2 ln(1/ǫ) + d ln 2 (1/ǫ)Again, this is essentially the bound of  #b11  with θ(2ν * (D) + ǫ) replaced by the smaller quantitysup k≤⌈log(1/ǫ)⌉ φ(2ν * (D) + ǫ k , ǫ k /256) 2ν * (D) + ǫ k ,[Han13] has provided a more refined analysis of disagreement-based active learning that gives a label complexity ofÕ(θ(ν * (D) + ǫ)( ν * (D) 2 ǫ 2 + ln 1 ǫ )(d ln θ(ν * (D) + ǫ) + ln ln 1 ǫ )); observe that their dependence is still on θ(ν * (D) + ǫ). We leave a more refined label complexity analysis of our algorithm for future work.

Tsybakov Noise Conditions
An important sub-case of learning from noisy data is learning under the Tsybakov noise conditions [Tsy04].Definition 1. (Tsybakov Noise Condition) Let κ ≥ 1. A labelled data distribution D over X × Y satisfies (C 0 , κ)-Tsybakov Noise Condition with respect to a hypothesis class H for some constant C 0 > 0, if for all h ∈ H, ρ D (h, h * (D)) ≤ C 0 (err D (h) − err D (h * (D))) 1 κ .The following theorem shows the performance guarantees achieved by Algorithm 1 under the Tsybakov noise conditions. Theorem 5. Suppose (C 0 , κ)-Tsybakov Noise Condition holds for D with respect to H. Then Algorithm 1 with inputs example oracle U, labelling oracle O, hypothesis class H, confidence-rated predictor P of Algorithm 3, target excess error ǫ and target confidence δ satisfies the following properties. There exists a constant c 5 > 0 such that with probability 1 − δ, the total number of labels queried by Algorithm 1 is at most:c 5 ⌈log 1 ǫ ⌉ k=1 (d ln(φ(C 0 ǫ 1 κ k , ǫ k /256)ǫ 1 κ −2 k ) + ln( ⌈log 1 ǫ ⌉ − k + 1 δ ))φ(C 0 ǫ 1 κ k , ǫ k /256)ǫ 1 κ −2 kComparison.[Han13] provides a label complexity bound ofÕ(θ(C 0 ǫ 1 κ )ǫ 2 κ −2 ln 1 ǫ (d ln θ(C 0 ǫ 1 κ ) + ln ln 1 ǫ )) for disagreement-based active learning. For κ > 1, by Proposition 2, our label complexity is at most:O sup k≤⌈log(1/ǫ)⌉ φ(C 0 ǫ 1/κ k , ǫ k /256) ǫ 1/κ k · ǫ 2/κ−2 k · d ln(1/ǫ) ,For κ = 1, our label complexity is at mostO ln 1 ǫ · sup k≤⌈log(1/ǫ)⌉ φ(C 0 ǫ k , ǫ k /256) ǫ k · d ln( sup k≤⌈log(1/ǫ)⌉ φ(C 0 ǫ k , ǫ k /256) ǫ k ) + ln ln 1 ǫ .In both cases, our bounds are better,as sup k≤⌈log(1/ǫ)⌉ · φ(C0ǫ 1/κ k ,ǫ k /256) C0ǫ 1/κ k ≤ θ(C 0 ǫ 1/κ ). In further work, [HY12]provides a refined analysis with a bound ofÕ(θ(C 0 ǫ 1 κ )ǫ 2 κ −2 d ln θ(C 0 ǫ 1 κ )); however, this work is not directly comparable to ours, as they need prior knowledge of C 0 and κ.

Case Study: Linear Classification under the Log-concave Distribution
We now consider learning linear classifiers with respect to log-concave data distribution on R d . In this case, for any r, the disagreement coefficient θ(r) ≤ O( √ d ln(1/r))  #b5 ; however, for any η > 0, φ(r,η) r ≤ O(ln(r/η)) (see Lemma 14 in the Appendix), which is much smaller so long as η/r is not too small. This leads to the following label complexity bounds.Corollary 1. Suppose D X is isotropic and log-concave on R d , and H is the set of homogeneous linear classifiers on R d . Then Algorithm 1 with inputs example oracle U, labelling oracle O, hypothesis class H, confidence-rated predictor P of Algorithm 3, target excess error ǫ and target confidence δ satisfies the following properties. With probability 1 − δ:(1) In the realizable case, there exists some absolute constant c 8 > 0 such that the total number of labels queried is at most c 8 ln 1 ǫ (d + ln ln 1 ǫ + ln 1 δ ).(2) In the agnostic case, there exists some absolute constant c 9 > 0 such that the total number of labels queried is at most c 9 ( ν * (D) 2ǫ 2 + ln 1 ǫ ) ln ǫ+ν * (D) ǫ (d ln ǫ+ν * (D) ǫ + ln 1 δ ) + ln 1 ǫ ln ǫ+ν * (D) ǫ ln ln 1 ǫ . (3) If (C 0 , κ)-TsybakovNoise condition holds for D with respect to H, then there exists some constant c 10 > 0 (that depends on C 0 , κ) such that the total number of labels queried is at most c 10 ǫ 2 κ −2 ln 1 ǫ (d ln 1 ǫ + ln 1 δ ). In the realizable case, our bound matches  #b5 . For disagreement-based algorithms, the bound is O(d 3 2 ln 2 1 ǫ (ln d + ln ln 1 ǫ )), which is worse by a factor of O( √ d ln(1/ǫ)).  #b5  does not address the fully agnostic case directly; however, if ν * (D) is known a-priori, then their algorithm can achieve roughly the same label complexity as ours.For the Tsybakov Noise Condition with κ > 1,  #b2  #b5  provides a label complexity bound for O(ǫ 2 κ −2 ln 2 1 ǫ (d + ln ln 1 ǫ )) with an algorithm that has a-priori knowledge of C 0 and κ. We get a slightly better bound. On the other hand, a disagreement based algorithm [Han13] gives a label complexity of O(d 3 2 ln 2 1 ǫ ǫ 2 κ −2 (ln d + ln ln 1 ǫ )). Again our bound is better by factor of Ω( √ d) over disagreement-based algorithms. For κ = 1, we can tighten our label complexity to get aÕ(ln 1 ǫ (d + ln ln 1 ǫ + ln 1 δ )) bound, which again matches  #b5 , and is better than the ones provided by disagreement-based algorithm -O(d 3 2 ln 2 1 ǫ (ln d + ln ln 1 ǫ )) [Han13].

Related Work
Active learning has seen a lot of progress over the past two decades, motivated by vast amounts of unlabelled data and the high cost of annotation [Set10,  #b9 Han13]. According to  #b9 , the two main threads of research are exploitation of cluster structure [UWBD13,  #b10 , and efficient search in hypothesis space, which is the setting of our work. We are given a hypothesis class H, and the goal is to find an h ∈ H that achieves a target excess generalization error, while minimizing the number of label queries. Three main approaches have been studied in this setting. The first and most natural one is generalized binary search  #b16  #b7  #b8 Now11], which was analyzed in the realizable case by  #b8  and in various limited noise settings by [Kää06, Now11, NJC13]. While this approach has the advantage of low label complexity, it is generally inconsistent in the fully agnostic setting  #b10 . The second approach, disagreement-based active learning, is consistent in the agnostic PAC model.  #b6  provides the first disagreement-based algorithm for the realizable case.  #b1  provides an agnostic disagreement-based algorithm, which is analyzed in [Han07] using the notion of disagreement coefficient.  #b11  reduces disagreement-based active learning to passive learning;  #b3  and  #b4  further extend this work to provide practical and efficient implementations. [Han09,Kol10] give algorithms that are adaptive to the Tsybakov Noise condition. The third line of work [BBZ07, BL13, ABL14], achieves a better label complexity than disagreement-based active learning for linear classifiers on the uniform distribution over unit sphere and logconcave distributions. However, a limitation is that their algorithm applies only to these specific settings, and it is not apparent how to apply it generally.Research on confidence-rated prediction has been mostly focused on empirical work, with relatively less theoretical development. Theoretical work on this topic includes KWIK learning [LLW08], conformal prediction [SV08] and the weighted majority algorithm of  #b15 . The closest to our work is the recent learning-theoretic treatment by  #b12  #b13 .  #b12  addresses confidence-rated prediction with guaranteed error in the realizable case, and provides a predictor that abstains in the disagreement region of the version space. This predictor achieves zero error, and coverage equal to the measure of the agreement region.  #b13  shows how to extend this algorithm to the non-realizable case and obtain zero error with respect to the best hypothesis in H. Note that the predictors in  #b12  #b13  generally achieve less coverage than ours for the same error guarantee; in fact, if we plug them into our Algorithm 1, then we recover the label complexity bounds of disagreement-based algorithms  #b11 Han09,Kol10].A formal connection between disagreement-based active learning in realizable case and perfect confidencerated prediction (with a zero error guarantee) was established by  #b14 . Our work can be seen as a step towards bridging these two areas, by demonstrating that active learning can be further reduced to imperfect confidence-rated prediction, with potentially higher label savings.[ 

A Additional Notation and Concentration Lemmas
We begin with some additional notation that will be used in the subsequent proofs. Recall that we define:σ(n, δ) = 8 n (2d ln 2en d + ln 24 δ ),(3)where d is the VC dimension of the hypothesis class H.The following lemma is an immediate corollary of the multiplicative VC bound; we pick the version of the multiplicative VC bound due to [Hsu10].Lemma 7. Pick any n ≥ 1, δ ∈ (0, 1). Let S n be a set of n iid copies of (X, Y ) drawn from a distribution D over labelled examples. Then, the following hold with probability at least 1 − δ over the choice of S n :(1) For all h ∈ H,|err D (h) − err Sn (h)| ≤ min(σ(n, δ) + σ(n, δ)err D (h), σ(n, δ) + σ(n, δ)err Sn (h))(4)In particular, all classifiers h in H consistent with S n satisfieserr D (h) ≤ σ(n, δ)(5)(2) For all h, h ′ in H,|(err D (h) − err D (h ′ )) − (err Sn (h) − err Sn (h ′ ))| ≤ σ(n, δ) + min( σ(n, δ)ρ D (h, h ′ ), σ(n, δ)ρ Sn (h, h ′ )) (6) |ρ D (h, h ′ ) − ρ Sn (h, h ′ )| ≤ σ(n, δ) + min( σ(n, δ)ρ D (h, h ′ ), σ(n, δ)ρ Sn (h, h ′ ))(7)Where σ(n, δ) is defined in Equation (3).We occasionally use the following (weaker) version of Lemma 7.Lemma 8. Pick any n ≥ 1, δ ∈ (0, 1). Let S n be a set of n iid copies of (X, Y ). The following holds with probability at least 1 − δ:(1) For all h ∈ H,|err D (h) − err Sn (h)| ≤ 4σ(n, δ) (8) (2) For all h, h ′ in H, |(err D (h) − err D (h ′ )) − (err Sn (h) − err Sn (h ′ ))| ≤ 4σ(n, δ) (9) |ρ D (h, h ′ ) − ρ Sn (h, h ′ )| ≤ 4σ(n, δ)(10)Where σ(n, δ) is defined in Equation (3).For an unlabelled sample U k , we useŨ k to denote the joint distribution over X × Y induced by uniform distribution over U k and D Y |X . We have:Lemma 9. If the size of n k of the unlabelled dataset U k is at least 192( 256 ǫ k ) 2 (d ln 256 ǫ k + ln 288 δ k ), then with probability 1 − δ k /4, the following conditions hold for all h, h ′ ∈ V k :|err D (h) − errŨ k (h)| ≤ ǫ k 64 (11) |(err D (h) − err D (h ′ )) − (errŨ k (h) − errŨ k (h ′ ))| ≤ ǫ k 32 (12) |ρ D (h, h ′ ) − ρŨ k (h, h ′ )| ≤ ǫ k 64 (13)Lemma 10. If the size of n k of the unlabelled dataset U k is at least 192( 256 ǫ k ) 2 (d ln 256 ǫ k + ln 288 δ k ), then with probability 1 − δ k /4, the following hold:(1) The outputs {(ξ k,i , ζ k,i , γ k,i )} n k i=1 of any confidence-rated predictor with inputs hypothesis set V k , unlabelled data U k , and error bound ǫ k /64 satisfy:1 n k n k i=1 [I(h(x i ) = h ′ (x i ))(1 − γ k,i )] ≤ ǫ k 32 ;(14)(2) The outputs {(ξ k,i , ζ k,i , γ k,i )} n k i=1 of the confidence-rated predictor of Algortihm 3 with inputs hypothesis set V k , unlabelled data U k , and error bound ǫ k /64 satisfy:φ k ≤ Φ D (V k , ǫ k 128 ) + ǫ k 256(15)We useΓ k to denote the joint distribution over X ×Y induced by Γ k and D Y |X . Denote γ k (x) :X → [0, 1], where γ k (x i ) = γ k,i , and 0 elsewhere. Clearly, Γ k ({x}) = γ k (x) n k φ k andΓ k ({(x, y)}) =Ũ k ({(x,y)})γ k (x) φ k. Also, Equations (14) and (15) of Lemma 10 can be restated as∀h, h ′ ∈ V k , EŨ k [(1 − γ k (x))I(h(x) = h ′ (x))] ≤ ǫ k 32 EŨ k [γ k (x)] = φ k ≤ Φ D (V k , ǫ k 128 ) + ǫ k 256In the realizable case, define event  Proof. By Equation (5) of Lemma 7, with probability 1 − δ k /2, if h ∈ V k is consistent with S k , thenerrΓ k (h) ≤ σ(m k , δ k /2) Because m k = 768φ k ǫ k (d ln 768φ k ǫ k + ln 48 δ k ), we have errΓ k (h) ≤ ǫ k /8φ k .The fact follows from combining the fact above with Lemma 9 and Lemma 10, and the union bound.In the non-realizable case, define event E a = {For all k = 1, 2, . . . , k 0 : Equations (11), (12), (13), (14), (15) hold forŨ k , and Algorithm 2 succeeds with inputs hypothesis set V = V k , example distribution ∆ = Γ k , labelling oracle O, target excess errorǫ = ǫ k 8φ k and target confidenceδ = δ k 2 }.Fact 2. P(E a ) ≥ 1 − δ.Proof. This is an immediate consequence of Lemma 9, Lemma 10, Lemma 4 and union bound.Recall that we assume the hypothesis space is "dense", in the sense that ∀r > 0, sup h∈BD(h * (D),r) ρ(h, h * (D)) = r. We will call this the "denseness assumption".

B Proofs related to the properties of Algorithm 2
We first establish some properties of Algorithm 2. The inputs to Algorithm 2 are a set V of hypotheses of VC dimension d, an example distribution ∆, a labeling oracle O, a target excess errorǫ and a target confidenceδ.We define the event E = {For all j = 1, 2, . . . : Equations (4)-(7) hold for sample S j with n = n j and δ =δ j } By union bound, P(Ẽ) ≥ 1 − jδ j ≥ 1 −δ.Proof. (of Lemma 4) AssumeẼ happens. For the proof of (1), define j max as the smallest integer j such that σ(n j ,δ j ) ≤ǫ 2 /144. Since n jmax is a power of 2, n jmax ≤ 2 min{n = 1, 2, . . . : 8(2d ln 2en d + ln 24 log n(log n+1) δ ) n ≤ ǫ 2 144 } Thus, n jmax ≤ 192 144 ǫ 2 (d ln 144 ǫ + ln 24 δ ). Then in round j max , the stopping criterion (6) of Algorithm 2 is satisified; thus, Algorithm 2 halts with j 0 ≤ j max .To prove (2.1), we observe that as h * (∆) is the risk minimizer in V , if h satisfies err∆(h) − err∆(h * (∆)) ≤ ǫ 2 , then err∆(h) − err∆(ĥ j0 ) ≤ǫ 2 . By Equation (6) of Lemma 7, (err Sj 0 (h) − err Sj 0 (ĥ j0 )) ≤ (err∆(h) − err∆(ĥ j0 )) + σ(n j0 ,δ j0 ) + σ(n j0 ,δ j0 )ρ Sj 0 (h,ĥ j0 )≤ǫ 2 + σ(n j0 ,δ j0 ) + σ(n j0 ,δ j0 )ρ Sj 0 (h,ĥ j0 ) Hence h ∈ V j0 .For the proof of (2.2), note first that by (2.1), in particular, h * (∆) ∈ V j0 . Hence by Equation (6) of Lemma 7, and the stopping criterion Equation (6), (err∆(ĥ j0 ) − err∆(h * (∆))) − (err Sj 0 (ĥ j0 ) − err Sj 0 (h * (∆))) ≤ σ(n j0 ,δ j0 ) + σ(n j0 ,δ j0 )ρ Sj 0 (ĥ j0 , h * (∆)) ≤ǫ 6Thus,err∆(ĥ j0 ) − err∆(h * (∆)) ≤ǫ 6(16)On the other hand, if h ∈ V j0 , then(err∆(h) − err∆(ĥ j0 )) − (err Sj 0 (h) − err Sj 0 (ĥ j0 )) ≤ σ(n j0 ,δ j0 ) + σ(n j0 ,δ j0 )ρ Sj 0 (h,ĥ j0 ) ≤ǫ 6By definition of V j0 ,(err Sj 0 (h) − err Sj 0 (ĥ j0 )) ≤ σ(n j0 ,δ j0 ) + σ(n j0 ,δ j0 )ρ Sj 0 (h,ĥ j0 ) +ǫ 2 ≤ 2ǫ 3 Hence, err∆(h) − err∆(ĥ j0 ) ≤ 5ǫ 6(17)Combining Equations (16) and (17), we haveerr∆(h) − err∆(h * (∆)) ≤ǫProof. (of Lemma 5) AssumeẼ happens. For each j, by triangle inequality, we have that ρ Sj (ĥ j , h) ≤ err Sj (ĥ j ) + err Sj (h). If h ∈ V j , then, by defintion of V j , err Sj (h) − err Sj (ĥ j ) ≤ǫ 2 + σ(n j ,δ j ) + σ(n j ,δ j )err Sj (ĥ j ) + σ(n j ,δ j )err Sj (h)

Using the fact that
A ≤ B + C √ A ⇒ A ≤ 2B + C 2 ,err Sj (h) ≤ǫ + 2err Sj (ĥ j ) + 2 σ(n j ,δ j )err Sj (ĥ j ) + 3σ(n j ,δ j ) ≤ 3err Sj (ĥ j ) + 4σ(n j ,δ j ) +ǫ Since err Sj (ĥ j ) ≤ err Sj (h * (∆)) ≤ ν * (∆) + σ(n j ,δ j )ν * (∆) + σ(n j ,δ j ) ≤ 2ν * (∆) + 2σ(n j ,δ j ), by the triangle inequality, we get that for all h ∈ V j ,ρ Sj (h,ĥ j ) ≤ err Sj (h) + err Sj (ĥ j ) ≤ 8ν * (∆) + 12σ(n j ,δ j ) +ǫ(18)Now observe that for any j, sup h∈Vj σ(n j ,δ j )ρ Sj (h,ĥ j ) + σ(n j ,δ j ) ≤ sup h∈Vj max(2 σ(n j ,δ j )ρ Sj (h,ĥ j ), 2σ(n j ,δ j ))≤ max(2 (8ν * (∆) + 12σ(n j ,δ j ) +ǫ)σ(n j ,δ j ), 2σ(n j ,δ j )) ≤ max(12 2ν * (∆)σ(n j ,δ j ),ǫ/6, 216σ(n j ,δ j )),Where the first inequality follows from A + B ≤ 2 max(A, B), the second inequality follows from Equation (18), the third inequality follows from√ A + B ≤ √ A + √ B, A + B + C ≤ 3 max(A, B, C) and √ AB ≤ max(A, B).It can be easily seen that there exists some constant c 1 > 0, such that taking j 1 = ⌈log c1 2 (d ln 1 ǫ + ln 1 δ )( ν * (∆)+ǫǫ 2 ) ⌉ ensures that n j1 ≥ c1 2 (d ln 1 ǫ + ln 1 δ )( ν * (∆)+ǫ ǫ 2); this, in turn, suffices to make max(12 2ν * (∆)σ(n j ,δ j ), 216σ(n j ,δ j )) ≤ǫ/6Hence the stopping criterion sup h∈Vj σ(n j ,δ j )ρ Sj (h,ĥ j ) + σ(n j ,δ j ) ≤ǫ/6 is satisfied in iteration j 1 , andAlgorithm 2 exits at iteration j 0 ≤ j 1 , which ensures that n j0 ≤ n j1 ≤ c 1 (d ln 1ǫ + ln 1 δ )( ν * (∆)+ǫ ǫ 2).The following lemma examines the behavior of Algorithm 2 under the Tsybakov Noise Condition and is crucial in the proof of Theorem 5. We observe that even if the (C 0 , κ)-Tsybakov Noise Conditions hold with respect to D, they do not necessarily hold with respect to Γ k . In particular, it is not necessarily true that:ρΓ k (h, h * (D)) ≤ C 0 (errΓ k (h) − errΓ k (h * (D))) 1 κ , ∀h ∈ V kHowever, we show that an "approximate" Tsybakov Noise Condition with a significantly larger "C 0 ", namely Condition (19) is met byΓ k and V k , with C = max(8C 0 , 4)φ can be ω(1) in our particular application.Lemma 11. Suppose we run Algorithm 2 with inputs hypothesis set V , example distribution∆, labelling oracle O, excess generalization errorǫ and confidenceδ. Then there exists some absolute constant c 2 > 0 (independent of C) such that the following holds. Suppose there exist C > 0 and a classifierh ∈ V , such that∀h ∈ V, ρ∆(h,h) ≤ C max(ǫ, err∆(h) − err∆(h)) 1 κ ,(19)whereǫ is the target exccess error parameter in Algorithm 2. Then, on the event that Algorithm 2 succeeds,n j0 ≤ c 2 max((d ln 1 ǫ + ln 1 δ )ǫ −1 , (d ln(Cǫ 1 κ −2 ) + ln 1 δ )Cǫ 1 κ −2 )Observe that Condition (19), the approximate Tsybakov Noise Condition in the statement of Lemma 11, is with respect toh, which is not necessarily the true risk minimizer in V with respect to∆. We therefore prove Lemma 11 in three steps; first, in Lemma 12, we analyze the difference err∆(ĥ) − err∆(h), whereĥ is the empirical risk minimizer. Then, in Lemma 13, we bound the difference err∆(h) − err∆(h) for any h ∈ V j for some j. Finally, we combine these two lemmas to provide sample complexity bounds for the V j0 output by Algorithm 2.Proof. (of Lemma 11) Assume the eventẼ happens. Then, Consider iteration j, by Lemma 13, if h ∈ V j , thenρ∆(h,ĥ j ) ≤ ρ∆(h,h) + ρ∆(ĥ j ,h) ≤ max(2C(36ǫ) 1 κ , 2C(52σ(n j ,δ j )) 1 κ , 2C(6400Cσ(n j ,δ j )) 1 2κ−1 ).(20)We can write:sup h∈Vj σ(n j ,δ j ) + σ(n j ,δ j )ρ Sj (h,ĥ j ) ≤ sup h∈Vj 3σ(n j ,δ j ) + 2σ(n j ,δ j )ρ∆(h,ĥ j )≤ sup h∈Vj max(6σ(n j ,δ j ), 2 2σ(n j ,δ j )ρ∆(h,ĥ j )),where the first inequality follows from Equation (23) and the second inequality follows A + B ≤ 2 max (A, B).

We can further use Equation (20) to show that this is at most:
≤ max(6σ(n j ,δ j ), (16Cσ(n j ,δ j )) 1 2 (36ǫ) 1 2κ , (16Cσ(n j ,δ j )) 1 2 (52σ(n j ,δ j )) 1 2κ , (6400Cσ(n j ,δ j )) κ 2κ−1 ) ≤ max(6σ(n j ,δ j ),ǫ/6, (6400Cσ(n j ,δ j )) κ 2κ−1 )Here the last inequality follows from the fact that (16Cσ(n j ,δ j )) 1 2 (36ǫ) 1 2κ ≤ max((3456Cσ(n j ,δ j )) κ 2κ−1 ,ǫ/6) and (16Cσ(n j ,δ j )) 1 2 (52σ(n j ,δ j )) (A, B). It can be easily seen that there exists c 2 > 0, such that taking j 1 = ⌈log c2 2 (d ln max(C,1)1 2κ ≤ max((144Cσ(n j ,δ j )) κ 2κ−1 , 6σ(n j ,δ j )), since A 2κ−1 2κ B 1 2κ ≤ maxǫ + ln 1 δ )(Cǫ 1 κ −2 + ǫ −1 )⌉, so that n j ≥ c2 2 (d ln max(C,1) ǫ + ln 1 δ )(Cǫ 1 κ −2 +ǫ −1 ) suffices to make max(6σ(n j ,δ j ), (6400Cσ(n j ,δ j )) κ 2κ−1 ) ≤ǫ/6Hence the stopping criterion sup h∈Vj σ(n j ,δ j )ρ Sj (h,ĥ j ) + σ(n j ,δ j ) ≤ǫ/6 is satisfied in iteration j 1 . Thus the number of the exit iteration j 0 satisfies j 0 ≤ j 1 , and n j0 ≤ n j1 ≤ c 2 max((d ln 1ǫ + ln 1 δ )ǫ −1 , (d ln(Cǫ 1 κ −2 )+ ln 1 δ )Cǫ 1 κ −2 ).Lemma 12. Suppose there exist C > 0 and a classifierh ∈ V , such that Equation (19) holds. Suppose we draw a set S of n examples, denote the empirical risk minimizer over S asĥ, then with probability 1 − δ:err∆(ĥ) − err∆(h) ≤ max(2σ(n, δ), (4Cσ(n, δ)) κ 2κ−1 , 2ǫ) ρ∆(ĥ,h) ≤ max(C(2σ(n, δ)) 1 κ , C(4Cσ(n, δ)) 1 2κ−1 , C(2ǫ) 1 κ )Proof. By Lemma 7, with probability 1 − δ, Equation (6)  ≤ max(2σ(n, δ), (4Cσ(n, δ)) κ 2κ−1 , 2ǫ)Where the first inequality is by Equation (6) of Lemma 7; the second inequality follow from Equation (19) and A + B ≤ 2 max(A, B). The third inequality follows from 2 σ(n, δ)Cǫ (A, B). As a consequence, by Equation (19),1 κ ≤ max(2(Cσ(n, δ)) κ 2κ−1 , 2ǫ), since A 2κ−1 2κ B 1 2κ ≤ maxρ∆(ĥ,h) ≤ max(C(2σ(n, δ)) 1 κ , C(4Cσ(n, δ)) 1 2κ−1 , C(2ǫ) 1 κ )Lemma 13. Suppose there exist a C > 0 and a classifierh ∈ V such that Equation (19) holds. Suppose we draw a set S of n iid examples, and letĥ denote the empirical risk minimizer over S. Moreover, we define:V = h ∈ V : err S (h) ≤ err S (ĥ) +ǫ 2 + σ(n, δ) + σ(n, δ)ρ S (h,ĥ)then with probability 1 − δ, for all h ∈Ṽ ,err∆(h) − err∆(h) ≤ max(52σ(n, δ), 36ǫ, (6400Cσ(n, δ)) κ 2κ−1 ) ρ∆(h,h) ≤ max(C(36ǫ)1 κ , C(52σ(n, δ)) 1 κ , C(6400Cσ(n, δ)) 1 2κ−1 )Proof. First, by Lemma 12,err∆(ĥ) − err∆(h) ≤ max(2σ(n, δ), (4Cσ(n, δ)) κ 2κ−1 , 2ǫ) (21) ρ∆(ĥ,h) ≤ max(C(2σ(n, δ)) 1 κ , C(4Cσ(n, δ)) 1 2κ−1 , C(2ǫ) 1 κ ) (22) Next, if h ∈Ṽ , then err S (h) − err S (ĥ) ≤ σ(n, δ) + σ(n, δ)ρ S (h,ĥ) +ǫ 2Combining it with Equation (6) of Lemma 7: err∆(h) − err∆(ĥ) ≤ err S (h) − err S (ĥ) + σ(n, δ)ρ S (h,ĥ) + σ(n, δ), we get err∆(h) − err∆(ĥ) ≤ 2σ(n, δ) + 2 σ(n, δ)ρ S (h,ĥ) +ǫ 2 By Equation (7) of Lemma 7,ρ S (h,ĥ) ≤ ρ∆(h,ĥ) + σ(n, δ)ρ∆(h,ĥ) + σ(n, δ) ≤ 2ρ∆(h,ĥ) + 2σ(n, δ)(23)Therefore,err∆(h) − err∆(ĥ) ≤ 5σ(n, δ) + 3 σ(n, δ)ρ∆(h,ĥ) +ǫ 2 (24) Hence err∆(h) − err∆(h) = (err∆(h) − err∆(ĥ)) + (err∆(ĥ) − err∆(h)) ≤ (4Cσ(n, δ)) κ 2κ−1 + 7σ(n, δ) + 3ǫ + 3 σ(n, δ)ρ∆(h,ĥ)≤ (4Cσ(n, δ)) κ 2κ−1 + 7σ(n, δ) + 3ǫ + 3 σ(n, δ)ρ∆(h,h) + 3 σ(n, δ)ρ∆(h,ĥ)Here the first inequality follows from Equations (21) and (24) and max(A, B, C) ≤ A + B + C, and the second inequality follows from triangle inequality and (22), σ(n, δ)ρ∆(ĥ,h) is at most: ≤ Cσ(n, δ) · ((2ǫ) 1/κ + (2σ(n, δ)) 1/κ + (4Cσ(n, δ)) 1/(2κ−1) ) ≤ (4Cσ(n, δ)) 2κ/(2κ−1) + Cσ(n, δ)((2ǫ) 1/κ + (2σ(n, δ)) 1/κ ) ≤ (4Cσ(n, δ)) 2κ/(2κ−1) + max(4ǫ 2 , (Cσ(n, δ)) 2κ/(2κ−1) ) + max(4σ(n, δ) 2 , (Cσ(n, δ)) 2κ/(2κ−1) ), where the first step follows from Equation (22), the second step from algebra, and the third step from using the fact that A Combining this with the fact that A + B + C + D ≤ 4 max(A, B, C, D), we get that this is at most:√ A + B ≤ √ A + √ B. From Equation≤ max(40(4Cσ(n, δ)) κ/(2κ−1) , 36ǫ, 52σ(n, δ), 12 σ(n, δ)ρ∆(h,h))Combining this with Condition (19), we get that this is at most: max(40(4Cσ(n, δ)) κ/(2κ−1) , 36ǫ, 52σ(n, δ), 12 Cσ(n, δ)ǫ 1/κ , 12 Cσ(n, δ)(err∆(h) − err∆(h)) 1/κ ) Using A (2κ−1)/2κ B 1/2κ ≤ max (A, B), we get that Cσ(n, δ)ǫ 1/κ ≤ max(ǫ, (Cσ(n, δ)) κ/(2κ−1) ). Also note err∆(h) − err∆(h) ≤ 12 Cσ(n, δ)(err∆(h) − err∆(h)) 1/κ implies err∆(h) − err∆(h) ≤ (144Cσ(n, δ)) κ/(2κ−1) . Thus we have err∆(h) − err∆(h) ≤ max(36ǫ, 52σ(n, δ), (6400Cσ(n, δ)) κ 2κ−1 ) Invoking (19) again, we have that: Proof. (Of Lemma 2) We useh k = argmin h∈V k errΓ k (h) to denote the optimal classifier in V k with respect to the distributionΓ k . Assuming E a happens, we prove the lemma by induction. Base Case: For k = 1, clearly h * (D) ∈ V 1 = H. Inductive Case: Assume h * ∈ V k . In order to show the inductive case, our goal is to show that:ρ∆(h,h) ≤ max(C(36ǫ)PΓ k (h * (D)(x) = y) − PΓ k (h k (x) = y) ≤ ǫ k 16φ k(25)If (25) holds, then, by (2.1) of Lemma 4, we know that if Algorithm 2 succeeds when called in iteration k of Algorithm 1, then, it is guaranteed that h * ∈ V k+1 . We therefore focus on showing (25). First, from Equation (12) of Lemma 9, we have:(errŨ k (h * (D)) − errŨ k (h k )) − (err D (h * (D)) − err D (h k )) ≤ ǫ k 32As err D (h * (D)) ≤ err D (h k ), we get:errŨ k (h * (D)) ≤ errŨ k (h k ) + ǫ k 32(26)On the other hand, by Equation (14) of Lemma 10 and triangle inequality,EŨ k [I(h k (x) = y)(1 − γ k (x))] − EŨ k [I(h * (D)(x) = y)(1 − γ k (x))] (27) ≤ EŨ k [I(h * (D)(x) =h k (x))(1 − γ k (x))] ≤ ǫ k 32(28)Combining Equations (26) and (27), we get:EŨ k [I(h * (D)(x) = y)γ k (x)] = errŨ k (h * (D)(x)) − EŨ k [I(h * (D)(x) = y)(1 − γ k (x))] ≤ errŨ k (h k (x)) + ǫ k /32 − EŨ k [I(h * (D)(x) = y)(1 − γ k (x))] ≤ EŨ k [I(h k (x) = y)γ k (x)] + EŨ k [I(h(x) = y)(1 − γ k (x))] + ǫ k /32 −EŨ k [I(h * (D)(x) = y)(1 − γ k (x))] ≤ EŨ k [I(h k (x) = y)γ k (x)] + ǫ k /16Dividing both sides by φ k , we get:PΓ k (h * (D)(x) = y) − PΓ k (h k (x) = y) ≤ ǫ k 16φ k ,from which the lemma follows.Proof. (of Lemma 3) Assuming E r happens, we prove the lemma by induction. Base Case: For k = 1, clearly err D (h) ≤ 1 ≤ ǫ 1 = ǫ2 k0 , ∀h ∈ V 1 = H. Inductive Case: Note that ∀h, h ′ ∈ V k+1 ⊆ V k , by Equation (14) of Lemma 10, we have:EŨ k [I(h(x) = h ′ (x))(1 − γ k (x))] ≤ ǫ k 8By the proof of Lemma 1, h * (D) ∈ V k+1 on event E r , thus ∀h ∈ V k+1 ,EŨ k [I(h(x) = h * (D)(x))(1 − γ k (x))] ≤ ǫ k 8 (29) Since any h ∈ V k+1 , h is consistent with S k of size m k = 768φ k ǫ k (d ln 768φ k ǫ k + ln 48 δ k ), we have that for all h ∈ V k+1 , PΓ k (h(x) = h * (D)(x)) ≤ ǫ k 8φ k That is, EŨ k [I(h(x) = h * (D)(x))γ k (x)] ≤ ǫ k 8 Combining this with Equation (29) above, PŨ k (h(x) = h * (D)(x)) ≤ ǫ k 4By Equation (11) of Lemma 9,P D (h(x) = h * (D)(x)) ≤ ǫ k 2 = ǫ k+1The lemma follows.Proof. (of Lemma 6) Assuming E a happens, we prove the lemma by induction.Base Case: (14) of Lemma 10,For k = 1, clearly err D (h) − err D (h * (D)) ≤ 1 ≤ ǫ 1 = ǫ2 k0 , ∀h ∈ V 1 = H. Inductive Case: Note that ∀h, h ′ ∈ V k+1 ⊆ V k , by EquationEŨ k [I(h(x) = y)(1 − γ k (x))] − EŨ k [I(h ′ (D)(x) = y)(1 − γ k (x))] ≤ EŨ k [I(h(x) = h ′ (D)(x))(1 − γ k (x))] ≤ ǫ k 8From Lemma 2, h * (D) ∈ V k whenever the event E a happens. Thus ∀h ∈ V k+1 ,EŨ k I(h(x) = y)(1 − γ k (x)) − EŨ k I(h * (D)(x) = y)(1 − γ k (x)) ≤ ǫ k 8(30)On the other hand, if Algorithm 2 succeeds with target excess error ǫ k 8φ k , by item(2.2) of Lemma 4, for any h ∈ V k+1 ,PΓ k (h(x) = y) − min h∈V k PΓ k (h(x) = y) ≤ ǫ k 8φ k Moreover, as h * (D) ∈ V k from Lemma 2, PΓ k (h(x) = y) − PΓ k (h * (D)(x) = y) ≤ ǫ k 8φ kIn other words,EŨ k [I(h(x) = y)γ k (x)] − EŨ k [I(h * (D)(x) = y)γ k (x)] ≤ ǫ k 8 Combining this with Equation (30), we get that for all h ∈ V k+1 , PŨ k (h(x) = y) − PŨ k (h * (D)(x) = y) ≤ ǫ k 4Finally, combining this with Equation (12) of Lemma 9, we have that:P D (h(x) = y) − P D (h * (D)(x) = y) ≤ ǫ k 2 = ǫ k+1The lemma follows.Proof. (of Theorem 1) In the realizable case, We observe that for example z i , ζ i = P(P (z i ) = −1), ξ i = P(P (z i ) = 1), and γ i = P(P (z i ) = 0). Suppose h * ∈ H is the true hypothesis which has 0 error with respect to the data distribution. By the realizability assumption, h * ∈ V . Moreover, P U (P (x) = h * (x), P (x) = 0) = 1 m ( i:h * (zi)=+1 ζ i + i:h * (zi)=−1 ξ i ) ≤ η by Algorithm 3.In the non-realizable case, we still have P x∼U (h * (x) = P (x), P (x) = 0) ≤ η, hence by triangle inequality,P x∼U (P (x) = x, P (x) = 0) − P x∼U (h * (x) = y, P (x) = 0) ≤ η. Thus P x∼U (P (x) = y, P (x) = 0) ≤ P x∼U (h * (x) = y) + η Proof. (of Theorem 2) Suppose P ′ assigns probabilities {[ξ ′ i , ζ ′ i , γ ′ i ], i = 1, .. . , m} to the unlabelled examples z i , and suppose for the sake of contradiction thatm i=1 ξ ′ i + ζ ′ i > m i=1 ξ i + ζ i . Then, {ξ ′ i , ζ ′ i , γ ′ i }'s cannot satisfy the LP in Algorithm 3, and thus there exists some h ′ ∈ V for which constraint (2) is violated. The true hypothesis that generates the data could be any h ∈ V ; if this true hypothesis is h ′ , then P x∼U (P ′ (x) = h ′ (x), P ′ (x) = 0) > δ.

D Proofs from Section 3
Proof. (of Theorem 4) (1) In the realizable case, suppose that event E r happens. Then from Equation (15) of Lemma 10, while running Algorithm 3, we have that:φ k ≤ Φ D (V k , ǫ k 128 ) + ǫ k 256 ≤ Φ D (B D (h * , ǫ k ), ǫ k 128 ) + ǫ k 256 ≤ Φ D (B D (h * , ǫ k ), ǫ k 256 ) = φ(ǫ k , ǫ k 256 )where the second inequality follows from the fact that V k ⊆ B D (h * (D), ǫ k ), and third inequality follows from Lemma 18 and denseness assuption. Thus, there exists c 3 > 0 such that, in round k,m k = (d ln 768φ k ǫ k + ln 48 δ k ) 768φ k ǫ k ≤ c 3 (d ln φ(ǫ k , ǫ k /256) ǫ k + ln( k 0 − k + 1 δ )) φ(ǫ k , ǫ k /256) ǫ kHence the total number of labels queried by Algorithm 1 is at most⌈log 1 ǫ ⌉ k=1 m k ≤ c 3 ⌈log 1 ǫ ⌉ k=1 (d ln φ(ǫ k , ǫ k /256) ǫ k + ln( k 0 − k + 1 δ )) φ(ǫ k , ǫ k /256) ǫ k(2) In the agnostic case, suppose the event E a happens. First, given E a , from Equation (15) of Lemma 10 when running Algorithm 3,φ k ≤ Φ D (V k , ǫ k 128 ) + ǫ k 256 ≤ Φ D (B D (h * , 2ν * (D) + ǫ k ), ǫ k 256 ) = φ(2ν * (D) + ǫ k , ǫ k 256 )(31)where the second inequality follows from the fact that V k ⊆ B D (h * (D), 2ν * (D) + ǫ k ) and the third inequality follows from Lemma 18 and denseness assumption.Second, recall thath k = argmin h∈V k errΓ k (h),errΓ k (h k ) = min h∈V k errΓ k (h) ≤ errΓ k (h * (D)) = EŨ k [I(h * (D)(x) = y)γ k (x)] φ k ≤ PŨ k (h * (D)(x) = y) φ k ≤ ν * (D) + ǫ k /64 φ kHere the first inequality follows from the suboptimality of h * (D) under distributionΓ k , the second inequality follows from γ k (x) ≤ 1, and the third inequality follows from Equation (11). Thus, conditioned on E a , in iteration k, Algorithm 2 succeeds by Lemma 5, and there exists a constant c 4 > 0 such that the number of labels queried ism k ≤ c 1 ǫ k 8φ k + errΓ k (h k ) ( ǫ k 8φ k ) 2 (d ln 1 ǫ k 8φ k + ln 2 δ k ) ≤ c 4 (d ln φ(2ν * (D) + ǫ k , ǫ k /256) ǫ k + ln( k 0 − k + 1 δ )) φ(2ν * (D) + ǫ k , ǫ k /256) ǫ k (1 + ν * (D) ǫ k )Here the last line follows from Equation (31). Hence the total number of examples queried is at most:⌈log 1 ǫ ⌉ k=1 m k ≤ c 4 ⌈log 1 ǫ ⌉ k=1 (d ln φ(2ν * (D) + ǫ k , ǫ k /256) ǫ k + ln( k 0 − k + 1 δ )) φ(2ν * (D) + ǫ k , ǫ k /256) ǫ k (1 + ν * (D) ǫ k )Proof. (of Theorem 5) Assume E a happens. First, from Equation (15) of Lemma 10 when running Algorithm 3,φ k ≤ Φ D (V k , ǫ k 128 )+ ǫ k 256 ≤ Φ D (B D (h * , C 0 ǫ 1 κ k ), ǫ k 128 )+ ǫ k 256 ≤ Φ D (B D (h * , C 0 ǫ 1 κ k ), ǫ k 256 ) = φ(C 0 ǫ 1 κ k , ǫ k 256 ) (32)where the second inequality follows from the fact that V k ⊆ B D (h * (D), C 0 ǫ 1 κ k ), and the third inequality follows from Lemma 18 and denseness assumption.Second, for all h ∈ V k , φ k ρΓ k (h, h * (D)) = EŨ k I(h(x) = h * (D)(x))γ k (x) ≤ ρŨ k (h, h * (D)) ≤ ρ D (h, h * (D)) + ǫ k /32 ≤ C 0 (err D (h) − err D (h * (D))) 1 κ + ǫ k /32 ≤ C 0 (errŨ k (h) − errŨ k (h * (D)) + ǫ k /64) 1 κ + ǫ k /32 = C 0 (EŨ k [I(h(x) = y)γ k (x)] − EŨ k [I(h * (D)(x) = y)γ k (x)] +EŨ k [I(h(x) = y)(1 − γ k (x))] − EŨ k [I(h * (D)(x) = y)(1 − γ k (x))] + ǫ k /16) 1 κ + ǫ k /32Here the first inequality follows from γ k (x) ≤ 1, the second inequality follows from Equation (13) of Lemma 9, the third inequality follows from Definition 1 and the fourth inequality follows from Equation (12) of Lemma 9. The above can be upper bounded by:≤ C 0 (EŨ k [I(h(x) = y)γ k (x)] − EŨ k [I(h * (D)(x) = y)γ k (x)] + ǫ k /16) 1 κ + ǫ k /32 ≤ 2C 0 (EŨ k [I(h(x) = y)γ k (x)] − EŨ k [I(h * (D)(x) = y)γ k (x)]) 1 κ + 2C 0 (ǫ k /16) 1 κ + ǫ k /32 ≤ max(8C 0 , 4) max((EŨ k [I(h(x) = y)γ k (x)] − EŨ k [I(h * (D)(x) = y)γ k (x)]), ǫ k 16 ) 1 κ = max(8C 0 , 4)(φ k ) 1 κ max(PΓ k (h(x) = y) − PΓ k (h * (D)(x) = y), ǫ k 8φ k ) 1 κHere the first inequality follows from Equation (14) of Lemma 10 and triangle inequality EŨk [I(h(x) = y)γ k (x)]−EŨ k [I(h * (D)(x) = y)γ k (x)] ≤ EŨ k [I(h(x) = h * (D)(x))γ k (x)] ≤ ǫ k /32, and the last two inequalities follow from simple algebra.Dividing both sides by φ k , we get:ρΓ k (h, h * (D)) ≤ C 1 (φ k ) 1 κ −1 max(errΓ k (h) − errΓ k (h * (D)), ǫ k 8φ k ) 1 κwhere C 1 = max(8C 0 , 4). Thus in iteration k, Condition (19) in Lemma 11 holds with C := C 1 (φ k ) 1 κ −1 and h := h * (D). Thus, from Lemma 11, Algorithm 2 succeeds, and there exists a constant c 5 > 0, such that the number of labels queried ism k ≤ c 2 max((d ln(C 1 (φ k ) 1 κ −1 ( ǫ k 8φ k ) 1 κ −2 ) + ln 2 δ k )(C 1 (φ k ) 1 κ −1 ( ǫ k 8φ k ) 1 κ −2 ), (d ln( ǫ k 8φ k ) −1 + ln 2 δ k )( ǫ k 8φ k ) −1 ) ≤ c 5 (d ln(φ k ǫ 1 κ −2 k ) + ln( k 0 − k + 1 δ ))φ k ǫ 1 κ −2 k ≤ c 5 (d ln(φ(C 0 ǫ 1 κ k , ǫ k 256 )ǫ 1 κ −2 k ) + ln( k 0 − k + 1 δ ))φ(C 0 ǫ 1 κ k , ǫ k 256 )ǫ 1 κ −2 kWhere the last line follows from Equation (31). Hence the total number of examples queried is at most⌈log 1 ǫ ⌉ k=1 m k ≤ c 5 ⌈log 1 ǫ ⌉ k=1 (d ln(φ(C 0 ǫ 1 κ k , ǫ k 256 )ǫ 1 κ −2 k ) + ln( k 0 − k + 1 δ ))φ(C 0 ǫ 1 κ k , ǫ k 256 )ǫ 1 κ −2 kThe following lemma is an immediate corollary of Theorem 21, item (a) of Lemma 2 and Lemma 3 of  #b5 :Lemma 14. Suppose D is isotropic and log-concave on R d , and H is the set of homogeneous linear classifiers on R d , then there exist absolute constants c 6 , c 7 > 0 such that φ(r, η) ≤ c 6 r ln c7r η .Proof. (of Lemma 14) Denote w h as the unit vector w such that h(x) = sign(w · x), and θ(w, w ′ ) to be the angle between vectors w and w ′ . If h ∈ B D (h * , r), then by Lemma 3 of  #b5 , there exists some constant c 11 > 0 such that θ(w h , w h * ) ≤ r c11 . Also, by Lemma 21 of  #b5 , there exists some constants c 12 , c 13 > 0, such that, if θ(w, w ′ ) = α thenP D (sign(w · x) = sign(w ′ · x), |w · x| ≥ b) ≤ c 12 α exp(−c 13 b α )We define a special solution (ξ, ζ, γ) as follows:ξ(x) := I(w h * · x ≥ r c 11 c 13 ln c 12 r c 11 η )ζ(x) := I(w h * · x ≤ − r c 11 c 13 ln c 12 r c 11 η )γ(x) := I(|w h * · x| ≤ r c 11 c 13 ln c 12 r c 11 η )Then it can be checked that for all h ∈ B D (h * , r), E[I(h(x) = +1)ζ(x) + I(h(x) = −1)ξ(x)] = P D (sign(w h * · x) = sign(w h · x), |w h * · x| ≥ r c 11 c 13 ln c 12 r c 11 η ) ≤ ηV 1 = h ∈ V : err S (h) ≤ err S (ĥ) + 3ǫ 4 5: return V 1 .It is immediate that we have the following lemma.Lemma 15. Suppose we run Algorithm 4 with inputs hypothesis set V , example distribution ∆, labelling oracle O, target excess errorǫ and target confidenceδ. Then there exists an eventẼ, P(Ẽ) ≥ 1 −δ, such that onẼ, the set V 1 has the following property.(1) If for h ∈ H, err∆(h) − err∆(h * (∆)) ≤ǫ/2, then h ∈ V 1 .(2) On the other hand, if h ∈ V 1 , then err∆(h) − err∆(h * (∆)) ≤ǫ.WhenẼ happens, we say that Algorithm 4 succeeds.Proof. By Equation (9) of Lemma 8 and because n = 6144 ǫ 2 (d ln 6144 ǫ 2 + ln 24 δ ), we have for all h, h ′ ∈ H,(err∆(h) − err∆(h ′ )) − (err S (h) − err S (h ′ )) ≤ ǫ 4For the proof of (1), for any h ∈ V , err∆(h) − err∆(h * (∆)) ≤ǫ/2, then err∆(h) − err∆(ĥ) ≤ǫ/2Thus err S (h) − err S (ĥ) ≤ 3ǫ 4 proving h ∈ V 1 . For the proof of (2), for any h ∈ V 1 ,err S (h) − err S (h ′ ) ≤ 3ǫ 4 Thus err S (h) − err S (h * (∆)) ≤ 3ǫ 4 Combining with the fact that (err∆(h) − err∆(h * (∆))) − (err S (h) − err S (h * (∆))) ≤ ǫ 4 we have err∆(h) − err∆(h * (∆)) ≤ǫCorollary 2. Suppose we replace the calls to Algorithm 2 with Algorithm 4 in Algorithm 1, then run it with inputs example oracle U, labelling oracle O, hypothesis class V , confidence-rated predictor P of Algorithm 3, target excess error ǫ and target confidence δ. Then the modified algorithm has a label complexity ofO( ⌈log 1/ǫ⌉ k=1 (d( φ(2ν * (D) + ǫ k , ǫ k /256) ǫ k ) 2 )in the agnostic case andÕ( ⌈log 1/ǫ⌉ k=1 d( φ(C 0 ǫ 1 κ k , ǫ k 256 ) ǫ 1 κ k ) 2 ǫ 2 κ −2 k ) under (C 0 , κ)-Tsybakov Noise Condition.Under denseness assumption, by Lemma 17, we have φ(r, η) ≥ r − 2η, the label complexity bounds given by Corollary 2 is always no better than the ones given by Theorem 4 and 5.Proof. (Sketch) Define event E a = {For all k = 1, 2, . . . , k 0 : Equations (11), (12), (13), (14), (15) hold forŨ k with confidence δ k /2, and Algorithm 4 succeeds with inputs hypothesis set V = V k , example distribution ∆ = Γ k , labelling oracle O, target excess errorǫ = ǫ k 8φ k and target confidenceδ = δ k 2 }.Clealy, P(E a ) ≥ 1 − δ. On the event E a , there exists an absolute constant c 13 > 0, such that the number of examples queried in interation k ism k ≤ c 13 (d ln 8φ k ǫ k + ln 2 δ )( 8φ k ǫ k ) 2Combining it with Equation (15) of Lemma 10φ k ≤ Φ D (V k , ǫ k 128 ) + ǫ k 256 we have m k ≤ O((d ln Φ D (V k , ǫ k 128 ) + ǫ k 256 ǫ k + ln 2 δ k )( Φ D (V k , ǫ k 128 ) + ǫ k 256 ǫ k ) 2 )The rest of the proof follows from Lemma 18 and denseness assumption, along with algebra.

F Proofs of Concentration Lemmas
Proof. (of Lemma 9) We begin by observing that:errŨ k (h) = 1 n k n k i=1 [P D (Y = +1|X = x i )I(h(x i ) = −1) + P D (Y = −1|X = x i )I(h(x i ) = +1)] Moreover, max(S({I(h(x) = 1, h ∈ H)}, n), S({I(h(x) = −1, h ∈ H)}, n)) ≤ ( en d ) d .Combining this fact with Lemma 16, the following equations hold simultaneously with probability 1 − δ k /6:1 n k n k i=1 P D (Y = +1|X = x i )I(h(x i ) = −1) − P D (h(x) = −1, y = +1) ≤ 8(d ln en k d + ln 24 δ k ) n k ≤ ǫ k 128 1 n k n k i=1 P D (Y = −1|X = x i )I(h(x i ) = +1) − P D (h(x) = +1, y = −1) ≤ 8(d ln en k d + ln 24 δ k ) n k ≤ ǫ k 128Thus Equation (11) holds with probability 1 − δ k /6. Moreover, we observe that Equation (11) implies Equation (12). To show Equation (13), we observe that by Lemma 8, with probability 1 − δ k /12,|ρ D (h, h ′ ) − ρŨ k (h, h ′ )| = |ρ D (h, h ′ ) − ρ S k (h, h ′ )| ≤ 2 σ(n k , δ k /12) ≤ ǫ k 64Thus, Equation (13) holds with probability ≥ 1 − δ k /12. By union bound, with probability 1 − δ k /4, Equations (11), (12), and (13) hold simultaneously.Proof. (of Lemma 10) (1) Given a confidence-rated predictor with inputs hypothesis set V k , unlabelled data U k , and error bound ǫ k /64, the outputs {(ξ k,i , ζ k,i , γ k,i )} n k i=1 must satisfy that for all h, h ′ ∈ V k , [I(h(x k,i ) = h ′ (x k,i ))(ξ k,i + ζ k,i )] ≤ ǫ k 32That is,1 n k n k i=1 [I(h(x k,i ) = h ′ (x k,i ))(1 − γ k,i )] ≤ ǫ k 32(2) By definition of Φ D (V, η), there exist nonnegative functions ξ, ζ, γ such that ξ(x) + ζ(x) + γ(x) ≡ 1, E D [γ(x)] = Φ D (V k , ǫ k /128) and for all h ∈ V k , Consider the linear progam in Algorithm 3 with inputs hypothesis set V k , unlabelled data U k , and error bound ǫ k /64. We consider the following special (but possibly non-optimal) solution for this LP: ξ k,i = ξ(z k,i ), ζ k,i = ζ(z k,i ), γ k,i = γ(z k,i ). We will now show that this solution is feasible and has coverage Φ D (V k , ǫ k /128) plus O(ǫ k ) with high probability. Observe that max(S({I(h(x) = 1, h ∈ H)}, n), S({I(h(x) = −1, h ∈ H)}, n)) ≤ ( en d ) d . Therefore, from Lemma 16 and the union bound, with probability 1 − δ k /4, the following hold simultaneously for all h ∈ H: Thus {(ξ(z k,i ), ζ(z k,i )} n k i=1 is a feasible solution of the linear program of Algorithm 3. Also, by Equation (33),1 n k n k i=1 γ(z k,i ) − E D γ(x) ≤ ln 2 δ k 2n k ≤ ǫ k 256(33)1 n k n k i=1 γ(z k,i ) ≤ Φ D (V k , ǫ k 128 ) + ǫ k 64 .Thus, the outputs {(ξ k,i , ζ k,i , γ k,i )} n k i=1 of the linear program in Algorithm 3 satisfyφ k = 1 n k n k i=1 γ k,i ≤ 1 n k n k i=1 γ(z k,i ) ≤ Φ D (V k , ǫ k 128 ) + ǫ k 256due to their optimality. where S(F , n) = max z1,...,zn∈Z |{(f (z 1 ), . . . , f (z n )) : f ∈ F }| is the growth function of F .Proof. The proof is fairly standard, and follows immediately from the proof of additive VC bounds. With probability 1 − δ, sup f ∈F Proof. Suppose (ξ 1 , ζ 1 , γ 1 ) are nonnegative functions satisfying ξ 1 + ζ 1 + γ 1 ≡ 1, and for all h ∈ V , E D [ζ 1 (x)I(h(x) = +1)+ξ 1 (x)I(h(x) = −1)] ≤ η−λ, and E D γ 1 (x) = Φ D (V, η−λ). Notice by Lemma 17,Φ D (V, η− λ) ≥ 2η − λ − 2(η − λ) = λ.Then we pick nonnegative functions (ξ 2 , ζ 2 , γ 2 ) as follows. Let ξ 2 = ξ 1 , γ 2 = (1 − λ ΦD (V,η−λ) )γ 1 , and ζ 2 = 1 − ξ 2 − γ 2 . It is immediate that (ξ 2 , ζ 2 , γ 2 ) is a valid confidence rated predictor and ζ 2 ≥ ζ 1 , γ 2 ≤ γ 1 , E D γ 2 (x) = Φ D (V, η − λ) − λ. It can be readily checked that the confidence rated predictor (ξ 2 , ζ 2 , γ 2 ) has error guarantee η, specifically: E D [ζ 2 (x)I(h(x) = +1) + ξ 2 (x)I(h(x) = −1)] ≤ E D [(ζ 2 (x) − ζ 1 (x))I(h(x) = +1) + (ξ 2 (x) − ξ 1 (x))I(h(x) = −1)] + η − λ ≤ E D [(ζ 2 (x) − ζ 1 (x)) + (ξ 2 (x) − ξ 1 (x))] + η − λ ≤ λ + η − λ = η Thus, Φ D (V, η), which is the minimum abstention probability of a confidence-rated predictor with error guarantee η with respect to hypothesis set V and data distribution D, is at most Φ D (V, η − λ) − λ.

G Detailed Derivation of Label Complexity Bounds
G.1 Agnostic Proposition 1. In agnostic case, the label complexity of Algorithm 1 is at mostO( sup k≤⌈log(1/ǫ)⌉ φ(2ν * (D) + ǫ k , ǫ k /256) 2ν * (D) + ǫ k (d ν * (D) 2 ǫ 2 ln 1 ǫ + d ln 2 1 ǫ )),where theÕ notation hides factors logarithmic in 1/δ.Proof. Applying Theorem 5, the total number of labels queried is at most:c 4 ⌈log 1 ǫ ⌉ k=1(d ln φ(2ν * (D) + ǫ k , ǫ k /256) ǫ k + ln( ⌈log(1/ǫ)⌉ − k + 1 δ )) φ(2ν * (D) + ǫ k , ǫ k /256) ǫ k (1 + ν * (D) ǫ k )Using the fact that φ(2ν * (D) + ǫ k , ǫ k /256) ≤ 1, this is c 4 ⌈log 1 ǫ ⌉ k=1 (d ln φ(2ν * (D) + ǫ k , ǫ k /256) ǫ k + ln( ⌈log(1/ǫ)⌉ − k + 1 δ )) φ(2ν * (D) + ǫ k , ǫ k /256) ǫ k (1 + ν * (D) where the last line follows as ǫ k is geometrically decreasing.

Footnote
4 : Here theÕ() notation hides factors logarithmic in 1/δ