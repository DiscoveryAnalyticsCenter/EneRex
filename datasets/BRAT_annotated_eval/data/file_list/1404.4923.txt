Unified Structured Learning for Simultaneous Human Pose Estimation and Garment Attribute Classification

Abstract
In this paper, we utilize structured learning to simultaneously address two intertwined problems: 1) human pose estimation (HPE) and 2) garment attribute classification (GAC), which are valuable for a variety of computer vision and multimedia applications. Unlike previous works that usually handle the two problems separately, our approach aims to produce an optimal joint estimation for both HPE and GAC via a unified inference procedure. To this end, we adopt a preprocessing step to detect potential human parts from each image (i.e., a set of "candidates") that allows us to have a manageable input space. In this way, the simultaneous inference of HPE and GAC is converted to a structured learning problem, where the inputs are the collections of candidate ensembles, the outputs are the joint labels of human parts and garment attributes, and the joint feature representation involves various cues such as pose-specific features, garment-specific features, and cross-task features that encode correlations between human parts and garment attributes. Furthermore, we explore the "strong edge" evidence around the potential human parts so as to derive more powerful representations for oriented human parts. Such evidences can be seamlessly integrated into our structured learning model as a kind of energy function, and the learning process could be performed by standard structured Support Vector Machines (SVM) algorithm. However, the joint structure of the two problems is a cyclic graph, which hinders efficient inference. To resolve this issue, we compute instead approximate optima by using an iterative procedure, where in each iteration the variables of one problem are fixed. In this way, satisfactory solutions can be efficiently computed by dynamic programming. Experimental results on two benchmark datasets show the stateof-the-art performance of our approach.


 Fig. 1. Overview of the proposed approach. For a given image, first we detect candidates that have potentials to be valid human parts using the HPE algorithm proposed in  #b2 . This preprocessing step simplifies the representation of the image and converts the joint inference of HPE and GAC into a structured learning problem. Second, we design the joint feature representation for structured learning using various visual cues. Meanwhile, potential strong edges in the image are detected by utilizing a well-established algorithm presented in  #b4 . Finally, we use the prediction function learned from structured SVM to produce the joint labels of human parts and garment attributes.H UMAN-ORIENTED technologies play important roles in many computer vision and multimedia applications that require interactions between persons and electronic devices. The significance of human-oriented technologies naturally drives the research community to extensively investigate human-related topics, such as face recognition  #b0 , human tracking  #b1 , pose estimation  #b2 , clothing technology  #b3 , etc. In this work, we are interested in two of them: human pose estimation (HPE) and clothing technology (CT). Both problems have been studied extensively, and a review of previous work is presented in the following section.A. Previous Work 1) Human Pose Estimation: The literature about HPE can trace back to 40 years ago. Fischler and Elschlager  #b5  proposed to represent the articulated human pose as a collection of rigid body parts. This classical model, called pictorial structure (PS) in  #b6 , provides a straightforward representation for articulated objects and owns a tree structure that can facilitate efficient inference. Hence, PS is still adopted as a basic tool by recently established approaches, e.g.,  #b7 -  #b12 . These recent works mainly pursue better feature description, arXiv:1404.4923v3 [cs.CV]  $b22  Sep 2014 more efficient computation, more complex human structures and more effective contextual information.Feature description is one of the key elements for various vision tasks and so HPE  #b13 . In  #b11 , an iterative parsing paradigm was introduced to obtain an increasingly finer feature scheme for describing human parts. Other works, e.g.,  #b7 ,  #b9 , employed shape-based feature descriptors such as Shape Context  #b14  and Histogram of Oriented Gradients (HOG)  #b15 , which proved to be more effective than colorbased features. In  #b8 , Eichner and Ferrari considered the appearance consistence between connected/symmetric limbs and developed a better appearance model.The majority of early works on HPE emphasized on detection performance, i.e., a more effective inference schema, and the focus of later works was placed on the efficiency of inference. Typically, the search space of human pose is the main bottleneck for improving the efficiency, because one must search over the location and orientation space for each human part, as well as over the image pyramid. To reduce the search space of human poses, Ferrari et al.  #b10  utilized a generic upper-body detector and the GrabCut  #b16  algorithm, targeting at a reduced search space. In  #b7 ,  #b17 ,  #b18 , the tree structure is used to make the inference procedure more efficient, which can also well handle the spatial association between different human parts. One advantage of the tree structure is the ability to allow a fast computation via a dynamic programming  #b6 . Furthermore, deformable cost computation between connected human parts can be accelerated by performing a distance transform if the pair-wise cost suffices some specific conditions  #b6 .Although the tree-based PS models can draw a general representation for human body, it does not explore the implicit connections between rigid parts that do not share joints  #b19 . Therefore, graph-based structures are further proposed and explored by recent works. Such models are hard to be learnt exactly because of their high computational cost over the graph structure. Usually, Markov Random Filed (MRF) is used to achieve an approximate inference. By taking a branch and bound step in  #b19 , inference on a graph is nearly as efficient as the tree models. Recently, Yang and Ramanan  #b2  proposed a method that represents each human part by a mixture of templates. Unlike the previous limb models with articulated orientations, their templates are non-oriented and can well capture near-vertical and near-horizontal limbs. By tuning the part type and location, their model can handle the in-plane rotation and foreshortening.Another drawback of the original PS model is that the contextual information is not explicitly considered. In the work of Sapp et al.  #b18 , various visual cues (e.g., boundary and segmentation) were employed in their coarse-to-fine model. In  #b12 , Rothrock et al. incorporated background context into the PS model. Their model encourages a high contrast of a part region from its surroundings. Experimental results in their paper demonstrated the effectiveness of such contextual information.2) Clothing Technology: The clothing technologies, mainly including clothing segmentation  #b20 -  #b22 , clothing recommendation  #b23  and garment attribute classification  #b3 , play an important role in many multimedia systems such as clothing search engines  #b24 , online shopping  #b25  and human recognition  #b26 .The work of Chen et al.  #b27  is one of the representative works in the related literature, which introduced an And-Or graph to generate a large set of composite clothing components for further recognition. In  #b25 , Liu et al. proposed a crossscenario clothing retrieval system which can search similar garments from the online shop by using a person's daily life photo as input. In their work, a well trained human part detector was employed for part alignment and an offline transfer learning scheme was introduced to handle the discrepancy between images from daily life and online shops. Recently, they proposed a practical system called "magic closet" in  #b23  which can automatically recommend garments according to occasions. They utilized the latent SVM algorithm and modeled the clothing attributes as latent variables to provide mid-level features, rather than directly bridging the raw image features to the occasions.Recent progress in clothing techniques has witnessed the benefit of HPE due to the strong relations between human parts and garment attributes. In other words, it is a popular way to perform clothing study based on the results of human part detection  #b3 ,  #b25 . Moreover, Yamaguchi et al.  #b22  and Chen et al.  #b3  used well trained human part detectors to produce a large number of garment types, aiming to achieve precise clothing recognition. Bourdev et al.  #b28  trained an SVM classifier over each human part with the purpose of indicating whether or not a human part has specific garment attributes.3) HPE and CT: There are few works addressing the interrelations between HPE and CT. Yamaguchi et al.  #b22  tried to refine both HPE and clothing parsing by using a threestage scheme: first, they obtained some initial results of HPE; second, they used those initial HPE results as the basis to gain more reliable clothing segmentation; finally, the produced segmentation results were used to further refine HPE. However, since the quality of clothing segmentation largely depends on the success of HPE and vice versa, such a separate modeling approach may fail to capture the correlations between the two tasks and cannot achieve significant improvements over the competing baseline, as can be seen from the reports in  #b22 . In the recent work of Ladicky et al.  #b29 , they combined the partbased approach of pose estimation and pixel-based approach of image labeling into a principle way so as to inherit advantages of both. Inference for their model was performed with two steps: first, they iteratively added the next best pose candidate by computing an energy function of their model; second, they refined the final solution over the selected candidates of the first step.

B. Contributions of This Work
It is indeed natural to anticipate that HPE and CT are intertwined problems and can help each other. For example, in Figure 2, depending on the garment type, one can erase a large number of incorrect HPE candidates. However, existing approaches that individually use pose information to refine CT or use clothing knowledge to help HPE cannot fully Such prior knowledge about limb appearance was considered in  #b8 . However, it can't distinguish which human estimation is right from (b) and (c) in this way, where the lower arms' appearances differ slightly. Given that the garment attribute is known, e.g., sleeve type is sleeveless, one can easily exclude (b) because the lower arms in (b) have few skin colors.capture the advantages of modeling the correlations between the two tasks. In this paper, we therefore propose to integrate HPE and garment attribute classification (GAC) into a unified framework, with the purpose of making effective use of the possible correlations between human parts and garment attributes. Also, we aim to provide an effective way to jointly model multiple visual cues, including the features specific for human parts (pose-specific features) and for garment attributes (garment-specific features), as well as the cross-task features that encode the correlations between human parts and garment attributes. For a more informative description for oriented human part, we explore the "strong edge" evidence as an energy function so as to incorporate the contextual information around a human part. This motivation is based on the following observation: Since our representation for a human part is an oriented bounding box, it generally holds that there exist parallel edges sharing a similar orientation with an underlying correct part candidate, as illustrated in Figure 5.To this end, we use the HPE algorithm presented in  #b2  to obtain from each image a set of bounding boxes (called "candidates") that have potentials to be correct human parts, resulting in a basic representation for each image -one image is represented by one set of candidate ensembles. In this way, the joint inference for HPE and GAC is converted to a structured learning  #b30  problem, where the input is the image represented by a collection of candidate ensembles, the output is the joint labels of human parts and garment attributes, and the joint feature representation involves the aforementioned multiple visual cues.Given a set of annotated training images, the prediction function of structured learning is learnt by using the structured Support Vector Machines (SVM) algorithm. Inference for a new test samples can be performed efficiently by iteratively computing a local optimum on a tree with the dynamic programming algorithm. Experimental results on two benchmark datasets show the state-of-the-art performance of our approach.One may want to take HPE and GAC into the multitask learning (MTL) framework  #b31 . We remark here that our problem cannot be solved via existing MTL methods since models of MTL always assume that the underlying tasks share the same feature space. In our case, however, this assumption is not valid as we address two different tasks: human pose estimation (a detection task) and garment attribute classification (a recognition task). Each of the two tasks has its own feature space that can not be shared with the other, i.e. pose-specific features and garment specific features (see Section II-B). Also, methods from domain adaption (DA)  #b32  cannot be applied as DA algorithms deal with the variations in some combinations of factors, including scene, object location and pose, view angle, resolution etc. Obviously, our problem is not under the setting of DA algorithms.The rest of the paper is organized as follows. Section II elaborates on our approach for combined HPE and GAC, including feature design, parameter learning and inference algorithm. Section III presents the experiments and results. Section IV concludes this paper and discusses our future work.

II. JOINT INFERENCE OF HPE AND GAC
As Figure 1 shows, our approach contains three major procedures, including a preprocessing step that detects candidates from each image, an engineering step that forms a joint feature representation from various visual cues, and an inference step that uses structured SVM learned from a set of training images. In the following, we shall detail each step one by one.

A. Preprocessing and Problem Formulation
We do not build our approach by directly utilizing the images represented by raw pixels, and instead, we use the existing HPE method  #b2  to produce some initial results as input to our approach. More precisely, for each human part i (e.g., head), we perform a non-maximum suppression on the output of  #b2  and take the top K i (each K i = 40 in our work) candidates (denoted by b i ) from each image, where each candidate is a bounding box (x, y, θ, s), with (x, y), θ and s denoting the center coordinates, the angle and the size of the   bounding box, respectively 1 . This step allows us to obtain a manageably sized state space and simplifies the representation of a given instance. Suppose there are m human parts in total (m = 6 in this work), and then each image is represented by m candidate ensembles, each of which contains K i candidates respectively. Thus, the input space (i.e., sample space) X of our approach is defined asX = {x | x = (b 1 , b 2 , · · · , b m )},(1)where x refers to an image and b i denotes the candidate ensemble for the i-th human part (there are K i candidates in b i ). Furthermore, we introduce the following notation:P = {p | p = (p 1 , p 2 , · · · , p m ), ∀i, 1 ≤ p i ≤ K i },(2)where p i is a positive integer that indicates the index of the candidate for the i-th human part. In this way, the task of HPE is formulated as the problem of learning a prediction function from X to P. The goal of GAC in our work is to determine the garment attributes possessed by each image. We consider five types of attributes, including "Collar", "Color", "Neckline", "Pattern" and "Sleeve" that are most relevant with the upper body limbs.  $b1  The original output of  #b2  is a set of non-oriented bounding boxes. We transform them to the oriented ones using the online code that  #b2  provides.Each attribute has multiple styles, e.g., short sleeve and long sleeve for the "Sleeve" attribute. We use T k to denote the number of attribute values for the k-th attribute. The attribute values we consider in this paper are given in Figure 3. For the ease of presentation, we introduce the following notation:C = {c | c = (c 1 , c 2 , · · · , c n ), ∀k, 1 ≤ c k ≤ T k }, (3)where n is the number of garment attributes (n = 5 in this work), and c k is the label for the k-th attribute (e.g., c 5 = 1 means short sleeve, and c 5 = 2 means long sleeve). In this way, similar with HPE, the task of GAC can also be formulated as a problem of learning a prediction function from X to C.Hence, the task of performing combined HPE and GAC can be formulated as follows:f : X → Y,(4)where Y is the joint output space defined byY = {y | y = (p, c), p ∈ P, c ∈ C}.(5)Regarding the prediction function f , we first presume that there is a compatibility function S that measures the fitness between an input-output pair (x, y):S(x, y; w) = w · J(x, y) + αQ(x, p),(6)where w · J(x, y) denotes the inner product of two vectors, J(x, y) is the joint feature representation (which should be designed carefully), w is an unknown weight vector (which should be learned from training samples), Q(x, p) is the energy function indicating the response of a strong edge around the potential human parts and α is a parameter (which can be hand-tuned by cross-validation).In this way, the mapping function f in Eq. (4) can be written as:f (x; w) = arg max y∈Y S(x, y; w).B. Joint Feature RepresentationThe joint feature representation J(x, y) is an important component of the prediction function. In our approach, J(x, y) consists of three types of features, including the pose-specific features denoted by J p (x, p), the garment-specific features denoted by J c (c), and the cross-task features denoted by J pc (x, y); that is,w ·J(x, y) = w p ·J p (x, p)+w c ·J c (c)+w pc ·J pc (x, y). (8)In the following, we shall present our techniques used to design each type of feature.1) Pose-Specific Features: Given an image represented as m candidate ensembles (each candidate is a bounding box), we extract some features specifically useful for HPE, called pose-specific features, as follows:w p · J p (x, p) = m i=1 w u,i p · φ p (x, p i ) + (i,j)∈E d p w d,ij p · ψ d p (x, p i , p j ) + (i,j)∈E c p w c,ij p · ψ c p (x, p i , p j ),(9)Torso Head LL.arm RL.arm Fig. 4. Part-part relations. The solid lines are used to indicate spatially connected parts, e.g., torso and right upper arm (RU.arm). We mainly consider the geometry constraint for this case, e.g., relative position and relative rotation. The dashed lines mean the relations between symmetric parts, e.g., right upper arm (RU.arm) and left upper arm (LU.arm). For these parts, we model the appearance constraint, i.e., appearance similarity in color and texture descriptors.where φ p (x, p i ) denotes the unary feature for the i-th human part, ψ d p (x, p i , p j ) models the pairwise relations between spatially connected human parts, E d p is the collection of all pairs of connected parts, ψ c p (x, p i , p j ) contains the appearance consistency message between symmetric parts and E c p is the set of all symmetric parts (see Figure 4 for details). The three terms are called unary score, deformation score and consistency score respectively.In our approach, the unary feature φ p (x, p i ) is chosen as the HOG descriptor  #b15  which has been proved quite effective for object detection  #b2 ,  #b9 .The design of ψ d p (x, p i , p j ) concerns some basic geometry constraints between connected parts, including relative position, rotation and distance of part candidate p i with respect to p j . More concretely, we divide the image space into 3 by 3 regions, with p j at the central region. Then we use a 9 dimensional one-zero vector as the relative position feature, where there is only one element with value "1" that indicates the region where p i is located. To describe the relative rotation, we divide the range of angles [0, 360] into 20 bins and use a 20 dimensional one-zero vector as the feature. Our relative distance feature is the euclidean distance between the center of p i and p j .For the symmetric parts, we assume some consistency constraints between them which hold for most cases; that is, they should share similar appearance. In this work, we compute the divergence of the color histogram in RGB and LAB space and take it as the feature descriptors.In Figure 4, we mark all the parts that are related to each other.2) Garment-Specific Features: There are some features only specific for garment, i.e., garment-specific features. In this work, we consider the co-occurrences between different garment attribute values:w c · J c (c) = k,l w kl c · ψ c (c k , c l ),(10)where ψ c (c k , c l ) is a binary vector that indicates whether or not c k and c l co-occur in an image. For example, the texture type "drawing" (usually belongs to T-shirt style) often cooccurs with the collar type "round".3) Cross-Task Features: The cross-task features encode the correlations between human parts and garment attributes. In our approach, we model the part-garment relations manually specified as in Table I. For a given attribute k, we denote the human part(s) associated with it asp(k) and the corresponding configuration(s) asp k . Then the cross-task features are formulated as:w pc · J pc (x, y) = n k=1 w k pc · Ψ k pc (x,p k , c k ),(11)where Ψ k pc (x,p k , c k ) denotes the features extracted from x under the constraints of part configurationp k and attribute label c k . Note that here we write the cross-task score as a summary by the attribute order. Since the dependency between part and attribute is cyclic, one can also write it by the human part order.To describe the design of Ψ k pc (x,p k , c k ), we first convert the garment attribute label c k to a T k dimension vector I(c k ), with only one dimension assigned with value one and others with zeros. From Table I, the low-level feature descriptors of the k-th garment attribute depend on two aspects: the corresponding human part(s) and the feature type (denoted by F k and specified in Table I). We use F k (p k ) to denote features of the k-th garment attribute under the part candidate(s)p k . Then our cross-task feature Ψ k pc (x,p k , c k ) is represented as follows:Ψ k pc (x,p k , c k ) = F k (p k ) ⊗ I(c k ),(12)where the "⊗" operator is the outer product of two vectors. In fact, we map the resulting matrix to a vector by the row order. Note that in Table I, a garment attribute depends exclusively on the some of the limbs, not all ones. This technique that feature descriptors draw from both the labels of human parts and garment attributes, provides us a simple way to capture the correlations between HPE and GAC and makes it a unified approach towards the two intertwined problems.

C. Learning with Structured SVM
We perform our joint estimation for HPE and GAC using the prediction function f in Eq.  #b6 . The weight vector w is a critical component of the prediction function. Given N training samples {(x r , y r )} N r=1 , we compute w by solving the following structured SVM problem:minimize w,ξ 1 2 w 2 + C N r=1 ξ r ,subject to ∀r ∈ pos, w · J(x r , y r ) ≥ 1 − ξ r , ∀r ∈ neg, w · J(x r , y r ) ≤ −1 + ξ r .where C is the parameter that controls the trade-off between margin and accuracy, and ξ r ≥ 0 is a slack variable. In our experiments, we set C with a fixed value 0.01 that allows a soft margin. (c) a correct candidate for the right-lower arm (green rectangle); (d) strong edge evidence for the correct candidate; (e) an incorrect prediction for the same arm (red rectangle); (f) strong edge evidence for the incorrect candidate. Given a part candidate (denoted by an oriented rectangle), we extract the strong edges that connect the regions of the joints of the part (denoted by two yellow rounds).(a) (b) (c) (d) (e) (f)

D. Strong Edge Evidence
Now we give details on the design of our energy function Q(x, p) in Eq.  #b5 . First, we utilize a boundary detector  #b4  to detect all potential edges in an image. Then for a candidate human part, we try to find the long edges that connect the two joint regions as the strong edge evidence (see Figure 5). Our energy function mainly considers three factors: the consistency of orientation between the part p i and the strong edges se i (denoted by Q o (p i , se i )), the distance of the part away from the strong edges (denoted by Q d (p i , se i )) and the strength of the strong edges themselves. That is,Q(x, p) = m i=1 Q o (p i , se i ) + β m i=1 Q d (p i , se i ),(14)where β is a parameter which can be tuned by cross validation. Given a part candidate p i = (x, y, θ, s), the first term in Eq. (14) is computed as:Q o (p i , se i ) = 1 Z e∈sei cos(θ − θ e ) · strg e ,(15)where e is an image pixel on the strong edges se i , Z is the number of pixels on se i , θ e is the orientation of the strong edge at pixel e and strg e is the edge strength (which is produced by the algorithm  #b4 ). The measurement of the distance from the given part to the strong edges is represented as follows:Q d (p i , se i ) = 1 Z e∈sei min{dt(e, l), dt(e, r)} · strg e ,(16)where l and r are the two parallel edges of the part bounding box whose angles are θ, and dt(e, l) is the closest distance of pixel e from edge l which can be efficiently computed by a distance transform algorithm  #b34 . Note that if there is no strong edge associated with the underlying part, we force the energy to be zero.

E. Inference
Now that we have clarified how to design the joint feature, the strong edge energy function, as well as the learning algorithm for weight vector w in Eq. (6), we propose our inference algorithm which is quite efficient (for each input sample, our algorithm only needs 2 seconds for the joint estimation) and effective.

Algorithm 1 Approximate Inference for Joint Estimation
Input: An input sample x, the weight vector w, parameter α and β. Output: Optimal joint estimation y * .1: Set the optimal joint estimation y * = ∅. 2: Set the optimal score S * = −∞. 3: Initialize the parts estimation: p 0 = arg max p∈P w p · J p (x, p) + αQ(x, p). 4: repeat 5: Compute the garment attributes: c t = arg max c∈C w c · J c (c) + w pc · J pc (x, p t−1 , c).

6:
if i ∈ E c p then 7:  m 3 = w s,i p · ψ s p (x, p i ),m 4 = k,p(k)=i w k pc · Ψ k pc (x,p k , c k ).l 1 = i∈i,j∈j,(i,j)∈E d p w d,ij p · ψ d p (x, p i , p j ).16:l 2 = k,p(k)=i∪j w k pc · Ψ k pc (x,p k , c k ).17:set l(p i , p j ) = l 1 + l 2 .

7:
Compute the local score: S = S(x, y t ; w). In Figure 6, we represent our problem as a factor graph G, where the black-rectangle node denotes a human part, the black-circle node denotes a garment attribute and the colored node denotes a potential. As our original problem is a cyclic graph, it cannot be optimized exactly and efficiently. Therefore, in Algorithm 1, we propose an iterative algorithm to search for an approximate solution. Our algorithm receives a sample x (defined in Eq. (1)), the SVM weight w, parameter α and β as inputs and outputs the optima for the joint problem. In each iteration, by fixing one type of the variable (either human part or garment attribute, see step 5 and step 6), our inference procedure can be performed on a tree structure which yields an efficient computation by dynamic programming  #b6 . This procedure is also illustrated in Figure 7 and Figure 8.1) Inference for Pose: In the work of  #b6 , the PS model is restricted as a tree: each node has a unary term that describes how suitable a configuration is assigned to this part, and each edge encodes the deformation cost for a pair of connected parts. In Figure 6, we demonstrate our extension for the traditional PS model:• appearance consistency between symmetric parts (green nodes)  6. The factor graph representation for our problem. We denote our variables with black nodes, those of which with number 0-5 represent the human parts: torso, RU/LU/RL/LL arm and head, while those with number 6-10 denote the garment attributes: collar, color, neckline, pattern and sleeve. We denote our potentials with colored nodes, with purple ones denoting the unary potential, red denoting the deformation potential, green denoting the consistency potential, orange denoting the attribute co-occurrence potential and cyan denoting the cross potential.• joint compatibility across the human part(s) and the garment attribute(s) (blue nodes) Adding the edges connecting symmetric parts will destroy the tree structure. In Figure 7, however, we propose a trick to group the symmetric parts as a super-node so that the global structure remains to be a "tree". On the other hand, an edge across human part and garment attribute is used to measure how compatible a human part configuration is with a given attribute. We call this kind of cost as cross score. When the attribute variables are fixed, we can remove the garmentspecific potentials as they do not contribute to searching for the best pose. In addition, we can group some deformation and cross potentials for a more concise representation, e.g. as what we do for node 1 and 2 in Figure 7.In Algorithm 2, we describe our computation procedure. For a super-node i 2 , we denote its children nodes as C i . In the line 3-13, we first compute the scores with respect to a single node i. This step involves calculation of unary score, strong edge score, consistency score and cross score. Note that we have grouped the symmetric parts as one node. In this way, the consistency score is a self description towards the node i. In line 15, we compute the deformation score of node i and j. For example, if the super-node i = {1, 2} whereas the super-node j = {0}, the deformation score between i and j is the sum of deformation score of (1, 0) and (2, 0). In line 16, we compute the cross score for all attributes whose associated human parts are exactly i∪j. For example, the attribute node 6 is associated with part node 5 and 0, so we will compute the cross score with respect to nodes {5, 0, 6}. Line 18-27 is a conventional message passing procedure that can be computed efficiently by dynamic programming  #b6 .2) Inference for Attributes: Referring to Figure 8, we stretch the part variables and remove redundant edges associated with the stretched variables from the original graph as  $b2  For simplicity, here we call all variable nodes as super-nodes.

Algorithm 2 Exact Inference for Human Pose Estimation (Extended Pictorial Structure Inference)
Input: An input sample x, the weight vector w, parameter α, β and garment attributes c. Output: Optimal pose estimation p * .1: Set the optimal joint estimation p * = ∅. 2: Set the node 0 as the root node. 3: for each configuration p i of super-node i do 4:m 1 = i∈i w u,i p · φ p (x, p i ).

5:
m 2 = α i∈i Q(x, p i ).

18:
if i is a leaf node then 19:B i (p j ) = max p i (m(p i ) + l(p i , p j )), 20: else 21: B i (p j ) = max p i m(p i ) + l(p i , p j ) + v∈C i B v (p i ) .

22:
end if 23: end for 24: Compute the best configuration for the root node: p * 0 = arg max p0 m(p 0 ) + v∈C0 B v (p 0 ) . 25: for each parent-child pair (p * j , p i ) do 26: p * i = arg max p i B i (p * j ). 27: end for  Fig. 7. The factor graph representation for inferring human pose. Circle nodes with double boundaries are assigned with fixed values. Symmetric parts are grouped into a super-node, denoted by a dashed oval. For some part nodes, their deformation and cross potential can also be grouped as the associated attribute nodes are now fixed. Note that we don't draw the garment-specific potentials as they don't contribute for searching a best pose. they contribute nothing to this inference step. Note that for the attribute-attribute pairs (i.e. the garment-specific feature), we manually model them as a tree structure. In this way, we can still perform an efficient computation like Algorithm 2.3) Implementation and Computation Complexity: We write K = max 1≤i≤m K i and T = max 1≤k≤n T k . Now we propose a computation analysis for our Algorithm 2 and give some optimization tricks. In line 3-13, one has to loop over all possible configuration p i for the super-node i, and there are at most 2 nodes in a super-node (see Figure 7); this makes the computation O(K 2 ). In fact, note that the computation of unary and strong edge score can be decomposed into a summation of each node (line 4 and 5), which indicates that we can separately compute these scores for each node configuration, yielding a computation O(K). Also note that actually we only compute cross score for node 0 in line 11 (see Figure 6). Based on this observation, computation onm(p i ) is reduced from O(K 2 ) + O(K 2 ) + O(K 2 ) + O(K 2 ) to O(K) + O(K) + O(K 2 ) + O(K).In line 14-16, we compute the deformation and cross score for each pair (p i , p j ), which yields a computation complexity O(K 4 ) if without any optimization. For the deformation score, as the decomposition property still holds, the computation is O(K 2 ). For the cross score in line 16, when k ∈ {6, 8}, the computation is O(K 2 ) since we have to loop over all the configurations for nodes 0 and 5. When k = 10, however, it is not necessary to enumerate the O(K 4 ) combinations of node 1, 2, 3 and 4 if we design a suitable cross-task feature. In our case, F 10 (p 10 ) is the concatenation of the color histogram of each p i ∈p 10 , which implicitly owns the decomposition property. Thus, the computation can be reduced to O(K), if we omit the summation operation of these separate scores.

F. Parameter Sharing
Our work is distinct from other works which address pose estimation and garment attribute in two aspects. First, our carefully designed structured learning model integrates the pose feature and garment attributes into a principle fashion, facilitating a global optimal model. Second, although we derive an iterative inference algorithm to approximate the optima, we allow the parameter sharing between the two steps, i.e. the cross-task features are shared and contribute to Fig. 9. Our joint approach v.s. YDR  #b2 . Left column: results from  #b2 . Right column: results from our joint approach.  #b2  estimates incorrect lower arm(s), which subsequently results in an incorrect prediction for the sleeve attribute. Our joint approach captures the co-relations between the arms and the sleeve attribute and makes a correct estimation for both.both pose estimation and attribute classification (line 5 and 6 in Algorithm 1). Therefore, our approach is a paradigm of learning globally and inferring locally, achieving both effectiveness and efficiency (see Section III for experimental justification).

III. EXPERIMENTS


A. Experimental Settings
In this section, we introduce our experimental settings, including the used datasets, the baselines, the evaluation metrics and the scheme for training structured SVM and inferring for a testing image.

1) Datasets:
We conduct experiments on two datasets. The first one is the widely used Buffy dataset  #b10  consisting of 748 annotated video frames from Buffy TV show. This dataset is proposed as a standard one for HPE task but not originally for GAC task. We manually annotate the garment attributes for the Buffy dataset. The second dataset, called "DL", contains 1000 daily life photos we collect from websites. Compared with Buffy, the DL dataset possesses more various garment attribute values. In order to obtain quantitative evaluation results, we also manually annotate the human parts and garment attributes for the images in the DL dataset.Some garment attributes cannot be labeled for the images in which the person does not wear any garment or some attributes' visual cues cannot be described. In Figure 11, we illustrate some of such samples and list the statistical information for the attributes we annotate in Figure 10.   #b18 , Yang and Ramanan (YDR)  #b2  and Ladicky tl al. (LTZ)  #b29 . As the code of LTZ is not publicly available, we only evaluate our DL dataset by the first three methods.  . Examples lacking some garment attributes. Characters in these images all lack visual cues for specific garment attributes. Persons of the first row wear no garment, thus cannot be labeled with any garment attribute while those of the second row cannot be labeled with part of attributes, e.g., neckline types.Although all these algorithms are primarily designed for HPE, they can actually produce results for GAC: as discussed in  #b3 ,  #b25 , the results of HPE can be used for part alignment, which enables the extraction of attribute features. Then for each attribute, we individually train an SVM multi-class model  #b35 . We use the features described in Table I to train each SVM model. We also compare our GAC results with CGG [4] 3 , which designed a specific pipeline to recognize semantic clothing attributes.3) Evaluation Metrics: We evaluate the HPE results with the standard metric of Probability of Correct Part (PCP)  #b8 . The GAC results are evaluated by the Garment Attribute Precision (GAP) criterion, i.e., the classification accuracy for each garment attribute (there are 5 attributes in this work). 4) Training/Testing: For the Buffy dataset, like  #b2 ,  #b10 ,  #b36 , we select the images from Episode 3, 4 for training, and Episode 2, 5 and 6 for testing. For our DL dataset, we select randomly 300 images for training and use the remaining 700 images for testing.As we have discussed in Section III-A1, some images cannot be annotated with some garment attributes. For an image without the garment attribute c j , we set all the features related to c j to a zero vector in Eq. (8) when training our structured SVM model and skip evaluation on such attributes. For a person wearing two (or more) garments, we label each garment's attribute values. Thus the image has several groups of labels in terms of these garments. When training the model, all the groups of labels are used to construct the constraints. When testing a new instance, any attribute value which the algorithm produces is acceptable if the value belongs to any of the groups. Figure 12 shows some exemplar results produced by our approach. In the following, we shall analyze our approach and compare it with the baselines.

B. Results
1) Examining the Advantages of Joint Learning: To show the advantages of combining HPE and GAC together, we compare our joint learning approach with its two separated versions: one is an HPE algorithm created by removing the garment-related features in Eq.  #b7 ; the other is a GAC algorithm created by ignoring all part-related features. Figure 13 shows the comparison results, which demonstrate the significant advantages of the joint learning over the separated schemes.Note that the basic appearance constraints of human parts (as  #b8  considered) have been modeled in the pose-specific features (see Section II-B). Thus, the improvement on HPE is attributed to the integration of garment attribute evidence modeled in Eq. (11), i.e., joint inference. The improvement on GAC can be examined in the same way.2) Examining the Effectiveness of Strong Edge: We demonstrate the effectiveness of the strong edge evidence in this section. By setting the α in Eq. (6) with value zero, our inference algorithm 2 produces the results without strong edge

Head Torso
Upper Arm Lower Arm Fig. 12. Examples of our results obtained on the Buffy and DL datasets. We demonstrate some good results from Buffy and DL in the first and second panels respectively. Some failure cases are showed in the bottom panel. We use the oriented line to denote the pose estimation. If an HPE result is incorrect, the line is red. We visualize three attributes (neckline, pattern and sleeve) of our GAC results by some icons (see Figure 3 for the icon definition). If a GAC result is incorrect, we use a dashed red rectangle to mark it. Examining the failure cases, we find our algorithm is confused when some human parts are occluded or the human pose is largely variational. Attributes are misclassified when the corresponding parts are mis-detected, or occluded by some objects.  Fig. 13. Demonstrating the advantages of combining HPE and GAC together. X-axis: human part and garment attribute. Y-axis: error reduction rate. We compare our joint learning approach with its separated versions on the Buffy and DL dataset, which deal with HPE and GAC individually. The joint approach improves all of the human parts and a majority of garment attributes. This is because there exists a strong correlation between human parts and garment attributes, and our algorithm captures their inter-dependency that improves both simultaneously.evidence. Then we use 3-fold cross validation to tune the parameters α and β in Eq.  #b13 . The error reduction rate for employing the strong edge evidence on two datasets is demonstrated in Figure 14. For the Buffy dataset, by using the strong edge evidence, the lower arm accuracy is refined and for the DL dataset, all of the human parts are predicted more precisely.3) Comparisons with State-of-the-art Algorithms: In this section, we compare our joint approach (with strong edge evidence) with the state-of-the-art algorithm. Note that the results of GAC produced by HPE algorithms have been explained in Section III-A2. Figure 9 gives the exemplar comparison of HPE and GAC results from YDR  #b2  and our joint approach. On the Buffy dataset, Table II shows that our approach consistently outperforms YDR  #b2  which is a recently established algorithm and provides the candidates for our approach. We also compare our approach with LTZ  #b29  which combines pose estimation and segmentation for computation. We improve the lower arms performance and achieve the highest overall accuracy. Table III shows the comparison results on the DL dataset. It can be seen that our approach outperforms all the competing baselines on the task of HPE.To examine the effectiveness of our approach on GAC, we also compare it with CGG  #b3 , which is a real GAC method. Demonstrating the effectiveness of strong edge evidence. X-axis: human part. Y-axis: error reduction rate. We demonstrate the error reduction rate for employing the strong edge evidence on both Buffy and DL dataset. The strong edge evidence improves the detection rate because it captures the contextual information for a human part, which is complementary to other features.On the Buffy dataset, there is a significant improvement of our approach on the attributes of "color" and "sleeve", and on the DL dataset, we also reach a competitive performance. The reason of the surprising gain on the Buffy dataset is that human pose of Buffy is more variational than DL. And our model can capture the inter-dependency between human part(s) and garment attributes. However, the pipeline of CGG is step by step, like the work of  #b22 . Thus it is depressed if the human pose is largely unconstrained. One may notice that in Table II and Table III, compared with the baselines except CGG  #b3 , the GAC results of our approach are significantly improved when we gain less improvement for HPE on average (see YDR  #b2  for example). The reasons are twofold. First, the training procedure of our approach is different from that of theirs. Our model is trained in a unified manner, which allows us to integrate more useful features. That is, during the training procedure, our model captures the dependency between human parts and garment attributes (i.e. cross-task features), as well as that between different garment attributes (i.e. garment-specific features). As a result, we finally have a global optimal model for HPE and GAC. However, the competing baselines are trained in a separate manner. That is, given the HPE, each garment attribute is trained individually. Therefore, neither the inter-dependency between human parts and garment attributes, nor that between different garment attributes can be utilized, resulting in a marginal model for multiclass SVM. Second, we search for the optimal prediction for GAC by iteratively updating HPE and GAC, reaching a (local) optimal state of HPE and GAC 4 . However, the baselines can only make a prediction for GAC by the given HPE result.

IV. CONCLUSIONS AND FUTURE WORK
Based on the observation that there exist correlations between human parts and garment attributes, we propose to integrate HPE and GAC into a unified procedure and handle both tasks simultaneously. We show that such integration can be seamlessly achieved by using the framework of structured SVM. First, due to the joint feature representation, it is convenient to involve various visual cues such as posespecific features, garment-specific features and cross-task features. Second, the structured nature of the output space of structured SVM provides us a straightforward way to jointly infer the solutions for several problems (e.g., HPE and GAC). Benefiting from these superiorities, our approach achieves state-of-the-art performance in both HPE and GAC problems, as demonstrated in the experiments. Obviously, the boosted performance can benefit quite many multimedia applications, e.g., online clothing retrieval, clothing recommendation, and we are planning to extend our proposed approach for these applications in our future work.

Footnote
3 : This code is not publicly available. We thank the authors Chen et al.[4] for providing us the source code for performance comparison.
4 : Empirically, the local optima are good enough as we have demostrated in our experiments.