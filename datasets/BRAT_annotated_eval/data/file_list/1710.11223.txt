Fast and Scalable Learning of Sparse Changes in High-Dimensional Gaussian Graphical Model Structure

Abstract
We focus on the problem of estimating the change in the dependency structures of two p-dimensional Gaussian Graphical models (GGMs). Previous studies for sparse change estimation in GGMs involve expensive and difficult non-smooth optimization. We propose a novel method, DIFFEE for estimating DIFFerential networks via an Elementary Estimator under a high-dimensional situation. DIFFEE is solved through a faster and closed form solution that enables it to work in large-scale settings. We conduct a rigorous statistical analysis showing that surprisingly DIFFEE achieves the same asymptotic convergence rates as the state-of-the-art estimators that are much more difficult to compute. Our experimental results on multiple synthetic datasets and one real-world data about brain connectivity show strong performance improvements over baselines, as well as significant computational benefits.

Introduction
Learning the change of interactions between random variables is an essential task in many real-world applications. For instance, identifying the difference in brain connectivity networks of subjects from different groups can shed light on understanding psychiatric diseases  #b1 . As another example in gene expression analysis, interests may not center on a particular graph representing interactions among genes, but instead on how gene interactions change when external stimuli change  #b5 . Such change detection can significantly simplify network-driven studies about diseases, drugs or system understanding. In this paper we consider Gaussian graphical models (GGMs) and focus on estimating changes in the dependency structure of two p-dimensional GGMs, based on n c and n d samples drawn from the models, respectively. Recent literature has made significant advances on estimating the statistical dependency structure of GGMs based on samples drawn from the model [1]  #b4  (reviewed in 2.1). Detecting structural changes naturally involves two sets of data samples. Given two sets of data (in the form of two matrices) X c ∈ R nc×p and X d ∈ R n d ×p identically and independently drawn from normal distributions N p (µ c , Σ c ) and N p (µ d , Σ d ) respectively, our goal is to estimate the structural change ∆ (defined by [35]) 1 : ∆ = Ω d − Ω c (1.1) Here µ c , µ d ∈ R p describes the mean and Σ c , Σ d ∈ R p×p represents covariance matrices. In Eq. (1.1), the precision matrix Ω c := (Σ c ) −1 and Ω d := (Σ d ) −1 . The conditional dependency graph structure of a GGM is encoded by the sparsity pattern of its precision matrix. The entries of ∆ describe if the magnitude of conditional dependency of a pair of random variables changes between two conditions. They can also be interpreted as the differences in the partial covariance of each pair of random variables between the two conditions.In particular, we focus on estimating the change ∆ under a high-dimensional situation, where the number of variables p may exceed the number of observations:p > max(n c , n d ). In such high-dimensional settings, it is still possible to conduct consistent estimation by leveraging low-dimensional structure such as sparsity constraints. A sparse ∆ indicates few of its entries are non-zero. In the context of estimating structural changes of two GGMs, this translates into a differential network with few edges. However, we do not assume the individual structures Ω c and Ω d to be sparse, and they may both correspond to dense matrices. Our main objective is to get an estimated ∆ of the true change ∆ * such that the estimation error ( ∆ − ∆ * ) is bounded.A naive approach to detecting structural changes in GGMs is a two-step procedure in which we estimate Ω d and Ω c from two sets of samples separately and obtain ∆ = Ω d − Ω c . However, in a high-dimensional setting, this strategy needs to assume that both Ω d and Ω c are sparse (in order to achieve consistent estimation). This is not necessarily true even if the change ∆ is sparse. A motivating example from identifying the difference in connectivity networks among brain regions (functional networks) of subjects from different groups. Recent literature in neuroscience has suggested functional networks are not sparse. On the other hand, differences in functional connections across subjects should be sparse [2]. In the application of estimating genetic networks of two conditions, each individual network might contain hub nodes and therefore not entirely sparse.This has motivated a few recent studies to directly estimate the changes of structures from two sets of samples. Zhang et al. used the fused norm for regularizing maximum likelihood estimation (MLE) to simultaneously learn two GGMs with a sparsity-inducing penalty on the difference [33]. The resulting penalized MLE framework is a log-determinant program, which can be solved by block coordinate descent algorithms [33] or the alternating direction method of multipliers (ADMM) by the JGL-fused package  #b0 . Later Liu et al. proposed to use density ratio estimation (DRE) to directly learn structural changes without having to identify the structures of each individual graphical model. The authors focused on exponential family-based pairwise Markov networks  #b8  and solve the resulting optimization using proximal gradient descent  #b7 . A follow-up study showed that under certain conditions the DRE method recovers the correct parameter sparsity with high probability  #b7 . More recently, Fazayeli et al. introduced a regularized density ratio estimator for direct structured change estimation in Ising model structure. Theoretically, the authors showed that the estimation error converges to zero under milder conditions than DRE  #b3 . Another related regularized convex program to directly learn structural changes without going through the learning of the individual GGMs is the Diff-CLIME method [34]. Diff-CLIME uses an 1 minimization formulation constrained by the covariance-precision matching. Diff-CLIME reduces the estimation problem to solving linear programs (LP) and can be solved by using any standard LP solvers. Another recent work relaxes the Gaussian assumption in Diff-CLIME model to a semiparametric distribution [30]. All previous studies have used 1 regularized convex formulation for estimating structural changes. While state-of-the-art optimization methods have been developed to solve the resulting non-smooth programs, their iterative algorithms are very expensive for large-scale problems.In this paper, we propose a simple estimator, namely DIFFerential networks via an Elementary Estimator (DIFFEE) for fast and scalable learning of sparse structural change in high-dimensional GGMs. Briefly speaking, DIFFEE provides the following benefits: Notations: Given a p-dimensional vector x = (x 1 , x 2 , . . . ,•x p ) T ∈ R p , x 1 = i |x i | represents the 1 -norm of x. x ∞ = max i |x i | is the ∞ -norm of x. x 2 = i x 2 i describes the 2 -norm of x.

Background: Elementary Estimator for Estimating Sparse GGM in Closed Form
Sparse Gaussian Graphical Model(sGGM)  #b6  #b9 32] assumes data samples are independently and identically drawn from N p (µ, Σ), a multivariate normal distribution with mean µ and covariance matrix Σ. The conditional dependency graph structure among its p random variables is encoded by the sparsity pattern of the inverse covariance matrix (precision matrix) Ω. Ω := (Σ) −1 . An edge does not connect j-th node (variable) and k-th node (variable) if and only if Ω jk = 0 (i.e., conditionally independent). sGGM imposes an 1 penalty on the parameter Ω. Here B * ( φ) is the so-called proxy of backward mapping for the target GM (more details in Section A.1). λ n is a regularization parameter. φ is the empirical mean of the sufficient statistics. For example, in the case of Gaussian GM, φ is the sample covariance matrix.The key idea in Eq. (2.4) (summarized in Figure 1) θ = S λn (B * ( φ))where the function S(·) is an element-wise softthresholding with parameter λ:[S λ (A)] ij = sign(A ij ) max(|A ij | − λ, 0) (2.5) The optimization in Eq. (2.4) is decomposable into independent element-wise subproblems. Each subproblem corresponds to soft-thresholding. Essentially the final estimators are obtained by performing simple thresholding operations on the proxy backward maps. This class of estimators is thus both computationally practical and highly scalable. Using the theoretical framework proposed by  #b11  for regularized M-estimators, Yang et al. further proved that the resulting algorithms achieve strong statistical guarantees with sharp convergence rates.

Previous Estimators for Change Estimation in GGM Structure
Multiple estimators have been proposed to estimate sparse differential network from two sets of samples.

FusedGLasso (Regularized MLE):
The most straightforward estimator for differential network was to extend the classic Graphical lasso estimator [32] for sparse GGM with an added sparsity penalty on the differential network (i.e., fused norm). argminΩc,Ω d 0,∆ n c (− log det(Ω c )+ < Ω c , Σ c >) +n d (− log det(Ω d )+ < Ω d , Σ d >) +λ 2 (||Ω c || 1 + ||Ω d || 1 ) + λ n ||∆|| 1 (2.6)This was solved by block coordinate descent algorithms in [33]. Later the alternating direction method of multipliers (ADMM) was used to solve Eq. (2.6) that needs to run SVD in one sub-procedure  #b0 .Diff-CLIME: Another recent study [35] extended the CLIME estimator to directly learn the ∆ through a constrained optimization formulation. argmin∆ ||∆|| 1 Subject to: || Σ c ∆ Σ d − ( Σ c − Σ d )|| ∞ ≤ λ n (2.7)This reduces the estimation to solving multiple linear programming problems.

DensityRatio:
The third category of estimators optimizes the following loss: argmax ∆ L KLIEP (∆) − λ n ∆ 1 −λ 2 ∆ 2 (2.8) Here KLIEP minimizes the KL divergence between the true probability density p d (x) and the estimated p d (x) = r(x; ∆)p c (x) without explicitly modeling the true p c (x) and p d (x). Its key idea is the formulation of density ratio term r(x; ∆) for directly estimating sparse differential network of graphical models in exponential families. This DensityRatio estimator uses the elastic-net penalty for enforcing ∆ to be sparse. The resulting optimization was solved using proximal gradient descent methods in  #b7 .

Proposed Method: DIFFEE
The aforementioned studies cannot avoid certain steps involving expensive computation in their iterative optimization, such as SVD operations in the FusedGLasso, linear programming in the Diff-CLIME, and calculating the normalization term in the Density-Ratio estimator. We aim to propose a scalable and theoreticallyguaranteed estimator for estimating sparse differential network under large-scale settings.

Differential Network by Elementary Estimators (DIFFEE):
Computationally elementary estimators are much faster than their regularized convex program peers for graphical model estimation. Therefore we extend it to the following general estimator for estimating sparse change in GGM structure: argmin∆ ||∆|| 1 Subject to: ||∆ − B * ( Σ d , Σ c )|| ∞ ≤ λ n (2.9)The basic idea in Eq. (2.9) is to use a well-defined proxy function B * ( Σ d , Σ c ) to approximate the backward mapping (the vanilla graphical model MLE solution), so that B * ( Σ d , Σ c ) is both well-defined under high-dimensional situations and also has a simple closed-form.As shown by Figure 1, there are three components in the estimation pipeline of elementary estimator for GM: (1) Backward mapping that is the vanilla MLE solution for estimating an exponential graphical model;(2) Proxy backward mapping B * ( Σ d , Σ c for dimensional settings; and (3) The closed-form solution of Eq. (2.9) as the final estimator.(1) Backward Mapping: The density ratio of two Gaussian distributions is naturally an exponentialfamily distribution (see Section A.1.1). Based on [29], learning an exponential family distribution from data means to estimate its canonical parameter. For an exponential family distribution, computing the canonical parameter through vanilla graphical model MLE can be expressed as a backward mapping (the first step in Figure 1). Through simple derivations in Eq. (A.8), we can easily conclude that the differential network ∆ is one entry of the canonical parameter for this distribution. When using vanilla MLE to learn this exponential distribution (i.e., estimating canonical parameter), the backward mapping of ∆ can be easily inferred from the two sample covariance matrices usingΣ −1 d − Σ −1 c )(Section A.1).(2) Proxy Backward Mapping: Now the key is to find a closed-form and statistical guaranteed estimator as proxy backward mapping of ∆ under highdimensional cases. Inspired by the elementary estimator for sGGM, wechoose [T v ( Σ d )] −1 − [T v ( Σ c )] −1 ) as the proxy backward mapping for ∆. Here [T v (A)] ij := ρ v (A ij ) (2.10) where ρ v (·)is chosen to be a soft-thresholding function. We therefore obtain the following DIFFEE objective function for estimating sparse changes in GGM structure: argmin∆ ||∆|| 1 Subject to: ||∆ − [T v ( Σ d )] −1 − [T v ( Σ c )] −1 || ∞ ≤ λ n(2.11) Here λ n > 0 is the tuning parameter.The optimization in Eq. (2.11) seeks an estimator with minimum complexity with regard to the 1 regularization, at the same time being close enough to the 'initial estimator'[T v ( Σ d )] −1 − [T v ( Σ c )] −1 according to the element-wise ∞ norm.This formulation ensures that the final estimator (solution of Eq. (2.11)) has the desired sparse structure.Theoretically, the choice of 1 and ∞ in Eq. (2.9) connects to the asymptotic error bounds of the final estimators. In Section 2.5, we theoretically prove that the statistical convergence rate of DIFFEE achieves the same sharp convergence rate as the state-of-the-art estimators for differential network. Our proofs are inspired by the unified framework of the high-dimensional statistics  #b11  and EE for sGGM [31].[31] proved that when (p>n), the proxy backward mapping [T v ( Σ)] −1 in their EE-sGGM achieves the sharp convergence rate to its truth (i.e., by proving (3) Closed Form Solution: To solve Eq. (2.11), we get the following closed form solution:||T v ( Σ)) −1 − Σ * −1 || ∞ = O( log p n )).∆ = S λn ([T v ( Σ d )] −1 − [T v ( Σ c )] −1 ) (2.12) Where [T v ( Σ d )] −1 − [T v ( Σ c )] −1 is the pre-computed proxy backward mapping. Here [S λ (A)] ij = sign(A ij ) max(|A ij |−λ, 0)is the same soft-thresholding function in Eq. (2.5). Algorithm 1 shows the detailed steps of the DIFFEE estimator. Being non-iterative, the closed form solution helps DIFFEE achieve significant computational advantages over other estimators.

Algorithm 1 DIFFEE
input Two data matrices Xc and X d . input Hyper-parameter: λn and v output ∆ 1: Compute [Tv( Σc)] −1 and [Tv( Σ d )] −1 from Σc and Σ d .2: Compute ∆ = S λn ([Tv( Σ d )] −1 − [Tv( Σc)] −1 ) output ∆

Analysis of Computational Complexity
The closed form solution (Eq. (2.12)) brings significant advantages in hyper-parameter tuning. This is because we only need to compute the proxy backward mapping[T v ( Σ d )] −1 − [T v ( Σ c )] −1 once.Then the model selection just executes a fast and simple element-wise softthresholding operator using different values of hyperparameter λ n ( Eq. (2.12)).In details, DIFFEE includes four non-iterative operations in its computation:1. Estimating two covariance matrices. The computational complexity is O(max(n c , n d )p 2 ). 2. The element-wise soft-thresholding operations [T v (·)], that cost O(p 2 ).O(p 3 ) O(T * p 3 ) O((nc + p 2 ) 3 ) O(p 8 ) 3. The matrix inversions 2 [T v (·)] −1 to get the proxy backward mapping, that cost O(p 3 ). 4. The element-wise soft-thresholding operation S λn that costs O(p 2 ).Therefore, the total asymptotic computational complexity of DIFFEE estimator is O(p 3 ) .In Table 1, we compare the asymptotic computational complexity of our method to the baselines. DIFFEE achieves the best computational complexity compared to the state-of-the-art baselines. This is because:• All existing estimators for differential network estimation have used an iterative optimization procedure to find the solution. In each iteration, their estimations require at least O(p 3 ) computational cost.• For tuning the sparsity hyperparameter λ n , DIF-FEE only needs to re-run its element-wise softthresholding operation S λn that cost O(p 2 ). In contrast, all the baselines have to re-run the whole algorithm for each value of the hyper-parameter λ n .• Most estimators have two hyperparameters for tuning. FusedGlasso (Eq. (2.6)) and DensityRatio (Eq. (2.8)) both need to tune the hyperparameter λ 2 3 . Both tuning are much more expensive than DIFFEE in computation. DIFFEE needs to tune the hyperparamter v, but it costs only O(p 2 ).• Diff-CLIME has one hyperparameter λ n for tuning, however, its asymptotic time cost (O(p 8 )) is significantly more demanding than DIFFEE 4 . In summary, Diff-CLIME can not handle large-scale cases, like p > 100. For example, in our experiments Diff-CLIME can not even finish on a case of p = 200 after two days of running.

Strong Statistical Guarantees of DIFFEE
In this section, we provide a statistical convergence analysis of DIFFEE Eq. (2.9) under the following struc-2 Many faster algorithms exist for speeding up matrix inversion and matrix multiplication. The best known asymptotic cost of matrix inversion is O(p 2.373 ) (Wikipedia). Besides both operations can be further improved up by parralelization 3 The optimization problem of DensityRatio is a quadratic programming problem with nc + p 2 variables. Based on the result from [4], the computational complexity of quadratic problem with b variables is O(b 3 ). Therefore, the time complexity of DensityRatio is O((nc + p 2 ) 3 ). 4 The optimization problem of Diff-CLIME is a linear programming problem with p 2 variables. Based on the result from [6], the computational complexity of linear problem with b variables is O(b 4 ). Therefore, the time complexity of Diff-CLIME is O((p 2 ) 4 ).tural assumption:(C-Sparsity): The 'true' canonical exponential family parameter for ∆ * (sparse change between two GGM structures) is exactly sparse with k non-zero entries indexed by a supported set S. All other elements equal to 0 (in S c ).Theorem 2.1. Consider any differential network in Eq. (1.1) whose sparse canonical parameter ∆ * satisfies the (C-Sparsity) assumption. Suppose we compute the solution of Eq. (2.9) with a bounded λ n such that λ n ≥ ||∆ * − B * ( Σ d , Σ c )|| ∞ , then the optimal solution ∆ satisfies the following error bounds:|| ∆ − ∆ * ||∞ ≤ 2λn || ∆ − ∆ * ||F ≤ 4 √ kλn || ∆ − ∆ * ||1 ≤ 8kλn(2.  #b4  Proof. See detailed proof in Section A.2.2Theorem (2.1) provides a general bound for any selection of λ n and B * ( Σ d , Σ c ). We then use Theorem (2.1) to derive the statistical convergence rate of DIFFEE whose choice of the proxy backward map- ping is B * ( Σ d , Σ c ) = [T v ( Σ d )] −1 − [T v ( Σ c )] −1 .λ n := 8κ1a κ2log p min(nc,n d ) and min(n c , n d ) > c log p, with a probability of at least 1 − 2C 1 exp(−C 2 Kp log(Kp)), the estimated optimal solution ∆ has the following error bound:|| ∆ − ∆ * ||∞ ≤ 16κ1a κ2 log p min(nc, n d ) || ∆ − ∆ * ||F ≤ 32κ1a κ2 k log p min(nc, n d ) || ∆ − ∆ * ||1 ≤ 64κ1a κ2 k log p min(nc, n d ) (2.14)where a, c, κ 1 and κ 2 are constants. Proof. See detailed proof in Section A.2.4 (especially from Eq. (A.31) to Eq. (A.36)). DIFFEE has achieved the same convergence rates as the Diff-CLIME[35] and the DensityRatio estimator  #b7 . The FusedGLasso estimator has not provided such convergence rate analysis.To derive the statistical error bound of DIFFEE, we need to assume that [T v ( Σ c )] −1 and [T v ( Σ d )] −1 are well-defined. This is ensured by assuming that the true Ω * c and Ω * d satisfy the following conditions [31]:(C-MinInf−Σ):The true Ω * c and Ω *d of Eq. (1.1) have bounded induced operator norm, i.e., |||Ω c * ||| ∞ := sup w =0∈R p ||Σc * w||∞ ||w||∞ ≤ κ 1 and |||Ω d * ||| ∞ := sup w =0∈R p ||Σ d * w||∞ ||w||∞ ≤ κ 1 .

(C-Sparse-Σ):
The two true covariance matrices Σ * c and Σ * d are "approximately sparse" (following [3]). For some constant 0 ≤ q < 1 and c 0 (p), maxi p j=1 |[Σ * c ] ij | q ≤ c 0 (p) and max i p j=1 |[Σ * d ] ij | q ≤ c 0 (p). 5We additionally require infw =0∈R p ||Ω * c w||∞ ||w||∞ ≥ κ 2 and inf w =0∈R p ||Ω * d w||∞ ||w||∞ ≥ κ 2 .

Experiments
We use two models of simulated datasets as well as a real world dataset for empirical comparisons.• The first model mimics real world networks with a sparse differential network containing only hub nodes. This model can evaluate whether the method can efficiently infer the hub nodes in the differential network or not. In [35], the authors claim that if the change estimator also assumes the sparsity structure in Ω c and Ω d , then the estimator cannot achieve a good result on datasets generated by this data model.• The second data simulation model, in contrast, generates random graphs that differ by a sparse random differential network. It evaluates the estimation performance of a certain estimator for inferring the randomly-generated differential networks.• The real world dataset is a human brain fMRI dataset with two groups of subjects: autism and control. Our choice of this dataset is motivated by the recent literature in neuroscience that has suggested functional networks are not sparse. On the other hand, differences in functional connections across subjects should be sparse [2].The two simulation models allow for a thorough evaluation of DIFFEE vs the baseline methods. The realworld data allows us to compare DIFFEE versus the baselines through classification using the estimated differential graph.

Experimental Setup
Baselines: We compare DIFFEE with (1) Fused-GLasso  #b0 , (2) DensityRatio  #b8 , and (3) Diff-CLIME [35].

Evaluation metrics:
We evaluate DIFFEE and the baseline methods on F1-score and running time cost.More details in Section B.

Hyper-parameters:
We need to tune the value of three hyper-parameters in these experiments: v, λ n and λ 2 . In detail:• v is used for soft-thresholding in DIFFEE. We choose v from the set {0.001i|i = 1, 2, . . . , 1000} and pick a value that makes T v (Σ c ) and T v (Σ d ) invertible.• λ n is the main hyper-parameter that control the sparsity of the estimated differential network. Based on our convergence rate analysis in Section 2.5, λ n ≥ • λ 2 controls individual graph's sparsity in Fused-GLasso. We choose λ 1 = 0.0001 (a very small value) for all experiments to ensure only the differential network is sparse. λ 2 in the DensityRatio is set to 0.2 according to their package.Two models to generate simulated datasets: Using the following two graph models, we generate multiple sets of synthetic multivariate-Gaussian datasets.• Model 1 -mimic real-world networks with hub nodes: Inspired by [35], this model assumes that the graphs mimic real-world networks  #b12 . We first generate Ω d as a network with s · p(p−1) 2 edges following a power-law degree distribution with an expected power parameter of 2. Here s is a parameter that controls the sparsity of the two graphs. A larger value of s corresponds to denser graphs. Next, the value of each nonzero entry of Ω d is generated from a uniform distribution with [−10/p, −4/p]∪[4/p, 10/p], where division by p ensures the positive definiteness of Ω c and Ω d . The diagonals are then set to 1 and Ω d is symmetrized by averaging it with its transpose ( 1 2 (Ω d +Ω T d )). The differential network ∆ is generated by the top 20% edges of the top 2 hub nodes in Ω d . Ω c = Ω d − ∆. The shared part B S is generated independently and equal to 0.5 with probability 0.1s and 0 with probability 1 − 0.1s. Similar to Model 1, s controls the sparsity of the two graphs. δ c and δ d are selected large enough to guarantee the positive definiteness.A clear differential network structure ∆ = B d − B c exists between these two graphs.Following Model 1 or Model 2, for each case of simulated data generation, we generate two blocks of data samples following the distribution N (0, (Ω c ) −1 ) andN (0, (Ω d ) −1 ). Details see Section B.

Experiments on Simulated Datasets
Experimental Design: By varying the number of features p, amount of sparsity s, and the number of samples (n c ,n d ), we can generate many cases of simulated datasets. This allows us to comprehensively evaluate DIFFEE across a wide range of data situations. To this end, we design the following three sets of synthetic experiments by varying p, s, n c , and n d :    Figure 3 shows that in general the time costs of FusedGLasso and DensityRatio are roughly comparable. DIFFEE is about 100 times better than both (detailed numbers are provided in Table 3 to Table 10). Diff-CLIME is extremely slow when p increases. Because Figure 3 (c),(d),(e) and (f) are about data cases with p = 200, we can not run Diff-CLIME on these cases (it cannot finish any p = 200 case for a single value of λ n by one day). Interestingly, the empirical time results match the computational analysis in Table 1. Especially DensityRatio's time cost grows quickly when n c increases. In contrast the running time of DIFFEE and FusedGLasso are not connected strongly to the size of samples. Overall DIFFEE costs much less computation time than the baselines and can significantly scale up to larger p.

A Real-World Dataset about Functional Connectivity among Brain Regions
We then use DIFFEE for a classification task on a well-known human brain fMRI dataset: ABIDE  #b1 . . The subjects are randomly partitioned into three equal sets: a training set, a validation set, and a test set. Each estimator produces ∆ using the training set. Then, these differential networks are used as inputs to linear discriminant analysis (LDA), which is tuned via cross-validation on the validation set. Finally, accuracy is calculated by running LDA on the test set. This classification process aims to assess the ability of an estimator to learn the differential patterns of the connectome structures. Notably, the DensityRatio method cannot be compared on this data, because the method does not provide the precision matrices necessary for LDA.Classification Results: Table 2 displays the maximum accuracy achieved by DIFFEE, FusedGLasso, and Diff-CLIME, after tuning over hyperparameters. DIF-FEE yields a classification accuracy of 57.58% distinguishing the autism and control groups, outperforming the FusedGLasso and Diff-CLIME estimators. 

A Appendix of Method


A.1 Backward mapping for Exponential Families
The solution of vanilla graphical model MLE can be expressed as a backward mapping [29] for an exponential family distribution. It estimates the model parameters (canonical parameter θ) from certain (sample) moments.We provide detailed explanations about backward mapping of exponential families, backward mapping for Gaussian special case and backward mapping for differential network of GGM in this section.Backward mapping:Essentially the vanilla graphical model MLE can be expressed as a backward mapping that computes the model parameters corresponding to some given moments in an exponential family distribution. For instance, in the case of learning GGM with vanilla MLE, the backward mapping is Σ −1 that estimates Ω from the sample covariance (moment) Σ.Suppose a random variable X ∈ R p follows the exponential family distribution: P(X; θ) = h(X)exp{< θ, φ(θ) > −A(θ)} (A.1) Where θ ∈ Θ ⊂ R d is the canonical parameter to be estimated and Θ denotes the parameter space. φ(X) denotes the sufficient statistics as a feature mapping function φ : R p → R d , and A(θ) is the log-partition function. We then define mean parameters v as the expectation of φ(X): v(θ) := E[φ(X)], which can be the first and second moments of the sufficient statistics φ(X) under the exponential family distribution. The set of all possible moments by the moment polytope:M = {v|∃p is a distribution s.t. E p [φ(X)] = v} (A.2) Mostly, the graphical model inference involves the task of computing moments v(θ) ∈ M given the canonical parameters θ ∈ H. We denote this computing as forward mapping :A : H → M (A.3)The learning/estimation of graphical models involves the task of the reverse computing of the forward mapping, the so-called backward mapping [29]. We denote the interior of M as M 0 . backward mapping is defined as:A * : M 0 → H (A.4) which does not need to be unique. For the exponential family distribution,A * : v(θ) → θ = ∇A * (v(θ)). (A.5) Where A * (v(θ)) = sup θ∈H < θ, v(θ) > −A(θ).Connecting Eq. (2.11) and Eq. (A.15), R() is the 1 norm, R * () is the ∞ -norm, and ∞ -norm is the dual norm of 1 -norm. θ n represents a backward mapping (or proxy backward mapping well-defined in highdimensional settings) of θ , which is a close approximation of θ * .Following the unified framework  #b11 , we first decompose the parameter space into a subspace pair(M,M ⊥ ), whereM is the closure of M. HereM ⊥ := {v ∈ R p | < u, v >= 0, ∀u ∈M}. M is the model subspace that typically has a much lower dimension than the original high-dimensional space.M ⊥ is the perturbation subspace of parameters. For further proofs, we assume the regularization function in Eq. (A.15) is decomposable w.r.t the subspace pair (M,M ⊥ ).(C1) R(u + v) = R(u) + R(v), ∀u ∈ M, ∀v ∈M ⊥ .[20] showed that most regularization norms are decomposable corresponding to a certain subspace pair. For simplicity, we assume there exists a true parameter θ * which has the exact structure w.r.t a certain subspace pair. Concretely: R * ( θ − θ * ) ≤ 2λ n (A.17) || θ − θ * || 2 ≤ 4λ n Ψ(M) (A.18) R( θ − θ * ) ≤ 8λ n Ψ(M) 2 (A.19)For the proposed DIFFEE model, R = || · || 1 . Based on the results in  #b11 , Ψ(M) = √ k, where k is the total number of nonzero entries in ∆. Using R = || · || 1 in Theorem (A.2), we have the following theorem (the same as Theorem (2.1)), Theorem A.3. Suppose that R = || · || 1 and the true parameter ∆ * satisfy the conditions (C1)(C2) and λ n ≥ R * ( ∆ − ∆ * ), then the optimal point ∆ of Eq. (2.11) has the following error bounds:|| ∆ − ∆ * || ∞ ≤ 2λ n , || ∆ − ∆ * || 2 ≤ 4 √ kλ n , and || ∆ − ∆ * || 1 ≤ 8kλ n

A.2.2 Proof of Theorem (A.2)
Proof. Let δ := θ − θ * be the error vector that we are interested in.R * ( θ − θ * ) = R * ( θ − θ n + θ n − θ * ) ≤ R * ( θ n − θ) + R * ( θ n − θ * ) ≤ 2λ n (A.20)By the fact that θ * M ⊥ = 0, and the decomposability of R with respect to (M,M ⊥ )R(θ * ) = R(θ * ) + R[ΠM⊥ (δ)] − R[ΠM⊥(δ)] = R[θ * + ΠM⊥(δ)] − R[ΠM⊥ (δ)] ≤ R[θ * + ΠM⊥ (δ) + ΠM(δ)] + R[ΠM(δ)] − R[ΠM⊥ (δ)] = R[θ * + δ] + R[ΠM(δ)] − R[ΠM⊥ (δ)] (A.21)Here, the inequality holds by the triangle inequality of norm. Since Eq. (A.15) minimizes R( θ), we have R(θ * + ∆) = R( θ) ≤ R(θ * ). Combining this inequality with Eq. (A.21), we have:R[ΠM⊥ (δ)] ≤ R[ΠM(δ)] (A.22)Moreover, by Hölder's inequality and the decomposability of R(·), we have:||∆|| 2 2 = δ, δ ≤ R * (δ)R(δ) ≤ 2λ n R(δ) = 2λ n [R(ΠM(δ)) + R(ΠM⊥ (δ))] ≤ 4λ n R(ΠM(δ)) ≤ 4λ n Ψ(M)||ΠM(δ)|| 2 (A.23)where Ψ(M) is a simple notation for Ψ(M, || · || 2 ).Since the projection operator is defined in terms of || · || 2 norm, it is non-expansive: ||ΠM(∆)|| 2 ≤ ||∆|| 2 . Therefore, by Eq. (A.23), we have: ||ΠM(δ)|| 2 ≤ 4λ

A.2.3 Useful lemma(s) Lemma A.4. (Theorem 1 of [26]). Let δ be
max ij |[ X T X n ] ij − Σ ij |. Suppose that v > 2δ.Then, under the conditions (C-SparseΣ), and as ρ v (·) is a soft-threshold function, we can deterministically guarantee that the spectral norm of error is bounded as follows:|||T v ( Σ) − Σ||| ∞ ≤ 5v 1−q c 0 (p) + 3v −q c 0 (p)δ (A.26)Lemma A.5. (Lemma 1 of [23]). Let A be the event that|| X T X n − Σ|| ∞ ≤ 8(max i Σ ii ) 10τ log p n (A.27)where p := max n, p and τ is any constant greater than 2. Suppose that the design matrix X is i.i.d. sampled from Σ-Gaussian ensemble with n ≥ 40 max i Σ ii . Then, the probability of event A occurring is at least 1 − 4/p τ −2 .To prove the bound of  ||∆ * − ([T v ( Σ d )] −1 − [T v ( Σ c )] −1 )|| ∞ , we first prove the bound of ||Ω * c − [T v ( Σ c )] −1 || ∞ . In the following proof, we first derive the inequality ||Ω * c − [T v ( Σ c )] −1 || ∞ ≤ |||[T v ( Σ c )] −1 ||| ∞ |||Ω * c ||| ∞ ||T v ( Σ c ) − Σ * c || ∞ ,||Ω * c − [T v ( Σ c )] −1 || ∞ = ||[T v ( Σ c )] −1 (T v ( Σ c )Ω * c − I)|| ∞ ≤ |||[T v ( Σ c )w]||| ∞ ||T v ( Σ c )Ω * c − I|| ∞ = |||[T v ( Σ c )] −1 ||| ∞ ||Ω * c (T v ( Σ c ) − Σ * c )|| ∞ ≤ |||[T v ( Σ c )] −1 ||| ∞ |||Ω * c ||| ∞ ||T v ( Σ c ) − Σ * c || ∞ .(||T v ( Σ c )w|| ∞ = ||T v ( Σ c )w − Σw + Σw|| ∞ ≥ ||Σw|| ∞ − ||(T v ( Σ c ) − Σ)w|| ∞ ≥ κ 2 ||w|| ∞ − ||(T v ( Σ c ) − Σ)w|| ∞ ≥ (κ 2 − ||(T v ( Σ c ) − Σ)w|| ∞ )||w|| ∞ (A.29)Where the second inequality uses the condition (C-SparseΣ). Now, by Lemma (A.4) with the selection of v, we have|||T v ( Σ c ) − Σ||| ∞ ≤ c 1 ( log p n c ) (1−q)/2 c 0 (p) (A.30)where c 1 is a constant related only on τ and max i Σ ii . Specifically, it is defined as 6.5 × (16(max i Σ ii ) √ 10τ ) 1−q . Hence, as long as n c >( 2c1c0(p) κ2 ) 2 1−q log p as stated, so that |||T v ( Σ c )−Σ||| ∞ ≤ κ2 2 , we can conclude that ||T v ( Σ c )w|| ∞ ≥ κ2 2 ||w|| ∞ , which implies |||[T v ( Σ c )] −1 ||| ∞ ≤ 2 κ2 . The remaining term in Eq. (A.28) is ||T v ( Σ c ) − Σ * c || ∞ ; ||T v ( Σ c ) − Σ * c || ∞ ≤ ||T v ( Σ c ) − Σ c || ∞ + || Σ c − Σ * c || ∞ . By construction of T v (·) in (C-Thresh) and by Lemma (A.5), we can confirm that ||T v ( Σ c ) − Σ c || ∞ as well as || Σ c − Σ * c || ∞ can be upper-bounded by v. Similarly, the [T v ( Σ d )] −1 has the same result.Finally,||∆ * − [T v ( Σ d )] −1 − [T v ( Σ c )] −1 || ∞ (A.31) ≤||Ω d − [T v ( Σ d )] −1 || ∞ + ||Ω c − [T v ( Σ c )] −1 || ∞ (A.32) ≤ 4κ 1 a κ 2 log p n c + 4κ 1 a κ 2 log p n c (A.33)Suppose p > max(n c , n d ), we have that||∆ * − [T v ( Σ d )] −1 − [T v ( Σ c )] −1 || ∞ ≤ 8κ 1 a κ 2 log p min(n c , n d ) (A.34)Similarly, we also have that , and||∆ * − [T v ( Σ d )] −1 − [T v ( Σ c )] −1 || F ≤||∆ * − [T v ( Σ d )] −1 − [T v ( Σ c )] −1 || 1 ≤ 64κ 1 a κ 2 k log p min(n c , n d ) (A.36)By combining all together, we can confirm that the selection of λ n satisfies the requirement of Theorem (A.3), which completes the proof.

B Details of Experimental Setup
Evaluation Metrics: We evaluate DIFFEE and the baseline methods on both contexts of effectiveness and scalability.• F1-score: We first use the edge-level F1-score to compare the predicted versus true differential graph.Here, F1 = 2·Precision·Recall Precision+Recall , where Precision = TP TP+FP and Recall = TP TP+FN . TP (true positive) means the number of true edges correctly estimated by the predicted differential network. FP (false positive) and FN (false negative) are the number of incorrectly predicted nonzero entries and zero entries respectively. We repeat the experiment 10 times for each method and use the average metrics for comparison. The better method achieves a higher F1-score.• Time Cost: We use the execution time (measured in seconds or log(seconds)) for a method as a measure of its scalability. To ensure a fair comparison, we try 30 different λ (or λ 2 ) and measure the total time of execution for each method. The better method uses less time 6 .• Low F1 values on Model 1 datasets: The F1-score of all cases in Figure 2(a) appear quite low. This is due to the fact that simulated differential networks from Model 1 are extremely sparse (e.g., only 0.1% non-zero edges among all possible edges). For example, if the estimated ∆ only predicts 5% zero entries incorrectly (i.e., FP=5%) and correctly predicts all the rest entries (TP = 0.1%, TN = 94.9%). The precision equals to TP TP + FP = 0.1% 0.1%+5% ≈ 0.02, which is a small number. The recall equals to TP TP + FN = 0.1% 0.1%+0% = 1. Then F1 = precision·recall 2(precision+recall) ≈ 0.01, which is also a relatively small number. However, the estimator only wrongly inferred 5% zero entries, which is still a good result. Therefore, low F1-score doesn't mean that the estimator is bad when the differential network is extremely sparse.This extreme sparsity also influences other evaluation metrics. For instance, if the estimated ∆ only includes 1% zero entries and 0.05% non-zero entries incorrectly (i.e., FP=5% and FN=0.05%) and correctly predicts all the rest entries (TP=0.05% and TN=94.9%). The TPR = 0.05% 0.05%+0.05% = 0.5 and FPR = 5% 5%+94.9% ≈ 0.2. If you plot this point in the FPR vs. TPR curve, it is not good. However from the angle of accuracy, this method only predicts wrongly around 5% edges, which indicates that it performs well. 6 The machine that we use for experiments is an Intel(R) Core(TM) i7-6850k CPU @ 3.60GHz with a 64GB memory. To simulate data for the control block, we generate n c data samples following multivariate gaussian distribution with mean 0 and covariance matrix (Ω c ) −1 . We use the multivariate distribution method from stochastic simulation [24] to sample the simulated data blocks. In our implementation, we directly use the R function "mvrnorm" in MASS package. We repeat the same process for the case group with Ω d . Then, we apply DIFFEE and baseline methods to obtain the estimated differential networks.   {1, 2, 3, . . . , 30}}. The Diff-CLIME cannot finish any tasks in one day. So all the results in the column "Diff-CLIME" are indicated by "NA". In most of the synthetic datasets, DIFFEE achieves a higher F1-Score and less computation time than other baselines. This proves that DIFFEE outperforms the baselines in both effectiveness and scalability. Table 5 and Table 6 present the detailed performance results of our proposed method DIFFEE and others by varying the sparsity level s. The Table 5 and Table 6 are obtained by Model 1 and Model 2 respectively. We vary the sparsity parameter s in the set of {0.1, 0.2, . . . , 0.7}. The computation time and F1-Score are measured similar to Table 3 and Table 4. In all of the synthetic datasets, DIFFEE performs better as indicated by its higher F1-score and lesser computation time than other baselines. Table 7 and Table 8 present the detailed results of our proposed method-DIFFEE versus the corresponding baselines FusedGLasso, Density Ratio, and Diff-CLIME on the simulated datasets varying different n c and n d in a high-dimensional setting (p > max(n c , n d )). The Table 7 and Table 8 are obtained by Model 1 and Model 2, respectively. We vary the number of samples n c and n d in the set of {p/2, p/4}. The computation time and F1-Score are measured similar to Table 3 and  Table 4. In most of the synthetic datasets, DIFFEE achieves a higher F1-Score and less computation time than other baselines. Table 9 and Table 10 present the performance of our proposed method-DIFFEE and other methods with varying n c and n d in a low-dimensional setting (p > max(n c , n d )). The Table 9 and Table 10 correspond to Model 1 and Model 2, respectively. We vary the number of samples n c and n d in the set of {p, 2p, 3p}. The computation time and F1-Score are measured similar to Table 3 and Table 4. In most of the synthetic datasets, DIFFEE achieves a higher F1-Score and less computation time than other baselines.          

C Detailed Empirical Results


Footnote
1 : Using which of the two sample sets as 'c' set (or 'd' set) does not affect the computational cost and the statistical convergence rates of our model. For instance, on samples from a controlled disease study 'c' may represent the 'control' group and 'd' may represent the 'disease' group.
5 : This indicates for some positive constant d, [Σ * c ]jj ≤ d and [Σ * d ]jj ≤ d for all diagonal entries. Moreover, if q = 0, then this condition reduces to Σ * d and Σ * c being sparse.