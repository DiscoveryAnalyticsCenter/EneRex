Neural Variational Inference and Learning in Undirected Graphical Models

Abstract
Many problems in machine learning are naturally expressed in the language of undirected graphical models. Here, we propose black-box learning and inference algorithms for undirected models that optimize a variational approximation to the log-likelihood of the model. Central to our approach is an upper bound on the logpartition function parametrized by a function q that we express as a flexible neural network. Our bound makes it possible to track the partition function during learning, to speed-up sampling, and to train a broad class of hybrid directed/undirected models via a unified variational inference framework. We empirically demonstrate the effectiveness of our method on several popular generative modeling datasets.

Introduction
Many problems in machine learning are naturally expressed in the language of undirected graphical models. Undirected models are used in computer vision  #b0 , speech recognition  #b1 , social science  #b2 , deep learning  #b3 , and other fields. Many fundamental machine learning problems center on undirected models  #b4 ; however, inference and learning in this class of distributions give rise to significant computational challenges.Here, we attempt to tackle these challenges via new variational inference and learning techniques aimed at undirected probabilistic graphical models p. Central to our approach is an upper bound on the log-partition function of p parametrized by a an approximating distribution q that we express as a flexible neural network  #b5 . Our bound is tight when q = p and is convex in the parameters of q for interesting classes of q. Most interestingly, it leads to a lower bound on the log-likelihood function log p, which enables us to fit undirected models in a variational framework similar to black-box variational inference  #b6 .Our approach offers a number of advantages over previous methods. First, it enables training undirected models in a black-box manner, i.e. we do not need to know the structure of the model to compute gradient estimators (e.g., as in Gibbs sampling); rather, our estimators only require evaluating a model's unnormalized probability. When optimized jointly over q and p, our bound also offers a way to track the partition function during learning  #b7 . At inference-time, the learned approximating distribution q may be used to speed-up sampling from the undirected model my initializing an MCMC chain (or it may itself provide samples). Furthermore, our approach naturally integrates with recent variational inference methods  #b5  #b8  for directed graphical models. We anticipate our approach will be most useful in automated probabilistic inference systems  #b9 .As a practical example for how our methods can be used, we study a broad class of hybrid directed/undirected models and show how they can be trained in a unified black-box neural variational inference framework. Hybrid models like the ones we consider have been popular in the early deep learning literature  #b3  #b10  and take inspiration from the principles of neuroscience  #b11 . They also 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. arXiv:1711.02679v2 [cs.LG] 16 Nov 2017 possess a higher modeling capacity for the same number of variables; quite interestingly, we identify settings in which such models are also easier to train.

Background
Undirected graphical models. Undirected models form one of the two main classes of probabilistic graphical models  #b12 . Unlike directed Bayesian networks, they may express more compactly relationships between variables when the directionality of a relationship cannot be clearly defined (e.g., as in between neighboring image pixels).In this paper, we mainly focus on Markov random fields (MRFs), a type of undirected model corresponding to a probability distribution of the form p θ (x) =p θ (x)/Z(θ), wherep θ (x) = exp(θ · x) is an unnormalized probability (also known as energy function) with parameters θ, and Z(θ) = p θ (x)dx is the partition function, which is essentially a normalizing constant. Our approach also admits natural extensions to conditional random field (CRF) undirected models.Importance sampling. In general, the partition function of an MRF is often an intractable integral overp(x). We may, however, rewrite it asI := xp θ (x)dx = xp θ (x) q(x) q(x)dx = x w(x)q(x)dx,(1)where q is a proposal distribution. Integral I can in turn be approximated by a Monte-Carlo estimatêI := 1 n n i=1 w(x i ), where x i ∼ q.This approach, called importance sampling  #b13 , may reduce the variance of an estimator and help compute intractable integrals. The variance of an importance sampling estimateÎ has a closed-form expression:1 n E q(x) [w(x) 2 ] − I 2 .By Jensen's inequality, it equals 0 when p = q.Variational inference. Inference in undirected models is often intractable. Variational approaches approximate this process by optimizing the evidence lower boundlog Z(θ) ≥ max q E q(x) [logp θ (x) − log q(x)]over a distribution q(x); this amounts to finding a q that approximates p in terms of KL(q||p). Ideal q's should be expressive, easy to optimize over, and admit tractable inference procedures. Recent work has shown that neural network-based models possess many of these qualities  #b14  #b15  #b16 .Auxiliary-variable deep generative models. Several families of q have been proposed to ensure that the approximating distribution is sufficiently flexible to fit p. This work makes use of a class of distributions q(x, a) = q(x|a)q(a) that contain auxiliary variables a  #b17  #b18 ; these are latent variables that make the marginal q(x) multimodal, which in turn enables it to approximate more closely a multimodal target distribution p(x).

Variational Bounds on the Partition Function
This section introduces a variational upper bound on the partition function of an undirected graphical model. We analyze its properties and discuss optimization strategies. In the next section, we use this bound as an objective for learning undirected models.

A Variational Upper Bound on Z(θ)
We start with the simple observation that the variance of an importance sampling estimator (1) of the partition function naturally yields an upper bound on Z(θ):E q(x) p(x) 2 q(x) 2 ≥ Z(θ) 2 .(2)As mentioned above, this bound is tight when q = p. Hence, it implies a natural algorithm for computing Z(θ): minimize (2) over q in some family Q.We immediately want to emphasize that this algorithm will not be directly applicable to highly peaked and multimodal distributionsp (such as an Ising model near its critical point). If q is initially very far fromp, Monte Carlo estimates will tend to under-estimate the partition function.However, in the context of learning p, we may expect a random initialization ofp to be approximately uniform; we may thus fit an initial q to this well-behaved distribution, and as we gradually learn or anneal p, q should be able to track p and produce useful estimates of the gradients ofp and of Z(θ).Most importantly, these estimates are black-box and do not require knowing the structure ofp to compute. We will later confirm that our intuition is correct via experiments.

Properties of the Bound
Convexity properties. A notable feature of our objective is that if q is an exponential family with parameters φ, the bound is jointly log-convex in θ and φ. This lends additional credibility to the bound as an optimization objective. If we choose to further parametrize φ by a neural net, the resulting non-convexity will originate solely from the network, and not from our choice of loss function.To establish log-convexity, it suffices to look atp θ (x) 2 /q(x) for one x, since the sum of log-convex functions is log-convex. Note that logp θ (x) 2 q(x) = 2θ T x − log q φ (x). One can easily check that a non-negative concave function is also log-concave; since q is in the exponential family, the second term is convex, and our claim follows.Importance sampling. Minimizing the bound on Z(θ) may be seen as a form of adaptive importance sampling, where the proposal distribution q is gradually adjusted as more samples are taken  #b13  #b19 . This provides another explanation for why we need q ≈ p; note that when q = p, the variance is zero, and a single sample computes the partition function, demonstrating that the bound is indeed tight. This also suggests the possibility of taking 1 n n i=1p(xi) q(xi) as an estimate of the partition function, with the x i being all the samples that have been collected during the optimization of q.χ 2 -divergence minimization. Observe that optimizing (2) is equivalent to minimizing E q (p−q) 2 q 2 ,which is the χ 2 -divergence, a type of α-divergence with α = 2  #b20  #b21 . This connections highlights the variational nature of our approach and potentially suggests generalizations to other divergences. Moreover, many interesting properties of the bound can be easily established from this interpretation, such as convexity in terms of q,p (in functional space).

Auxiliary-Variable Approximating Distributions
A key part of our approach is the choice of approximating family Q: it needs to be expressive, easy to optimize over, and admit tractable inference procedures. In particular, sincep(x) may be highly multi-modal and peaked, q(x) should ideally be equally complex. Note that unlike earlier methods that parametrized conditional distributions q(z|x) over hidden variables z (e.g. variational autoencoders  #b14 ), our setting does not admit a natural conditioning variable, making the task considerably more challenging.Here, we propose to address these challenges via an approach based on auxiliary-variable approximations  #b17 : we introduce a set of latent variables a into q(x, a) = q(x|a)q(a) making the marginal q(x) multi-modal. Computing the marginal q(x) may no longer be tractable; we therefore apply the variational principle one more time and introduce an additional relaxation of the formE q(a,x) p(a|x) 2p (x) 2 q(x|a) 2 q(a) 2 ≥ E q(x) p(x) 2 q(x) 2 ≥ Z(θ) 2 ,(3)where, p(a|x) is a probability distribution over a that liftsp to the joint space of (x, a). To establish the first inequality, observe thatE q(a,x) p(a|x) 2p (x) 2 q(x|a) 2 q(a) 2 = E q(x)q(a|x) p(a|x) 2p (x) 2 q(a|x) 2 q(x) 2 = E q(x) p(x) 2 q(x) 2 · E q(a|x) p(a|x) 2 q(a|x) 2 .The factor E q(a|x)p(a|x) 2 q(a|x) 2is an instantiation of bound (2) for the distribution p(a|x), and is therefore lower-bounded by 1.This derivation also sheds light on the role of p(a|x): it is an approximating distribution for the intractable posterior q(a|x). When p(a|x) = q(a|x), the first inequality in (3) is tight, and we are optimizing our initial bound.

Instantiations of the Auxiliary-Variable Framework
The above formulation is sufficiently general to encompass several different variational inference approaches. Either could be used to optimize our objective, although we focus on the latter, as it admits the most flexible approximators for q(x).Non-parametric variational inference. First, as suggested by Gershman et al.  #b22 , we may take q to be a uniform mixture of K exponential families:q(x) = K k=1 1 K q k (x; φ k ).This is equivalent to letting a be a categorical random variable with a fixed, uniform prior. The q k may be either Gaussians or Bernoulli, depending on whether x is discrete or continuous. This choice of q lets us potentially model arbitrarily complex p given enough components. Note that for distributions of this form it is easy to compute the marginal q(x) (for small K), and the bound in (3) may not be needed.MCMC-based variational inference. Alternatively, we may set q(a|x) to be an MCMC transition operator T (x |x) (or a sequence of operators) as in Salimans et al.  #b23 . The prior q(a) may be set to a flexible distribution, such as normalizing flows  #b24  or another mixture distribution. This gives a distribution of the formq(x, a) = T (x|a)q(a).(4)For example, if T (x|a) is a Restricted Boltzmann Machine (RBM; Smolensky  #b25 ), the Gibbs sampling operator T (x |x) has a closed form that can be used to compute importance samples. This is in contrast to vanilla Gibbs sampling, where there is no closed form density for weighting samples.The above approach also has similarities to persistent contrastive divergence (PCD; Tieleman and Hinton  #b26 ), a popular approach for training RBM models, in which samples are taken from a Gibbs chain that is not reset during learning. The distribution q(a) may be thought of as a parametric way of representing a persistent distribution from which samples are taken throughout learning; like the PCD Gibbs chain, it too tracks the target probability p during learning.Auxiliary-variable neural networks. Lastly, we may also parametrize q(a|x) by an flexible function approximator such as a neural network  #b17 . More concretely, we set q(a) to a simple continuous prior (e.g. normal or uniform) and set q φ (x|a) to an exponential family distribution whose natural parameters are parametrized by a neural net. For example, if x is continuous, we may set q(x|a) = N (µ(a), σ(a)I), as in a variational auto-encoder. Since the marginal q(x) is intractable, we use the variational bound (3) and parametrize the approximate posterior p(a|x) with a neural network. For example, if a ∼ N (0, 1), we may again set p(a|x) = N (µ(x), σ(x)I).

Optimization
In the rest of the paper, we focus on the auxiliary-variable neural network approach for optimizing bound (3). This approach affords us the greatest modeling flexibility and allows us to build on previous neural variational inference approaches.The key challenge with this choice of representation is optimizing (3) with respect to the parameters φ, φ of p, q. Here, we follow previous work on black-box variational inference  #b5  #b6  and compute Monte Carlo estimates of the gradient of our neural network architecture.The gradient with respect to p has the form 2E qp (x,a)q(x,a) 2 ∇ φp (x, a) and can be estimated directly via Monte Carlo. We use the score function estimator to compute the gradient of q, which can be written as −E q(x,a)p (x,a) 2 q(x,a) 2 ∇ φ log q(x, a) and estimated again using Monte Carlo samples. In the case of a non-parametric variational approximation K k=1 1 K q k (x; φ k ), the gradient has a simple expression∇ φ k E qp (x) 2 q(x) 2 = −E q k p(x) 2 q(x) 2 d k (x) , where d k (x)is the difference of x and its expectation under q k .Note also that if our goal is to compute the partition function, we may collect all intermediary samples for computing the gradient and use them as regular importance samples. This may be interpreted as a form of adaptive sampling  #b19 .Variance reduction. A well-known shortcoming of the score function gradient estimator is its high variance, which significantly slows down optimization. We follow previous work  #b5  and introduce two variance reduction techniques to mitigate this problem.We first use a moving averageb ofp(x) 2 /q(x) 2 to center the learning signal. This leads to a gradient estimate of the form E q(x) (p (x) 2 q(x) 2 −b)∇ φ log q(x); this yields the correct gradient by well known properties of the score function  #b6 . Furthermore, we use variance normalization, a form of adaptive step size. More specifically, we keep a running averageσ 2 of the variance of thep(x) 2 /q(x) 2 and use a normalized form g = g/ max(1,σ 2 ) of the original gradient g.Note that unlike the standard evidence lower bound, we cannot define a sample-dependent baseline, as we are not conditioning on any sample. Likewise, many advanced gradient estimators  #b8  do not apply in our setting. Developing better variance reduction techniques for this setting is likely to help scale the method to larger datasets.

Neural Variational Learning of Undirected Models
Next, we turn our attention to the problem of learning the parameters of an MRF. Given dataD = {x (i) } n i=1 , our training objective is the log-likelihood log p(D|θ) := 1 n n i=1 log p θ (x (i) ) = 1 n n i=1 θ T x (i) − log Z(θ).(5)We can use our earlier bound to upper bound the log-partition function by log E x∼qpθ (x) 2 q(x) 2. By our previous discussion, this expression is convex in θ, φ if q is an exponential family distribution. The resulting lower bound on the log-likelihood may be optimized jointly over θ, φ; as discussed earlier, by training p and q jointly, the two distributions may help each other. In particular, we may start learning at an easy θ (where p is not too peaked) and use q to slowly track p, thus controlling the variance in the gradient.Linearizing the logarithm. Since the log-likelihood contains the logarithm of the bound (2), our Monte Carlo samples will produce biased estimates of the gradient. We did not find this to pose problems in practice; however, to ensure unbiased gradients one may further linearize the log using the identity log(x) ≤ ax − log(a) − 1, which is tight for a = 1/x. Together with our bound on the log-partition function, this yieldslog p(D|θ) ≥ max θ,q 1 n n i=1 θ T x (i) − 1 2 aE x∼qp θ (x) 2 q ψ (x) 2 − log(a) − 1 .(6)This expression is convex in each of (θ, φ) and a, but is not jointly convex. However, it is straightforward to show that equation (6) and its unlinearized version have a unique point satisfying first-order stationarity conditions. This may be done by writing out the KKT conditions of both problems and using the fact that a * = (E x∼qp θ (x) 2 q(x) 2 ) −1 at the optimum. See Gopal and Yang  #b27  for more details.

Variational Inference and Learning in Hybrid Directed/Undirected Models
We apply our framework to a broad class of hybrid directed/undirected models and show how they can be trained in a unified variational inference framework.The models we consider are best described as variational autoencoders with a Restricted Boltzmann Machine (RBM; Smolensky  #b25 ) prior. More formally, they are latent-variable distributions of the form p(x, z) = p(x|z)p(z), where p(x|z) is an exponential family whose natural parameters are parametrized by a neural network as a function of z, and p(z) is an RBM. The latter is an undirected latent variable model with hidden variables h and unnormalized log-probabilitylogp(z, h) = z T W h + b T z + c T h, where W, b, c are parameters.We train the model using two applications of the variational principle: first, we apply the standard evidence lower bound with an approximate posterior r(z|x); then, we apply our lower bound on the RBM log-likelihood log p(z), which yields the objectivelog p(x) ≥ E r(z|x) [log p(x|z) + logp(z) + log B(p, q) − log r(z|x)] .(7)Here, B denotes our bound (3) on the partition function of p(z) parametrized with q. Equation (7) may be optimized using standard variational inference techniques; the terms r(z|x) and p(x|z) do not appear in B and their gradients may be estimated using REINFORCE and standard Monte Carlo, respectively. The gradients ofp(z) and q(z) are obtained using methods described above. Note also that our approach naturally extends to models with multiple layers of latent directed variables.Such hybrid models are similar in spirit to deep belief networks  #b10 . From a statistical point of view, a latent variable prior makes the model more flexible and allows it to better fit the data distribution. Such models may also learn structured feature representations: previous work has shown that undirected modules may learn classes of digits, while lower, directed layers may learn to represent finer variation  #b28 . Finally, undirected models like the ones we study are loosely inspired by the brain and have been studied from that perspective  #b11 . In particular, the undirected prior has been previously interpreted as an associative memory module  #b10 .

Experiments


Tracking the Partition Function
We start with an experiment aimed at visualizing the importance of tracking the target distribution p using q during learning. We use Equation 6 to optimize the likelihood of a 5 × 5 Ising MRF with coupling factor J and unaries chosen randomly in {10 −2 , −10 −2 }. We set J = −0.6, sampled 1000 examples from the model, and fit another Ising model to this data. We followed a non-parametric inference approach with a mixture of K = 8 Bernoullis. We optimized (6) using SGD and alternated between ten steps over the φ k and one step over θ, a. We drew 100 Monte Carlo samples per q k . Our method converged in about 25 steps over θ. At each iteration we computed log Z via importance sampling.The adjacent figure shows the evolution of log Z during learning. It also plots log Z computed by exact inference, loopy BP, and Gibbs sampling (using the same number of samples). Our method accurately tracks the partition function after about 10 iterations. In particular, our method fares better than the others when J ≈ −0.6, which is when the Ising model is entering its phase transition.

Learning Restricted Boltzmann Machines
Next, we use our method to train Restricted Boltzmann Machines (RBMs) on the UCI digits dataset  #b29 , which contains 10,992 8 × 8 images of handwritten digits; we augment this data by moving each image 1px to the left, right, up, and down. We train an RBM with 100 hidden units using ADAM  #b30  with batch size 100, a learning rate of 3 · 10 −4 , β 1 = 0.9, and β 2 = 0.999; we choose q to be a uniform mixture of K = 10 Bernoulli distributions. We alternate between training p and q, performing either 2 or 10 gradient steps on q for each step on p and taking 30 samples from q per step; the gradients of p are estimated via adaptive importance sampling.We compare our method against persistent contrastive divergence (PCD; Tieleman and Hinton  #b26 ), a standard method for training RBMs. The same ADAM settings were used to optimize the model with the PCD gradient. We used k = 3 Gibbs steps and 100 persistent chains. Both PCD and our method were implemented in Theano  #b31 .In Figure 1, we plot the true log-likelihood of the model (computed with annealed importance sampling with step size 10 −3 ) as a function of the epoch; we use 10 gradient steps on q for each step on p. Both PCD and our method achieve comparable performance. Interestingly, we may use our  approximating distribution q to estimate the log-likelihood via importance sampling. Figure 1 (right) shows that this estimate closely tracks the true log-likelihood; thus, users may periodically query the model for reasonably accurate estimates of the log-likelihood. In our implementation, neural variational inference was approximately eight times slower than PCD; when performing two gradient steps on q, our method was only 50% slower with similar samples and pseudo-likelihood; however log-likelihood estimates were noisier. Annealed importance sampling was always more than order of magnitude slower than neural variational inference.Visualizing the approximating distribution. Next, we trained another RBM model performing two gradient steps for q for each step of p. The adjacent figure shows the mean distribution of each component of the mixture of Bernoullis q; one may distinguish in them the shapes of various digits. This confirms that q indeed approximates p.Speeding up sampling from undirected models. After the model has finished training, we can use the approximating q to initialize an MCMC sampling chain. Since q is a rough approximation of p, the resulting chain should mix faster. To confirm this intuition, we plot in the adjacent figure samples from a Gibbs sampling chain that has been initialized randomly (top), as well as from a chain that was initialized with a sample from q (bottom). The latter method reaches a plausible-looking digit in a few steps, while the former produces blurry samples.

Learning Hybrid Directed/Undirected Models
Next, we use the variational objective (7) to learn two types of hybrid directed/undirected models: a variational autoencoder (VAE) and an auxiliary variable deep generative model (ADGM)  #b17 . We consider three types of priors: a standard set of 200 uniform Bernoulli variables, an RBM with 64 visible and 8 hidden units, and an RBM with 64 visible and 64 hidden units. In the ADGM, the approximate posterior r(z, u|x) = r(z|u, x)r(u|x) includes auxiliary variables u ∈ R 10 . All the conditional probabilities r(z|u, x), r(u|x), r(z|x), p(x|z) are parametrized with dense neural networks with one hidden layer of size 500. We train all neural networks for 200 epochs with ADAM (same parameters as above) and neural variational inference (NVIL) with control variates as described in Mnih and Rezende  #b8 . We parametrize q with a neural network mapping 10-dimensional auxiliary variables a ∈ N (0, I) tox via one hidden layer of size 32. We show in Table 1 the test set negative log-likelihoods on the binarized MNIST  #b32  and 28 × 28 Omniglot  #b16  datasets; we compute these using 10 3 Monte Carlo samples and using annealed importance sampling for the 64 × 64 RBM.Overall, adding an RBM prior with as little as 8 latent variables results in significant log-likelihood improvements. Most interestingly, this prior greatly improves sample quality over the discrete latent variable VAE (Figure 2). Whereas the VAE failed to generate correct digits, replacing the prior with a small RBM resulted in smooth MNIST images. We note that both methods were trained with exactly the same gradient estimator (NVIL). We observed similar behavior for the ADGM model. This suggests that introducing the undirected component made the models more expressive and easier to train.

Related Work and Discussion
Our work is inspired by black-box variational inference  #b6  for variational autoencoders and related models  #b14 , which involve fitting approximate posteriors parametrized by neural networks. Our work presents analogous methods for undirected models. Popular classes of undirected models include Restricted and Deep Boltzmann Machines  #b3  #b25  as well as Deep Belief Networks  #b10 . Closest to our work is the discrete VAE model; however, Rolfe  #b28  seeks to efficiently optimize p(x|z), while the RBM prior p(z) is optimized using PCD; our work optimizes p(x|z) using standard techniques and focuses on p(z). Our bound has also been independently studied in directed models  #b21 .More generally, our work proposes an alternative to sampling-based learning methods; most variational methods for undirected models center on inference. Our approach scales to small and medium-sized datasets, and is most useful within hybrid directed-undirected generative models. It approaches the speed of the PCD method and offers additional benefits, such as partition function tracking and accelerated sampling. Most importantly, our algorithms are black-box, and do not require knowing the structure of the model to derive gradient or partition function estimators. We anticipate that our methods will be most useful in automated inference systems such as Edward  #b9 .The scalability of our approach is primarily limited by the high variance of the Monte Carlo estimates of the gradients and the partition function when q does not fit p sufficiently well. In practice, we found that simple metrics such as pseudo-likelihood were effective at diagnosing this problem. When training deep generative models with RBM priors, we noticed that weak q's introduced mode collapse (but training would still converge). Increasing the complexity of q and using more samples resolved these problems. Finally, we also found that the score function estimator of the gradient of q does not scale well to higher dimensions. Better gradient estimators are likely to further improve our method.

Conclusion
In summary, we have proposed new variational learning and inference algorithms for undirected models that optimize an upper-bound on the partition function derived from the perspective of importance sampling and χ 2 divergence minimization. Our methods allow training undirected models in a black-box manner and will be useful in automated inference systems  #b9 .Our framework is competitive with sampling methods in terms of speed and offers additional benefits such as partition function tracking and accelerated sampling. Our approach can also be used to train hybrid directed/undirected models using a unified variational framework. Most interestingly, it makes generative models with discrete latent variables both more expressive and easier to train.