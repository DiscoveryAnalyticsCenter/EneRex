Adaptive Representation Selection in Contextual Bandit

Abstract
We consider an extension of the contextual bandit setting, motivated by several practical applications, where an unlabeled history of contexts can become available for pre-training before the online decisionmaking begins. We propose an approach for improving the performance of contextual bandit in such setting, via adaptive, dynamic representation learning, which combines offline pre-training on unlabeled history of contexts with online selection and modification of embedding functions. Our experiments on a variety of datasets and in different nonstationary environments demonstrate clear advantages of our approach over the standard contextual bandit.

Introduction
Sequential decision making is a common problem in many practical applications where the agent must choose the best action to perform at each iteration in order to maximize the cumulative reward over some period of time. One of the key challenges is achieving a good trade-off between the exploration of new actions and the exploitation of known actions. This exploration vs. exploitation trade-off in sequential decision making problems is often formulated as the multi-armed bandit (MAB) problem: given a set of bandit "arms" (actions), each associated with a fixed but unknown reward probability distribution  #b14 Auer et al., 2002a], an agent selects an arm to play at each iteration, and receives a reward, drawn according to the selected arm's distribution, independently from the previous actions.A particularly useful version of MAB is the contextual multi-armed bandit (CMAB), or simply the contextual bandit problem, where at each iteration, before choosing an arm, the agent observes an N -dimensional context, or feature vector. Over time, the goal is to learn the relationship between the context vectors and the rewards, in order to make better prediction which action to choose given the context  #b1 .For example, the contextual bandit approach is commonly used in various practical sequential decision problems with side information (context), from clinical trials  #b21  to recommender system  #b18 , where the patient's information (medical history, etc.) or an online user's profile provide a context for making a better decision about the treatment to propose or an ad to show, and the reward represents the outcome of the selected action, such as, for example, success or failure of a particular treatment option.However, in certain real-life applications, before the online decision-making starts, an agent may have an access to a unlabeled context history (i.e., contexts without the associated rewards), which can be potentially used as a prior knowledge to improve the subsequent online decision-making. For instance, in medical decision-making settings, the doctor may have an access to medical records of different patients, which can be used to gain a better understanding of the patients population. A different example of unlabeled context history can occur in an online recommender setting, where the system may have some previous information about the users, although the reward feedback (e.g., whether the user clicked on the suggested link or not) might be missing.Having an access to unlabeled data makes it possible to pre-train some model of the input (contexts) in an offline mode, and use it later to improve the online decision making. For example, we can learn an autoencoder to map the raw inputs into potentially better representations. Moreover, when the inputs are non-homogeneous, we may want to cluster the unlabeled data and learn separate representations for each cluster. Then, in the online mode, we can decide which representation to use for a given context; such context-driven representation selection has a potential to further improve the subsequent decision-making. These representation models (e.g., autoencoders) can (and should) continue to be updated online as more contexts become available, especially in nonstationary environments abundant in practical applications, where both the context distribution and the reward distribution can change in various ways.Motivated by the above scenarios, we consider here a contextual bandit setting, called Contextual Bandit with Representation learning and unlabeled History (CBRH). In this setting, it is assumed that (1) a set of unlabeled contexts is available for pre-training before the online decision-making starts, (2) the bandit's performance can be improved by learning a good context representation (embedding) rather than using the raw input, the (3) embedding functions are pre-trained on the unlabeled history and adaptively selected (and updated) based on the context during the online decision-making. Next, we propose an algorithm for the above CBRH setting, called Adaptive Bandit with Context-Driven Embeddings (ABaCoDE), which implements online, clustering-based embedding selection and learning coupled with Thompson-Sampling contextual bandit approach.As demonstrated by empirical results on multiple datasets, our approach consistently (and quite considerably) outperforms the standard contextual bandit, and appears to be particularly beneficial in nonstationary environments of several types, involving both context and reward nonstationarity.

Related Work
The multi-armed bandit problem has been extensively studied. Optimal solutions have been provided using a stochastic formulation  #b14 ; Auer et al. [2002a], a Bayesian formulation  #b20 ;  #b7 ;  #b0 , or using an adversarial formulation  #b3 Auer et al., 2002b]. However, these approaches do not take into account the context which may affect to the arm's performance. In LINUCB  #b17  #b9  and in Contextual Thompson Sampling style (CTS) algorithms  #b1  #b8 , the authors assume a linear dependency between the expected reward of an action and its context; the representation space is modeled using a set of linear predictors. This assumption is not used in Neural Bandit  #b2 . Authors in  #b6  studies considering some kind of incomplete feedback called "Partial Monitoring (PM)", which is a general framework for sequential decision making problems with incomplete feedback that allows the learner, when it is possible, to retrieve the expected value of actions through an analysis of the feedback matrix, both of which are assumed to be known to the learner.In  #b12  authors study a variant of the stochastic multi-armed bandit (MAB) problem in which the rewards are corrupted. In this framework, motivated by privacy preserving in online recommender systems, the goal is to maximize the sum of the (unobserved) rewards, based on the observation of transformation of these rewards through a stochastic corruption process with known parameters.We can say that our setting is similar to the on-line semisupervised learning  #b22  #b19 , which is a field of machine learning that studies learning from both labeled and unlabeled examples in an on-line setting. However in the their setting they receive the true label at each iteration, and we receive a bandit feedback.

Background
This section introduces some background concepts our approach builds upon, such as contextual bandit and Thompson Sampling.The contextual bandit problem Following  #b15 , this problem is defined as follows. At each time point (iteration) t ∈ {1, ..., T }, an agent is presented with a context (feature vector) x t ∈ R N before choosing an arm k ∈ A = {1, ..., K}. We will denote by X = {X 1 , ..., X N } the set of features (variables) defining the context. Let r t = (r 1 t , ..., r K t ) denote a reward vector, where r k t ∈ [0, 1] is a reward at time t associated with the arm k ∈ A. Herein, we will primarily focus on the Bernoulli bandit with binary reward, i.e. r k t ∈ {0, 1}. Let π : X → A denote a policy. Also, D c,r denotes a joint distribution over (x, r). We will assume that the expected reward is a linear function of the context, i.e. E[r k t |x t ] = µ T k x t , where µ k is an unknown weight vector (to be learned from the data) associated with the arm k.

Contextual Thompson Sampling
In this setting, we consider the general Thompson Sampling, where the reward r i t for choosing arm i at time t follows a parametric likelihood function P r(r t |μ i ). Following  #b1 , the posterior distribution at time t + 1,P r(μ i |r t ) ∝ P r(r t |μ i )P r(μ i ) is given by a multivariate Gaussian distribution N (μ i (t + 1), v 2 B i (t + 1) −1 ), where B i (t) = I d + t−1 τ =1 x τ x τ , and where d is the size of the context vectors x i , v = R 24 dln( 1 γ ) with R > 0, ∈]0, 1], γ ∈]0, 1] constants, andμ i (t) = B i (t) −1 ( t−1 τ =1 x τ r τ ). At every step t, the algorithm generates a d-dimensional samplẽ µ i from N (μ i (t), v 2 B i (t) −1 ), for each arm, selects the arm i that maximizes x tμi ,and obtains reward r t .

Algorithm 1 The Contextual Thompson Sampling Algorithm
1: Initialize: for i = 1, ..., k, B i = I d ,μ i = 0 d , f i = 0 d . 2: for t = 1, 2, ..., T do 3:Receive context x t 4:for i = 1, ..., k, sampleμ i from the N (μ i , v 2 B −1 i ) 5: Choose arm i t = arg max i⊂I x(t) μ i 6:Receive reward r i k 7:B i = B i + x t x T t , f i = f i + x t r i t ,μ i = B −1 i f i 8: end4 Problem FormulationUsing the notation introduced in the previous section, we now define our novel bandit setting: Contextual Bandit with Representation learning and unlabeled History (CBRH) (outlined in Alg. 2), based on the following key assumptions.First, we assume that a context x t ∈ R N is mapped into its representation z t ∈ R Ni using an embedding function e i (c t ), selected from a set E = {e 1 , ..., e k } of currently available embedding functions. Second, we assume that the set of embedding functions E can be modified online. And third, an access to a set D of unlabeled contexts, i.e. contexts without the associated rewards, is assumed. This dataset can be used, for example, for pre-training embedding functions e(x). We then define a set Π = ∪ ei∈E {π : R N → A, π(c) = π(e i (c))} of compound-function policies, where the function π : R Ni → A maps z t = e i (c t ) to an action in A. The objective is to learn a hypothesis π over T iterations maximizing the cumulative reward. (x t , r t ) is drawn according to distribution D c,r

5:
A context representation z t is obtained 6:The player chooses an arm k t =π(z t ) 7:The reward r k t is revealed 8:The player updates its policy π 9: t = t + 1 10: Until t=T 5 Adaptive Bandit with Context-Dependent Embeddings (ABaCoDE)We now describe an adaptive, context-driven embedding selection approach to solving the CBRU problem introduced in the previous section. It has two variants, based on onlineand offline clustering, respectively; the choice is controlled by a Boolean input parameter isOnline in Algorithm 3. Two more inputs include: an unlabeled pre-training dataset D, as well as the number of embeddings k. The algorithm processes the input contexts sequentially, one by one, but at the end of each mini-batch of data it updates the embeddings to reflect possible changes in the data distribution.Algorithm 3 Adaptive Bandit with Context-Dependent Embeddings (ABaCoDE) 1: Input: unlabeled dataset D, a set of unlabeled contexts for pre-training; k, the number of clusters (and corresponding embeddings); a Boolean variable isOnline. if not(isOnline) then recomputeClusters(C, B) 14:

updateEmbedding(M, C) end
The initialization step consists of clustering the pre-training dataset D into k clusters (line 3), training an autoencoder for each cluster, which results into k encoding (embedding) functions (line 4), and initializing parameters of the contextual Thompson Sampling bandit, used later to make classification decisions based on embedded context (line 5).Next, the algorithm switches to the online mode, processing an online stream of incoming samples (contexts). As mentioned above, we assume that at the end of each fixed-length time window, i.e. a fixed-size mini-batch of data, we update our embeddings.Within each data mini-batch M (line 7), once the next input sample x i arrives, it is first assigned to one of the existing clusters c j (line 8), associated with the corresponding embedding function e j . Next, an online clustering is performed if isOnline is true, i.e. the centroid of the cluster c j is recomputed, but no changes are made to other clusters (line 9). Otherwise, there are no changes to clusters, until the end of the batch, as we will see shortly. Based on the cluster assignment c j , the corresponding embedding function e j is used to compute the representation vector z for given input x i (line 10); given the context z, the contextual bandit B makes a decision (line 11), obtains the reward r i (line 12), and updates its parameters (line 13) using the contextual Thompson Sampling described in the previous section.After the end of the mini-batch M is reached (line 14), if isOnline was false, the clusters will be recomputed from scratch using all data points received so far (however, no such re-clustering is performed if the online clustering was selected). Finally, the embeddings (i.e., their corresponding autoencoder parameters) are updated respectively using the updated set of clusters C.In the next section, we present empirical results comparing both online and offline clustering methods outlined above with two baseline approaches:• Contextual Bandit (CB): as the baseline, we use the standard contextual multi-armed bandit with Thompson Sampling, based on the raw input (i.e., no embeddings).• universal embedding (uE): a universal embedding denotes a single embedding computed based on all data, and always recomputed to include the data from the most recent mini-batch; no clustering is performed.• mini-batch embedding (mE): this is our offline clustering approach presented in Algorithm 3, when isOnline is f alse.• online embedding (oE): this is the online version of our algorithm described above, i.e. isOnline is true.  Table 1). To simulate an online data stream, we draw samples from each dataset sequentially, starting from the beginning each time we draw the last sample. At each round, the algorithm receives reward 1 if the instance is classified correctly, and 0 otherwise. We compute the total number of classification errors as a performance metric.It is important to keep in mind that the bandit feedback (correct/incorrect classification) makes the classification problem significantly more challenging, as compared to the standard supervised learning, since the true label is never revealed in bandit setting unless the classification is correct. Thus, the classification accuracy in a bandit setting is expected to be lower than in the supervised learning setting. We now describe some details of the experiments. For MNIST, we took 10,000 samples from the original test dataset (clearly, not using them later for testing) to pre-train the encodings, and 60,000 samples from the training dataset to simulate the online bandit with 10 arms corresponding to different digits. For STL-10, 100,000 samples of unlabeled data are used to pre-train the encodings; then the 5,000 test samples together with 8,000 training samples are combined to simulate the online bandit, again with 10 different arms corresponding to image classes 1 . For Caltech-101 Silhouettes-28 dataset, out of the original 8671 samples, 671 are used for pre-training and 8000 for online learning with 101 different arms (class labels). For CIFAR-10 dataset, 10,000 test set samples are used for pre-training, and 50,000 training samples are left for the online bandit with 10 arms (classes). For Warfarin dataset, 528 test set samples are used for pre-training, and 5,000 training samples are left for the online bandit with 3 arms (classes).

Nonstationary Environments
We simulated several types of nonstationarity using the above datasets. As mentioned before, we assume that the input data arrive in batches, and the data distribution (i.e., the joint distribution of the context and reward) may change across those batches, while remaining stationary within each batch. We used the batch size of 1,000, and varied the number of embeddings k, using k = 2, 4, or 8, presenting average results over all k.

Nonstationary context: varying cluster distribution
To simulate changes in the context (input) distribution, we first clustered all samples in the corresponding pre-training data subset into k clusters. Next, we generate a sequence of batches, where each batch contained a certain fraction of samples from different clusters, and these fractions were changing across the batches, i.e. the probability distribution of cluster membership was changing, simulating nonstationary input.Nonstationary context: negative images Another type of input nonstationarity involved introducing negative images as inputs with same semantics but different textures. Namely, with probability p, the negative image of the original image was presented as an input. Experiments were performed in two settings: half (p = 0.5) and rand (0 < p < 1 randomly assigned for each mini-batch), in both stationary and nonstationary context conditions, with both shuffled and unshuffled rewards (described later). 1 To speed up the computation, we squeezed input 27648dimensional vectors into 1000-dimensional ones via linear stretching.Nonstationary reward: multi-task environment Another type of nonstationarity was assuming that input samples may come from different domains (tasks), and thus can be associated with different subsets of labels (arms). For example, we combined 5,000 randomly selected training samples from each of the two selected domains, MNIST and Warfarin datasets, and extended the set of possible labels (arms) to include 10 labels from MNIST and 3 labels from Warfarin. We used linear stretching to make the input dimensions equal across the two domains. The algorithm had to assign a label to each input without any information about which domain the input came from.Nonstationary reward: shuffled class labels We further explored the multi-task setting by introducing a different type of nonstationary reward, where the class labels were shuffled, i.e. randomly permuted, in each batch.

Results
We explored different combination of the above nonstationarities. Table 2 summarizes our results for the nonstationary context due to varying cluster distribution, and for mixeddomain (multi-task) settings, with unshuffled reward function.As we can see, on three out of six datasets, baseline was still outperforming our embeddings. However, if we consider the mean accuracy in the entire set of experiments, the top three algorithms were: universal embedding (mean accuracy 28.83%), baseline (mean accuracy 27.78%), mini-batch embedding (mean accuracy 27.58%), respectively, suggesting the advantage of representation learning (embedding computation). Moreover, if we take a look at the whole iteration history, for example, for MNIST dataset (Figure 1), we observe that initially, the baseline CB (solid line) is considerably worse than embedding-based approaches, and requires a large number of iteration to finally catch up with them. Figures 2  and 3 show the history of reward accumulation for the STL-10 and CIFAR-10, demonstrating that the baseline is consistently outperformed by embedding selection methods.     Table 3 summarizes our results with shuffled reward function, for the nonstationary context due to varying cluster distribution, and for mixed-domain (multi-task) settings. Based on the mean accuracy in the entire experiment, the top three algorithms were: universal embedding (mean accuracy 28.46%), mini-batch embedding (mean accuracy 26.62%), online embedding (mean accuracy 25.52%), respectively. Furthermore, in this experiment, our embedding-based approaches always outperformed the baseline, suggesting that in a setting where reward functions are nonstationary, in addition to the nonstationary input environment, the advantage of representation learning is quite significant, as compared to standard CB (mean accuracy 16.28%). Note that, with nonstationary (shuffled) labels, the reward accumulated by the baseline CB remains significantly below the reward of embedding-based approaches, at all iterations (Figures 4-6). Thus, in a more challenging setting with both context and reward nonstationarities, the embedding-based approaches clearly outperform the standard contextual bandit. Table 4 summarizes our results for the nonstationary online learning setting with negative environments and unshuffled reward. Based on the mean accuracy in the entire experiment, the top three algorithms were: online embedding (mean accuracy 12.78%), universal embedding (mean accuracy 12.61%), mini-batch embedding (mean accuracy 12.23%), respectively. Again, the embedding-based approaches are always superior to the baseline CB; online embedding achieved the best performance among all methods on MNIST, while universal and batch embeddings were taking their turns outperforming the baseline on other datasets and settings.  Table 5 summarizes our results for the nonstationary online learning setting with negative environments and shuffled reward function. Based on the mean accuracy in the entire experiment, the top three algorithms were: universal embedding (mean accuracy 12.61%), online embedding (mean accuracy 12.14%), mini-batch embedding (mean accuracy 11.80%), respectively, further confirming the advantage of adaptive encoding over standard CB (mean accuracy 7.12%). In addition, the difference of textures under the same semantics introduced in this experiments demonstrated that embedding selection outperforms single universal embedding in most nonstationary cases.Figures 7-14 visualize the details of reward accumulation over time by different methods, on MNIST data and all the settings from the Tables 4 and 5. The performance gap between the embedding-based approaches and the baseline is especially large in those settings. Furthermore, we can see that both adaptive, context-dependent embedding approaches (oE and mE) consistently ourperform the single-embedding approach (uE), with the online embedding emerging as the best one, especially with increasing number of iterations.

Conclusions
We introduced an extension of the contextual bandit problem motivated by several real-world applications in non-stationary environments, including recommendation systems, health monitoring and medical diagnosis, and others. In this setting, which we refer to as Contextual Bandit with Representation learning and unlabeled History (CBRH), a set of unlabeled contexts is available prior to online decision making, which allows, instead of using the raw context, to learn context representations. Next, during the online phase, embeddings are selected adaptively, depending on each context, and updated based on the contexts observed so far. We propose two specific algorithms for the CBRH problem, based on online and offline clustering, which combine online embedding selection and learning with contextual Thompson Sampling bandit. The algorithms are evaluated in several types of nonstationary environments and compared to the standard contextual bandit, as well as universal (single) embedding, on several datasets. Overall, we observe clear advantages of the embedding-based approaches over the standard contextual bandit; moreover, the proposed adaptive embedding selection and learning methods frequently outperform the universal embedding in multiple nonstationary settings.