Part-based R-CNNs for Fine-grained Category Detection

Abstract
Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.

Introduction
The problem of visual fine-grained categorization can be extremely challenging due to the subtle differences in the appearance of certain parts across related categories. In contrast to basic-level recognition, fine-grained categorization aims to distinguish between different breeds or species or product models, and often requires distinctions that must be conditioned on the object pose for reliable identification. Facial recognition is the classic case of fine-grained recognition, and it is noteworthy that the best facial recognition methods jointly discover facial landmarks and extract features from those locations.Localizing the parts in an object is therefore central to establishing correspondence between object instances and discounting object pose variations and camera view position. Previous work has investigated part-based approaches to this problem  #b6  #b15  #b29  #b43  #b46  #b21 . The bottleneck for many pose-normalized representations is indeed accurate part localization. The Poselet  #b7  and DPM  #b16  methods have previously been utilized to obtain part localizations with a modest degree of success; methods generally report adequate part localization only when given a known bounding box at test time  #b10  #b19  #b35  #b36  #b42 . By developing a novel deep part detection scheme, we propose an end-to-end fine grained categorization system which requires no knowledge of object bounding box at test time, and can achieve performance rivaling previously reported methods requiring the ground truth bounding box at test time to filter false positive detections. The recent success of convolutional networks, like  #b26 , on the ImageNet Challenge  #b22  has inspired further work on applying deep convolutional features to related image classification  #b13  and detection tasks  #b20 . In  #b20 , Girshick et al. achieved breakthrough performance on object detection by applying the CNN of  #b26  to a set of bottom-up candidate region proposals  #b40 , boosting PASCAL detection performance by over 30% compared to the previous best methods. Independently, OverFeat  #b37  proposed localization using a CNN to regress to object locations. However, the progress of leveraging deep convolutional features is not limited to basic-level object detection. In many applications such as fine-grained recognition, attribute recognition, pose estimation, and others, reasonable predictions demand accurate part localization.Feature learning has been used for fine-grained recognition and attribute estimation, but was limited to engineered features for localization. DPD-DeCAF  #b47  used DeCAF  #b13  as a feature descriptor, but relied on HOG-based DPM  #b16  for part localization. PANDA  #b48  learned part-specific deep convolutional networks whose location was conditioned on HOG-based poselet models. These models lack the strength and detection robustness of R-CNN  #b20 . In this work we explore a unified method that uses the same deep convolutional representation for detection as well as part description.We conjecture that progress made on bottom-up region proposal methods, like selective search  #b40 , could benefit localization of smaller parts in addition to whole objects. As we show later, average recall of parts using selective search proposals is 95% on the Caltech-UCSD bird dataset.In this paper, we propose a part localization model which overcomes the limitations of previous fine-grained recognition systems by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns part appearance models and enforces geometric constraints between parts. An overview of our method is shown in Figure 1. We have investigated different geometric constraints, including a non-parametric model of joint part locations conditioned on nearest neighbors in semantic appearance space. We present state-of-the-art results evaluating our approach on the widely used fine-grained benchmark Caltech-UCSD bird dataset  #b41 .

Related work


Part-based models for detection and pose localization
Previous work has proposed explicit modeling of object part appearances and locations for more accurate recognition and localization. Starting with pictorial structures  #b17  #b18 , and continuing through poselets  #b7  and related work, many methods have jointly localized a set of geometrically related parts. The deformable parts model (DPM)  #b16 , until recently the state-of-the-art PASCAL object detection method, models parts with additional learned filters in positions anchored with respect to the whole object bounding box, allowing parts to be displaced from this anchor with learned deformation costs. The "strong" DPM  #b2  adapted this method for the strongly supervised setting in which part locations are annotated at training time. A limitation of these methods is their use of weak features (usually HOG  #b11 ).

Fine-grained categorization
We first present results on the standard fine-grained categorization task associated with the Caltech-UCSD birds dataset. The first set of results in Table 1 are achieved in the setting where the ground truth bounding box for the entire bird is known at test time, as most state-of-art methods assume, making the categorization task somewhat easier. In this setting, our part-based method with the local non-parametric geometric constraint Î´ N P works the best without fine-tuning, achieving 68.1% classification accuracy without fine-tuning. Finetuning improves this result by a large margin, to over 76%. We compare our results against three state-of-the-art baseline approaches with results assuming the ground truth bounding box at test time. We use deep convolutional features as the authors of  #b13 , but they use a HOG-based DPM as their part localization method. The increase in performance is likely due to better part localization (see Table 4). Oracle method uses the ground truth bounding box and part annotations for both training and test time.The second set of results is in the less artificial setting where the bird bounding box is unknown at test time. Most of the literature on this dataset doesn't report performance in this more difficult, but more realistic setting. As Table 1  Table 1. Fine-grained categorization results on CUB200-2011 bird dataset. -ft means extracting deep features from finetuned CNN models using each semantic part. Oracle method uses the ground truth bounding box and part annotations for both training and test time.Bounding Box Given DPD  #b47  50.98% DPD+DeCAF feature  #b13  64.96% POOF  #b6  56.78% Symbiotic Segmentation  #b9  59.40% Alignment  #b19  62 shows, in this setting our part-based method works much better than the baseline DPD model. We achieve 66.0% classification accuracy without finetuning , almost as good as the accuracy we can achieve when the ground truth bounding box is given. This means there is no need to annotate any box during test time to classify the bird species. With finetuned CNN models, our method achieves 73.89% classification accuracy. We are unaware of any other published results in this more difficult setting, but we note that our method outperforms previous state-of-the-art even without knowledge of the ground truth bounding box. Another interesting experiment we did is to remove the part descriptors by only looking at the image descriptors inside the predicted bounding box. By having geometric constraints over part locations relative to object location, our method is able to help localize the object. As Table 2 shows, our method outperforms a single object detector using R-CNN, which means the geometric constraints helps our method better localize the object window. The detection of strong DPM is not as accurate as our method, which explains the performance drop. The "oracle" method uses the ground truth bounding box and achieves 57.94% accuracy, which is still much lower than the method in Table 1 of using both image descriptors inside object and parts. 

Convolutional networks
In recent years, convolutional neural networks (CNNs) have been incorporated into a number of visual recognition systems in a wide variety of domains. At least some of the strength of these models lies in their ability to learn discriminative features from raw data inputs (e.g., image pixels), in contrast to more traditional object recognition pipelines which compute hand-engineered features on images as an initial preprocessing step. CNNs were popularized by LeCun and colleagues who initially applied such models to digit recognition  #b27  and OCR  #b28  and later to generic object recognition tasks  #b23 . With the introduction of large labeled image databases  #b22  and GPU implementations used to efficiently perform the massive parallel computations required for learning and inference in large CNNs, these networks have become the most accurate method for generic object classification  #b26 .Most recently, generic object detection methods have begun to leverage deep CNNs and outperformed any competing approaches based on traditional features. OverFeat  #b37  uses a CNN to regress to object locations in a coarse sliding-window detection framework. Of particular inspiration to our work is the R-CNN method  #b20  which leverages features from a deep CNN in a region proposal framework to achieve unprecedented object detection results on the PASCAL VOC dataset. Our method generalizes R-CNN by applying it to model object parts in addition to whole objects, which our empirical results will demonstrate is essential for accurate fine-grained recognition.

Part-based R-CNNs
While  #b20  demonstrated the effectiveness of the R-CNN method on a generic object detection task (PASCAL VOC), it did not explore the application of this method to simultaneous localization and fine-grained recognition. Because our work operates in this regime, we extend R-CNN to detect objects and localize their parts under a geometric prior. With hypotheses for the locations of individual semantic parts of the object of interest (e.g., the location of the head for an animal class), it becomes reasonable to model subtle appearance differences which tend to appear in locations that are roughly fixed with respect to these parts.In the R-CNN method, for a particular object category, a candidate detection x with CNN feature descriptor Ï(x) is assigned a score of w 0 Ï(x), where w 0 is the learned vector of SVM weights for the object category. In our method, we assume a strongly supervised setting (e.g.,  #b2 ) in which at training time we have ground truth bounding box annotations not only for full objects, but for a fixed set of semantic parts {p 1 , p 2 , ..., p n } as well.Given these part annotations, at training time all objects and each of their parts are initially treated as independent object categories: we train a one-versusall linear SVM on feature descriptors extracted over region proposals, where regions with â¥ 0.7 overlap with a ground truth object or part bounding box are labeled as positives for that object or part, and regions with â¤ 0.3 overlap with any ground truth region are labeled as negatives. Hence for a single object category we learn whole-object ("root") SVM weights w 0 and part SVM weights {w 1 , w 2 , ..., w n } for parts {p 1 , p 2 , ..., p n } respectively. At test time, for each region proposal window we compute scores from all root and part SVMs. Of course, these scores do not incorporate any knowledge of how objects and their parts are constrained geometrically; for example, without any additional constraints the bird head detector may fire outside of a region where the bird detector fires. Hence our final joint object and part hypotheses are computed using the geometric scoring function detailed in the following section, which enforces the intuitively desirable property that pose predictions are consistent with the statistics of poses observed at training time.

Geometric constraints
Let X = {x 0 , x 1 , . . . , x n } denote the locations (bounding boxes) of object p 0 and n parts {p i } n i=1 , which are annotated in the training data, but unknown at test time. Our goal is to infer both the object location and part locations in a previously unseen test image. Given the R-CNN weights {w 0 , w 1 , . . . , w n } for object and parts, we will have the corresponding detectors {d 0 , d 1 , . . . , d n } where each detector score is d i (x) = Ï(w i Ï(x)), where Ï(Â·) is the sigmoid function and Ï(x) is the CNN feature descriptor extracted at location x. We infer the joint configuration of the object and parts by solving the following optimization problem:X * = arg max X â(X) n i=0 d i (x i )(1)where â(X) defines a scoring function over the joint configuration of the object and root bounding box. We consider and report quantitative results on several configuration scoring functions â, detailed in the following paragraphs.Box constraints. One intuitive idea to localize both the object and parts is to consider each possible object window and all the windows inside the object and pick the windows with the highest part scores. In this case, we define the scoring functionâ box (X) = n i=1 c x0 (x i )(2)where c x (y) = 1 if region y falls outside region x by at most pixels 0 otherwiseIn our experiments, we let = 10.Geometric constraints. Because the individual part detectors are less than perfect, the window with highest individual part detector scores is not always correct, especially when there are occlusions. We therefore consider several scoring functions to enforce constraints over the layout of the parts relative to the object location to filter out incorrect detections. We defineâ geometric (X) = â box (X) n i=1 Î´ i (x i ) Î±(4)where Î´ i is a scoring function for the position of the part p i given the training data. Following previous work on part localization from, e.g.  #b3  #b16  #b18 , we experiment with three definitions of Î´:-Î´ M G i (x i )fits a mixture of Gaussians model with N g components to the training data for part p i . In our experiments, we set N g = 4.Î´ N P i (x i ) finds the K nearest neighbors in appearance space tox 0 , wheráº½ x 0 = arg max d 0 (x 0 ) is the top-scoring window from the root detector. We then fit a Gaussian model to these K neighbors. In our experiments, we set K = 20. Figure 2 illustrates some examples of nearest neighbors.The DPM  #b16  models deformation costs with a per-component Gaussian prior. R-CNN  #b20  is a single-component model, motivating the Î´ M G or Î´ N P definitions. Our Î´ N P definition is inspired by Belhumeur et al.  #b3 , but differs in that we index nearest neighbors on appearance rather than geometry. 

Evaluation
In this section, we present a comparative performance evaluation of our proposed method. Specifically, we conduct experiments on the widely-used fine-grained benchmark Caltech-UCSD birds dataset  #b41  (CUB200-2011). The classification task is to discriminate among 200 species of birds, and is challenging for computer vision systems due to the high degree of similarity between categories. It contains 11,788 images of 200 bird species. Each image is annotated with its bounding box and the image coordinates of fifteen keypoints: the beak, back, breast, belly, forehead, crown, left eye, left leg, left wing, right eye, right leg, right wing, tail, nape and throat. We train and test on the splits included with the dataset, which contain around 30 training samples for each species. Following the protocol of  #b47 , we use two semantic parts for the bird dataset: head and body.We use the open-source package Caffe  #b24  to extract deep features and finetune our CNNs. For object and part detections, we use the Caffe reference model, which is almost identical to the model used by Krizhevsky et al. in  #b26 . We refer deep features from each layer as convn, pooln, or fcn for the nth layer of the CNN, which is the output of a convolutional, pooling, or fully connected layer respectively. We use fc6 to train R-CNN object and part detectors as well as image representation for classification. For Î´ N P , nearest neighbors are computed using pool5 and cosine distance metric.

Part localization
We now present results evaluating in isolation the ability of our system to accurately localize parts. Our results in Table 4 are given in terms of the Percentage of Correctly Localized Parts (PCP) metric. For the first set of results, the whole object bounding box is given and the task is simply to correctly localize the parts inside of this bounding box, with parts having â¥ 0.5 overlap with ground truth counted as correct.For the second set of results, the PCP metric is computed on top-ranked parts predictions using the objective function described in Sec. 3.2. Note that in this more realistic setting we do not assume knowledge of the ground truth bounding box at test time -despite this limitation, our system produces accurate part localizations.As shown in Table 4, for both settings of given bounding box and unknown bounding box, our methods outperform the strong DPM  #b2  method. Adding a geometric constraint Î´ N P improves our results (79.82% for body localization compared to 65.42%). In the fully automatic setting, the top ranked detection and part localization performance on head is 65% better than the baseline method. â null = 1 is the appearance-only case with no geometric constraints applied. Although the fine-grained classification results don't show a big gap between â geometric and â box , we can see the performance gap for part localiza-  tion. The reason for the small performance gap might be that deep convolutional features are invariant to small translations and rotations, limiting the impact of small localization errors on our end-to-end accuracy.We also evaluate the recall performance of selective search region proposals  #b40  for bounding box and semantic parts. The results of recall given different overlapping thresholds are shown in Table 3. Recall for the bird head and body parts is high when the overlap requirement is 0.5, which provides the foundation for localizing these parts given the region proposals. However, we also observe that the recall for head is below 40% when the overlap threshold is 0.7, indicating the bottom-up region proposals could be a bottleneck for precise part localization.Other visualizations are shown in Figure 4. We show three detection and part localization for each image, the first column is the output from strong DPM, the second column is our methods with individual part predictions and the last column is our method with local prior. We used the model pretrained from  #b2  to get the results. We also show some failure cases of our method in Figure 5.

Component Analysis
To examine the effect of different values of Î± and K used in â geometric , we conduct cross-validation experiments. Results are shown in Figure 3. We fix K = 20 in Figure 3, left and fix Î± = 0.1 in Figure 3, right. All the experiments on conducted on training data in a cross-validation fashion and we split the training data into 5 folds. As the results show, the end-to-end fine-grained classification results are sensitive to the choice of Î± and Î± = 0 is the case of â box predictions without any geometric constraints. The reason why we have to pick a small Î± is the pdf of the Gaussian is large compared to the logistic score function output from our part detectors. On the other hand, the choice of K cannot be too small and it is not very sensitive when K is larger than 10.

Conclusion
We have proposed a system for joint object detection and part localization capable of state-of-the-art fine-grained object recognition. Our method learns detectors and part models and enforces learned geometric constraints between parts and with the object frame. Our experimental results demonstrate that even with a very strong feature representation and object detection system, it is highly beneficial to additionally model an object's pose by means of parts for the difficult task of fine-grained discrimination between categories with high semantic similarity. In future extensions of this work, we will consider methods which jointly model at training time the object category and each of its parts and deformation costs. We also plan to explore the weakly supervised setting in which we automatically discover and model parts as latent variables from only the object bounding box annotations. Finally, we will consider relaxing the use of selective search for smaller parts and employing dense window sampling.

Strong DPM
Ours (â box ) Ours (Î´ N P ) Fig. 4. Examples of bird detection and part localization from strong DPM  #b2  (left); our method using â box part predictions (middle); and our method using Î´ N P (right). All detection and localization results without any assumption of bounding box.