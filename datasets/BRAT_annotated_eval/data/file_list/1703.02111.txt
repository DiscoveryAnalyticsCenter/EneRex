Classification and clustering for observations of event time data using non-homogeneous Poisson process models

Abstract
Data of the form of event times arise in various applications. A simple model for such data is a non-homogeneous Poisson process (NHPP) which is specified by a rate function that depends on time. We consider the problem of having access to multiple independent observations of event time data, observed on a common interval, from which we wish to classify or cluster the observations according to their rate functions. Each rate function is unknown but assumed to belong to a finite number of rate functions each defining a distinct class. We model the rate functions using a spline basis expansion, the coefficients of which need to be estimated from data. The classification approach consists of using training data for which the class membership is known, to calculate maximum likelihood estimates of the coefficients for each group, then assigning test observations to a group by a maximum likelihood criterion. For clustering, by analogy to the Gaussian mixture model approach for Euclidean data, we consider mixtures of NHPP and use the expectation-maximisation algorithm to estimate the coefficients of the rate functions for the component models and group membership probabilities for each observation. The classification and clustering approaches perform well on both synthetic and real-world data sets. Code associated with this paper is available at https://github.com/duncan-barrack/NHPP.

Introduction
Much real world data is amenable to modelling as a non-homogeneous Poisson process (NHPP) (e.g. stock purchase transaction times  #b8 , email traffic  #b16  as well as computer network traffic  #b21 ). In various applications a natural unit of observation is a sample of event times observed on some common interval {t : 0 < t ≤ T } and a challenge is to label each observation as belonging to one of a small number of distinct classes. A particular example we consider later is the categorisation of stores by till transaction times. This can provide retailers valuable insights in to the different demands placed on different store types and help to inform various decisions from stock management to appropriate store staffing levels. Another example we consider is the categorisation of transportation hubs according to the times at which they are used. Such information helps inform the type and number of services (transportation, catering, retail and others) that should be made available at each type of hub. When a 'training set', containing observations with known class memberships is available, such a task is termed 'classification', when no labels are available for the data it is termed a 'clustering' problem  #b9 . Previous approaches to classification and clustering of event time data have focused on deriving summary statistics from the data such as the mean and variance of the inter-event times and using these as inputs in statistical learning models  #b19  #b23  #b18 . In addition to studies of temporal data a number of authors have considered spatial point processes. Illian et al used functional principal component analysis on second order summary statistics of spatial point process data to classify plant species  #b11  #b10 . Closer in approach to the present work  #b15  used a maximum likelihood (ML) criterion to classify the textures of images modelled using homogeneous Poisson processes (HPPs).HPPs, like those used in the work of  #b15  assume rates are constant for each class which is too restrictive for many applications. For example stock transaction times and human communication behaviour both exhibit periodic patterns  #b8  #b16 . Computer network traffic is better modelled by non-homogeneous NHPP models rather than HPPs  #b21 . Hence, in this work, we consider instead NHPP models, which are defined in terms of rate functions that vary with time. We model these rate functions using a spline basis expansion which imposes an assumption of smoothness on the functions and means estimating them amounts to estimating a finite number of basis coefficients. We go beyond previous work which has focused purely on rate function estimation  #b1  #b12  #b25  #b28  #b17  #b13  by considering the tasks of classification and clustering. For classification we show that once intensity estimates for the NHPP rate functions corresponding to each class are obtained by solving a convex optmisation problem, it is a simple task to obtain the posterior class membership probabilities for test data. For clustering, where class labels are not known a priori, we assume all data to have been generated from a mixture of k NHPP models. Again the rate functions for each model are represented by a combination of basis functions but his time the expectation-maximisation (EM) algorithm is used to obtain the coefficients of the basis functions as well as k membership probabilities for each distinct observation of event times. Our approaches are validated on synthetic and three real data sets.The paper is structured as follows. In Section 2 we describe our procedure for estimating NHPP rate functions. Then in Sections 3 and 4 we outline our classification and clustering methods. The results of applying these to synthetic and real data are shown in Sections 5. We conclude with a discussion in Section 6.

Rate function estimation
Suppose that we assume that a sample of event times χ = {χ 1 , χ 2 , . . . , χ mχ }, observed on the interval {t : 0 < t ≤ T }, arises from a NHPP with rate function λ(t), then the log likelihood for the rate function isl(λ(t)|χ) = −M (t) + mχ i=1 ln(λ(χ i )),(1)where M (t) = t 0 λ(t)dt (see for example  #b26 ). In estimating λ(t) in equation (1) we impose that it belongs to a class of smooth functions by assuming it is a linear combination of smooth basis functionsλ(t) = n b m=1 c m B m (t).(2)In this paper, for the basis functions B 1 (t), . . . , B n b (t), we use B-splines (De  #b5  which offer control over smoothness and have finite support. We estimate the basis function coefficients c = {c 1 , c 2 , . . . , c n b } by minimising −l(λ(t)|χ) i.e. by minimisingf (c) = −l(λ(t)|χ) = M (t) − mχ i=1 ln n b m=1 c m B m (χ i ) ,(3)subject to the constraint that the rate function λ(t) is non-negative i.e.g(c) = −λ(t) = − n b m=1 c m B m (t) ≤ 0.(4)Minimising (3) subject to (4) is a convex optimisation problem which we show as follows. First, we derive the gradient of f (c)f = ( 1 (f ), . . . , n b (f )) ,(5)wherej (f ) = ∂ ∂cj M (t) − mχ i=1 Bi(χi) n b m=1 cmBm(χi) . The n b × n b Hessian is H =    H 1,1 (f ) . . . H 1,n b (f ) . . . . . . . . . H n b ,1 (f ) . . . H n b ,n b (f )    ,(6)whereH s,u (f ) = mχ i=1 Bs(χi)Bu(χi) ( n b m=1 cmBm(χi)) 2 as ∂ 2 ∂cs∂cu M (t) = t t1 ∂ 2 ∂cs∂cu n b m=1 c m B m (t) dt = 0 for every s and u. As we may express H = X X, where the χ mχ × n b matrix X =      B1(χ1) n b m=1 cmBm(χ1) . . . Bn b (χ1) n b m=1 cmBm(χ1) . . . . . . . . . B1(χm χ ) n b m=1 cmBm(χm χ ). . .Bn b (χm χ ) n b m=1 cmBm(χm χ )      ,(7)thus H is positive semi-definitive as y Hy = y X Xy = (yX) (Xy) = ||Xy|| 2 ≥ 0 for every non-zero column vector y = {y 1 , y 2 , . . . , y n b } of real numbers. Therefore f (c) (equation (3)) is convex in c. Constraint (4) is linear in c and hence convex in c. Hence estimating the basis function coefficients of the NHPP rate function λ(t) is a convex optimisation problem.Furthermore, as f (c) and g(c) are twice differentiable, the minimisation to perform is straightforward using anyone of several numerical techniques designed for solving non-linear convex optimisation problems where the objective function and inequality constraint are both twice differentiable. For the results in this paper we used the interior point method  #b3  #b4  #b27 .

Classification
Using the same CV procedure as outlined in Section 5.2.1, we obtained an average classification and true positive rates for both classes of 1. The estimated rate functions are shown in Figure 4 from which it can be seen that admissions for poisoning peak around midnight. This is most likely due to instances of Bike trip start times Figure 3: Classification and clustering results for k = 2 ('member' and 'casual' station classes) and 3 ('morning (AM) member', 'afternoon (PM) member' and 'casual' station classes) for the Hubway bike share data. Plots show the performance of the classification and clustering methods as a function of κ (see text for details of this parameter) and estimated rate functions for the station classes for κ = 20. Estimated rate functions for stations with ids 9 and 15, which were the most misclassified 'member' stations for k = 2, are also shown. alcohol poisoning amongst revellers in the late evenings. Cardiac conditions peak in the morning, which is consistent with evidence in the medical literature  #b22 .

Classification of test data
The posterior probability that a test observation χ j is a member of class ν can be derived from Bayes' theoremP (g j = ν|χ j ) = e − Mν (t) mj i=1 λ ν (χ j,i ) k q=1 e − Mq(t) mj i=1 λ q (χ j,i ) .(10)Observations are then assigned to the class for which this probability is maximal, i.e. g = argmax ν {P (g j = ν|χ j )}.

Clustering
Estimated rate functions from the cluster analysis are also shown in Figure  4 and these are very similar to the estimates obtained from the classification procedure. In particular, instances of poisoning peak around night time and cardiac conditions reach a high point in the morning. Assigning poisoning and cardiac months to the model for which their membership probabilities are maximal gives a clustering accuracy of 100%.  Figure 4: Times of hospital admissions for a selection of observations as well as estimated rate functions for the A&E diagnoses of poisoning (blue) and cardiac (red) obtained from our classification and clustering methods with k = 2. The rate function estimates indicate that poisoning instances peak around midnight and cardiac conditions are maximal in the morning.

Expectation step
Given a current estimate for the model parameters θ c we find the so called 'auxiliary function' which is the expected value of the data log likelihood (11) of the mixture model with respect to the conditional distribution of z given the data D.Q(θ, θ c ) = E z|D, θ c [l(θ|D, z)] = n l=1 E z|D, θ c [l(θ|χ l , z l )], = n l=1 k q=1 p(z l = q|x l , θ c )l(θ|x l , z l ), = n l=1 k q=1 R c q,l (ln(τ q ) + l(λ q (t)|χ l ))) .(13)HereR c ν,l = p(z l = ν|χ l , θ c ) is the 'membership probability' of observation l to group ν. l(λ ν (t)|χ l ) = −M ν (t) + m l i=1 λ ν (χ l,i )is the log likelihood for rate function ν for observation l. Plugging in this latter expression for the log likelihood into (13) we getQ(θ, θ c ) = n l=1 k q=1 R c q,l ln(τ q ) − M q (t) + m l i=1 ln(λ q (χ l,i )) ,(14)To obtain the membership probabilities for each observation l of model ν we regard the mixing weights τ as prior probabilities of each mixture component and use Baye's theorem to obtainR c ν,l = p(z l = ν|χ l , θ c ) = p(z l = ν)p(χ l |z l = ν, θ c ) k q=1 p(z l = q)p(χ l |z l = q, θ c ) = τ c ν e − M c ν (t) m l i=1 λ c ν (χ l,i ) k q=1 τ c q e − M c q (t) m l i=1 λ c q (χ l,i ) .(15)

Maximisation step
After obtaining values for the membership probabilities, we obtain subsequent estimates for the parameters θ c+1 by finding the θ which maximise the auxiliary function (14) subject to the constraint that the mixing weights must sum to 1, i.e. k q=1 τ q = 1. To address this constraint we consider the Lagrange functionL(θ, θ c , δ) = n l=1 k q=1 R c q,l ln(τ q ) − M q (t) + m l i=1 ln(λ q (χ l,i )) + δ k q=1 τ q − 1 ,(16)where δ is the Lagrange multiplier. Updates for the mixing weights τ are found by taking the partial derivative of (16) with respect to τ ν , setting it to zero and solving for τ . The partial derivative of L(θ, θ c , δ) is∂L(θ, θ c , δ) ∂τ ν = ∂ ∂τ ν n l=1 k q=1 R c q,l ln(τ q ) − M q (t) + m l i=1 ln(λ q (χ l,i )) + ∂ ∂τ ν δ k q=1 τ q − 1 .(17)Evaluating (17) and removing all constant terms we get∂L(θ, θ c , δ) ∂τ ν = n l=1 d dτ ν R c ν,l ln(τ ν ) + d dτ ν δτ ν , = n l=1 R c ν,l τ ν + δ.(18)Setting equation (18) to 0 and solving for τ ν we getτ ν = n l=1 R c ν,l −δ .(19)Using the constraint that k q=1 τ q = 1, then −δ = n l=1 k q=1 R c q,l = n l=1 1 = n. Hence the updated estimate for the mixing weight is τ c+1ν = n l=1 R c ν,ln . Next we seek the C which maximise L(θ, θ c , δ) subject to the constraints that all rate functions are non-negative, i.e.n b m=1 c q,m B m (t), . . . , n b m=1 c k,m B m (t) ≥ 0,(20)for all q = 1, . . . , k. This is a convex minimisation problem of the same variety as encountered in Section 2. To see this, consider the term (16). f (c ν , l) is identical to equation (3) which we showed is convex in the set of basis function coefficients in Section 2. As R ν,l ≥ 0 for every ν and l,−f (c ν,l )= −M q (t) + m l i=1 ln ( n b m=1 c q,m B m (χ l,i )) from equationR ν,l f (c ν,l ) = R ν,l (M q (t) − m l i=1 ln( n b m=1 c q,m B m (χ l,i ))) is also convex. Furthermore, disre- garding terms which do not involve C, −L(θ, θ c , δ) = n l=1 k q=1 R c q,l f (c ν,l )is convex in C as sums of convex functions are also convex. The functions in the constraints given in (20) are linear in C and hence convex. Therefore to find the C which maximise L(θ, θ c , δ) (16) we can use the same numerical approach as used to minimise (3). The expectation and maximisation steps are applied iteratively until the the change in value of the Lagrangian function from one step to the next is less than a prescribed threshold. We used a value of 1e-4 for the results in this paper.

Initialisation
Depending on the starting conditions the EM algorithm may converge to a local optimum. To overcome this issue we use a form of the random restart approach commonly used with the EM algorithm for Gaussian mixtures  #b2  described as follows. To set initial estimates for C, each observation χ l is randomly assigned a class label g l ∈ 1, 2, . . . , k. Rate function estimates for the 'random' classes are obtained using the classification procedure outlined in Section 3 and the basis function estimates which correspond to these are used for the initial estimates for C. All mixing weights in τ are initially set to 1/k. The EM algorithm is then initialised from a number of different sets of initial conditions corresponding to different 'random' class assignments and the solution with the largest log likelihood is retained. Using this procedure, for all the examples considered in this paper, 3 random restarts were typically plenty to avoid local maxima.

Empirical results
In this section we present the results of our methods applied to various synthetic and real world data sets.

Synthetic data
We first applied our methods to synthetically generated event data with prescribed rate functions. None of the data generating rate functions belong to the class of smooth functions we assumed in our methods. We generated three difference data sets based on the following four rate functions.λ 1 (t) = 100sin 2 (t/2) λ 2 (t) = 100sin 2 (t) λ 3 (t) =          20 if t < t2 4 40 if t2 4 ≤ t < t2 2 60 if t2 3 ≤ t < 3t2 4 80 if t ≥ 3t2 4 λ 4 (t) =          80 if t < t2 4 60 if t2 4 ≤ t < t2 2 40 if t2 3 ≤ t < 3t2 4 20 if t ≥ 3t2 4For synthetic data set 1 we generated event times for observations with class label 1 (resp. 2) separately using rate λ 1 (t) (λ 2 (t)). As λ 1 (t) and λ 2 (t) are both periodic functions this experiment mimics real world examples of point processes which exhibit periodic patterns (e.g. time of stock trades  #b8  and the times at which emails are sent  #b16 ).For synthetic data set 2, λ 3 (t) (resp. λ 4 (t)) was used to generate class 1 (2) observations. These rate functions mimic computer network traffic for which a NHPP model with rate function defined by a step function is a suitable model  #b21 .Finally for data set 3 we considered a four class problem where rate functions λ 1 (t), λ 2 (t), λ 3 (t) and λ 4 (t) were used to generate data in classes 1,2,3 and 4 respectively. The thinning algorithm (Lewis and Shedler, 1979) was used to generate all data. Synthetic event time data for 20 observations for each rate function is shown in Figure 1.

Retail transaction data results
Next we introduce our first real data set which is made up of the till transaction times from the 6 th of September 2011 to the 28 th of February 2015 for 74 UK stores (10,000 transactions per store) belonging to a large UK retailer. Half of these stores have been categorised by the retailer as 'High street' (HS) which are large stores found in city centres and the remainder as 'travel' stores which are situated in airports and railway stations. These categorisations serve as the ground truth for our analysis.

Till transaction times Classification results
Sep  Figure 2: Retail store results. The first row of plots include raster plots of till transaction event times for a selection of stores for each store class ('high street store' and 'travel store' ). Also included within this row is a plot of the estimated rate functions for travel (blue) and high street (red) stores obtained from the classification procedure. These rate functions indicate peak demand for travel stores is around summer while demand peaks around Christmas for high streets stores. The final row of plots are the estimated rate functions obtained from the clustering method for a mixture of 2 and 3 NHPP models. For the clustering results for k = 2 the blue rate function peaks around summer and the membership probabilities for most travel stores are greatest for this model. The red function peaks around Christmas and this rate function corresponds predominantly to high street stores.

Hubway bike share data set
This data set contains bike trip information for the Hubway bike share program located in Boston, Massachusetts and its environs. The data, which covers the period from the 28 th of July 2011 to the 30 th of November 2013, contains trip duration time, date and time of the trip as well the the id of the start and finishing bike share stations of cycle journeys. Additionally the user type (casual -'24-hour or 72-hour pass user' or member -'annual or monthly member') was also recorded. In our analysis we ranked each station according to the proportion of station users classed as either casual or members and classified the κ stations with the highest casual-member proportions as 'casual' stations and the κ stations with the lowest casual-member proportions as 'member' stations. We only considered stations with more than 5,000 recorded trips to avoid any spurious results caused by stations with only a handful of recorded trips. This left a total of 89 station observations. The times of day at which the bike trips began at each station served as the sample of event times for each station observation.

A&E data set
This data set contains the time of day (00:00:00 to 23:59:59) in every month from April 2011 to the end of December 2014 at which patients arrived at Accident and Emergency (A&E) departments in hospitals throughout England. Each patient was diagnosed as either suffering from poisoning (including alcohol poisoning) or a cardiac condition and these are used as the ground truths for the class labels. For each label there are 45 observations corresponding to each of the 45 months spanning April 2011 to the end of December 2014. Each monthly observation contains 10,000 A&E admission events.

Hospital admissions times
We have made Matlab code for the classification and clustering procedures outlined in Sections 3-4 and for the synthetic data set results in Section 5.1 available at https://github.com/duncan-barrack/NHPP. The Hubway bike share data set is available at https://www.thehubway.com/system-data. The store data and NHS A&E data are not publicly available.

Discussion
In this paper, we have detailed principled approaches for the classification and clustering of observations of event data using NHPP models. Results on synthetic and real data were presented which show the effectiveness of our methods. The focus of this work has been on temporal point process data observed over a fixed interval {t : 0 < t ≤ T }. Another possibility, particularly suited for periodic data, would be to assume the domain of the data is periodic (e.g. a circle). The approach we have described here should generalise easily to the circular case. Furthermore, our method is not restricted to temporal data and, in principle, can be generalised to multi-dimensional spatial and spatial-temporal point process data.For the clustering procedure choosing the appropriate number of models in the NHPP mixture may not always be straightforward. In principle, the Bayesian information criterion  #b24  or Akaike information criterion  #b0  could be used for this. Future work could investigate the suitability of these methods for choosing the number of NHPP components for our clustering procedure.