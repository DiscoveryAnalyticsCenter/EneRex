DISTRIBUTED MAPPER

Abstract
The construction of Mapper has emerged in the last decade as a powerful and effective topological data analysis tool that approximates and generalizes other topological summaries, such as the Reeb graph, the contour tree, split, and joint trees. In this paper we study the parallel analysis of the construction of Mapper. We give a provably correct algorithm to distribute Mapper on a set of processors and discuss the performance results that compare our approach to a reference sequential Mapper implementation. We report the performance experiments that demonstrate the efficiency of our method.

Introduction and Motivation
The topology of data is one of the fundamental originating principle in studying data. Consider the classical problem of fitting data set of point in R n using linear regression. In linear regression one usually assumes that data is almost distributed near a hyperplane in R n . See Figure 1 (a). If the data does not meet this assumption then the model chosen to fit the data may not work very well.(b) (c) (a) Figure 1. (a) The linear shape of the data is a fundamental assumption underlying the linear regression method. (b) Clustering algorithms assume that the data is clustered in a certain way. (c) Data can come in many other forms and shapes.On the hand if the clustering algorithms make the shape assumption that the data forms into clusters. See Figure 1 (b). Data can come in many other forms and shapes, see Figure 1 (c). It is the shape of data  #b8  that drives the meaning of the analytical methods and underlines the fundamental assumptions.Topology is the field in Mathematics that rigorously defines and studies the notion of shape. Over the past two decades, topology has found enormous applications in data analysis and the application of topological techniques to study data has become is now considered a vibrant area of research called as Topological Data Analysis (TDA)  #b7  #b8  #b9  #b10  #b11  #b12  #b16 . Many popular tools have been invented in the last two decades to study the shape of data, most notably Persistence Homology  #b19  #b40  and the construction of Mapper  #b44 . Persistent Homology has been successfully used to study a wide range of data problems including three-dimensional structure of the DNA  #b20 , financial networks  #b22 , material science  #b27  and many other applications  #b38 . The construction of Mapper has emerged recently as a powerful and effective topological data analysis tool to solve a wide variety of problems  #b30  #b37  and it has been studied from multiple points of view  #b13  #b18  #b34 . Mapper works as a tool of approximation of a topological space by mapping this space via a "lens", or a sometimes called a filter, to another domain. One uses properties of the lens and the range to then extract a topological approximation of the original space. We give the precious notion in section 3. Mapper generalizes other topological summaries such as the Reeb graph, the contour tree, split, and joint trees. Moreover, Mapper is the core software developed by Ayasdi, a data analytic company whose main interest is promoting the usage of methods inspired by topological constructions in data science applications.As the demand of analyzing larger data sets grows, it is natural to consider parallelization of topological computations. While there are numerous parallel algorithms that tackle the less general topological constructions, such as Reeb graph and contour tree, we are not aware of similar attempts targeting the parallel computation of Mapper in the literature. Our work here is an attempt to fill in this gap.This article addresses the parallel analysis of the construction of Mapper. We give a provably correct algorithm to distribute Mapper on a set of processors and discuss the performance results that compare our approach to a reference sequential implementation for the computation of Mapper. Finally, we report the performance analysis experiments that demonstrate the efficiency of our method.

Prior Work
While there are numerous algorithms to compute topological constructions sequentially, the literature of parallel computing in topology is relatively young. One notable exception is parallelizing Morse-Smale complex computations  #b25  #b43 . Parallelization of merge trees is studied in  #b23  #b31  #b39  #b42 . Other parallel algorithms in topology include multicore homology computation  #b28  spectral sequence parallelization  #b29 , distributed contour tree  #b32 . There are several other attempts to speed up the serial computation of topological constructions including an optimized Mapper sequential algorithm for large data  #b45 , a memory efficient method to compute persistent cohomology  #b5 , efficient data structure for simplicial complexes  #b1 , optimized computation of persistent homology  #b14  and Morse-Smale complexes  #b24 .

Preliminaries and Definitions
We start this section by recall basic notions from topology. For more details the reader is referred to standard texts in topology. See for instance  #b35 . All topological spaces we consider in this paper will be compact unless otherwise specified. An open cover of a topological space is a collection of open sets U = {A α } α∈I such that ∪ α∈I A α = X. All covers in this article will consist of a finite number of sets unless otherwise specified. Given a topological space X with a cover U, one may approximate this space via an abstract simplicial complex construction called the nerve of the cover U. The nerve of a cover is a simplicial complex whose vertices are represented by the open sets the cover. Each non-empty intersection between two sets in the cover defines an edge in the nerve and each non-empty intersection between multiple sets defines higher order simplicies. See Figure 4 for an illustrative example. Under mild conditions the nerve of a cover can be considered as an approximation of the underlying topological space. This is usually called the Nerve Theorem  #b21 . The Nerve Theorem plays an essential role in TDA: it gives a mathematically justified approximation of the topological space, being thought as the data under study, via simplicial complexes which are suitable for data structures and algorithms. In  #b44  Singh et al proposed using a continuous map f : X −→ Z to construct a nerve of the space X. Instead of covering X directly, Singh et al suggested covering the codomain Z and then use the map f to pull back this cover to X. This perspective has multiple useful points of view. On one hand, choosing different maps on X can be used to capture different aspects of the space X. In this sense the function f is thought of as a "lens" or a "filter" in which we view the space X. On the other hand, fixing the map f and choosing different covers for the codomain Z can be used to obtain multi-level resolution of the Mapper structure. This has been recently studied in details in  #b17  #b18  and utilized to obtain a notion of persistence-based signature based on the definition of Mapper.As we mentioned in the introduction, Mapper is related to Reeb graphs. In fact, it has been proven that under certain conditions Mapper and the Reeb graph give essentially the same topological summery for the underlying space  #b13  #b34 . While Mapper is a more general topological construction than Reeb graph, we choose to motivate the main idea behind Mapper by illustrating its relationship with Reeb graph because the latter has found numerous applications. See for instance  #b4  and the references therein.For the sake of illustration X will be considered to be a surface. Let f : X −→ [a, b] be a Morse scalar function on X. The Reeb graph R(X, f ) of the Morse function f defined on X is constructed as follows. Define the equivalence relation ∼ on X by x ∼ y if and only if x and y belong to the same connected component of a level set f −1 (c) for the same c ∈ [a, b]. The set X/∼ with the standard quotient topology induced by the function π : X −→ X/∼ is called the Reeb graph of f . See Figure 2 for an illustrative example of a Reeb graph on a surface. If X is an embedded surface, orientable and without a boundary then X can be recovered up to a homeomorphism from R(X, f ) as the boundary of an oriented 3-dimensional regular neighborhood of the graph R(X, f )  #b26 . This is a perspective from which a Reeb graph may be considered as a topological summary of the original space. Other properties of Reeb graph can be found in  #b4  #b15 . Due to the successfulness of Reeb graph in various applications on meshes, there have been several attempts to give similar constructions on point clouds. In  #b36  Natali et el gave a method to mimic the Reeb graph construction for a point cloud in R 3 . Similar attempts were made in  #b47 . See also  #b3  and the references within. The problem with most of these methods is the lack of theoretical justification. Moreover it is not clear how these constructions can be generalized to a higher dimensional data. Besides being theoretically justified and applicable to data of any dimension, the Mapper construction gives a data structure that generalizes both Reeb graphs and merge/split trees and operates in practice on a metric space. We illustrate here how the construction of Mapper can realize in the special case of the Reeb graph of a space. We need first the following definition. The connected-components of the inverse images are identified as well as the intersection between these sets. (c) Mapper is defined as a graph whose vertices represent the connected component and whose edge represent the intersection between these components.N 1 (f (U)) f (U) U X (a) (b) (c)Definition 3.1. Let X be a topological space and let U be an open cover for X. The 1-nerve N 1 (U) of U is a graph whose nodes are represented by the elements of U and whose edges are the pairs A, B of U such that A ∩ B = ∅.N (U) U X Figure 4. Each open set defines a vertex in the nerve simplicial complex. Each non-empty intersection between two sets define an edge and each non-empty intersection between multiple sets define higher order simplicies.A scalar function f on X and a cover for the codomain [a, b] of f give rise to a natural cover of X in the following way. Start by defining an open cover for the interval [a, b] and take the inverse image of each open set to obtain an open cover for X. This is illustrated in Figure 3 (a). In otherwords if U = {(a 1 , b 1 ), ..., (a n , b n )} is a finite collection of open sets that cover the interval [a, b] then f (U) := {f −1 ((a 1 , b 1 )), ..., f −1 ((a n , b n ))} is an open cover for the space X.The open cover f (U) can now be used to obtain the 1-nerve graph N 1 (f (U)). With an appropriate choice of the cover U, the graph N 1 (f (U)) is a version of the Reeb graph R(X, f )  #b13  #b34 . This is illustrated in Figure 3. Clearly, for a fixed function f the graph N 1 (f (U)) depends on the choice of the cover U of the interval [a, b]. Figure 5 shows how different choices affect the graph G.Observe that the different covers for [a, b] give various "resolution" of the graph N 1 (f (U)). The idea of mapper presented in Definition 3.1 can be generalized to encompass a larger set of problems. One can replace the interval [a, b] in Definition 3.1 by any parametization domain Z to obtain more sophisticated insights on the data X. This requires introducing the definition of a nerve of a cover of a topological space. Definition 3.2. Let X be a topological space and let U be a finite cover for X. The nerve of U is the abstract simplicial complex N (U) whose vertices are the elements of U and whose simplicies are the finite subcollections A 1 , ...., A k of U such that :A 1 ∩ ... ∩ A k = ∅.In this paper we will deal with nerves of multiple topological spaces simultaneously. For this reason we will sometimes refer to the nerve of a cover U of a space X by N (X, U). Figure 4 shows an illustrative example of nerve on a topological space X. We will denote the vertex in N (U) that corresponds to an open set A in U by v A .Let f : X −→ Z be a continuous map between two topological spaces X and Z. Let U be a finite cover of Z. The cover that consists of f −1 (U ) for all open sets U ∈ U will be called the pullback of U under f and will be denoted by f * (U). A continuous map f : X −→ Z is said to be well-behaved if the inverse image of any path-connected set U in Z, consists of finitely many path-connected sets in X  #b17 . All maps in this paper will be assumed to be well-behaved. 3.1. Some Graph Theory Notions. Our construction requires a few definitions from graph theory. We include these notions here for completeness. See  #b2  for a more thorough treatment.Definition 3.4. Let G = (V, E) be a graph.Let ∼ be an equivalence relation defined on the node set V . The quotient graph of G with respect to the equivalence relation is a graph G/ ∼ whose node set is the quotient set V / ∼ and whose edge set is The quotient graph induced by ∼ is the cyclic graph C 3 . See Figure 6.{([u], [v])|(u, v) ∈ E}.We will also need the definition of disjoint union of two graphs. We will denote to the disjoint union of two sets A and B by A B.Definition 3.5. Let G 1 = (V 1 , E 1 ) and G 2 = (V 2 , E 2 ) be two graphs. The disjoint union of G 1 and G 2 is the graphG 1 G 2 defined by (V 1 V 2 , E 1 E 2 ).

Distributed Computing of Topological Constructions
The idea of parallelizing the computation of Mapper lies in decomposing the space of interest into multiple smaller subspaces. The subspaces will be chosen to overlap on a smaller portion to insure a meaningful merging for the individual pieces. A cover of each space is then chosen. Each subspace along with its cover is then processed independently by a processing unit. The final stage consists of gathering the individual pieces and merging them together to produce the final correct Mapper construction on the entire space. Since Mapper is essentially a nerve of a certain cover we will start our discussion by laying our some theorems that will facilitate the computation of nerve of a topological space on two units. Our discussion here operates on a general topological space X. The algorithmic aspects will be discussed in later sections.  Figure 7. Let U and V be covers for the spaces Y and Z respectively. The cover U ∪ V is clearly a cover for X. Under which conditions can we build the 1-nerve N 1 (X, U ∪V) from the 1-nerves N 1 (Y, U) and N 1 (Z, V)? This is an important question for us because we want to be able to build the 1-nerve N 1 (X, U ∪ V) from the "building blocks" N 1 (Y, U) and N 1 (Z, V).A natural initial choice is to consider the disjoint union of N 1 (Y, U) and N 1 (Z, V). The disjoint union N 1 (Y, U) N 1 (Z, V) shares some vertices and edges with N 1 (X, U ∪ V) but these two graphs are not identical in general. The problems that could arise when trying to rebuild N 1 (X, U ∪ V) from the disjoint union of N 1 (Y, U) and N 1 (Z, V) are the following :(1) There may exist some duplicate nodes in A, B with A ∈ U but A ∈ V and B ∈ V but B ∈ U. Such an edge will not exist inN 1 (Y, U) N 1 (Z, V).The following definition addresses the previous problems and allows a correct reconstruction of N 1 (X, U ∪ V) from N 1 (X, U) and N 1 (X, V).Definition 4.1. Let X be a topological space. Let Y, Z be two subspaces of X such that X = Y ∪Z and Y ∩ Z = ∅. Suppose that the intersection Y ∩ Z consists of a finite collection of maximal pathconnected open sets A 1 , · · · , A n . Let U, V be open covers for Y and Z respectively. We say that the covers U and V are nerve-consistent if the following two conditions are satisfied :(1) A ∈ U ∩ V for every A in {A 1 , · · · , A n }. (2) If U ∈ U and V ∈ V such that U ∩ V = ∅, then U = V ∈ {A 1 , · · · , A n }.Note that it is necessary to impose the condition of maximal path-connectedness on the open sets of A := {A 1 , · · · , A n } in the definition above. Otherwise, the opens sets that form A may not be unique and the nerve-consistency definition would have to depend on the open sets that form A. In the definition above nerve-consistency depends only on the spaces Y and Z. Moreover, this condition implies A ∩ B = ∅ for every A, B ∈ A with A = B because otherwise that would contradict the fact that A and B are maximal path-connected. We now have the following theorem. Theorem 4.2. Let X be a topological space. Let Y and Z be two open sets of X such that X = Y ∪ Z and Y ∩ Z = ∅ intersecting on a finite union of maximal path-connected open sets A 1 ∪ · · · ∪ A n . Let U, V be open covers for Y and Z that are nerve-consistent. Then N 1 (X, U ∪ V) = (N 1 (Y, U) N 1 (Z, V))/ ∼ where v A ∼ v B for all A ∈ U and B ∈ V such that A = B and A ∈ U ∩ V.Proof. We show that both N 1 (X, U ∪V) and (N 1 (Y, U) N 1 (Z, V))/ ∼ have essentially the same node and edge sets.Let A = {A i } n i=1 . Every node v A in N 1 (X, U ∪ V) corresponds to an open set A in U ∪ V. If A is not in A then there is a unique corresponding node of v A in (N 1 (Y, U) N 1 (Z, V))/ ∼. Otherwise A ∈ A.In this case A must be in both U and V, since the covers U and V are nerveconsistent. Hence there are two copies of the vertex v A inside N 1 (Y, U) N 1 (Z, V). By the definition of the equivalence relation ∼ these two nodes are identified in N 1 (Y, U) N 1 (Z, V)/ ∼. Hence there is a one to one correspondence between the node sets of N 1 (X, U ∪ V) and N 1 (Y, U) N 1 (Z, V)/ ∼. On the other hand let (v A , v B ) be an edge in N 1 (X, U ∪ V). We distinguish four cases:• If A and B are in U but not in V then there is a unique edge that corresponds to(v A , v B ) in N 1 (Y, U) N 1 (Z, V)/ ∼. •The case when A and B are in V but not in U is similar to case (2).• The case when A ∈ U and B ∈ V. We show that no such edge can exist in N 1 (X, U ∪ V).Assume otherwise that (v A , v B ) an edge in N 1 (X, U ∪ V) then we must have A ∩ B = ∅.Since U and V are nerve-consistent then we have A = B. • The case when when A ∈ V and B ∈ U is similar to the previous case.With this in mind, we now address the problem of distributed Mapper on two concurrent units. For this reason also have the following condition : if U 1 ∈ U 1 and U 2 ∈ U 2 such that U 1 ∩ U 2 = ∅ then U 1 ∩ U 2 = A. See Remark 4.4 below. (3) We compute the Mapper construction on the covers f * (U i ) for i = 1, 2. We obtain two graphs G 1 and G 2 . See Figure 8 (b). (4) We merge the graphs G 1 , G 2 as follows. By the construction of A, U 1 and U 2 , the set A presents in both covers U i , i=1,2. Let C 1 , ..., C n be the path-connected components of f −1 (A). Since A appears in both of the covers every connected component C i in f −1 (A) occurs in both graphs G 1 and G 2 . In other words, the nodes v 1 , ..., v n that corresponds to the components C 1 , ..., C n occur in both G 1 and G 2 where each vertex v i corresponds to the set C i . The merge of the graph is done by considering the disjoint union G 1 G 2 and then take the quotient of this graph by identifying the duplicate nodes v 1 , ..., v k presenting in both G 1 and G 2 . See Figure 8 (c). The steps of the previous algorithm are summarized in Figure 8. Remark 4.4. Note that the covers U 1 and U 2 we chose for the sets A 1 and A 2 are nerve-consistent in the sense of Definition 4.1. Figure 8. The steps of the distributed Mapper on two units. (a) The space X is decomposition based on a decomposition of the codomain (b) Each part is sent to a processing unit and the Mapper graphs are computed on the subspaces (c) The graphs are merged by identifying the corresponding the nodes.A 1 A 2 A (a) (b) (c) G 1 G 2 GIt is immediate from the construction of the distributed Mapper on two units above that the covers f * (U 1 ) and f * (U 2 ) are nerve-consistent. We record this fact in the following Lemma.   4.6. N 1 (f * (U 1 ) ∪ f * (U 2 )) = (N 1 (f * (U 1 ) N 1 (f * (U 2 ))/ ∼ where v C ∼ v D for all C ∈ f * (U 1 ) and D ∈ f * (U 1 ) such that C = D and C ∈ f −1 (A 1 ∩ A 2 ). Now define an N -chain cover of [a, b] to be a cover U of [a, b] that consists of N open intervals A 1 , ..., A N such that A i,j := A i ∩ A j = ∅

The Design of the Algorithm
In this section we discuss the computational details of the distributed Mapper algorithm we discussed in the previous section from the topological perspective. Before we give our algorithm we recall quickly the reference sequential version. 5.1. The Sequential Mapper Algorithm. The serial Mapper algorithm can be obtained by a straightforward change of terminology of the topological mapper introduced in Section 3. To this end, the topological space X is replaced by the data under investigation. The lens, or the filter, f is chosen to reflect a certain property of the data. Finally, the notion of path-connectedness is replaced by an appropriate notion of clustering. This is algorithm is summarized in the Algorithm 1.

Algorithm 1: Sequential Mapper [45]
Input: A dataset X with a notion of metric between the data points; a scalar function f : X −→ R n ; a finite cover U = {U 1 , ..., U k } of f (X); Output: A graph that represents N 1 (f (U)).

The Main Algorithm.
We now give the details of the Distributed Mapper algorithm. To guarantee that the output of the distributed Mapper is identical to the sequential Mapper we need to do some processing on the cover that induces the final Mapper output. In distributed Mapper, we consider an N -chain cover A 1 , · · · , A N of the interval [a, b] along with the their cover U 1 ,...,U N . The justification of the previous choice of covers will be given when we discuss the correctness of Algorithm 3. The details of the cover preprocessing are described in Algorithm 2.

Algorithm 2: Cover Preprocessing
Input: A point cloud X; a scalar function f : X −→ [a, b]; a set of N processors (P); Output: A collection of pairs {(A i , U i )} N i=1 where {A i } N i=1 is an N -chain cover of [a, b] and U i is a cover of A i . 1 Construct an N -chain cover of [a, b]. That is, cover [a, b] by N open intervals A 1 , · · · , A N such that A i,j := A i ∩ A j = ∅U i ∩ U i+1 = {A i,i+1 } and (2) if U i ∈ U i and U i+1 ∈ U i+1 such that U i ∩ U i+1 = ∅ then U i ∩ U i+1 = A i,i+1 for each i = 1, ..., N − 1;After doing the preprocessing of the cover and obtaining the collection{(A i , U i )} N i=1 , every pair (A i , U i )is then mapped to a specific processor P i which performs some calculations to produce a subgraph G i . At the end, we merge the subgraphs into one graph G. The details of the algorithm are presented in Algorithm 3.

Algorithm 3: Distributed Mapper
Input: A point cloud X; a scalar function f : X −→ [a, b]; a set of N processors (P); a collection of pairs {(A i , U i )} N i=1 obtained from the cover preprocessing algorithm; Output: Distributed Mapper Graph.1 for ( i ← 1 to i = N ) do 2 P i ← (A i , U i ); //Map each A i ,and its cover U i to the processor P i .  $b3  Determine the set of point X i ⊂ X that maps to A i via f and run the sequential Mapper construction concurrently on the covers (f | X i ) * (U i ) for i = 1, .., N . We obtain N graphs G 1 , ...G N . If N = 1, return the graph G 1 ; 4 Let C i j 1 , ..., C i j i be the clusters obtained from f −1 (A i,i+1 ). These clusters are represented by the vertices v i j 1 , ..., v i j i in both G i and G i+1 (each vertex v i k corresponds to the cluster C i k ) by the choice of the coverings U i and U i+1 ; 5 Merge the graphs G 1 , ..., G N as follows. By the construction of A i,i+1 , U i and U i+1 , each one of the sets f * (U i ) and f * (U i+1 ) share the clusters C i j k in f * (A i,i+1 ) . Hence C i j k is represented by a vertex in both graphs G i and G i+1 . The merging is done by considering the of the disjoint union graph G 1 ... G N and then take the quotient of this graph that identifies the corresponding vertices in G i and G i+1 for 1 ≤ i ≤ N − 1.

5.3.
Correctness of the Algorithm. The correctness of the previous algorithm follows basically from Corollary 4.6. That being said, we give here a detailed proof of the correctness that discusses the steps of the algorithms in details.Proposition 5.1. The Distributed Mapper algorithm returns a graph identical to the sequential Mapper.Proof. We will prove that the Distributed Mapper performs the computations on X and correctly produces a graph G that is identical to the graph obtained by the sequential Mapper algorithm using induction.Denote by N to the number of units of initial partitions of interval I, which is the same number of processing units. If N = 1, then the Distributed Mapper works exactly like the sequential Mapper. In this case A 1 = X and the single cover U 1 for X is used to produce the final graph which Algorithm 3 returns at step  #b2 . Now assume the hypothesis is true on k unit. We show that it holds on k+1 units. In step (1) and (2) Algorithm 3 constructs a k + 1-chain cover for [a, b] consisting of the open sets A 1 , ..., A k , A k+1 . Denote by U i to the cover of A i for 1 ≤ i ≤ k + 1. We can run Algorithm 3 on the collection{(A i , U i )} k i=1and produce a sequential Mapper graphs G i 1 ≤ i ≤ k in step (3). By the induction hypothesis, Algorithm 3 produces correctly a graph G obtained by merging the sequential Mapper graphs G 1 , ..., G k . In other words the graph G obtained from Algorithm 3 is identical to the graph obtain by running the sequential Mapper construction on the cover ∪ k i Ui. We want to show that combining G and G k+1 using our algorithm produces a graph G that is identical to running the sequential Mapper on the covering consists of ∪ k+1 ) corresponds to a node that exists in both graphs G and G k+1 . This means that each connected component of f −1 (A ∩ A k+1 ) is processed twice : one time on the first k processor and one time on the k + 1 processor and for each such component corresponds to a node in both G and G k+1 . In step (5) the algorithm checks the graphs G and G k+1 for node duplication and merge them according to their correspondence to produce the graph G.

Performance of the Algorithm
In this section, we discuss the performance improvement obtained using the Distributed Mapper algorithm. Generally there are some systems and applications where parallelism cannot be applied on all data or processes. In this case, part of data can be processed in parallel, while the other should be sequential. This may happen because of the nature of data (e.g. dependencies), the natures of processors (e.g. heterogeneity) or some other factors. In this case we can rely on a well-known formula which is the Amedahl's law to calculate the speedup ratio upper bound that comes from parallelism and the improvement percentage  #b0 . The Amedahl's law is formulated as follows:S = 1 (1 − part) + part/N ,where S is the theoretical speedup ratio, part is the proportion of system or program that can be made in parallel, 1 − part is the proportion that remains sequential, and N is the number of processors. In Table 1 we use Amedahl's law to calculate the theoretic speedup ratios using different numbers of processors and different ratios of parallelism. The table shows that the speedup increases as a response of the increase in the number of processors and/or in the proportion of the program that can be made parallel. For instnace, based on the table, when part = 0.25 the speedup of the program increases by 1.33. Notice however that at some points the performance stops improving even if we increase the number of processors.In the distributed Mapper, there are two computational pieces which are the clustering piece that is about 94% of the execution, and cover construction/merging subgraphs piece that is about 4% of the execution. Our algorithm makes the clustering piece completely in parallel while the cover construction/merging subgraphs piece is processed sequentially. Table 1 shows that the speedup of the Distributed Mapper achieves 6.49 when N = 10 and it goes up to 16.40 when N = 1000. 6.1. Experimentations. In this section we present practical results obtained using a Python implementation. We ran our experimentations on a Dell OptiPlex 7010 machine with 4-core i7-3770 Intel CPU @ 3.40GHz and with a 24GiB System Memory. The distributed Mapper algorithm was tested on different models and compared their run-time with a publicly available data available at  #b46 . The size of the point cloud data are shown in Table 2.   Data  Size  camel  21887  cat  7277  elephant 42321  horse  8431  face  29299  head 15941 Table 2. The size of the datasets used in our experiments.The sequential Mapper algorithm relies on 3 inputs: the data X, the scalar function f : X −→ [a, b] and the choice of cover U of [a, b]. The existing publicly available Mapper implementations, see for instance  #b33 , do not satisfy the level of control that we require for the cover choice and so we relied on our own Mapper implementation. The clustering algorithm that we used to specify the Mapper nodes is a modified version of the DBSCAN  #b6 .Using our implementation on the data given in Table 2 we obtained a minimum speedup of 2.41, a maximum one of 3.35 and an average speedup of 2.95 on 4 cores. This gives 73% average parallel efficiency. This is detailed in Figure 9. The figure shows the results of distributed Mapper speedup obtained using our experiments. The x-axis represents the number of processes while the y-axis shows the speedup. It is clear from the figure that the curves are increasing in a monotonic fashion as we increase the number of processes. Moreover, the experimental results matches the theoretical calculations that appears in Table 1. Figure 9. Speedups obtained by the distributed Mapper using number of processes that run concurrently.

Conclusion and Future Work
In this work we gave a provably correct algorithm to distribute Mapper on a set of processors. Our algorithm relies on a divide an conquer strategy for the codomain cover which gets pulled back to the domain cover. This work has several potential directions of the work that we have not discussed here. For instance, the recursive nature of the main algorithm was implied throughout the paper but never discussed explicitly. On the other hand the algorithm can be utilized to obtain a multi-resolution Mapper construction. In other words, using this algorithm we have the ability to increase the resolution of Mapper for certain subsets of the data and decrease at others. This is potentially useful for interactive Mapper applications.

Footnote
1 : For each set X i := f −1 (U i ), its clusters X ij ⊂ X i are computed using the chosen clustering algorithm.; 2 Each cluster is considered as a vertex in the Mapper graph. Moreover we insert an edge between two nodes X ij and X kl whenever X ij ∩ X ij = ∅;We will refer the mapper graph obtained using Algorithm 1 by the sequential Mapper.