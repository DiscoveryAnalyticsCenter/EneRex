Sharper Bounds for Regularized Data Fitting

Abstract
We study matrix sketching methods for regularized variants of linear regression, low rank approximation, and canonical correlation analysis. Our main focus is on sketching techniques which preserve the objective function value for regularized problems, which is an area that has remained largely unexplored. We study regularization both in a fairly broad setting, and in the specific context of the popular and widely used technique of ridge regularization; for the latter, as applied to each of these problems, we show algorithmic resource bounds in which the statistical dimension appears in places where in previous bounds the rank would appear. The statistical dimension is always smaller than the rank, and decreases as the amount of regularization increases. In particular, for the ridge low-rank approximation problem min Y,X Y X ε is an error parameter, and nnz(A) is the number of nonzero entries of A. This is faster than prior work, even when λ = 0. We also study regularization in a much more general setting. For example, we obtain sketchingbased algorithms for the low-rank approximation problem min X,Y Y X − A 2 F + f (Y, X) where f (·, ·) is a regularizing function satisfying some very general conditions (chiefly, invariance under orthogonal transformations).

Introduction
The technique of matrix sketching, such as the use of random projections, has been shown in recent years to be a powerful tool for accelerating many important statistical learning techniques. Indeed, recent work has proposed highly efficient algorithms for, among other problems, linear regression, low-rank approximation  #b28  #b37  and canonical correlation analysis  #b0 . In addition to being a powerful theoretical tool, sketching is also an applied one; see  #b40  for a discussion of state-of-the-art performance for important techniques in statistical learning.Many statistical learning techniques can benefit substantially, in their quality of results, by using some form of regularization. Regularization can also help by reducing the computing resources needed for these techniques. While there has been some prior exploration in this area, as discussed in §1.1, commonly it has featured sampling-based techniques, often focused on regression, and often with analyses using distributional assumptions about the input (though such assumptions are not always necessary). Our study considers fast (linear-time) sketching methods, a breadth of problems, and makes no distributional assumptions. Also, where most prior work studied the distance of an approximate solution to the optimum, our guarantees are concerning approximation with respect to a relevant loss function -see below for more discussion.It is a long-standing theme in the study of randomized algorithms that structures that aid statistical inference can also aid algorithm design, so that for example, VC dimension and sample compression have been applied in both areas, and more recently, in cluster analysis the algorithmic advantages of natural statistical assumptions have been explored. This work is another contribution to this theme. Our high-level goal in this work is to study generic conditions on sketching matrices that can be applied to a wide array of regularized problems in linear algebra, preserving their objective function values, and exploiting the power of regularization.

Results
We study regularization both in a fairly broad setting, and in the specific context of the popular and widely used technique of ridge regularization. We discuss the latter in sections 2, 3 and 4; our main results for ridge regularization, Theorem 16, on linear regression, Theorem 28, on low-rank approximation, and Theorem 36, on canonical correlation analysis, show that for ridge regularization, the sketch size need only be a function of the statistical dimension of the input matrix, as opposed to its rank, as is common in the analysis of sketching-based methods. Thus, ridge regularization improves the performance of sketching-based methods.Next, we consider regularizers under rather general assumptions involving invariance under left and/or right multiplication by orthogonal matrices, and show that sketching-based methods can be applied, to regularized multiple-response regression in §5 and to regularized low-rank approximation, in §6. Here we obtain running times in terms of the statistical dimension. Along the way, in §6.1, we give a "base case" algorithm for reducing low-rank approximation, via singular value decomposition, to the special case of diagonal matrices.Throughout we rely on sketching matrix constructions involving sparse embeddings  #b12  #b30  #b29  #b4  #b11 , and on Sampled Randomized Hadamard Transforms (SRHT)  #b1  #b32  #b13  #b14  #b34  #b6  #b15  #b39 . Here for matrix A, its sketch is SA, where S is a sketching matrix. The sketching constructions mentioned can be combined to yield a sketching matrix S such that the sketch of matrix A, which is simply SA, can be computed in time O(nnz(A)), which is proportional to the number of nonzero entries of A. Moreover, the number of rows of S is small. Corollary 15 summarizes our use of these constructions as applied to ridge regression.A key property of a sketching matrix S is that it be a subspace embedding, so that SAx 2 ≈ Ax 2 for all x. Definition 22 gives the technical definition, and Definition 24 gives the definition of the related property of an affine embedding that we also use. Lemma 25 summarizes the use of sparse embeddings and SRHT for subspace and affine embeddings.In the following we give our main results in more detail. However, before doing so, we need the formal definition of the statistical dimension.Definition 1 (Statistical Dimension) For real value λ ≥ 0 and rank-k matrix A with singular values σ i , i ∈ [k], the quantity sd λ (A) ≡ i∈[k] 1/(1+λ/σ 2 i ) is the statistical dimension (or effective dimension, or "hat matrix trace") of the ridge regression problem with regularizing weight λ.Note that sd λ (A) is decreasing in λ, with maximum sd 0 (A) equal to the rank of A. Thus a dependence of resources on sd λ (A) instead of the rank is never worse, and will be much better for large λ.In §7, we give an algorithm for estimating sd λ (A) to within a constant factor, in O(nnz(A)) time, for sd λ (A) ≤ (n + d) 1/3 . Lnowing sd λ (A) to within a constant factor allows us to set various parameters of our algorithms.

Ridge Regression
Let A ∈ R n×d , b ∈ R n , and λ > 0. In this section we consider the ridge regression problem:min x∈R d Ax − b 2 + λ x 2 ,(5)Letx * ≡ argmin x∈R d Ax − b 2 + λ x 2 and ∆ * ≡ Ax * − b 2 + λ x * 2 .In generalx * = (A ⊤ A+λI d ) −1 A ⊤ b = A ⊤ (AA ⊤ +λI n ) −1 b, so x ⋆ can be found in O(nnz(A) min(n, d))time using an iterative method (e.g., LSQR). Our goal in this section is to design faster algorithms that find an approximatex in the following sense:Ax − b 2 + λ x 2 ≤ (1 + ε)∆ * .(6)In our analysis, we distinguish between two cases: n ≫ d and d ≫ n.Remark 10 In this paper we consider only approximations of the form (6). Although we do not explore it in this paper, our techniques can also be used to derive preconditioned methods. Analysis of preconditioned kernel ridge regression, which is related to the d ≫ n case, is explored in  #b2 .

Ridge Low-rank Approximation
In §3 we consider the following problem: for given A ∈ R n×d , integer k, and weight λ ≥ 0, find:min Y ∈R n×k X∈R k×d Y X − A 2 F + λ Y 2 F + λ X 2 F ,(1)where, as is well known (and discussed in detail later), this regularization term is equivalent to 2λ Y X * , where · * is the trace (nuclear) norm, the Schatten 1-norm. We show the following.Theorem 3 (Less detailed Thm. 28) Given input A ∈ R n×d , there is a sketching-based algorithm returningỸ ∈ R n×k ,X ∈ R k×d such that with constant probability,Ỹ andX form a (1 + ε)-approximate minimizer to (1), that is,ỸX − A 2 F + λ Ỹ 2 F + λ X 2 F (2) ≤ (1 + ε) min Y ∈R n×k X∈R k×d Y X − A 2 F + λ Y 2 F + λ X 2 F .(3)The matricesỸ andX can be found in O(nnz(A))+Õ((n+d)ε −1 k min{k,ε −1 sd λ (Y * )})+poly(ε −1 sd λ (Y * )) time, where Y * is an optimum Y in (1) such that sd λ (X * ) = sd λ (Y * ) ≤ rank(Y * ) ≤ k.This algorithm follows other algorithms for λ = 0 with running times of the form O(nnz(A)) + (n + d)poly(k/ε) (e.g.  #b12 ), and has the best known dependence on k and ε for algorithms of this type, even when λ = 0.Our approach is to first extend our ridge regression results to the multiple-response case min Z AZ − B 2 F + λ Z 2 F , and then reduce the multiple-response problem to a smaller one by showing that up to a cost in solution quality, we can assume that each row of Z lies in the rowspace of SA, for S a suitable sketching matrix. We apply this observation twice to the low-rank approximation problem, so that Y can be assumed to be of the form ARỸ , and X of the formXSA, for sketching matrix S and (right) sketching matrix R. Another round of sketching then reduces to a low-rank approximation problem of size independent of n and d, and finally an SVD-based method is applied to that small problem.Regarding related work: the regularization "encourages" the rank of Y X to be small, even when there is no rank constraint (k is large), and this unconstrained problem has been extensively studied; even so, the rank constraint can reduce the computational cost and improve the output quality, as discussed by  #b7 , who also give further background, and who give experimental results on an iterative algorithm. Pilanci et al.  #b31  consider only algorithms where the sketching time is at least Ω(nd), which can be much slower than our nnz(A) for sparse matrices, and it is not clear if their techniques can be extended. In the case of low-rank approximation with a nuclear norm constraint (the closest to our work), as the authors note, their paper gives no improvement in running time. While their framework might imply analyses for ridge regression, they did not consider it specifically, and such an analysis may not follow directly.

Regularized Canonical Correlation Analysis
Canonical correlation analysis (CCA) is an important statistical technique whose input is a pair of matrices, and whose solution depends on the Gram matrices A ⊤ A and B ⊤ B. If these Gram matrices are ill-conditioned it is useful to regularize them by instead using A ⊤ A + λ 1 I d and B ⊤ B + λ 2 I d ′ , for weights λ 1 , λ 2 ≥ 0. Thus, in this paper we consider a regularized version of CCA, defined as follows (our definition is in the same spirit as the one used by  #b0 ).Definition 4 Let A ∈ R n×d and B ∈ R n×d ′ , and let q = min(rank(A ⊤ A + λ 1 I d ), rank(B ⊤ B + λ 2 I d ′ )).Let λ 1 ≥ 0 and λ 2 ≥ 0. The (λ 1 , λ 2 ) canonical correlations σ (λ 1 ,λ 2 ) 1 ≥ · · · ≥ σ (λ 1 ,λ 2 ) q and (λ 1 , λ 2 ) canonical weights u 1 , . . . , u q ∈ R d and v 1 , . . . , v q ∈ R d ′ are ones that maximizetr(U ⊤ A ⊤ BV ) subject to U ⊤ (A ⊤ A + λ 1 I d )U = I q V ⊤ (B ⊤ B + λ 2 I d ′ )V = I q U ⊤ A ⊤ BV = diag(σ (λ 1 ,λ 2 ) 1 , . . . , σ (λ 1 ,λ 2 ) q )where U = [u 1 , . . . , u q ] ∈ R n×q and V = [v 1 , . . . , v q ] ∈ R d ′ ×q .One classical way to solve non-regularized CCA (λ 1 = λ 2 = 0) is the Björck-Golub algorithm  #b5 . In §4 we show that regularized CCA can be solved using a variant of the Björck-Golub algorithm.Avron et al.  #b0  showed how to use sketching to compute an approximate CCA. In §4 we show how to use sketching to compute an approximate regularized CCA.Theorem 5 (Loose version of Thm. 36) There is a distribution over matrices S ∈ R m×n with m = O(max(sd λ 1 (A), sd λ 2 (B)) 2 /ǫ 2 ) such that with constant probability, the regularized CCA of (SA, SB) is an ǫ-approximate CCA of (A, B). The matrices SA and SB can be computed in O(nnz(A) + nnz(B)) time.Our generalization of the classical Björck-Golub algorithm shows that regularized canonical correlation analysis can be computed via the product of two matrices whose columns are nonorthogonal regularized bases of A and B. We then show that these two matrices are easier to sketch than the orthogonal bases that arise in non-regularized CCA. This in turn can be tied to approximation bounds of sketched regularized CCA versus exact CCA.

General Regularization
A key property of the Frobenius norm · F is that it is invariant under rotations; for example, it satisfies the right orthogonal invariance condition AQ F = A F , for any orthogonal matrix Q (assuming, of course, that A and Q having dimensions so that AQ is defined). In §5 and §6, we study conditions under which such an invariance property, and little else, is enough to allow fast sketching-based approximation algorithms.For regularized multiple-response regression, we have the following.Theorem 6 (Implied by Thm. 52) Let f (·) be a real-valued function on matrices that is right orthogonally invariant, subadditive, and invariant under padding the input matrix by rows or columns of zeros. Let A ∈ R n×d , B ∈ R n×d ′ . Suppose that for r ≡ rank A, there is an algorithm that for general n, d, d ′ , r and ε > 0, in time τ (d, n, d ′ , r, ε) findsX withAX − B 2 F + f (X) ≤ (1 + ε) min X∈R d×d ′ AX − B 2 F + f (X).Then there is another algorithm that with constant probability finds such anX, taking time O(nnz(A) + nnz(B) + (n + d + d ′ )poly(r/ε)) + τ (d, poly(r/ε), poly(r/ε), r, ε).(Note that Thm. 52 seemingly requires an additional property called sketching inheritance. However this condition is implied by the conditions of the last theorem.) That is, sketching can be used to reduce to a problem in which the only remaining large matrix dimension is d, the number of columns of A.This reduction is a building block for our results for regularized low-rank approximation. Here the regularizer is a real-valued function f (Y, X) on matrices Y ∈ R n×k , X ∈ R k×d . We show that under broad conditions on f (·, ·), sketching can be applied tomin Y ∈R n×k X∈R k×d Y X − A 2 F + f (Y, X).(4)Our conditions imply fast algorithms when, for example, f (Y, X) = Y X (p) , where · (p) is a Schatten p-norm, or when f (Y, X) = min{λ 1 Y X (1) , λ 2 Y X (2) }, for weights λ 1 , λ 2 , and more. Of course, there are norms, such as the entriwise ℓ 1 norm, that do not satisfy these orthogonal invariance conditions.Theorem 7 (Implied by Thm. 59) Let f (Y, X) be a real-valued function on matrices that in each argument is subadditive and invariant under padding by rows or columns of zeros, and also right orthogonally invariant in its right argument and left orthogonally invariant in its left argument. Suppose there is a procedure that solves (4) when A, Y , and X are k × k matrices, and A is diagonal, and Y X is constrained to be diagonal, taking time τ (k) for a function τ (·).Then for general A, there is an algorithm that finds a (1 + ε)-approximate solution (Ỹ ,X) in time O(nnz(A)) +Õ(n + d)poly(k/ε) + τ (k).The proof involves a reduction to small matrices, followed by a reduction, discussed in §6.1, that uses the SVD to reduce to the diagonal case. This result, Corollary 58, generalizes results of  #b36 , who gave such a reduction for f (Y, X) = X 2 F + Y 2 F ; also, we give a very different proof.As for related work,  #b36  survey and extend work in this setting, and propose iterative algorithms for this problem. The regularizers f (Y, X) they consider, and evaluate experimentally, are more general than we can analyze.The conditions on f (Y, X) are quite general; it may be that for some instances, the resulting problem is NP-hard. Here our reduction would be especially interesting, because the size of the reduced NP-hard problem depends only on k.

Basic Definitions and Notation
We denote scalars using Greek letters. Vectors are denoted by x, y, . . . and matrices by A, B, . . . . We use the convention that vectors are column-vectors. We use nnz(·) to denote the number of nonzeros in a vector or matrix. We denote by [n] the set 1, . . . , n. The notation α = (1 ± γ)β means that (1 − γ)β ≤ α ≤ (1 + γ)β.Throughout the paper, A denotes an n × d matrix, and σ 1 ≥ σ 2 ≥ · · · ≥ σ min(n,d) its singular values.Definition 8 (Schatten p-norm) The Schatten p-norm of A is A (p) ≡ [ i σ p i ] 1/p . Note that the trace (nuclear) norm A * = A (1) , the Frobenius norm A F = A (2) , and the spectral normA 2 = A (∞) .The notation · without a subscript denotes the ℓ 2 norm for vectors, and the spectral norm for matrices. We use a subscript for other norms. We use range(A) to denote the subspace spanned by the columns of A, i.e. range(A) ≡ {Ax | x ∈ R d }. I d denotes the d × d identity matrix, 0 d denotes the column vector comprising d entries of zero, and 0 a×b ∈ R a×b denotes a zero matrix.The rank rank(A) of a matrix A is the dimension of the subspace range(A) spanned by its columns (equivalently, the number of its non-zero singular values). Bounds on sketch sizes are often written in terms of the rank of the matrices involved.Definition 9 (Stable Rank) The stable rank sr(A) ≡ A 2 F / A 2 2 . The stable rank satisfies sr(A) ≤ rank(A).

Large n
In this subsection we design an algorithm that is aimed at the case when n ≫ d. However, the results themselves are correct even when n < d. The general strategy is to design a distribution on matrices of size m-by-n (m is a parameter), sample an S from that distribution, and solvẽx ≡ argmin x∈R d S(Ax − b) 2 + λ x 2 .The following lemma defines conditions on the distribution that guarantee that (6) holds with constant probability (which can be boosted to high probability by repetition and taking the solution with minimum objective value).Lemma 11 Let x * ∈ R d , A and b as above. Let U 1 ∈ R n×d comprise the first n rows of an orthogonal basis forA √ λI d . Let sketching matrix S ∈ R m×n have a distribution such that with constant probability U ⊤ 1 S ⊤ SU 1 − U ⊤ 1 U 1 2 ≤ 1/4,(7)andU ⊤ 1 S ⊤ S(b − Ax * ) − U ⊤ 1 (b − Ax * ) ≤ ε∆ * /2. (8) Then with constant probability,x ≡ argmin x∈R d S(Ax − b) 2 + λ x 2 has Ax − b 2 + λ x 2 ≤ (1 + ε)∆ * .Proof:LetÂ ∈ R (n+d)×d have orthonormal columns with range(Â) = range( A √ λI d ). (An explicit expression for one suchÂ is given below.) Letb ≡ b 0 d . We have min y∈R d Â y −b 2 (9) equivalent to (5), in the sense that for anyÂy ∈ range(Â), there is x ∈ R d withÂy = A √ λI d x, so that Â y −b 2 = A √ λI d x −b 2 = b − Ax 2 + λ x 2 . Let y * = argmin y∈R d Â y −b 2 , so that Ay * = Ax * √ λx * . LetÂ = U 1 U 2 , where U 1 ∈ R n×d and U 2 ∈ R d×d , so that U 1 is as in the lemma statement. LetŜ ≡ S 0 m×d 0 d×n I d .Using (7), with constant probabilityÂ ⊤Ŝ⊤ŜÂ − I d 2 = U ⊤ 1 S ⊤ SU 1 + U ⊤ 2 U 2 − I d 2 = U ⊤ 1 S ⊤ SU 1 − U ⊤ 1 U 1 2 ≤ 1/4.(10)Using the normal equations for (9), we have0 =Â ⊤ (b −Ây * ) = U ⊤ 1 (b − Ax * ) − √ λU ⊤ 2 x * ,and soÂ⊤Ŝ⊤Ŝ (b −Ây * ) = U ⊤ 1 S ⊤ S(b − Ax * ) − √ λU ⊤ 2 x * = U ⊤ 1 S ⊤ S(b − Ax * ) − U ⊤ 1 (b − Ax * ).Using (8), with constant probabilityÂ ⊤Ŝ⊤Ŝ (b −Ây * ) = U ⊤ 1 S ⊤ S(b − Ax * ) − U ⊤ 1 (b − Ax * ) ≤ ε∆ * /2 = ε/2 b −Ây * .(11)It follows by a standard result from (10) and (11) that the solutionỹ ≡ argmin y∈R d Ŝ (Ây −b) has Âỹ −b ≤ (1 + ε) min y∈R d Â y −b , and therefore thatx satisfies the claim of the theorem. For convenience we give the proof of the standard result: (10) implies thatÂ ⊤Ŝ⊤ŜÂ has smallest singular value at least 3/4. The normal equations for the unsketched and sketched problems arêA ⊤ (b −Ây * ) = 0 =Â ⊤Ŝ⊤Ŝ (b −Âỹ).

The normal equations for the unsketched case imply Âỹ
−b 2 = Â (ỹ − y * ) 2 + b −Ây * 2 , so it is enough to show that Â (ỹ − y * ) 2 = ỹ − y * 2 ≤ ε∆ * . We have (3/4) ỹ − y * ≤ Â ⊤Ŝ⊤ŜÂ (ỹ − y * ) using (10) = Â ⊤Ŝ⊤ŜÂ (ỹ − y * ) −Â ⊤Ŝ⊤Ŝ (b −Âỹ) normal eqs = Â ⊤Ŝ⊤Ŝ (b −Ây * ) ≤ ε∆ * /2 from (11),so that ỹ − y * 2 ≤ (4/3) 2 ε∆ * /2 ≤ ε∆ * . The theorem follows.Lemma 12 For U 1 as in Lemma 11, U 1 2F = sd λ (A) = i 1/(1 + λ/σ 2 i ), where A has singular values σ i . Also U 1 2 = 1/ 1 + λ/σ 2 1 .This follows from (3.47) of  #b26 ; for completeness, a proof is given here.Proof: Suppose A = U ΣV ⊤ , the full SVD, so that U ∈ R n×n , Σ ∈ R n×d , and V ∈ R d×d . Let D ≡ (Σ ⊤ Σ + λI d ) −1/2 . ThenÂ = U ΣD V √ λD hasÂ ⊤Â = I d , and for given x, there is y = D −1 V ⊤ x withÂy = A √ λI d x. We have U 1 2 F = U ΣD 2 F = ΣD 2 F = i 1/(1 + λ/σ 2 i ) as claimed.Also U 1 2 = U ΣD 2 = ΣD 2 = 1/ 1 + λ/σ 2 1 , and the lemma follows.Definition 13 (large λ) Say that λ is large for A with largest singular value σ 1 , and error pa-rameter ε, if λ/σ 2 1 ≥ 1/ε.The following lemma implies that if λ is large, then x = 0 is a good approximate solution, and so long as we include a check that a proposed solution is no worse than x = 0, we can assume that λ is not large.Lemma 14 For ε ∈ (0, 1], large λ, and all x,Ax − b 2 + λ x 2 ≥ b 2 /(1 + ε). If λ is not large then U 1 2 2 ≥ ε/2. Proof: If σ 1 x ≥ b , then λ x 2 ≥ σ 2 1 x 2 ≥ b 2 . Suppose σ 1 x ≤ b . Then: Ax − b 2 + λ x 2 = Ax 2 + b 2 − 2b ⊤ Ax + λ x 2 ≥ ( b − Ax ) 2 + λ x 2 Cauchy-Schwartz ≥ ( b − σ 1 x ) 2 + λ x 2 assumption ≥ b 2 /(1 + σ 2 1 /λ) calculus ≥ b 2 /(1 + ε), large λas claimed. The last statement follows from Lemma 12.Below we discuss possibilities for choosing the sketching matrix S. We want to emphasize that the first condition in Lemma 11 is not a subspace embedding guarantee, despite having superficial similarity. Indeed, notice that the columns of U 1 are not orthonormal, since we only take the first n rows of an orthogonal basis of A √ λI d . Rather, the first condition is an instance of approximate matrix product with a spectral norm guarantee with constant error, for which optimal bounds in terms of the stable rank sr(U 1 ) were recently obtained  #b10 . As we discuss in the proof of part (i) of Corollary 15 below, sr(U 1 ) is upper bounded by sd λ (A)/ǫ.We only mention a few possibilities of sketching matrix S below, though others are possible with different tradeoffs and compositions.Corollary 15 Suppose λ is not large (Def. 13). There is a constant K > 0 such that fori. m ≥ K(ε −1 sd λ (A) + sd λ (A) 2 ) and S ∈ R m×n a sparse embedding matrix (see [CW13, MM13, NN13]) with SA computable in O(nnz(A)) time, or one can choose m ≥ K(ε −1 sd λ (A) + min((sd λ (A)/ǫ) 1+γ , sd λ (A) 2 )) an OSNAP (see [NN13, BDN15, Coh16]) with SA computable in O(nnz(A)) time, where γ > 0 is an arbitrarily small constant, orii. m ≥ Kε −1 (sd λ (A)+log(1/ε)) log(sd λ (A)/ε) and S ∈ R m×n a Subsampled Randomized Hadamard Transform (SRHT) embedding matrix (see, e.g.,  #b6 ), with SA computable in O(nd log n) time, or iii. m ≥ Kε −1 sd λ (A) and S ∈ R m×n a matrix of i.i.d. subgaussian values with SA computable in O(ndm) time,the conditions (7) and (8) of Lemma 11 apply, and with constant probability the correspondingx = argmin x∈R d S(Ax − b) + λ x 2 is an ε-approximate solution to min x∈R d b − Ax 2 + λ x 2 .Proof: Recall that sd λ (A) = U 1 2 F . For (i): sparse embedding distributions satisfy the bound for matrix multiplicationW ⊤ S ⊤ SH − W ⊤ H F ≤ C W F H F / √ m,for a constant C [CW13,  #b29  #b30 ; this is also true of OSNAP matrices. We set W = H = U 1 and use X 2 ≤ X F for all X and m ≥ K U 1 4 F to obtain (7), and set W = U 1 , H = b − Ax * and use m ≥ K U 1 2 F /ε to obtain (8). (Here the bound is slightly stronger than (8), holding for λ = 0.) With (7) and (8), the claim forx from a sparse embedding follows using Lemma 11.For OSNAP, Theorem 1 in  #b10  together with  #b30  imply that for m = O(sr(U 1 ) 1+γ ), condition (7) holds. Here sr(U 1 ) =U 1 2 F U 1 2 2, and by Lemma 12 and Lemma 14, sr(U 1 ) ≤ sd λ (A)/ǫ. We note that (8) continues to hold as in the previous paragraph. Thus, m is at most the min ofO((sd λ (A)/ǫ) 1+γ ) and O(sd λ (A)/ǫ + sd λ (A) 2 ).For (ii): Theorems 1 and 9 of  #b10  imply that for γ ≤ 1, with constant probabilityW ⊤ S ⊤ SH − W ⊤ H 2 ≤ γ W 2 H 2(12)for SRHT S, when m ≥ C(sr(W ) + sr(H) + log(1/γ)) log(sr(W ) + sr(H))/γ 2 for a constant C. We let W = H = U 1 and γ = min{1, 1/4 U 1 2 }. We haveU ⊤ 1 S ⊤ SU 1 − U ⊤ 1 U 1 2 ≤ min{1, 1/4 U 1 2 } U 1 2 2 = min{ U 1 2 2 , 1/4} ≤ 1/4, and sr(U 1 )/γ 2 = U 1 2 F U 1 2 2 max{1, 4 U 1 2 2 } = U 1 2 F max{1/ U 1 2 2 , 4} ≤ 2 U 1 2 F /εusing Lemma 14 and the assumption that λ is large. (And assuming ε ≤ 1/2.) Noting that log(1/γ) = O(log(1/ε)) and log(sr(U 1 )) = O(log U 1 F /ε) using Lemma 14, we have that m as claimed suffices for (7). For (8), we use (12) with W = U 1 , H = Ax * − b, and γ = ε/2/ U 1 2 ; note that using Lemma 14 and by the assumption that λ is large, γ ≤ 1 and so (12) can be applied. We haveU ⊤ 1 S ⊤ S(Ax * − b) ≤ ( ε/2/ U 1 2 ) U 1 2 Ax * − b ≤ ε∆ * /2, and sr(U 1 ) log(sr(U 1 ))/γ 2 ≤ U 1 2 F U 1 2 2 [2 log( U 1 F /ε)][2 U 1 2 2 /ε] = 4 U 1 2 F log( U 1 F /ε)/ε.Noting that since Ax * − b is a vector, its stable rank is one, we have that m as claimed suffices for (8). With (7) and (8), the claim forx from an SRHT follows using Lemma 11.The claim for (iii) follows as (ii), with a slightly simpler expression for m.Here we mention the specific case of composing a sparse embedding matrix with an SRHT.Theorem 16 Given A ∈ R n×d , there are dimensions within constant factors of those given in Cor. 15 such that for S 1 a sparse embedding and S 2 an SRHT with those dimensions,x ≡ argmin x∈R d S 2 S 1 (Ax − b) 2 + λ x 2 , satisfies Ax − b 2 + λ x 2 ≤ (1 + ε) min x∈R d Ax − b 2 + λ x 2 with constant probability. Therefore in O(nnz(A)) +Õ(d sd λ (A)/ε + sd λ (A) 2 )time, a ridge regression problem with n rows can be reduced to one withO(ε −1 (sd λ (A) + log(1/ε)) log(sd λ (A)/ε))rows, whose solution is a (1 + ε)-approximate solution.Proof: This follows from Corollary 15 and the general comments of Appendix A.3 of  #b10 ; the results there imply thatS i U 1 F = Θ( U 1 F ) and S i U 1 2 = Θ( U 1 2 ) for i ∈ [3]with constant probability, which implies that sr(S 1 U 1 ) and sr(S 2 S 1 U 1 ) are O(sr(U 1 )). Moreover, the approximate multiplication bounds of (7) and (8) have versions when using S 2 S 1 U 1 and S 2 S 1 (Ax * − b) to estimate products involving S 1 U 1 and S 1 (Ax * −b), so that for example, using the triangle inequality,U ⊤ 1 S ⊤ 1 S ⊤ 2 S 2 S 1 U 1 − U ⊤ 1 U 1 2 ≤ U ⊤ 1 S ⊤ 1 S ⊤ 2 S 2 S 1 U 1 − U ⊤ 1 S ⊤ 1 S 1 U 1 2 + U ⊤ 1 S ⊤ 1 S 1 U 1 − U ⊤ 1 U 1 2 ≤ 1/8 + 1/8 = 1/4.We have that S = S 2 S 1 satisfies (7) and (8), as desired.Similar arguments imply that a reduction also using a sketching matrix S 3 with subgaussian entries could be used, to reduce to a ridge regression problem with O(ε −1 sd λ (A)) rows.

Large d
If the number of columns is larger than the number of rows, it is more attractive to sketch the rows, i.e., to use AS ⊤ . In general, we can express (5) asmin x∈R d Ax 2 − 2b ⊤ Ax + b 2 + λ x 2 .We can assume x has the form x = A ⊤ y, yielding the equivalent problemmin y∈R n AA ⊤ y 2 − 2b ⊤ AA ⊤ y + b 2 + λ A ⊤ y 2 .(13)Sketching A ⊤ with S in the first two terms yieldsy ≡ argmin y∈R n λ SA ⊤ y 2 + AS ⊤ SA ⊤ y 2 − 2b ⊤ AA ⊤ y + b 2 (14) Now let c ⊤ ≡ b ⊤ AA ⊤ . Note that we can compute c in O(nnz(A)) time. The solution to (14) is, for B ≡ SA ⊤ with B ⊤ B invertible,ỹ = (λB ⊤ B + B ⊤ BB ⊤ B) + c/2.In the main result of this subsection, we show that provided λ > 0 then a sufficiently tight subspace embedding to range(A ⊤ ) suffices.Theorem 17 Suppose A has rank k, and its SVD isA = U ΣV ⊤ , with U ∈ R n×k , Σ ∈ R k×k and V ∈ R d×k . If S ∈ R m×d has 1. (Subspace Embedding) E ≡ V ⊤ S ⊤ SV − I k with E 2 ≤ ε/22. (Spectral Norm Approximate Matrix Product) for any fixed matrices C, D, each with d rows,C T S T SD − C T D 2 ≤ ε ′ C 2 D 2 , where ε ′ ≡ (ε/2)/(1 + 3σ 2 1 /λ). Then (14) hasx ≡ A ⊤ỹ approximately solving (5), that is, Ax − b 2 + λ x 2 ≤ (1 + ε)∆ * .

Proof:
(Given here for convenience.) Using the representation of P as a convex combination from the lemma just above,f (P A) = f ( j α j U j A) ≤ j f (α j U j A) = j α j f (U j A) = f (A),and f () is left reduced by contractions, as claimed.Definition 42 Fix p ≥ 1. The v-norm of matrix A is A v ≡ i∈[d] A i: p 2 1/p .This is also called the (2, p)-norm  #b36  or R 1 norm when p = 1  #b17 .Remark 43 Since · F , the spectral norm · 2 , and the trace norm · * are orthogonally invariant, they are reduced by contractions. Some f () are reduced by contractions on one side, without being orthogonally invariant: for example, the v-norm · v is right orthogonally invariant, and therefore by Lemma 41, right reduced by contractions, but not on the left.The v-norm can also be considered for p < 1; this is not subadditive, and so Lemma 41 does not apply, but even so, it is right orthogonally invariant and right reduced by contractions, just considering the invariance or contractions row-wise.Definition 44 (subspace embedding w.r.t. a matrix norm, poly-sized distributions) From Definition 22, a matrix S ∈ R m S ×n is a subspace ε-embedding for A with respect to the Euclidean norm if SAx 2 = (1 ± ε) Ax 2 for all x. More generally, S is a (left) subspace εembedding for A with respect to a matrix measure f () if f (SAX) = (1 ± ε)f (AX) for all matrices X. Say that R ∈ R d×m R is a right subspace ε-embedding for A with respect to f () if f (Y AR) = (1 ± ε)f (Y A) for all matrices Y . Say that a probability distribution over matrices S is a polysized sketching distribution if there is m S = poly(d/ε) so that with constant probability, S is a subspace ε-embedding. Similarly define for sketching on the right, where the size condition on m R is m R = poly(n/ε).Definition 45 (padding invariance) Say that a matrix measure f () is padding invariant if it is preserved by padding A with rows or columns of zeroes:f ( A 0 z×d ) = f ( A 0 n×z ′ ) = f (A).Lemma 46 Unitarily invariant norms and v-norms are padding invariant.Proof: For v-norms, this is direct from the definition. For unitarily invariant norms, this follows from their dependence on the singular values only, and that the singular values of a matrix don't change with padding: if A = U ΣV ⊤ , then the SVD of A 0 is U 0 ΣV ⊤ , and correspondingly for column padding.Definition 47 (piloi, piroi) Say that a matrix measure is piloi if it is padding invariant and left orthogonally invariant, and piroi if it is padding invariant and right orthogonally invariant.Definition 48 (embedding inheritance) Say that a matrix measure f () inherits a subspace εembedding from the Euclidean norm (on the left) if the condition that S ∈ R m×n is a subspace ε-embedding for A with respect to the Euclidean norm implies that S is a subspace O(ε)-embedding for f (). Define inheritance on the right similarly.

Multiple-response Ridge Regression
In multiple-response ridge regression one is interested in findingX * ≡ argmin X∈R d×d ′ AX − B 2 F + λ X 2 F , where B ∈ R n×d ′ .It is straightforward to extend the results and algorithms for large n to multiple regression. Since we use these results when we consider regularized low-rank approximation, we state them next. The proofs are omitted as they are entirely analogous to the proofs in subsection 2.1.Lemma 18 Let A, U 1 , U 2 as in Lemma 11, B ∈ R n×d ′ ,X * ≡ argmin X∈R d×d ′ AX − B 2 F + λ X 2 F ,and ∆ * ≡ AX * − B 2 F + λ X * 2 F . Let sketching matrix S ∈ R m×n have a distribution such that with constant probability,U ⊤ 1 S ⊤ SU 1 − U ⊤ 1 U 1 2 ≤ 1/4,(16)andU ⊤ 1 S ⊤ S(B − AX * ) − U ⊤ 1 (B − AX * ) F ≤ ε∆ * .(17)Then with constant probability,X ≡ argminX∈R d×d ′ S(AX − B) 2 F + λ X 2 F (18) has AX − B 2 + λ X 2 F ≤ (1 + ε)∆ * .Theorem 19 There are dimensions within a constant factor of those given in Thm. 16, such that for S 1 a sparse embedding and S 2 SRHT with those dimensions, S = S 2 S 1 satisfies the conditions of Lemma 18, therefore the correspondingX does as well. That is, in timeO(nnz(A) + nnz(B)) +Õ((d + d ′ )(sd λ (A)/ε + sd λ (A) 2 )time, a multiple-response ridge regression problem with n rows can be reduced to one withÕ(ε −1 sd λ (A)) rows, whose solution is a (1 + ε)-approximate solution.Remark 20 Note that the solution to (18), that is, the solution to min X Ŝ (ÂX −B) 2 F , whereŜ andÂ are as defined in the proof of Lemma 11, andB ≡ B 0 d×d ′ , isX = (ŜÂ) +ŜB ; that is, the matrixÂX =Â(ŜÂ) +ŜB whose distance toB is within 1 + ε of optimal has rows in the rowspace ofB, which is the rowspace of B. This property will be helpful building low-rank approximations.

Ridge Low-Rank Approximation
For an integer k we consider the problemmin Y ∈R n×k X∈R k×d Y X − A 2 F + λ Y 2 F + λ X 2 F .(19)From  #b36  (see also Corollary 58 below), this has the solutionY * = U k (Σ k − λI k ) 1/2 + X * = (Σ k − λI k ) 1/2 + V ⊤ k =⇒ sd λ (Y * ) = sd λ (X * ) = i∈[k] σ i >λ (1 − λ/σ i )(20)where U k Σ k V ⊤ k is the best rank-k approximation to A, and for a matrix W , W + has entries that are equal to the corresponding entries of W that are nonnegative, and zero otherwise.While  #b36  gives a general argument, it was also known (see for example  #b33 ) that when the rank k is large enough not to be an active constraint (say, k = rank(A)), then Y * X * for Y * , X * from (20) solves minZ∈R n×d Z − A 2 F + 2λ Z * ,where Z * is the nuclear norm of X (also called the trace norm). It is also well-known thatZ * = 1 2 ( min Y X=Z Y 2 F + X 2 F ),so that the optimality of (20) follows for large k.Lemma 21 Given integer k ≥ 1 and ε > 0, Y * and X * as in (20), there arem =Õ(ε −1 sd λ (Y * )) =Õ(ε −1 k) and m ′ =Õ(ε −1 min{k, ε −1 sd λ (Y * )}),such that there is a distribution on S ∈ R m×n and R ∈ R d×m ′ so that forZ * S , Z * R ≡ argmin Z S ∈R k×m Z R ∈R m ′ ×k ARZ R Z S SA − A 2 F + λ ARZ R 2 F + λ Z S SA 2 F , with constant probabilityỸ ≡ ARZ * R andX ≡ Z * S SA satisfy ỸX − A 2 F + λ Ỹ 2 F + λ X 2 F ≤ (1 + ε)( Y * X * − A 2 F + λ Y * 2 F + λ X * 2 F ).The products SA and AR take altogetherO(nnz(A)) +Õ((n + d)(ε −2 sd λ (Y * ) + ε −1 sd λ (Y * ) 2 ) to compute.Proof: Let Y * and X * be an optimal solution pair for (19). Consider the problem min H∈R k×dY * H − A 2 F + λ H 2 F .(21)Let H * be an optimal solution. We can apply Lemma 18 mapping A of the theorem to Y * , B to A, Y * to H * , andỸ toH ≡ SY * √ λI k + SA 0 k×d , so that for S satisfying the condition of Theorem 18, as noted in Remark 20,H is within 1 + ε of the cost of H * , and in the rowspace of SA. (That is, the rows of rowspan(H) ⊂ rowspan(SA).)Using Theorem 19, we have m =Õ(ε −1 (sd λ (Y * )) =Õ(ε −1 k). Now consider the problemmin W ∈R n×k WH − A 2 F + λ W 2 F .(22)We again apply Lemma 18, mapping A of the theorem toH ⊤ , B to A ⊤ , Y * to the transpose of an optimal solution W * to (22), and S ⊤ to a matrix R. This results inW ≡ AR0 k×m ′ H R √ λ+⊤ whose cost is within 1 + ε of that of W * . (Here Z +⊤ denotes the transpose of the pseudo-inverse of Z.) Moreover, the columns ofW are in the columspace of AR.SinceH can be written in the form Z S SA for some Z S ∈ R k×m , andW in the form ARZ R for some Z R ∈ R m ′ ×k , the quality bound of the lemma follows, after adjusting ε by a constant factor.Noting that rank(H) ≤ min{m, k}, there is big enoughm ′ =Õ(ε −1 sd λ (H)) =Õ(ε −1 min{m, k}) =Õ(min{ε −2 sd λ (Y * ), ε −1 k}).We apply Theorem 19 to obtain the time bounds for computing SA and AR.We can reduce to an even yet smaller problem, using affine embeddings, which are built using subspace embeddings. These are defined next.

Definition 22 (subspace embedding)
Matrix S ∈ R m S ×n is a subspace ε-embedding for A with respect to the Euclidean norm if SAx 2 = (1 ± ε) Ax 2 for all x.Lemma 23 There are sparse embedding distributions on matrices S ∈ R m×n with m = O(ε −2 rank(A) 2 ) so that SA can be computed in nnz(A) time, and with constant probability S is a subspace εembedding. The SRHT (of Corollary 15) is a distribution on S ∈ R m×n with m =Õ(ε −2 rank(A)) such that S is a subspace embedding with constant probability.Proof: The sparse embedding claim is from  #b12 , sharpened by  #b30  #b29 ; the SRHT claim is from for example  #b6 .Definition 24 (Affine Embedding) For A as usual and B ∈ R n×d ′ , matrix S is an affine ε-embedding for A, B if S(AX − B) 2 F = (1 ± ε) AX − B 2 F for all X ∈ R d×d ′ .A distribution over R m S ×n is a poly-sized affine embedding distribution if there is m S = poly(d/ε) such that constant probability, S from the distribution is an affine ε-embedding.Lemma 25 For A as usual, B ∈ R n×d ′ , suppose there is a distribution over S ∈ R m×n so that with constant probability, S is a subspace embedding for A with parameter ε, and for X * ≡ argmin X∈R d×d ′ AX−B 2 F and B * ≡ AX * −B, SB * 2 F = (1±ε) B * 2 F and U ⊤ S ⊤ SB * −U ⊤ B * ≤ ε B * 2 F .p ′ =Õ(ε −2 m) =Õ(ε −3 sd λ (Y * )) =Õ(ε −3 k) and p =Õ(ε −2 m ′ ) =Õ(ε −3 min{k, ε −1 sd λ (Y * )}),such that there is a distribution on S 2 ∈ R p×n , R 2 ∈ R d×p ′ so that forZ S ,Z R ≡ argmin Z S ∈R k×m Z R ∈R m ′ ×k S 2 ARZ R Z S SAR 2 − S 2 AR 2 2 F + λ S 2 ARZ R 2 F + λ Z S SAR 2 2 F , with constant probabilityỸ ≡ ARZ R andX ≡Z S SA satisfy ỸX − A 2 F + λ Ỹ 2 F + λ X 2 F ≤ (1 + ε)( Y * X * − A 2 F + λ Y * 2 F + λ X * 2 F ).The matrices S 2 AR, SAR, and SAR 2 can be computed in O(nnz(A)) + poly(sd λ (Y * )/ε) time.Proof: Apply Lemma 25, with A of the lemma mapping to AR, B of the lemma mapping to A, U to the left singular matrix of AR, S to S 2 , and d to m ′ . Also apply Lemma 25 in an analogous way, but in transpose, to SA. For the last statement: to compute SAR, apply the sparse embedding of S and the sparse embedding of R to A on each side, and then the SRHT components to the resulting small matrix; the claimed time bound follows. The other sketches are computed similarly. The theorem follows.Lemma 27 For C ∈ R p×m ′ , D ∈ R m×p ′ , G ∈ R p×p ′ , the problem of findingmin Z S ∈R k×m Z R ∈R m ′ ×k CZ R Z S D − G 2 F + λ CZ R 2 F + λ Z S D 2 F ,(23)and the minimizing CZ R and Z S D, can be solved inO(pm ′ r C + p ′ mr D + r D p(p ′ + r C ))time, where r C ≡ rank(C) ≤ min{m ′ , p}, and r D ≡ rank(D) ≤ min{m, p ′ }.Proof: Let U C be an orthogonal basis for colspace(C), so that every matrix of the form CZ R is equal to U C Z ′ R for some Z ′ R . Similarly let U ⊤ D be an orthogonal basis for rowspan(D), so that every matrix of the form Z S D is equal to one of the form Z ′ S U D . Let P C ≡ U C U ⊤ C and P D ≡ U D U ⊤ D . Then using P C (I − P C ) = 0, P D (I − P D ) = 0, and matrix Pythagoras,CZ R Z S D − G 2 F + λ CZ R 2 F + λ Z S D 2 F = P C U C Z ′ R Z ′ S U ⊤ D P D − G 2 F + λ U C Z ′ R 2 F + λ Z ′ S U ⊤ D 2 F = P C U C Z ′ R Z ′ S U ⊤ D P D − P C GP D 2 F + (I − P C )G 2 F + P C G(I − P D ) 2 F + λ Z ′ R 2 F + λ Z ′ S 2 F .

So minimizing (23) is equivalent to minimizing
P C U C Z ′ R Z ′ S U ⊤ D P D − P C GP D 2 F + λ Z ′ R 2 F + λ Z ′ S 2 F = U C Z ′ R Z ′ S U ⊤ D − U C U ⊤ C GU D U ⊤ D 2 F + λ Z ′ R 2 F + λ Z ′ S 2 F = Z ′ R Z ′ S − U ⊤ C GU D 2 F + λ Z ′ R 2 F + λ Z ′ S 2 F .This has the form of (19), mapping Y of (19) to Z ′ R , X to Z ′ S , and A to U ⊤ C GU D , from which a solution of the form (20) can be obtained.To recover Z R from Z ′ R : we have C = U C [ T C T ′ C ], for matrices T C and T ′ C , where upper triangular T C ∈ R r C ×r C . We recover Z R asT −1 CẐ ′ R 0 m−r C ×k , since then U C Z ′ R = CZ R .A similar back-substitution allows recovery of Z S from Z ′ S . Running times: to compute U C and U D , O(pm ′ r C + mp ′ r D ); to compute U ⊤ C GU D , O(r D p(p ′ + r C )); to compute and use the SVD of U ⊤ C GU D to to solve (19) via (20), O(r C r D min{r C , r D }); to recover Z R and Z S , O(k(r 2 C + r 2 D )). Thus, assuming k ≤ min{p, p ′ } and using r C ≤ min{p, m ′ } and r D ≤ min{m, p ′ }, the total running time is O(pm ′ r C + p ′ mr D + pp ′ (r C + r D )), as claimed.Theorem 28 The matricesZ S ,Z R of Theorem 26 can be found in O(nnz(A)) + poly(sd λ (Y * )/ε) time, in particular O(nnz(A))+Õ(ε −7 sd λ (Y * ) 2 min{k, ε −1 sd λ (Y * )}) time, such that with constant probability, ARZ R ,Z S SA is an ε-approximate minimizer to (19), that is,(ARZ R )(Z S SA) − A 2 F + λ ARZ R 2 F + λ Z S SA 2 F (24) ≤ (1 + ε) min Y ∈R n×k X∈R k×d Y X − A 2 F + λ Y 2 F + λ X 2 F .(25)With an additional O(n + d)poly(sd λ (Y * )/ε) time, and in particularO(ε −1 k sd λ (Y * )(n + d + min{n, d} min{k/ sd λ (Y * ), ε −1 }))time, the solution matricesỸ ≡ ARZ R ,X ≡Z S SA can be computed and output.Proof: Follows from Theorem 26 and Lemma 27, noting that for efficiency's sake we can use the transpose of A instead of A.4 Regularized Canonical Correlation Analysis  #b0  showed how to use sketching to compute an approximate canonical correlation analysis (CCA). In this section we consider a regularized version of CCA.Definition 29 Let A ∈ R n×d and B ∈ R n×d ′ , and let q = max(rank(A ⊤ A + λ 1 I d ), rank(B ⊤ B + λ 2 I d ′ )). Let λ 1 ≥ 0 and λ 2 ≥ 0. The (λ 1 , λ 2 ) canonical correlations σ (λ 1 ,λ 2 ) 1 ≥ · · · ≥ σ (λ 1 ,λ 2 ) q and (λ 1 , λ 2 ) canonical weights u 1 , . . . , u q ∈ R d and v 1 , . . . , v q ∈ R d ′ are ones that maximizetr(U ⊤ A ⊤ BV ) subject to U ⊤ (A ⊤ A + λ 1 I d )U = I q V ⊤ (B ⊤ B + λ 2 I d ′ )V = I q U ⊤ A ⊤ BV = diag(σ (λ 1 ,λ 2 ) 1 , . . . , σ (λ 1 ,λ 2 ) q )where U = [u 1 , . . . , u q ] ∈ R n×q and V = [v 1 , . . . , v q ] ∈ R d ′ ×q .One classical way to solve non-regularized CCA (λ 1 = λ 2 = 0) is the Björck-Golub algorithm  #b5 . The regularized problem can be solved using a variant of that algorithm, as is shown in the following.Definition 30 Let A ∈ R n×d with n ≥ d and let λ ≥ 0. A = QR is a λ-QR factorization if Q is full rank, R is upper triangular and R ⊤ R = A ⊤ A + λI d .Remark 31 A λ-QR factorization always exists, and R will be invertible for λ > 0. Q has orthonormal columns for λ = 0.

Fact 32 For a λ-QR factorization
A = QR we have Q ⊤ Q + λR −⊤ R −1 = I d .Proof: A direct consequence of R ⊤ R = A ⊤ A + λI d (multiply from the right by R −1 and the left by R −⊤ ).

Fact 33 For a λ-QR factorization
A = QR we have sd λ (A) = Q 2 F .Proof:Q 2 F = tr(Q ⊤ Q) = tr(I d − λR −⊤ R −1 ) = d − λ tr(R −⊤ R −1 ) = d − λ tr((A ⊤ A + λI d ) −1 ) = d − d i=1 λ σ 2 i + λ = d i=1 σ 2 i σ 2 i + λ = sd λ (A) .Theorem 34 (Regularized Björck-Golub) Let A = Q A R A be a λ 1 -QR factorization of A, and B = Q B R B be a λ 2 -QR factorization of B. Assume that λ 1 > 0 and λ 2 > 0. The (λ 1 , λ 2 ) canonical correlations are exactly the singular values of Proof: The constraints on U and V imply that R A U and R B V are orthonormal matrices, so the problem is equivalent to maximizing tr(Ũ ⊤ Q ⊤ A Q BṼ ) subject toŨ andṼ being orthonormal. A well-known result by Von Neumann (see  #b23 ) now implies that the maximum is bounded by the sum of the singular values of Q ⊤ A Q B and that quantity is attained by settingŨ = M and M =Ṽ . Simple algebra now establishes that U ⊤ A ⊤ BV = Σ and that the constraints hold.Q ⊤ A Q B . Furthermore, if Q ⊤ A Q B = M ΣN T is a thin SVD of Q ⊤ A Q B ,We now consider how to approximate the computation using sketching. The basic idea is similar to the one used in  #b0  to accelerate the computation of non-regularized CCA: compute the regularized canonical correlations and canonical weights of the pair (SA, SB) for a sufficiently large subspace embedding matrix S. Similarly to  #b0 , we define the notion of approximate regularized CCA, and show that for large enough S we find an approximate CCA with high probability.Definition 35 (Approximate (λ 1 , λ 2 ) regularized CCA)) For 0 ≤ η ≤ 1, an η-approximate (λ 1 , λ 2 ) regularized CCA of (A, B) is a set of positive numbersσ 1 ≥ · · · ≥σ q , and vectorŝ u 1 , . . . ,û q ∈ R d andv 1 , . . . ,v q ∈ R d ′ such that (a) For every i,σ i − σ (λ 1 ,λ 2 ) i ≤ η . (b) LetÛ = [û 1 , . . . ,û q ] ∈ R n×q andV = [v 1 , . . . ,v q ] ∈ R d ′ ×q . We have, Û ⊤ (A ⊤ A + λ 1 I d )Û − I q ≤ η and V ⊤ (B ⊤ B + λ 2 I d ′ )V s − I q ≤ η .In the above, the notation |X| ≤ α should be understood as entry-wise inequality.(c) For every i,û ⊤ i A ⊤ Bv i − σ (λ 1 ,λ 2 ) i ≤ η .Theorem 36 If S is a sparse embedding matrix with m = Ω(max(sd λ 1 (A), sd λ 2 (B)) 2 /ǫ 2 ) rows, then with high probability the (λ 1 , λ 2 ) canonical correlations and canonical weights of (SA, SB) form an ǫ-approximate (λ 1 , λ 2 ) regularized CCA for (A, B).Proof: We denote the approximate correlations and weights byσ 1 ≥ · · · ≥σ q ,û 1 , . . . ,û q ∈ R d andv 1 , . . . ,v q ∈ R d ′ . LetÛ = [û 1 , . . . ,û q ] ∈ R n×q andV = [v 1 , . . . ,v q ] ∈ R d ′ ×q . Let A = Q A R A be a λ 1 -QR factorization of A, B = Q B R Bbe a λ 2 -QR factorization of B, SA = Q SA R SA be a λ 1 -QR factorization of SA, and B = Q SB R SB be a λ 2 -QR factorization of SB. We use the notation σ i (·) to denote the ith singular values of a matrix. In the following we show that all three claims hold if the following three inequalities hold:Q ⊤ A S ⊤ SQ B − Q ⊤ A Q B F ≤ ǫ/2 Q ⊤ A S ⊤ SQ A − Q ⊤ A Q A F ≤ ǫ/4 Q ⊤ B S ⊤ SQ B − Q ⊤ B Q B F ≤ ǫ/4 .Since for sparse embeddings it holds with high probability thatW ⊤ S ⊤ SH − W ⊤ H F ≤ C W F H F / √ m,for a constant C, and since sd λ 1 (A) = Q A 2 F and sd λ 2 (B) = Q B 2 F , all three will hold with high probability with m that is large enough as in the theorem statement.Proof of (a). As a consequence of Theorem 34, we haveσ i − σ (λ 1 ,λ 2 ) i = σ i (Q ⊤ SA Q SB ) − σ i (Q ⊤ A Q B ) ≤ σ i (Q ⊤ SA Q SB ) − σ i (Q ⊤ A S ⊤ SQ B ) + σ i (Q ⊤ A S ⊤ SQ B ) − σ i (Q ⊤ A Q B )It is always the case that |σ i (Ψ) − σ i (Φ)| ≤ Ψ − Φ 2 [HJ13, Corollary 7.3.5] so with high probabilityσ i (Q ⊤ A S ⊤ SQ B ) − σ i (Q ⊤ A Q B ) ≤ Q ⊤ A S ⊤ SQ B − Q ⊤ A Q B 2 ≤ Q ⊤ A S ⊤ SQ B − Q ⊤ A Q B F ≤ ǫ/2 . To bound σ i (Q ⊤ SA Q SB ) − σ i (Q ⊤ A S ⊤ SQ B ) we use the fact [EI95, Theorem 3.3] that for nonsin- gular D L and D R we have |σ i (Ψ) − σ i (Φ)| ≤ γ · σ i (Ψ) for γ = max( D L D ⊤ L − I 2 , D ⊤ R D R − I 2 ).Let D L = R −⊤ A R ⊤ SA and D R = R SB R −1 B .Both are nonsingular because λ 1 > 0 and λ 2 > 0. We now haveD L D ⊤ L − I 2 = R −⊤ A R ⊤ SA R SA R −1 A − I 2 = R −⊤ A (A ⊤ S ⊤ SA + λ 1 I)R −1 A − I 2 = Q ⊤ A S ⊤ SQ A + λ 1 R −⊤ A R −1 A − I 2 = Q ⊤ A S ⊤ SQ A − Q ⊤ A Q A 2 ≤ ǫ/4 . Similarly, we bound D ⊤ R D R − I 2 ≤ ǫ/4. We now have σ i (Q ⊤ SA Q SB ) − σ i (Q ⊤ A S ⊤ SQ B ) = σ i (R −1 A A ⊤ S ⊤ SBR −1 B ) − σ i (R −1 SA A ⊤ S ⊤ SBR −1 SB ) ≤ ǫ/4 · σ i (R −1 A A ⊤ S ⊤ SBR −1 B ) = ǫ/4 · σ i (Q ⊤ A S ⊤ SQ ⊤ B ) ≤ ǫ/4 · σ i (Q ⊤ A Q B ) + σ i (Q ⊤ A Q B ) − σ i (Q ⊤ A S ⊤ SQ B ) ≤ ǫ/4 · (1 + ǫ/2) ≤ ǫ/2 .Proof of (b). We prove the claim forÛ . The proof forV is analogous. We need to show that with high probability Û ⊤ (A ⊤ A + λ 1 I d )Û − I q ≤ ǫ. Note, that sinceû 1 , . . . ,û q are canonical weights of (SA, SB), then we know thatÛ ⊤ (A ⊤ S ⊤ SA + λ 1 I d )Û = I q . So, the claim is equivalent to the claim that for all i, j we haveû ⊤ i (A ⊤ A + λ 1 I d )û j −û ⊤ i (A ⊤ S ⊤ SA + λ 1 I d )û j ≤ ǫ .For all i, j, we haveû ⊤ i (A ⊤ A + λ 1 I d )û j −û ⊤ i (A ⊤ S ⊤ SA + λ 1 I d )û j = û ⊤ i R ⊤ A R Aûj −û ⊤ i R ⊤ SA R SAûj = û ⊤ i R ⊤ A R A − R ⊤ SA R SA û j = û ⊤ i R ⊤ A I − R −⊤ A R ⊤ SA R SA R −1 A R Aûj If i = j, the Courant-Fischer theorem now implies that û ⊤ i (A ⊤ A + λ 1 I d )û i − 1 = û ⊤ i R ⊤ A I − R −⊤ A R ⊤ SA R SA R −1 A R Aûi ≤ I − R −⊤ A R ⊤ SA R SA R −1 A 2 ·û ⊤ i R ⊤ A R Aûi = I − R −⊤ A R ⊤ SA R SA R −1 A 2 ·û ⊤ i (A ⊤ A + λ 1 I)û i ≤ (ǫ/4) ·û ⊤ i (A ⊤ A + λ 1 I)û i .The last inequality is due to the fact that we already shown in the proof of (a) thatI − R −⊤ A R ⊤ SA R SA R −1 A 2 ≤ ǫ/4. Therefore, u ⊤ i (A ⊤ A + λ 1 I d )û i ≤ 1 + (ǫ/4) ·û ⊤ i (A ⊤ A + λ 1 I d )û i soû ⊤ i (A ⊤ A + λ 1 I d )û i ≤ 1 1 − ǫ/4 ≤ 2 .which now implies that û ⊤ i (A ⊤ A + λ 1 I d )û i − 1 ≤ ǫ/2. For i = j, the submultiplicativity property of matrix norms implies thatû ⊤ i (A ⊤ A + λ 1 I d )û j ≤ I − R −⊤ A R ⊤ SA R SA R −1 A 2 · R Aûi 2 · R Aûi 2 ≤ (ǫ/4) · û ⊤ i (A ⊤ A + λ 1 I)û i · û ⊤ j (A ⊤ A + λ 1 I)û j ≤ (ǫ/4) · max(û ⊤ i (A ⊤ A + λ 1 I)û i ,û ⊤ j (A ⊤ A + λ 1 I)û j ) ≤ ǫ/2Proof of (c). It is enough to show (after adjusting constants) thatû ⊤ i A ⊤ Bv i −σ i ≤ ǫ since we already shown that σ i − σ (λ 1 ,λ 2 ) i ≤ ǫ. We have, û ⊤ i A ⊤ Bv i −σ i = û ⊤ i A ⊤ Bv i −û ⊤ i A ⊤ S ⊤ SBv i = û ⊤ i R ⊤ A (Q ⊤ A Q B − Q ⊤ A S ⊤ SQ B )R Bvi ≤ Q ⊤ A Q B − Q ⊤ A S ⊤ SQ B 2 · R Aûi 2 · R Bvi 2 ≤ Q ⊤ A Q B − Q ⊤ A S ⊤ SQ B 2 · max(û ⊤ i (A ⊤ A + λ 1 I)û i ,v ⊤ i (B ⊤ B + λ 2 I)v i ) ≤ ǫ/2Taking an optimization point of view, the following Corollary shows that the suboptimality in the objective is not too big (the fact that the constraints are approximately held is established in the previous theorem).Corollary 37 Let U L and V L (respectively,Û L andV L ) denote the first L columns of U and V (respectively,Û andV . Then,tr(Û ⊤ L A ⊤ BV L ) ≤ tr(U ⊤ L A ⊤ BV L ) + ǫL .

General Regularization: Multiple-response Regression
In this section we consider the problemX * ≡ argmin X∈R d×d ′ AX − B 2 F + f (X)for a real-valued function f on matrices. We show that under certain assumptions on f (generalizing from f (X) = X h for some orthogonally invariant norm · h ), if we have an approximation algorithm for the problem, then via sketching the running time dependence of the algorithm on n can be improved. When a norm · g is orthogonally invariant, it can be expressed as A g = g(σ 1 , σ 2 , . . . , σ r ), where the σ i are the singular values of A, and g() is a symmetric gauge function: a function that is even in each argument, and symmetric, meaning that its value depends only on the set of input values and not their order.Lemma 40 If P is a contraction, then P is a convex combination of orthogonal matrices: P = j α j U j , where each U j is orthogonal, j α j = 1, and α j ≥ 0.Proof: Please see  #b24 , exercise 3.1.5(h). Briefly: the vector of singular values is contained in the hypercube [−1, 1] n , and so is a convex combination of n + 1 hypercube vertices; as diagonal matrices, these are orthogonal matrices, so if P has SVD P = U ΣV ⊤ , then Σ = j α j D j , where each D j is an orthogonal diagonal matrix, and so P = U ( j α j D j )V ⊤ = j α j U D j V ⊤ ; each summand is an orthogonal matrix.Lemma 41  #b16  If matrix measure f () is left orthogonally invariant and subadditive, then it is left reduced by contractions, and similarly on the right.

Lemma 49
If matrix measure f () is piloi then it inherits a left subspace ε-embedding from the Euclidean norm, and similarly on the right.Proof: Since the columns of AY are members of the columnspace of A, they can be expressed in terms of a basis for that columnspace; that is there is U with orthonormal columns so that for any AY there is some Z so that AY = U Z. So we will assume that A has orthonormal columns. Note that from padding invariance, if n > d we can expand A with orthonormal columnsĀ so that [ AĀ ] is an orthogonal matrix, and pad Y with zero rows, so thatf (AY ) = f ([ AĀ ] Y 0 ) = f ( Y 0 ) = f (Y ) We need to show that for S a subspace ε-embedding for A, it holds that (1 − O(ε))f (AY ) ≤ f (SAY ) ≤ (1 + O(ε))f (AY ) for all Y .For the upper bound on f (SAY ), since SAx 2 ≤ (1 + ε) Ax 2 = (1 + ε) x 2 , we know that SA 2 ≤ 1 + ε, so that 1 1+ε SA is a (nonsquare) contraction. Moreover, if we pad with zeros to make a square matrix, we do not not increase the spectral norm. Since f () is assumed padding invariant, if SA is padded with zero columns, we can also pad Y with rows of zeros. Suppose m > d, so we pad SA with m − d zero columns, and Y with m − d zero rows. So 1 1+ε [ SA 0 ] is a square contraction, and from the left orthogonal invariance of f () and Lemma 41, we have1 1 + ε f (SAY ) = f ( 1 1 + ε [ SA 0 ] Y 0 ) ≤ f ( Y 0 ) = f (Y ),and as noted above, f (Y ) = f (AY ), so that f (SAY ) ≤ (1 + ε)f (AY ), as desired.For the lower bound f (SAY )≥ (1 − ε)f (AY ): since SAx 2 ≥ (1 − ε) Ax 2 = (1 − ε) x 2 for all x, inf x A ⊤ S ⊤ SAx 2 = inf x SAx 2 2 / x 2 ≥ (1 − ε) 2 , so that (A ⊤ S ⊤ SA) −1 2 ≤ 1/(1 − ε) 2 , and (A ⊤ S ⊤ SA) −1 A ⊤ S ⊤ 2 ≤ (1 + ε)/(1 − ε) 2 ≤ 1 + O(ε). Thus f (AY ) = f (Y ) = f ((A ⊤ S ⊤ SA) −1 A ⊤ S ⊤ SAY ) ≤ (1 + O(ε))f (SAY ), and f (SAY ) ≥ (1 − O(ε))f (AY ), as claimed.Remark 50 Note that a sketching matrix that is a subspace ε-embedding on the right for the Euclidean norm is also a subspace embedding on the right for · v , even when p < 1, just applying the Euclidean embedding row-wise.Lemma 51 Let f () be a real-valued function on matrices that is right orthogonally invariant, right reduced by contractions, and inherits a sketching distribution from the Euclidean norm. (If f () is piroi and subadditive, these conditions hold by Lemmas 41 and 49.) Let B ∈ R n×d ′ . LetX * ≡ argmin X∈R d×d ′ AX − B 2 F + f (X),(26)and ∆ * ≡ AX * − B 2 F + f (X * ). Let S ∈ R m S ×n for parameter m S be an affine ε-embedding for A, B with respect to · F . ThenZ * ≡ argmin Z AZSB − B 2 F + f (ZSB) has (AZ * SB − B) 2 F + f (Z * SB) ≤ (1 + ε)∆ * , Proof: Let X * S ≡ argmin X∈R d×d ′ S(AX − B) 2 F + f (X). If S is an affine embedding for A, B, then X * S is a good approximate solution to (26), that is, AX * S − B 2 F + f (X * S ) ≤ (1 + ε)∆ * . Let P SB be the orthogonal projection onto rowspan(SB); note that P SB is a contraction. Then by hypothesis,(SAX * S − SB)P SB 2 F + f (X * S P SB ) ≤ SAX * S − SB 2 F + f (X * S ), using also that the Frobenius norm is reduced by contraction, as noted in Remark 43. That is, X * S P SB has cost no higher than that of X * S , or put another way, without loss of generality, X * S has rows in rowspan(SB). Since X * P SB can be expressed as ZSB for some Z, the lemma follows.The following is the main theorem of this section.Theorem 52 Let f () be a real-valued function on matrices that is right orthogonally invariant, right reduced by contractions, and inherits a sketching distribution from the Euclidean norm on the right. (If f () is piroi and subadditive, these conditions hold by Lemmas 41 and 49.) Let B ∈ R n×d ′ . Let X * and ∆ * as in Lemma 51. Suppose that for r ≡ rank A, there is an algorithm that for general n, d, d ′ , r and ε > 0, findsX with AX − B 2 F + f (X) ≤ (1 + ε)∆ * in time τ (d, n, d ′ , r, ε). Then there is an algorithm that with constant probability finds such aX, taking time O(nnz(A) + nnz(B) + (n + d + d ′ )poly(r/ε)) + τ (d, poly(r/ε), poly(r/ε), r, ε).A norm that is piroi satisfies the conditions of the theorem, using Lemmas 41 and 49. The v-norm for p < 1 also satisfies the conditions of the theorem, as noted in Remarks 43 and 50.Although earlier results for constrained least squares (e.g.  #b12 ) can be applied to obtain approximation algorithms for regularized multiple-response least squares, via the solution of min X∈R d×d ′ AX − B 2 F , subject to f (X) ≤ C for a chosen constant C, such a reduction yields a slower algorithm if properties of f (X) are not exploited, as here.Proof: Let S ∈ R m S ×n be an affine embedding as in Lemma 51; that lemma impliesZ * ≡ argmin Z AZSB − B 2 F + f (ZSB) has (AZ * SB − B) 2 F + f (Z * SB) ≤ (1 + ε)∆ * . Now supposeR ∈ R d ′ ×m Rcomes from a sketching distribution yielding a right subspace ε-embedding with respect to the Euclidean norm for SB, so that by Lemma 49 and hypothesis,R is a subspace embedding on the right for SB with respect to f (). Suppose also thatR ⊤ is an affine embedding for (SB) ⊤ , B ⊤ with respect to the Frobenius norm. For example a sparse embedding with m R = O(rank(SB) 2 /ε 2 ) satisfies these conditions with constant probability.SupposeŜ is an affine embedding for A, BR. TheñZ ≡ argmin Z Ŝ AZSBR −ŜBR 2 F + f (ZSBR) (27) has (AZSB − B) 2 F + f (ZSB) ≤(1 + ε) 3 ∆ * , so thatX ≡ZSB satisfies the conditions of the theorem, up to a constant factor in ε.We need to put (27) into the form of (26). Let D ≡ SBR, and let Q have m Q ≡ rank(D) orthogonal columns and m R rows, such that for upper triangular T ∈ R m Q ×m Q and T ′ ∈ R m Q ×(m R −m Q ) , D ⊤ = Q[T T ′ ]. Then any ZSBR ∈ rowspan(SBR) can be written as Z 1 Q ⊤ , for some Z 1 ∈ R d×m Q . (We can recover Z as in Lemma 27, with a back-solve on Z 1 using T .)Letting P Q ≡ QQ ⊤ , and using P Q (I − P Q ) = 0 and matrix Pythagoras, (27) can be solved by minimizingŜ AZ 1 Q ⊤ −ŜBR 2 F + f (Z 1 Q ⊤ ) = Ŝ AZ 1 Q ⊤ P Q −ŜBRP Q +ŜBR(P Q − I) 2 F + f (Z 1 Q ⊤ ) = Ŝ AZ 1 Q ⊤ P Q −ŜBRP Q 2 F + (P Q − I)ŜBR 2 F + f (Z 1 ), with respect to Z 1 , using also padding invariance and orthogonal invariance of f (). We could equivalently minimizeŜ AZ 1 Q ⊤ −ŜBRQQ ⊤ 2 F + f (Z 1 ) = Ŝ AZ 1 −ŜBRQ 2 F + f (Z 1 ), which has the form of (26).It remains to determine the sketching dimensions for S,Ŝ, andR. We need S ∈ R m S ×n andŜ ∈ R mŜ×n to be affine embeddings for A, B and for A, BR with respect to the Frobenius norm. Sparse embeddings (Def. 22, Lemma 25) have this property, with constant probability for m S , mŜ = O(r 2 /ε 2 ), where again r ≡ rank(A). By hypothesis, we have a distribution overR with mR = poly(m S /ε) = poly(r/ε) with the needed properties. Thus the algorithm of the theorem statement would be called with τ (d, mŜ , mR, r, ε), with the appropriate parameters in poly(r/ε), as claimed.

General Regularization: Low-rank Approximation
For an integer k we consider the problemmin Y ∈R n×k X∈R k×d Y X − A 2 F + f (Y, X),(28)where f (·, ·) is a real-valued function that is piloi in the left argument, piroi in the right argument, and left and right reduced by contraction in its left and right arguments, respectively. For examplef ( Y ℓ , X r ) for piloi · ℓ and piroi · r would satisfy these conditions, as would Y X g for orthogonally invariant norm · g . The functionf could be zero for arguments whose maximum is less than some µ, and infinity otherwise.

Via the SVD
First, a solution method relying on the singular value decomposition for a slightly more general problem than (28).Theorem 53 Let k be a positive integer, f 1 : R → R increasing, and f : R n×k × R k×d → R, where f is piloi and left reduced by contractions in its left argument, and piroi and right reduced by contractions in in its right argument.Let A have full SVD A = U ΣV ⊤ , Σ k ∈ R k×k the diagonal matrix of top k singular values of A. Let matrices W * , Z * ∈ R k×k solve min W ∈R k×k Z∈R k×k W Z diagonal f 1 ( W Z − Σ k (p) ) + f (W, Z),(29)and suppose there is a procedure taking τ (k) time to find W * and Z * . Then the solution tomin Y ∈R n×k X∈R k×d f 1 ( Y X − A (p) ) + f (Y, X)(30)is Y * = U W * 0 (n−k)×k and X * = [ Z * 0 k×(d−k) ] V ⊤ . Thus for general A, (30) can be solved in time O(nd min{n, d}) + τ (k).We will need a lemma. ) σ A , σ B , σ C . For any p ∈ [1, ∞], σ A − σ B p ≤ σ C p .Note that σ A p is the Schatten p-norm A (p) .

Proof: [Proof of Thm 53]
Suppose A has full SVD A = U ΣV ⊤ , and U ⊤ Y XV has full SVD RDS ⊤ , and let W ≡ R ⊤ U ⊤ Y and Z ≡ XV S, so that W Z = D. Then the invariance properties of · (p) and f (·, ·) implyf 1 ( Y X − A (p) ) + f (Y, X) = f 1 ( U RW ZS ⊤ V ⊤ − U ΣV ⊤ (p) ) + f (U RW, ZS ⊤ V ⊤ ) = f 1 ( RW ZS ⊤ − Σ (p) ) + f (W, Z) = f 1 ( RDS ⊤ − Σ (p) ) + f (W, Z).So the objective function is no larger at W, Z than at Y, X if W Z − Σ (p) = D − Σ (p) ≤ RDS ⊤ − Σ (p) . We apply Lemma 54, with A of the lemma mapped to RDS ⊤ and B to Σ, and use the the relation of the Schatten norm to the vector p-norm. The bound follows, and we can assume that W Z is a diagonal matrix D.Since D has rank at most k, it has at most k nonzero entries; we will assume rank(D) = k, but similar arguments go through for rank(D) < k. Let P D have ones where D is nonzero, and zeros otherwise. Then P D W is the projection of W onto the rowspace of D, and ZP D is the projection of Z onto D's columnspace. Since f (·, ·) is appropriately reduced by contractions, and P D W ZP D = D, we can assume that all but at most k rows of W and columns of Z are zero. Removing these zero rows and columns, we have k × k matrices D, W , Z, and Σ, and W and Z are invertible. (Here we use padding invariance, but only to extend f to smaller matrices.)Since the rows of W can be swapped by multiplying by an orthogonal matrix on the left, and the columns of Z via an orthogonal matrix on the right, the nonzero entries of D = W Z can be moved to correspond to the k largest diagonal entries of Σ without changing f (W, Z), and such moves can only decrease D − Σ (p) .We sharpen this result for the case that the regularization term comes from orthogonally invariant norms.Theorem 55 Consider (30) when f (·, ·) has the formf ( Y ℓ , X r ), where · ℓ and · r are orthogonally invariant, andf : R × R → R increasing in each argument. Suppose in that setting there is a procedure that solves (30) when A, Y , and X are diagonal matrices, taking time τ (r) for a function τ (·), with r ≡ rank(A). Then for general A, (30) can be solved by finding the SVD of A, and applying the given procedure to k × k diagonal matrices, taking altogether time O(nd min{n, d}) + τ (k).We will need a lemma.Lemma 56 If E, D, R ∈ R n×n with D and E diagonal, and R orthogonal, for any orthogonally invariant norm · g , there is a permutation π on [n] so tha π(E)D g ≤ ERD g , where π(E) i,i ≡ E π(i),π(i) .Proof: The permutation π we choose is the one that puts the i'th largest entry of |E| with the i'th smallest entry of |D|. Since the singular values of E and D are the nonzero entries of |E| and |D|, this means that the singular values of π(E)D have the form σ i (E)σ n−i+1 (D), where σ i (·) denotes the i'th largest singular value. We use an inequality of  #b38 , page 117, which implies that for any k ∈ [n] and S ⊂ [n] of size k, i∈[k] σ i (ERD) ≥ i∈S σ i (E)σ n−i+1 (D). Since S can be the set of indices of the k largest entries of |π(E)| * |D|, which are the k largest singular values of π(E)D, this implies that for all k, the sum of the k largest singular values of ERD is larger than the corresponding sum for π(E)D. Therefore by the Ky Fan dominance theorem  #b20 , the lemma follows.Proof: [Proof of Thm 55] Following up on the proof of Theorem 53, it suffices to show that when · ℓ and · r are orthogonally invariant, it can be assumed that W and Z are diagonal matrices.Let W have the SVD W = U W Σ W V ⊤ W . Then Z = W −1 D = V W Σ −1 W U ⊤ W D, so thatf ( W ℓ , Z r ) = f ( Σ W ℓ , Σ −1 W U ⊤ W D r ), using orthogonal invariance. We now apply Lemma 56, with E of the lemma mapping to Σ −1 W , R to U ⊤ W , and D to D. This yields a permutation π on the entries of Σ −1 W so that π(Σ −1 W )D r ≤ Σ −1 W U ⊤ W D r , so that the diagonal matrices π(Σ W ) and π(Σ −1 hW )D have product D and objective function value no larger than W and Z; that is, without loss of generality, W and Z are diagonal. Thus minimizing after obtaining the singular values Σ of A, the solution of W Z − Σ 2 F +f ( W ℓ , Z r ) with W and Z diagonal is sufficient to solve (30).Definition 57 (clipping to nonnegative (·) + ) For real number a, let (a) + denote a, if a ≥ 0, and zero otherwise. For matrix A, let (A) + denote coordinatewise application.Corollary 58 If the objective function in (30) is Y X − A 2 F + 2λ Y X (1) or Y X − A 2 F + λ( Y 2 F + X 2 F ), then the diagonal matrices W * and Z * from Theorem 55 yielding the solutionare W * = Z * = (Σ k − λI k ) + , where Σ k is the k × k diagonal matrix of top k singular values of A [UHZB14].If the objective function is Y X−A (p) +λ Y X (1) for p ∈ [1, ∞], then W * = Z * = (Σ k − αI k ) + , for an appropriate value α.If the objective function is Y X − A 2 F + λ Y X 2 F , then W * = Z * = Σ k /(1 + λ).Proof: Omitted.

Reduction to a small problem via sketching
Theorem 59 Suppose there is a procedure that solves (28) when A, Y , and X are k × k matrices, and A is diagonal, and Y X is constrained to be diagonal, taking time τ (k) for a function τ (·). Let f also inherit a sketching distribution on the left in its left argument, and on the right in its right argument. Then for general A, there is an algorithm that finds ε-approximate solution (Ỹ ,X) in time O(nnz(A)) +Õ(n + d)poly(k/ε) + τ (k).Proof: We follow a sequence of reductions similar to those for Theorem 52, but on both sides. Let (Y * , X * ) be an optimal solution pair:Y * , X * ≡ argmin Y ∈R n×k X∈R k×d Y X − A 2 F + f (Y, X),(31)and ∆ * ≡ Y * X * − A 2 F + f (Y * , X * ). Let S ∈ R m S ×n be an affine ε-embedding for Y * , A with respect to · F . From Lemma 51,Z * ≡ argmin Z∈R k×m S Y * ZSA − A 2 F + f (Y * , ZSA)has (Y * Z * SA − A) 2 F + f (Y * , Z * SA) ≤ (1 + ε)∆ * . Now suppose R ∈ R d×m R is a right affine ε-embedding for Z * SA, A with respect to · F . Then again by Lemma 51, applied on the right,W * ≡ argmin W ∈R m R ×k ARW Z * SA − A 2 F + f (ARW, Z * SA)has (ARW * Z * SA − A) 2 F + f (ARW * , Z * SA) ≤ (1 + ε) 2 ∆ * . It doesn't hurt to find the best W * , Z * simultaneously, so redefining them to beW * , Z * ≡ argmin W ∈R m R ×k Z∈R k×m S ARW ZSA − A 2 F + f (ARW, ZSA)(32)satisfies the same approximation property. SupposeR ∈ R d×mR comes from a sketching distribution yielding a right subspace ε-embedding with respect to the Euclidean norm for SA, so that by assumption,R is a subspace ε-embedding on the right for SA with respect to the right argument of f (·, ·). Suppose also thatR ⊤ is an affine embedding for (Z * SA) ⊤ , A ⊤ with respect to the Frobenius norm. SupposeŜ is similarly a left subspace ε-embedding for AR with respect to the left argument of f (·, ·), and an affine embedding on the left for ARW , AR with respect to the Frobenius norm, whereW is the solution to min W ∈R m R ×k ARW Z * SAR − AR 2 F + f (ARW, Z * SAR). TheñW ,Z ≡ argmin W ∈R m R ×k Z∈R k×m S Ŝ ARW ZSAR −ŜAR 2 F + f (ŜARW, ZSAR)(33)form a (1+O(ε))-approximate solution to (32), and therefore yield a (1+O(ε))-approximate solution to (31). We need to put the above into the form of (28). Suppose Q ℓ is an orthogonal basis for colspace(ŜAR), and Q ⊤ r an orthogonal basis for rowspan(SAR). Then any matrix of the form SARW can be written as Q ℓ W 1 for some W 1 ∈ R rank(SAR)×k , and similarly any matrix of the form ZSAR can be written as Z 1 Q ⊤ r for some Z 1 . Thus solving (33) is equivalent to solving W 1 ,Z 1 ≡ argminW 1 ,Z 1 Q ℓ W 1 Z 1 Q ⊤ r −ŜAR 2 F + f (Q ℓ W 1 , Z 1 Q ⊤ r ).(We can recoverW andZ fromW 1 andZ 1 via back-solves with the triangular portions of changeof-basis matrices, and padding by zeros, as in Lemma 27 and Theorem 52.) Using the properties of f (, ) we have f (Q ℓ W 1 , Z 1 Q ⊤ r ) = f (W 1 , Z 1 ). Let P ℓ ≡ Q ℓ Q ⊤ ℓ , and P r ≡ Q r Q ⊤ r . Using P ℓ (I − P ℓ ) = 0 and P r (I − P r ) = 0 and matrix Pythagoras, we haveQ ℓ W 1 Z 1 Q ⊤ r −ŜAR 2 F + f (Q ℓ W 1 , Z 1 Q ⊤ r ) = P ℓ Q ℓ W 1 Z 1 Q ⊤ r P r −ŜAR 2 F + f (W 1 , Z 1 ) = P ℓ Q ℓ W 1 Z 1 Q ⊤ r P r − P ℓŜ ARP r 2 F + (I − P ℓ )ŜAR 2 F + P ℓŜ AR(I − P r ) 2 F + f (W 1 , Z 1 )So we could equivalently minimizeP ℓ Q ℓ W 1 Z 1 Q ⊤ r P r − P ℓŜ ARP r 2 F + f (W 1 , Z 1 ) = Q ℓ W 1 Z 1 Q ⊤ r − Q ℓ Q ⊤ ℓŜ ARQ r Q ⊤ r 2 F + f (W 1 , Z 1 ) = W 1 Z 1 − Q ⊤ ℓŜ ARQ r 2 F + f (W 1 , Z 1 ),which has the form of (28).It remains to determine the sizes of S, R,R, andŜ, and the cost of their applications. We use the staged construction of Lemma 25, so each of these matrices is the product of a sparse embedding and an SHRT. We have m R and m S bothÕ(k/ε 2 ), and mR = mŜ =Õ(k/ε 4 ), noting that we needŜ to be a subspace ε-embedding for AR, of rankÕ(k/ε 2 ), and similarly forR. Moreover, to computeŜAR, SAR, andŜAR, we can first apply the sparse embeddings on either side, and then the SHRT components, so that the cost of computing these sketches is O(nnz(A)) +Õ(k 2 /ε 6 ). Since the remaining operations involve matrices withÕ(k/ε 4 ) rows and columns, the total work, up to computing ARW andZSA, is O(nnz(A)) +Õ(poly(k/ε)) + τ (k). The work to compute those products is O(n + d)poly(k/ε), as claimed. 

Estimation of statistical dimension
Let z ′ be the smallest z of the form 2 j for j = 0, 1, 2, . . ., with z ′ ≤ 6M , such that z ′ ≥γ z ′ /λ. Since M ≥ sd λ (A) ≥ 3 8 z for z ≤γ z /λ, there must be such a z ′ . Then by considering the lower bound of (34) for z ′ and for z ′ /2, we have sd λ (A) ≥ 3 8 max{z ′ /2,γ z ′ /λ} ≥ 1 16 (z ′ +γ z ′ /λ), which combined with the upper bound of (34) implies that z ′ +γ z ′ /λ is an estimator of sd λ (A) up to a constant factor.