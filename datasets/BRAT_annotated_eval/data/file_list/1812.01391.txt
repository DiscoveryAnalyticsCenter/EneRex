A DEEP LEARNING FRAMEWORK FOR SEMI-SUPERVISED CROSS-MODAL RETRIEVAL WITH LABEL PREDICTION

Abstract
Due to abundance of data from multiple modalities, cross-modal retrieval tasks with image-text, audioimage, etc. are gaining increasing importance. Of the different approaches proposed, supervised methods usually give significant improvement over their unsupervised counterparts at the additional cost of labeling or annotation of the training data. Semi-supervised methods are recently becoming popular as they provide an elegant framework to balance the conflicting requirement of labeling cost and accuracy. In this work, we propose a novel deep semi-supervised framework which can seamlessly handle both labeled as well as unlabeled data. The network has two important components: (a) the label prediction component predicts the labels for the unlabeled portion of the data and then (b) a common modality-invariant representation is learned for cross-modal retrieval. The two parts of the network are trained sequentially one after the other. Extensive experiments on three standard benchmark datasets, Wiki, Pascal VOC and NUS-WIDE demonstrate that the proposed framework outperforms the state-of-the-art for both supervised and semi-supervised settings.

Introduction
The steady increase in the amount of multimedia data like images, texts, sketches, audio, video, etc. over the last several years have made cross-modal retrieval a very active area of research. As an example, given an image or sketch, we may want to retrieve the textual documents semantically related to it. Though some of the data samples are associated with single labels (or tags)  #b22 , usually they can be better described using multiple labels  #b4  Chua et al. [2009] since non-binary similarity between examples can be better captured by using multiple labels. Few illustrative examples from different datasets are shown in Figure 1.The cross-modal retrieval algorithms can be broadly classified under three categories, namely (1) unsupervised (b) supervised and (c) semi-supervised. Supervised algorithms usually outperform their unsupervised counterparts due to the presence of labeled training data, which is often quite expensive to obtain. Semi-supervised frameworks  #b31   #b30   #b20  serve as a trade-off between these two important but conflicting requirements of performance and labeling cost. This is achieved by utilising a small amount of labeled data and a large amount of unlabeled data (which is easy to obtain but costly to annotate).In this work, we develop a deep learning framework for cross-modal retrieval which can seamlessly work under both supervised and semi-supervised settings. The proposed framework consists of two main components, namely (a) a label prediction module and (b) a common representation learning module. The first module is used to effectively predict the labels for the unlabeled portion of the training data, whereas a common modality-invariant representation corresponding to the input modalities is learned in the second module. We propose different losses, so that both the Observe that the presence of multiple tags helps to define non-binary relationships between the example data.labeled and unlabeled data can be utilized to learn an effective common representation for retrieving cross-modal data. The two parts of the network are trained sequentially one after the other. Extensive experiments on three standard benchmark datasets, namely Wiki  #b22 , Pascal VOC 2007 #b4  and NUS-WIDE Chua et al. [2009] and comparisons with state-of-the-art cross-modal techniques show the effectiveness of the proposed approach. The main contributions of the proposed work can be summarized as follows• We propose a deep learning framework for cross-modal retrieval for semi-supervised setting.• To the best of our knowledge, this is the first deep learning framework which has a label prediction framework for cross-modal retrieval to handle the unlabeled examples.• The proposed framework can seamlessly handle different scenarios -supervised and unsupervised settings, single-label and multi-label data which are either paired or unpaired.• Extensive experiments show the effectiveness of the proposed approach. Specifically, in the challenging scenario when the amount of labeled training data is less, it significantly outperforms the state-of-the-art.

Related Work
Cross-modal retrieval is an active area of research and the different approaches proposed in literature can be divided into unsupervised, supervised and semi-supervised. Unsupervised approaches do not have access to the training labels and in general utilizes the correspondence between the data of the two modalities to learn a common space. Canonical Correlation Analysis (CCA) and its kernelized version (KCCA) Hardoon et al. [2004] tries to project the data from the different modalities so that they become correlated. Partial Least Squares  #b24  linearly maps the different modalities into a common space. In  #b9 , connections between the objects in an image with keywords in the textual queries are established to design better cross-modal associations.The supervised methods uses the class labels (single or multiple labels) to generate more meaningful connections between the data of the two modalities. Deep CCA  #b0  and Deep CCA AE  #b29  integrate the concept of correlation learning into an encoder-decoder framework with reconstruction losses to learn the common space. The discriminant latent space is learned by using the label information in the work of  #b25 . Multiview Discriminant Analysis (MvDA)  #b13  uses both the intra and inter domain relationships to capture more discriminative information for effective design of the common space. The work in  #b28  uses the concept of l 2,1 penalties on the projection functions for selecting discriminative and relevant features from the learned common domain. Cluster CCA (CCCA)  #b23  uses group class label information to effectively design the common space and its enhanced version multi-label CCA (ml-CCA) Ranjan et al. [2015] can handle multi-label data. Another algorithm which can effortlessly handle multi-label data to compute good common domain representation is the work in  #b14 . Dictionary based supervised methods for cross-modal retrieval has been proposed in GCDL Mandal and Biswas [2016] and S2CDL Das et al. [2017]. CCCA Rasiwasia et al. [2014], ml-CCCA Ranjan et al. [2015] and GCDL Mandal and Biswas [2016] can handle both paired and unpaired data. Deep supervised cross-modal algorithms have also been developed in the works of  #b19  and  #b26 .Semi-supervised methods  #b30  #b20  #b31  try to bridge the gap between the supervised and unsupervised methods by utilizing both labeled as well as unlabeled data for training, and it is much less explored compared to the others.  #b30  establishes an unified optimization framework to jointly associate the relationship between the correlation and the semantic information of the training examples. Regularization is used in  #b30  to align the modalities more closely while making the model more robust to noise. In  #b20 , a special patch graph regularization term is used to capture the underlying graphical structure of the different modalities jointly, which helps to integrate the complementary information. GSS-SL  #b31  captures the intrinsic manifold of the different modalities by using a label graph constraint and utilizes the label space for the common representation. It uses a combination of label-linked loss and graph regularization to jointly predict the labels for the unlabeled data and learn the common space effectively.The concept of label cleaning and prediction from weakly annotated or unlabeled data for single modality has been explored in Veit  #b27  select a small subset of the data whose clean annotations are available and tries to learn functions so as to clean the label noise. The work in  #b27  jointly learns to clean noisy annotations and design an effective image classifier. Our work focuses on label prediction for cross-modal data and to the best of our knowledge is the first work in this field.Another active research area closely related to cross-modal retrieval is cross-modal hashing  #b11 Li [2016] Liong et al. [2017], where data from different modalities are projected into a discrete domain for fast and accurate retrieval. The cross-modal hashing techniques differ from the standard cross-modal retrieval in some important aspects, namely (a) projection into the discrete hamming space and (b) the retrieval data being the same as the training data. In cross-modal retrieval, the retrieval set is completely disjoint from the training set, making the problem more challenging.

The Proposed Approach
Here, we describe in details the proposed deep learning framework for semi-supervised cross-modal retrieval. The proposed framework consists of two main modules, the label prediction (LP) part and the common representation learning (CRL) part.

Notations
Let the data from the two modalities X , Y be denoted as D x ∈ R dx×Nx and D y ∈ R dy×Ny , where, d x and d y are the dimensions of the two modalities (d x = d y in general). The number of samples in both the modalities may be same i.e., N x = N y = N or different i.e. N x = N y . Out of them, N l data samples of both modalities are labeled, and the label matrix is denoted as L ∈ R dc×N l , where d c is the total number of categories. For single label (S-L) data, only one of the entries in d c is one, while for multi-label (M-L) data, multiple entries in d c can be one signifying the presence of multiple tags. Thus, the number of unlabeled data in X and Y modalities are N ul x = N x − N l and N ul y = N y − N l respectively. For ease of explanation, we will first consider the unlabeled data to be paired i.e., N ul x = N ul y = N ul and then extend it to handle the unpaired setting. We denote the labeled and unlabeled portion of the training data as D lx , D l y and D ul x , D ul y respectively. For performing cross-modal retrieval, first the data from the two modalities are projected to a common representation space, which is taken as the label space  #b31   #b3  in our work. In the proposed framework, two different deep networks are used to project the data of the two modalities. To handle the unlabeled training samples, we also design a label prediction module which is trained to predict the labels for both S-L and M-L data. The predicted labels are then fed to the main representation learning branch of the network. This is inspired from  #b27 , though there are significant differences as explained in the next section. The two parts of the network are trained sequentially one after the other (Figure 3). We now describe the two modules of the proposed framework in details below.

Label Prediction (LP)
We design a Label Prediction (LP) network to predict the labels (S-L or M-L) of the unlabeled portion of the training data of both the modalities. Our network is inspired by the label cleaning framework proposed in  #b27 , though in this work, they had access to the noisy labels and the goal was to clean the noisy labels for the task of single Figure 2: Illustration of the label prediction network. Here, the input to the network (with brown arrows) are image data, text data, noisy labels and the output are the predicted labels. The activation function helps to scale the prediction between [0, 1] in case of M-L data. For S-L data, the activation function is replaced with a softmax function i.e., f sof t (.). The loss function used is L wbce for M-L and L ce for the S-L dataset respectively. modality classification. In contrast, a portion of the training data in our work is completely unlabeled and the task is cross-modal retrieval.First, we generate weak labels/annotations of the unlabeled training data, i.e. D ul x , D ul y by utilizing the labeled portion using a simple, but effective approach. We randomly select a small portion of the labeled training data to form the Nearest Neighbor (NN) set as {D l,nn x , D l,nn y , L nn }. We also keep aside another portion as validation (val) set {D l,val x , D l,val y , L val } required for training the LP network. The remaining labeled data {D l,trx , D l,tr y , L tr } forms the train set for the LP network. Considering the NN set to be a set of anchor points, we use them to compute the weak/noisy annotations for the val and train set. For each paired data sample (x, y) ∈ (D l,trx , D l,tr y ), we find its closest match in (D l,nn x , D l,nn y ) and assign its label (L n,trx , L n,tr y ) as the noisy label for the pair (x, y). Since the training data is paired, we do a logical "OR" (f or ) operation to generate the final noisy label L n,tr = f or (L n,trx , L n,tr y ). The same approach is followed for generating the weak annotations for (D l,val x , D l,val y ).Thus, the problem of label prediction for the unlabeled data is transformed to a problem of label cleaning, where the noisy labels are generated as described above. The LP network consisting of fully connected (fc) layers and skip connection, takes the data from the two modalities D l,trx , D l,tr y , their noisy labels L n,tr as input, and tries to predict the correct labels i.e., L tr . We use both the modalities together to utilize the complementary information to better predict the missing annotations. An illustration of the LP network is given in Figure 2, where we first transform the three inputs using a fc layer and then concatenate the outputs before sending it through two additional fc layers. The output of the final fc layer for the i th data sample, g i has the same dimensionality as the label vector i.e., d c and is matched with the correct labels L tr i . Instead of learning the complete clean label from scratch, we learn the residual and then use an identity connection  #b8   #b27  from the input noisy label L n,tr i to get to the correct label L tr i . For M-L data, the predicted labels are computed asL tr i = clip(L n,tri + g i , [0, 1]),where the clip operation forces the predicted label to be within the correct range i.e., [0, 1]. We then compute the loss between the predictedL tr i = {L tr i,1 ,L tr i,2 , ...,L tr i,dc } and the correct L tr i = {L tr i,1 , L tr i,2 , ..., L tr i,dc } and back-propagate the gradients to train the network. We use the weighted binary cross entropy (WBCE) loss asL wbce (L tr i ,L tr i ) = − dc j=1 [w j L tr i,j log(L tr i,j ) + (1 − L tr i,j ) log(1 −L tr i,j )](1)where, the loss L(L tr i ,L tr i ) is computed across each label vector dimension i.e., j = {1, ..., d c }, L tr i ,L tr i are the two arguments to the loss and w j ≥ 1 denotes the positive weight for dimension j. For multi-label datasets, the absence of a tag is more common compared to it being present, and hence we want to penalize marking a present tag as absent considerably more than the opposite using this weight. The positive weights w j can be set empirically by studying the distribution of ones and zeros in the labeled portion of the data. We also tried using the standard BCE and the absolute distance error measure as in  #b27  in the LP framework, but the WBCE loss gave significant improvement in performance compared to the others. We have analyzed the importance of this loss in more details later.For S-L data, the noisy input to the LP network i.e., L n,tr i is one-hot encoding representation. We make corrections using the skip connection as beforeL tr i = f sof t (L n,tr i + g i ) where (f sof t (.)) is the softmax layer used to predict the final label. In this case, we use cross entropy loss (CE) to back-propagate the error.L ce (L tr i ,L tr i ) = − dc j=1 L tr i,j log(L tr i,j )(2)Thus, the final label prediction loss is given asL lp =      tr L wbce (L tr i ,L tr i ), for M-L tr L ce (L tr i ,L tr i ), for S-L(3)We train our LP network using {D l,tr x , D l,tr y , L n,tr } and track its performance progress by computing the accuracy on the validation set {D l,val x , D l,val y , L n,val }.As we will see in the next section, for training the CRL network, the labels of the unlabeled training data predicted by the LP network are used. One pertinent question here is how to feed the data to the LP network if the unlabeled training data is unpaired. In this case, for an unlabeled sample x ∈ D ulx whose paired sample in D ul y is unavailable, we compute its nearest neighbor in D l,nn x and mark its weak annotation as L n,ul x . We find the closest match of x in D l,tr x and set its corresponding data sample in Y domain as y and its label as L n,ul y . We then generate the noisy label L n,ul as previously done.

Common Representation Learning (CRL)
For cross-modal retrieval, we need to project the data from the two different modalities into a common space, where they have a shared representation. In this work, based on the success of  #b3  and  #b31 , we select the label space i.e., R dc for this purpose. For each modality, we use an encoder-decoder architecture with the the size of the bottleneck layer being equal to d c . An illustration showing our proposed architecture is given in Figure 3. For learning the common representation, we project both the labeled D l x , D l y and unlabeled D ul x , D ul y data into the label space. Ideally, we want the following (a) the projection of the data from the two modalities should be consistent with their labels, (b) similar data should lie close together while dissimilar data should be far apart and (c) the bottleneck code so generated should preserve most of the information from the original features. To satisfy the above three objectives we design our loss functions appropriately.Consider the neural network functions defined by the encoder and decoder for the X , Y modalities be denoted as P x (.), P y (.) and Q x (.), Q y (.). Both the encoder and decoder networks consists of three fc layers, with the decoder layers having mirrored sizes to that of the encoder. To make the projections of P x (.), P y (.) ∈ R dc , the final activation function used is the sigmoid layer (σ(.)) for the M-L data and softmax layer (f sof t (.)) for the S-L data. We now describe the different losses used in our work.Label Loss: The label loss L lab tries to make the final encoder outputs consistent with that of the label space by minimizing the following objective (for t = {x, y})L lab =      N l i=1 t L wbce (L i , σ(P t (D l t,i ))), for M-L N l i=1 t L ce (L i , f sof t (P t (D l t,i ))), for S-L(4)Cross Reconstruction Loss: The objective of the decoder is to reconstruct the input data from the common representation. We enforce the reconstruction losses to reduce the amount of information loss from the main feature domain to the bottleneck layer. In a standard encoder-decoder structure, the input to the decoders for the two modalities Q x (.), Q y (.) should be the outputs of the corresponding encoders P x (.), P y (.) respectively. Here, since the labeled data is given in pairs (or pairs can be generated by studying the labels), we instead send the common representation generated by P x (.) through Q y (.) to reconstruct the data of the other modality and vice-versa. This loss also helps to bridge the modality gap between the two inputs. We thus define our cross reconstruction loss as (for t = {x, y} and t = {y, x}) Figure 3: Illustration of the proposed framework. The pre-trained LP block (marked in blue) outputs the predicted labels for the unlabeled data and feeds it to the label lossL lab . The LP network also takes as input the noisy labels for the unlabeled data, which can be generated both for paired and unpaired data. The label loss for the labeled examples L lab is computed from the output of the activation layer. The generate pair block generates matched and non-matched pairs from the two modalities which is used to compute the similarity L sim and dissimilarity L dsim losses respectively. Also, the outputs of the decoders are used to compute the cross reconstruction losses L cross . (Figure best viewed in color).L cross = N l i=1 t,t ||D l t,i − Q t (P t (D l t ,i ))|| 1(5)Similarity Dissimilarity Learning Loss: The objective of this loss is to maintain the semantic structure of the data in the common representation. Though this is partly enforced by the label loss, here, we explicitly want to minimize the difference in the representations of all semantically similar data from the different modalities, while maximizing the distance between semantically different data. We observe that enforcing this at the outputs of the encoders P x (.), P y (.) before the final activation function actually helps in the projection to the label space. We enforce this loss only on the labeled portion of the data i.e., D l x , D l y . The semantically similar (S 1 ) and dissimilar sets (S 2 ) can be constructed from the provided labels L. For S-L data, we assume two data samples to be semantically similar if their labels are same, else they are dissimilar. For M-L data, we measure the similarity using normalized inner product (f s (.) =< ., . >) between the labels. If the similarity is more than a threshold τ 1 , we consider the samples as semantically similar, and if it is less than a threshold τ 2 (τ 1 > τ 2 ), we consider them as semantically dissimilar. Here we consider only the samples which are strongly similar or dissimilar for training, and we choose to ignore the small number of examples falling between the thresholds for which we are less confident. Formally, two data samples are considered similar or dissimilar based on the followingD l x,i , D l y,j ∈ S 1 if, L i = L j (S-L) or f s (L i , L j ) ≥ τ 1 (M-L) D l x,i , D l y,j ∈ S 2 if, L i = L j (S-L) or f s (L i , L j ) ≤ τ 2 (M-L)We finally define the similarity and dissimilarity losses over the sets S 1 and S 2 respectively asL sim = (xi,yj )∈S1 ||P x (x i ) − P y (y j )|| 2 2 (6) L dsim = (xi,yj )∈S2 max 0, µ − ||P x (x i ) − P y (y j )|| 2 2 (7)where, x i ∈ D l x , y j ∈ D l y and µ is a margin set by cross-validation. This implies that the distance between the representations of a non-matched or dissimilar pair must at least be µ distance apart, whereas the distance between matched pairs should be as small as possible.Total Loss: Thus the total loss for the CRL network is given as L crl = α 1 L lab + α 2 L cross + α 3 L sim + α 4 L dsim + βL lab (8) We have denoted the modes in which the methods are operating as -"s" (supervised), "us" (unsupervised) and "ss" (semi-supervised). The superscript "p" and "up" denotes whether the unlabeled training data is provided with paired correspondence or not. The algorithms in the "ss" mode is evaluated following the split as in  #b31 . where,L lab is the label loss with respect to the predicted labels provided by pre-trained LP for the unlabeled data D ulx , D ul y . The variables {α 1 , α 2 , α 3 , α 4 , β} shows the different weights of the losses and is set by cross-validation experiments.

Model training
Here, we briefly discuss the procedure to train the two parts of the network. The LP network in the proposed framework is trained initially based on the L lp loss whose training is stopped based on the accuracy on the validation set {D l,val x , D l,val y , L n,val }. The loss function for training the CRL network (8) however deals with both the labeled and unlabeled portion of the data. The unlabeled wbce lossL lab is based on the predicted labels of the pre-trained LP network. In this work we have trained the two deep sub-networks separately because studies in  #b27  showed no significant performance improvement on joint training of the two networks. During testing, the common representation of the query and the data in the retrieval set are computed, which are used for retrieving items similar to the query.

Experiments
Here, we report the results of the proposed framework and compare it with the state-of-the-art approaches. Specifically, we evaluate on three benchmark datasets, Wiki  #b22  which is single-label and Pascal VOC 2007  #b4  and NUS-WIDE Chua et al. [2009], which are annotated with multiple labels for both supervised and semi-supervised settings. Though all the experiments are on image-text retrieval, this method can work on any cross-modal retrieval application. Finally, we perform extensive analysis with respect to the amount of labeled data available and the loss functions used.

Datasets and Evaluation Protocol
The Wiki dataset  #b22  contains about 2, 866 articles with their corresponding images and textual documents spread across 10 different categories such as art, history, etc. collected from the Wikipedia repository. We consider 4096-d CNN Jia et al. [2014] feature representation for images and 100-d word vectors  #b18  for texts. The train:test split is taken to be 2000 : 866 out of which 1500 samples of the training data are assumed to be labeled  #b31 .The Pascal VOC 2007 dataset  #b4  consists of images and their textual queries annotated with multiple tags and the standard train:test split is 5011 : 4952 images-text pairs. As in  #b31 , we remove all the pairs whose textual features are entirely zeros, thus giving the final train:test split as 5000 : 4000. We consider the labeled and unlabeled split to be 4000 : 1000 and use 399-d word frequency features for text representation and 512-d GIST features to describe the images as in  #b31 .The NUS-WIDE dataset Chua et al. [2009] is a large dataset which has images and the associated tags spread across 81 unique categories. We follow the same protocol as in  #b31  and consider the data that belongs to the top 10 largest groups, with a train:test split of 40834 : 27159. We use 500-d SIFT and 1000-d word frequency vectors for image and text representation respectively and 35000 samples from the training data are considered to be labeled as in  #b31 .We use Mean Average Precision (MAP) as the evaluation metric which is defined as the mean of the average precision (AP) for all queries. AP can be defined asAP (q) = R r=1 Pq(r)δ(r) R r=1 δ(r), where q is the query, R is the number of retrieved items and P q (r) is the precision for query q at position r. MAP@R essentially measures the retrieval accuracy when R number of items from the database are being retrieved per query item. We report both MAP@50 and MAP@all for our experiments  #b31 . For S-L dataset like Wiki  #b22 , a retrieved sample is considered to be correct (i.e., δ(r) = 1) if it has the same class as the query. For M-L datasets, if the retrieved sample shares at least one common concept with the query, it is assumed to be correct. We have compared the proposed approach against several recent cross-modal retrieval algorithms such as CCA Hardoon et al. [2004], SCM Rasiwasia et al. [2010], GMLDA and GMMFA  #b25 , LCFS Wang et al. [2013], MvDA  #b13 , LGCFL Kang et al. [2015], ml-CCA Ranjan et al. [2015] and GSS-SL  #b31 . In this setting, the train and test sets are disjoint, unlike that in cross-modal hashing, and so comparisons with hashing techniques have not been performed.

Results under supervised settings
First, we evaluate the proposed approach for supervised setting where all the training samples are annotated with labels. The results of the proposed framework denoted as Ours s for the three datasets are provided in Table 1. The other numbers are directly taken from the work in  #b31 . We observe that the proposed approach outperforms the state-of-the-art for Wiki and Pascal VOC datasets and gives comparable performance for NUS-WIDE. We also observe that for R@all, the improvement obtained by the proposed approach over GSS1-SL is more which signifies that our performance degrades gracefully when larger number of items are retrieved.

Results under semi-supervised settings
Here, we evaluate the proposed framework under the semi-supervised setting on the three datasets by taking the same split as in  #b31 . Our results are denoted as Ours p ss and Ours up ss for the paired and unpaired scenarios respectively in Table 1. We observe that even for the semi-supervised scenario, the proposed framework outperforms GSS1-SL. As expected, the performance is slightly lower than the supervised counterpart. Another interesting observation is that the paired setting usually gives slightly better performance. This signifies the importance of the complementary information that is obtained from the correspondence information of the two modalities. Some retrieval results for the Pascal VOC dataset is shown in Figure 4. We observe that the unlabeled data helps to retrieve more meaningful images from the database. Notice that a completely irrelevant image of "motorbike" has been retrieved in the supervised case which has been corrected in the semi-supervised mode.We also evaluate the importance of each loss in our formulation on Wiki  #b22  and Pascal VOC  #b4  datasets for 20% labeled data. We observe from Table 2 that each of the losses contributes to the good performance of the proposed framework, though in varying amounts.

Effect of varying amount of labeled data
Here, we perform an experiment to analyze two things, (1) performance of the proposed framework for varying amounts of labeled data and (2) usefulness of the unlabeled data in improving the retrieval performance, which also is a measure of the effectiveness of the LP framework. To do this, we vary the percentage of labeled data starting from 20%, with increments of 10% to 90% and report the performance of the proposed framework. The remaining data is unlabeled and paired for this experiment. In the semi-supervised mode, we use the predictions of the LP network.The MAP@50 results for the S-L Wiki  #b22  dataset are reported in Figure 5, and for the M-L Pascal VOC  #b4  dataset are reported in Figure 6. We denote the results as I-Q (query is image), T-Q (query is text) and Avg (Average MAP). The superscript l and ul signifies that the algorithm is working in supervised Figure 4: Some text-image retrieval results for Pascal VOC Everingham et al. [2010] using our algorithm in semisupervised mode with 20% labeled data. The actual tags are shown. We observe a noticeable improvement in the retrieval performance using the additional unlabeled data.   VOC Everingham et al. [2010] with increasing percentage of labeled data. and semi-supervised mode respectively. We make the following observations from the results: (1) as expected, we notice that there is a monotonic increase in the retrieval percentage with the increase in the amount of labeled data; but after a certain stage it starts to saturate. This happens when the amount of labeled training data is sufficient to train the common domain projection functions i.e. P x (.), P y (.); (2) we observe that the unlabeled data indeed helps to improve  the performance, which also justifies the usefulness of the LP network; (3) when the amount of labeled data is less, the boost in performance provided by the unlabeled data is more as compared to when the labeled data is more. This is also partly because the amount of unlabeled data is more when there is less amount of labeled data. This is clearly evident from the results of Pascal VOC  #b4  dataset. We obtain similar observations when the unlabeled training data is also unpaired. We also report the performance of Ours up ss as compared to GSS1-SL  #b31  in case of unpaired unlabeled data in Figures 7 and 8 for both image and textual queries. We observe similar trend as the paired setting, though the unpaired unlabeled data leads to less improvement in performance compared to the paired unlabeled counterpart.We also analyze the performance of the label prediction network based on the amount of the labeled data. For S-L data, we consider a predicted label as correct if it matches with its ground truth. For M-L data, as previously discussed since the number of zeros is significantly higher than that of ones, we report the average number of errors in predicting ones and zeros as the accuracy. We measure the accuracy of the label prediction on the unlabeled paired data samples for the Wiki  #b22  and Pascal VOC Everingham et al. [2010] datasets, and the results as a function of amount of labeled data is given in Figures 9, 10. As expected, greater the number of labeled examples, the easier it is to predict the labels for the unlabeled data.  

Analysis of different losses for LP network
We analyzed several standard losses for training the LP network. The CE loss was a natural choice for the S-L dataset.For the M-L datasets, we analyzed the following loss functions (a) L1 loss (b) BCE and (d) WBCE. The average prediction error of the LP network for the Pascal VOC dataset for the different losses is shown in Figure 11. We observe that the WBCE loss performs much better than the other losses, and we get the same conclusion for the other datasets also. Thus, we finally selected WBCE for the M-L data for the label prediction task. Figure 11: Errors on Pascal VOC using different losses. 

Implementation details
We have implemented both of the networks LP and CRL in PyTorch. The LP network consist of a initial fc layer for the three inputs followed by a concatenation step and finally two additional fc layers. We have used fc layers of size 1, 000-d in all our experiments. The encoder-decoder architecture has three fc layers each of 5, 000-d. The hyper-parameters for the loss in (8)  We use stochastic gradient descent algorithm to train our networks.

Conclusion
In this work, we have proposed a deep-learning framework for the task of semi-supervised cross-modal retrieval. This has been achieved by designing two sub-networks (a) a label prediction network for predicting labels of the unlabeled training data (both S-L and M-L) and (b) a common domain representation learning framework to project the data from different modalities into a common space. We propose different losses to learn an effective and discriminative common representation using both the labeled and unlabeled training data. Extensive experiments performed on three standard benchmarking datasets have shown that the proposed framework outperforms the state-of-the-art for both semi-supervised as well as supervised settings. The proposed approach is able to effectively utilize the unlabeled data to give better retrieval performance, especially in the case of very small amounts of labeled data.