Optimal Binary Classifier Aggregation for General Losses

Abstract
We address the problem of aggregating an ensemble of predictors with known loss bounds in a semi-supervised binary classification setting, to minimize prediction loss incurred on the unlabeled data. We find the minimax optimal predictions for a very general class of loss functions including all convex and many non-convex losses, extending a recent analysis of the problem for misclassification error. The result is a family of semi-supervised ensemble aggregation algorithms which are as efficient as linear learning by convex optimization, but are minimax optimal without any relaxations. Their decision rules take a form familiar in decision theory -applying sigmoid functions to a notion of ensemble margin -without the assumptions typically made in margin-based learning.

Introduction
Consider a binary classification problem, in which we are given an ensemble of individual classifiers to aggregate into the most accurate predictor possible for data falling into two classes. Our predictions are measured on a large test set of unlabeled data, on which we know the ensemble classifiers' predictions but not the true test labels. Without using the unlabeled data, the prototypical supervised solution is empirical risk minimization (ERM): measure the errors of the ensemble classifiers with labeled data, and then simply predict according to the best classifier. But can we learn a better predictor by using unlabeled data as well?This problem is central to semi-supervised learning. The authors of this paper recently derived the worst-case-optimal solution for it when performance is measured with classification error (  #b0 ). However, this zero-one loss is inappropriate for other common binary classification tasks, such as estimating label probabilities, and handling false positives and false negatives differently. Such goals motivate the use of different evaluation losses like log loss and cost-weighted misclassification loss.In this paper, we generalize the setup of  #b0  to these loss functions and a large class of others. Like the earlier work, the choice of loss function completely specifies the minimax optimal ensemble aggregation algorithm in our setting, which is efficient and scalable.The algorithm learns weights over the ensemble classifiers by minimizing a convex function. The optimal prediction on each example in the test set is a sigmoid-like function of a linear combination of the ensemble predictions, using the learned weighting. Due to the minimax structure, this decision rule depends solely upon the loss function and upon the structure of the ensemble predictions on data, with no parameter or model choices.

Preliminaries
Our setting generalizes that of  #b0  F =    h 1 (x 1 ) h 1 (x 2 ) · · · h 1 (x n ). . . . . . . . . . . .h p (x 1 ) h p (x 2 ) · · · h p (x n )   (1)We use vector notation for the rows and columns of F: h i = (h i (x 1 ), · · · , h i (x n )) and x j = (h 1 (x j ), · · · , h p (x j )) . Each example j ∈ [n] has a binary label y j ∈ {−1, 1}, but the test labels are allowed to be randomized, represented by values in [−1, 1] instead of just the two values {−1, 1}; e.g. z i = 1 2 indicates y i = +1 w.p. 3 4 and −1 w.p. 1 4 . So the labels on the test data can be represented by z = (z 1 ; . . . ; z n ) ∈ [−1, 1] n , and are unknown to the predictor, which predicts g = (g 1 ; . . . ; g n ) ∈ [−1, 1] n .

Loss Functions
We incur loss on test example j according to its true label y j . If y j = 1, then the loss of predicting g j ∈ [−1, 1] on it is some function + (g j ); and if y j = −1, then the loss is − (g j ). To illustrate, if the loss is the expected classification error, then g j ∈ [−1, 1] can be interpreted as a randomized binary prediction in the same way as z j , so that + (g j ) = 1 2 (1 − g j ) and − (g j ) = 1 2 (1 + g j ). We call ± the partial losses here, following earlier work (e.g.  #b15 ). Since the true label can only be ±1, the partial losses fully specify the decision-theoretic problem we face, and changing them is tantamount to altering the prediction task.What could such partial losses conceivably look like in general? Observe that they intuitively measure discrepancy to the true label ±1. Consequently, it is natural for e.g. + (g) to be decreasing, as g increases toward the notional true label +1. This suggests that both partial losses + (·) and − (·) would be monotonic, which we assume hereafter in this paper (throughout we use increasing to mean "monotonically nondecreasing" and vice versa). Assumption 1. Over the interval (−1, 1), + (·) is decreasing and − (·) is increasing, and both are twice differentiable.We view Assumption 1 as very mild, as motivated above. Notably, convexity or symmetry of the partial losses are not required. In this paper, "general losses" refer to loss functions whose partial losses satisfy Assumption 1, to contrast them with convex losses or other subclasses.The expected loss incurred w.r.t. the randomized true labels z j is a linear combination of the partial losses:(z j , g j ) := 1 + z j 2 + (g j ) + 1 − z j 2 − (g j )(2)Decision theory and learning theory have thoroughly investigated the nature of the loss and its partial losses, particularly how to estimate the "conditional label probability" z j using (z j , g j ). A natural operation to do this is to minimize the loss over g j , and a loss such that arg ming∈[−1,1] (z j , g) = z j (for all z j ∈ [−1, 1])is called a proper loss (  #b16  #b15 ).

Minimax Formulation
As in  #b0 , we formulate the ensemble aggregation problem as a two-player zero-sum game between a predictor and an adversary. In this game, the first player is the predictor, playing predictions over the test set g ∈ [−1, 1] n . The adversary then sets the true labels z ∈ [−1, 1] n .The key idea is that any ensemble constituent i ∈ [p] known to have low loss on the test data gives us information about the unknown z, as z is constrained to be "close" to the test predictions h i . Each hypothesis in the ensemble represents such a constraint, and z is in the intersection of all these constraint sets, which interact in ways that depend on the ensemble predictions F.Accordingly, for now assume the predictor knows a vector of label correlations b such that∀i ∈ [p] : 1 n n j=1 h i (x j )z j ≥ b i(3)i.e. 1 n Fz ≥ b. When the ensemble is composed of binary classifiers which predict in [−1, 1], these p inequalities represent upper bounds on individual classifier error rates. These can be estimated from the training set w.h.p. when the training and test data are i.i.d. using uniform convergence, exactly as in the prototypical supervised ERM procedure discussed in the introduction (  #b4 ). So in our game-theoretic formulation, the adversary plays under ensemble constraints defined by b.The predictor's goal is to minimize the worst-case expected loss of g on the test data (w.r.t. the randomized labeling z), using the loss function as defined earlier in Equation (2):(z, g) := 1 n n j=1 (z j , g j )This goal can be written as the following optimization problem, a two-player zero-sum game:V := min g∈[−1,1] n max z∈[−1,1] n , 1 n Fz≥b (z, g) (4) = min g∈[−1,1] n max z∈[−1,1] n , 1 n Fz≥b 1 n n j=1 1 + z j 2 + (g j ) + 1 − z j 2 − (g j )(5)In this paper, we solve the learning problem faced by the predictor, finding an optimal strategy g * realizing the minimum in (4) for any given "general loss" . This strategy guarantees the best possible worst-case performance on the unlabeled dataset, with an upper bound of V on the loss. Indeed, for all z 0 and g 0 obeying the constraints, Equation (4) implies the tight inequalitiesmin g∈[−1,1] n (z 0 , g) (a) ≤ V ≤ max z∈[−1,1] n , 1 n Fz≥b (z, g 0 )(6)and g * attains the equality in (a), with a worst-case loss as good as any aggregated predictor.In our formulation of the problem, the constraints on the adversary take a central role. As discussed in previous work with this formulation (  #b0  #b1 ), these constraints encode the information we have about the true labels. Without them, the adversary would find it optimal to trivially guarantee error (arbitrarily close to) 1 2 by simply setting all labels uniformly at random (z = 0 n ). It is clear that adding information through more constraints will never raise the error bound V .  $b1  Nothing has yet been assumed about (z, g) other than Assumption 1. Our main results will require only this, holding for general losses. This brings us to this paper's contributions:1. We give the exact minimax g * ∈ [−1, 1] n for general losses (Section 2.1). The optimal prediction on each example j is a sigmoid function of a fixed linear combination of the ensemble's p predictions on it, so g * is a non-convex function of the ensemble predictions. By  #b5 , this incurs the lowest worst-case loss of any predictor constructed with the ensemble information F and b. 2. We derive an efficient algorithm for learning g * , by solving a p-dimensional convex optimization problem. This applies to a broad class of losses (cf. Lem. 2), including any with convex partial losses. Sec. 2 develops and discusses the results. 3. We extend the optimal g * and efficient learning algorithm for it, as above, to a large variety of more general ensembles and prediction scenarios (Sec. 3), including constraints arising from general loss bounds, and ensembles of "specialists" and heterogeneous features.

Results for Binary Classification
Based on the loss, define the function Γ : [−1, 1] → R as Γ(g) := − (g) − + (g). (We also write the vector Γ(g) componentwise with [Γ(g)] j = Γ(g j ) for convenience, so that Γ(h i ) ∈ R n and Γ(x j ) ∈ R p .) Observe that by Assumption 1, Γ(g) is increasing on its domain; so we can discuss its inverse Γ −1 (m), which is typically sigmoid-shaped, as will be illustrated.With these we will set up the solution to the game (4), which relies on a convex function. Both are shown for various losses, as listed in Section 2.3.Definition 1 (Potential Well). Define the potential wellΨ(m) :=    −m + 2 − (−1) if m ≤ Γ(−1) + (Γ −1 (m)) + − (Γ −1 (m)) if m ∈ (Γ(−1), Γ(1)) m + 2 + (1) if m ≥ Γ(1)Lemma 2. The potential well Ψ(m) is continuous and 1-Lipschitz. It is also convex under any of the following conditions:(A) The partial losses ± (·) are convex over (−1, 1). (B) The loss function (·, ·) is a proper loss. (C) − (x) + (x) ≥ − (x) + (x) for all x ∈ (−1, 1).Condition (C) is also necessary for convexity of Ψ, under 1.So the potential wells for different losses are shaped similarly, as seen in Figure 1. Lemma 2 tells us that the potential well is easy to optimize under any of the given conditions. Note that these conditions encompass convex surrogate losses commonly used in ERM, including all such "marginbased" losses (convex univariate functions of z j g j ), introduced primarily for their favorable computational properties.An easily optimized potential well benefits us, because the learning problem basically consists of optimizing it over the unlabeled data, as we will soon make explicit. The function that will actually be optimized is in terms of the dual parameters, so we call it the slack function.Definition 3 (Slack Function). Let σ ≥ 0 p be a weight vector over H (not necessarily a distribution). The vector of scores is F σ = (x 1 σ, . . . , x n σ), whose elements' magnitudes are the margins. The prediction slack function isγ(σ, b) := γ(σ) := −b σ + 1 n n j=1 Ψ(x j σ)(7)An optimal weight vector σ * is any minimizer of the slack function: σ * ∈ arg min 

Solution of the Game
These are used to describe the minimax equilibrium of the game (4), in our main result.Theorem 4. The minimax value of the game (4) ismin g∈[−1,1] n max z∈[−1,1] n , 1 n Fz≥b (z, g) = V = 1 2 γ(σ * ) = 1 2 min σ≥0 p   −b σ + 1 n n j=1 Ψ(x j σ)  The minimax optimal predictions are defined as follows: for all j ∈ [n],g * j := g j (σ * ) =    −1 if x j σ * ≤ Γ(−1) Γ −1 (x j σ * ) if x j σ * ∈ (Γ(−1), Γ(1)) 1 if x j σ * ≥ Γ(1) (8)g * j is always an increasing sigmoid, as shown in Figure 1.We can also redo the proof of Theorem 4 when g ∈ [−1, 1] n is not left as a free variable set in the game, but instead is preset to g(σ) as in (8) for some (possibly suboptimal) weight vector σ. Observation 5. For any weight vector σ 0 ≥ 0 p , the worst-case loss after playing g(σ 0 ) ismax z∈[−1,1] n , 1 n Fz≥b (z, g(σ 0 )) ≤ 1 2 γ(σ 0 )The proof is a simplified version of that of Theorem 4; there is no minimum over g to deal with, and the minimum over σ ≥ 0 p in Equation (13) is upper-bounded by using σ 0 . This result is an expression of weak duality in our setting, and generalizes Observation 4 of [1].

Ensemble Aggregation Algorithm
Theorem 4 defines a prescription for aggregating the given ensemble predictions on the test set.Learning: Minimize the slack function γ(σ), finding the minimizer σ * that achieves V . This is a convex optimization under broad conditions (Lemma 2), and when the test examples are i.i.d. the Ψ term is a sum of n i.i.d. functions. Therefore, it is readily amenable to standard first-order optimization methods which require only O(1) test examples at once. In practice, learning employs such methods to approximately minimize γ, finding some σ A such that γ(σ A ) ≤ γ(σ * ) + for some small . Standard convex optimization methods are guaranteed to do this for binary classifier ensembles, because the slack function is Lipschitz (Lemma 2) and b ∞ ≤ 1.Prediction: Predict g(σ * ) on any test example, as indicated in (8). This decouples the prediction task over each test example separately, which requires O(p) time and memory like p-dimensional linear prediction. After finding an -approximate minimizer σ A in the learning step as above, Observation 5 tells us that the prediction g(σ A ) has loss ≤ V + 2 .In particular, note that there is no algorithmic dependence on n in either step in a statistical learning setting. So though our formulation is transductive, it is no less tractable than a stochastic optimization setting in which i.i.d. data arrive one at a time, and applies to this common situation.

Examples of Different Losses
To further illuminate Theorem 4, we detail a few special cases in which ± are explicitly defined. These losses may be found throughout the literature (see e.g.  #b15 ). The key functions Ψ and g * are listed for these losses in Appendix A, and in many cases in Figure 1. The nonlinearities used for g * are sigmoids, arising solely from the intrinsic minimax structure of the classification game.• 0-1 Loss: Here g j is taken to be a randomized binary prediction; this case was developed in  #b0 , the work we generalize in this paper. • Log Loss, Square Loss • Cost-Weighted Misclassification (Quantile) Loss: This is defined with a parameter c ∈ [0, 1] representing the relative cost of false positives vs. false negatives, making the Bayesoptimal classifier the c-quantile of the conditional probability distribution (  #b18 ). • Exponential Loss, Logistic Loss • Hellinger Loss:This is typically given for p, y ∈ [0, 1] as1 2 √ p − √ y 2 + √ 1 − p − √ 1 − y 2 .Our formulation is equivalent when the prediction and label are rescaled to [−1, 1].• "AdaBoost Loss": If the goal of AdaBoost (  #b17 ) is interpreted as class probability estimation, the implied loss is proper and given in  #b5  #b15 . • Absolute Loss and Hinge Loss: The absolute loss can be defined by abs ∓ (g j ) = 1 ± g j , and the hinge loss also has abs ∓ (g j ) = 1 ± g j since the kink in the hinge loss only lies at g j = ∓1. These partial losses are the same as for 0-1 loss up to scaling, and therefore all our results for Ψ and g * are as well. So these losses are not shown in Appendix A.• Sigmoid Loss: This is an example of a sigmoid-shaped margin loss, a nonconvex smooth surrogate for 0-1 loss. Similar losses have arisen in a variety of binary classification contexts, from robustness (e.g.  #b8 ) to active learning (  #b9 ) and structured prediction (  #b13 ).

Related Work and Technical Discussion
There are two notable ways in which the result of Theorem 4 is particularly advantageous and general. First, the fact that (z, g) can be non-convex in g, yet solvable by convex optimization, is a major departure from previous work. Second, the solution has a convenient dependence on n (as in  #b0 ), simply averaging a function over the unlabeled data, which is not only mathematically convenient but also makes stochastic O(1)-space optimization practical. This is surprisingly powerful, because the original minimax problem is jointly over the entire dataset, avoiding further independence or decoupling assumptions.Both these favorable properties stem from the structure of the binary classification problem, as we can describe by examining the optimization problem constructed within the proof of Thm. 4 (Appendix C.1). In it, the constraints which do not explicitly appear with Lagrange parameters are all box, or L ∞ norm, constraints. These decouple over the n test examples, so the problem can be reduced to the one-dimensional optimization at the heart of Eq. (14), which is solved ad hoc. So we are able to obtain minimax results for these non-convex problems -the g i are "clipped" sigmoid functions because of the bounding effect of the [−1, 1] box constraints intrinsic to binary classification. We introduce Lagrange parameters σ only for the p remaining constraints in the problem, which do not decouple as above, applying globally over the n test examples. However, these constraints only depend on n as an average over examples (which is how they arise in dual form in Equation (16) of the proof), and the loss function itself is also an average (Equation (12)). This makes the box constraint decoupling possible, and leads to the favorable dependence on n, making an efficient solution possible to a potentially flagrantly non-convex problem.To summarize, the technique of optimizing only "halfway into" the dual allows us to readily manipulate the minimax problem exactly without using an approximation like weak duality, despite the lack of convexity in g. This technique was used implicitly for a different purpose in the "drifting game" analysis of boosting (  #b17 , Sec. 13.4.1). Existing boosting work is loosely related to our approach in being a transductive game invoked to analyze ensemble aggregation, but it does not consider unlabeled data and draws its power instead from being a repeated game (  #b17 ).The predecessor to this work (  #b0 ) addresses a problem, 0-1 loss minimization, that is known to be NP-hard when solved directly (  #b10 ). Using the unlabeled data is essential to surmounting this. It gives the dual problem an independently interesting interpretation, so the learning problem is on the always-convex Lagrange dual function and is therefore tractable.This paper's transductive formulation involves no surrogates or relaxations of the loss, in sharp contrast to most previous work. This allows us to bypass the consistency and agnostic-learning discussions (  #b21  #b2 ) common to ERM methods that use convex risk minimization. Convergence analyses of those methods make heavy use of convexity of the losses and are generally done presupposing a linear weighting over H (  #b20 ), whereas here such structure emerges directly from Lagrange duality and involves no convexity to derive the worst-case-optimal predictions.The conditions in 1 are notably general. Differentiability of the partial losses is convenient, but not necessary, and only used because first-order conditions are a convenient way to establish convexity of the potential well in Lemma 2. It is never used elsewhere, including in the minimax arguments used to prove Theorem 4. These manipulations are structured to be valid even if ± are non-monotonic; but in this case, g * j could turn out to be multi-valued and hence not a genuine function of the example's score.We emphasize that our result on the minimax equilibrium (Theorem 4) holds for general losses -the slack function may not be convex unless the further conditions of Lemma 2 are met, but the interpretation of the optimum in terms of margins and sigmoid functions remains. All this emerges from the inherent decision-theoretic structure of the problem (the proof of Appendix C.1). It manifests in the fact that the function g(x j σ) is always increasing in x j σ for general losses, because the function Γ is increasing. This monotonicity typically needs to be assumed in a generalized linear model (GLM;  #b14 ) and related settings. Γ appears loosely analogous to the link function used by GLMs, with its inverse being used for prediction.The optimal decision rules emerging from our framework are artificial neurons of the ensemble inputs. Helmbold et al. introduce the notion of a "matching loss" (  #b12 ) for learning the parameters of a (fully supervised) artificial neuron with an arbitrary increasing transfer function, effectively taking the opposite tack of this paper in using a neuron's transfer function to derive a loss to minimize in order to learn the neuron's weights by convex optimization. Our assumptions on the loss, particularly condition (C) of Lemma 2, have arisen independently in earlier online learning work by some of the same authors (  #b11 ); this may suggest connections between our techniques. We also note that our family of general losses was defined independently by  #b18  in the ERM setting (dubbed "F-losses") -in which condition (C) of Lemma 2 also has significance (  #b18 , Prop. 2) -but has seemingly not been revisited thereafter. Further fleshing out the above connections would be interesting future work.

Extensions
We detail a number of generalizations to the basic prediction scenario of Sec. 2. These extensions are not mutually exclusive, and apply in conjunction with each other, but we list them separately for clarity. They illustrate the versatility of our minimax framework, particularly Sec. 3.4.

Weighted Test Sets and Covariate Shift
Though our results here deal with binary classification of an unweighted test set, the formulation deals with a weighted set with only a slight modification of the slack function: Writing σ * r as the minimizer of the RHS above, the optimal predictions g * = g(σ * r ), as in Theorem 4. Such weighted classification can be parlayed into algorithms for general supervised learning problems via learning reductions (  #b3 ). Allowing weights on the test set for the evaluation is tantamount to accounting for known covariate shift in our setting; it would be interesting, though outside our scope, to investigate scenarios with more uncertainty.

General Loss Constraints on the Ensemble
So far in the paper, we have considered the constraints on ensemble classifiers as derived from their label correlations (i.e. 0-1 losses), as in (3). However, this view can be extended significantly with the same analysis, because any general loss (z, g) is linear in z (Eq. (2)), which allows our development to go through essentially intact.In summary, a classifier can be incorporated into our framework for aggregation if we have a generalization loss bound on it, for any "general loss." This permits an enormous variety of constraint sets, as each classifier considered can have constraints corresponding to any number of loss bounds on it, even multiple loss bounds using different losses. For instance, h 1 can yield a constraint corresponding to a zero-one loss bound, h 2 can yield one constraint corresponding to a square loss bound and another corresponding to a zero-one loss bound, and so on. Appendix B details this idea, extending Theorem 4 to general loss constraints.

Uniform Convergence Bounds for b
In our basic setup, b has been taken as a lower bound on ensemble classifier label correlations. But as mentioned earlier, the error in estimating b is in fact often quantified by two-sided uniform convergence (L ∞ ) bounds on b. Constraining z in this fashion amounts to L 1 regularization of the dual vector σ. Proposition 7. For any ≥ 0,min g∈[−1,1] n max z∈[−1,1] n , 1 n Fz−b ∞ ≤ (z, g) = min σ∈R p   −b σ + 1 n n j=1Ψ(x j σ) + σ 1   As in Thm. 4, the optimal g * = g(σ * ∞ ), where σ * ∞ is the minimizer of the right-hand side above.Here we optimize over all vectors σ (not just nonnegative ones) in an L 1 -regularized problem, convenient in practice because we can cross-validate over the parameter . To our knowledge, this particular analysis has been addressed in prior work only for the special case of the entropy loss on the probability simplex, discussed further in  #b7 .Prop. 7 is a corollary of a more general result using differently scaled label correlation deviations within the ensemble, i.e. 1 n Fz − b ≤ c for a general c ≥ 0 n . This turns out to be equivalent to regularizing the minimization over σ by its c-weighted L 1 norm c |σ| (Thm. 11), used to penalize the ensemble nonuniformly (  #b6 ). This situation is motivated by uniform convergence of heterogeneous ensembles comprised of e.g. "specialist" predictors, for which a union bound (  #b4 ) results in 1 n Fz − b with varying coordinates. Such ensembles are discussed next.

Heterogenous Ensembles of Specialist Classifiers and Features
All the results and algorithms in this paper apply in full generality to ensembles of "specialist" classifiers that only predict on some subset of the test examples. This is done by merely calculating the constraints over only these examples, and changing F and b accordingly (  #b1 ).To summarize this from  #b1 , suppose a classifier i ∈ [p] decides to abstain on an example x j (j ∈ [n]) with probability 1 − v i (x), and otherwise predict h i (x). Our only assumption on {v i (x 1 ), . . . , v i (x n )} is the reasonable one that n j=1 v i (x j ) > 0, so classifier i is not a useless specialist that abstains everywhere.The information about z contributed by classifier i is now not its overall correlation with z on the entire test set, but rather the correlation with z restricted to the test examples on which it predicts. On the test set, this is written as 1 n Sz, where the matrix S is formed by reweighting each row of F:S := n     ρ 1 (x 1 )h 1 (x 1 ) ρ 1 (x 2 )h 1 (x 2 ) · · · ρ 1 (x n )h 1 (x n ) ρ 2 (x 1 )h 2 (x 1 ) ρ 2 (x 2 )h 2 (x 2 ) · · · ρ 2 (x n )h 2 (x n ) . . . . . . . . . . . . ρ p (x 1 )h p (x 1 ) ρ p (x 2 )h p (x 2 ) · · · ρ p (x n )h p (x n )     , ρ i (x j ) := v i (x j ) n k=1 v i (x k )(S = F when the entire ensemble consists of non-specialists, recovering our initial setup.) Therefore, the ensemble constraints (3) become 1 n Sz ≥ b S , where b S gives the label correlations of each classifier restricted to the examples on which it predicts. Though this rescaling results in entries of S having different ranges and magnitudes ≥ 1, our results and proofs remain entirely intact.Indeed, despite the title, this paper applies far more generally than to an ensemble of binary classifiers, because our proofs make no assumptions at all about the structure of F. Each predictor in the ensemble can be thought of as a feature; it has so far been convenient to think of it as binary, following the perspective of binary classifier aggregation, but it could as well be e.g. real-valued, and the features can have very different scales (as in S above). An unlabeled example x is simply a vector of features, so arbitrarily abstaining specialists are equivalent to "missing features," which this framework handles seamlessly due to the given unlabeled data. Our development applies generally to semi-supervised binary classification.

A Examples of Optimal Decision Rules for Various Losses
Loss Partial LossesΓ(g) Ψ(m) gi(σ) 0-1 −(g) = 1 2 (1 + g) g max(1, |m|) clip(x i σ) + (g) = 1 2 (1 − g) Log − (g) = ln 2 1−g ln 1+g 1−g ln(1 + e m ) + ln(1 + e −m ) 1−e −x i σ 1+e −x i σ + (g) = ln 2 1+g Square −(g) = 1+g 2 2 g      −m m ≤ −1 1 2 (m 2 + 1) m ∈ (−1, 1) m m ≥ 1 clip(x i σ) + (g) = 1−g 2 2 CW (param. c) − (g) = c (1 + g) g + 2c − 1      −m m ≤ 2c − 2 (2c − 1)m + 4c(1 − c) m ∈ (2c − 2, 2c) m m ≥ 2cclip(x i σ + 1 − 2c)   (−1, x)).+ (g) = (1 − c) (1 − g) Exponential −(g) = e g e g − e −g      −m + 2/e m ≤ −e + 1 e √ 4 + m 2 m ∈ −e + 1 e , e − 1 e m + 2/e m ≥ e − 1 e clip ln 1 2 x i σ + 1 + 1 4 (x i σ) 2 + (g) = e −g Logistic −(g) = ln (1 + e g ) g      −m + 2 ln (1 + 1/e) m ≤ −1 ln(1 + e m ) + ln(1 + e −m ) m ∈ (−1, 1) m + 2 ln (1 + 1/e) m ≥ 1 clip(x i σ) + (g) = ln 1 + e −g Hellinger − (g) = 1 − 1−g 2 1+g 2 − 1−g 2        −m m ≤ −1 2 − 1−m √ 2−m 2 2 − 1+m √ 2−m 2 2 m ∈ (−1, 1) m m ≥ 1 (x i σ) 2 − (x i σ) 2 |x i σ| ≤ 1 sgn(x i σ) |x i σ| > 1 + (g) = 1 − 1+g 2 "AdaBoost" − (g) = 1+g 1−g 2g √ 1−g 2 √ m 2 +4+m √ m 2 +4−m + √ m 2 +4−m √ m 2 +4+m x i σ (x i σ) 2 +4 +(g) = 1−g 1+g

B Constraints on General Losses for Binary Classification
In all other sections of the paper, we allow the evaluation function of the game to be a general loss, but assume that the constraints (our information about the ensemble) are in terms of zero-one loss as written in (3). However, here we relax that assumption, allowing each classifier h i to constrain the test labels z not with the zero-one loss of h i 's predictions, but rather with some other general loss.This is possible because when the true labels are binary, all the losses we consider are linear in z, as seen in (5) [ + (h i (x j )) + − (h i (x j ))] − 2 i , then we can use the definition of (z, g) to write(z, h i ) ≤ i ⇐⇒ 1 n z [Γ(h i )] ≥ b i(9)Now (9) is a linear constraint on z, just like each of the error constraints earlier considered in (3). We can derive an aggregation algorithm with constraints like (9), using essentially the same analysis as employed in Section 2 to solve the game (4). As mentioned in Sec. 3.2,

Matching Objective and Constraint Losses
Though the ensemble constraints can be completely heterogeneous, we focus on a special case of them in the rest of this section to glean intuition. Suppose when each classifier contributes exactly one constraint to the problem, and the losses used for these constraints are all the same as each other and as the loss used in the objective function. In other words, the minimax prediction problem we now consider isV := min g∈[−1,1] n max z∈[−1,1] n , ∀i∈[p]: (z,hi)≤ i (z, g) = min g∈[−1,1] n max z∈[−1,1] n , ∀i∈[p]: 1 n z [Γ(hi)]≥b i (z, g)(10)The matrix F and the slack function from (1) are therefore redefined:F ij := Γ(h i (x j )) = − (h i (x j )) − + (h i (x j )) γ (σ, b ) := γ (σ) := −[b ] σ + 1 n n j=1 Ψ [Γ(x j )] σ where b = (b 1 , .. . , b p ) and the vectors x j are now from the new unlabeled data matrix F ij . The game (10) is clearly of the same form as the earlier formulation (4). Therefore, its solution has the same structure as in Theorem 4, proved using that theorem's proof: Theorem 8. The minimax value of the game (10) is V := 1 2 γ (σ * ) := min σ≥0 p 1 2 γ (σ). The minimax optimal predictions are defined as follows: for all j ∈ [n],g * j := g j (σ * ) =    −1 if [Γ(x j )] σ * ≤ Γ(−1) Γ −1 [Γ(x j )] σ * if [Γ(x j )] σ * ∈ (Γ(−1), Γ(1)) 1 if [Γ(x j )] σ * ≥ Γ(1)This provides a concise characterization of how to solve the semi-supervised binary classification game for general losses. Though on the face of it Theorem 8 is a much stronger result than even Theorem 4, we cannot overlook statistical issues. The loss bounds i on each classifier are estimated using a uniform convergence bound over the ensemble with loss , but the data now considered are not the ensemble predictions, but the predictions passed through function Γ. This can be impractical for losses like log loss, for which Γ is unbounded, and therefore uniform convergence to estimate b i in (9) is much less applicable than for 0-1 loss.But such issues are outside our scope here, and our constrained minimax results hold in any case given b. They may be useful to obtain semi-supervised learnability results for different losses from tighter statistical characterizations, which we consider an interesting open problem.

C Proofs and Supporting Results


C.1 Proof of Theorem 4
The main hurdle here is the constrained maximization over z. For this we use the following result, a basic application of Lagrange duality (from  #b0 , but proved below for completeness). Lemma 9. For any a ∈ R n ,max z∈[−1,1] n , 1 n Fz≥b 1 n z a = min σ≥0 p −b σ + 1 n F σ + a 1With this lemma, we prove the main theorem of this paper.Proof of Theorem 4. First note that (z, g) is linear in z,V = (5) = 1 2 min g∈[−1,1] n max z∈[−1,1] n , 1 n Fz≥b 1 n n j=1 [ + (g j ) + − (g j ) + z j ( + (g j ) − − (g j ))]Here we can rewrite the constrained maximization over z using Lemma 9:max z∈[−1,1] n , 1 n Fz≥b 1 n n i=1 z j ( + (g j ) − − (g j )) = max z∈[−1,1] n , 1 n Fz≥b − 1 n z [Γ(g)] = min σ≥0 p −b σ + 1 n F σ − Γ(g) 1(11)Substituting (11)  [ + (g j ) + − (g j )] + minσ≥0 p −b σ + 1 n F σ − Γ(g) 1   = 1 2 min σ≥0 p   −b σ + min g∈[−1,1] n   1 n n j=1 [ + (g j ) + − (g j )] + 1 n F σ − Γ(g) 1     (13) = 1 2 min σ≥0 p   −b σ + 1 n n j=1 min gj ∈[−1,1] + (g j ) + − (g j ) + x j σ − Γ(g j )  (14)The absolute value breaks down into two cases, so the inner minimization's objective can be simplified:+ (g j ) + − (g j ) + x j σ − Γ(g j ) = 2 + (g j ) + x j σ if x j σ ≥ Γ(g j ) 2 − (g j ) − x j σ if x j σ < Γ(g j )(15)Suppose g j falls in the first case, so that x j σ ≥ Γ(g j ). From Assumption 1, 2 + (g j ) + x j σ is decreasing in g j , so it is minimized for the greatest g * j ≤ 1 s.t. Γ(g * j ) ≤ x j σ. Since Γ(·) is increasing, exactly one of two subcases holds:1. g * j is such that Γ(g * j ) = x j σ, in which case the minimand (15) is + (g * j ) + − (g * j ) 2. g * j = 1 so that Γ(g * j ) = Γ(1) < x j σ, in which case the minimand (15) is 2 + (1) + x j σ A precisely analogous argument holds if g j falls in the second case, where x j σ < Γ(g j ). Putting the cases together, we have shown the form of the summand Ψ, piecewise over its domain, so (14) is equal to 1 2 min σ≥0 p [γ(σ)]. We have proved the dependence of g * j on x j σ * , where σ * is the minimizer of the outer minimization of  #b13 . This completes the proof.Proof of Lemma 9. We havemax z∈[−1,1] n , Fz≥nb 1 n z a = 1 n max z∈[−1,1] n min σ≥0 p z a + σ (Fz − nb) (a) = 1 n min σ≥0 p max z∈[−1,1] n z (a + F σ) − nb σ (16) = 1 n min σ≥0 p a + F σ 1 − nb σ = min σ≥0 p −b σ + 1 n F σ + a 1 (17)where d Γ −1 dm = 1 Γ (Γ −1 (m)) = 1 − (Γ −1 (m)) − + (Γ −1 (m)) ≥ 0(18)where the last inequality follows by Assumption 1. Therefore, by the chain rule and (18),F (m) = − (Γ −1 (m)) + + (Γ −1 (m)) − (Γ −1 (m)) − + (Γ −1 (m))(19)From this, we calculate F (m), writing ± (Γ −1 (m)) and ± (Γ −1 (m)) as simply ± and ± for clarity: to show that the other term is ≥ 0. But this is equal toF (m) = d[Γ −1 ]− − + − + + − − + + − − + = 2( − + − − + )(20)This proves that condition (C) of Lemma 2 is sufficient for convexity of F (and necessary also, under 1 on the partial losses).We now address the other conditions of Lemma 2. (A) implies (C), because by Assumption 1, − , + , − are ≥ 0 and + ≤ 0, so  #b19  is ≥ 0 as desired. Finally we prove that (B) implies (C). If is proper, then it is well known (e.g. Thm. 1 of  #b15 , and  #b5 ) that for all x ∈ (−1, 1),− (x) 1 + x = − + (x)1 − x (This is a simple and direct consequence of stationary conditions from the properness definition.) Define the function w(x) = − (x) 1+x = − + (x) 1−x ; we drop the argument and write it and its derivative as w and w for clarity. By direct computation, − + − − + = [(1 + x)w (w + (x − 1)w )] − [(w + (1 + x)w )(x − 1)w] = (1 + x)w 2 + (x 2 − 1)ww − (x − 1)w 2 + (x 2 − 1)ww = 2w 2 ≥ 0 so (C) is true as desired.Proof of Lemma 2. Continuity follows by checking Ψ(m) at m = ±1.For Lipschitzness, note that for m ∈ (Γ(−1), Γ(1)), by (19), ΨUsing Assumption 1 on the partial losses, equations (22) and (23) respectively make clear that Ψ (m) ≥ −1 and Ψ (m) ≤ 1 on this interval. Since Ψ (m) is −1 for m < Γ(−1) and 1 for m > Γ(1), it is 1-Lipschitz.As for convexity, since Ψ is linear outside the interval (Γ(−1), Γ(1)), it suffices to show that Ψ(m) is convex inside this interval, which is shown in Lemma 10.

C.3 Results and Proofs from Section 3
Proof of Theorem 6. The proof is similar to that of Theorem 4, which it generalizes. First note that r j [ + (g j ) + − (g j )] + x j σ − r j Γ(g j )  (25)As in the proof of Theorem 4, the inner minimization's objective can be simplified: r j ( + (g j ) + − (g j )) + x j σ − r j Γ(g j ) = 2r j + (g j ) + x j σ if x j σ ≥ r j Γ(g j ) 2r j − (g j ) − x j σ if x j σ < r j Γ(g j )Suppose g j falls in the first case, so that x j σ ≥ r j Γ(g j ). From Assumption 1, 2r j + (g j ) + x j σ is decreasing in g j , so it is minimized for the greatest g * j ≤ 1 s.t. Γ(g * j ) ≤x j σ rj . Since Γ(·) is increasing, exactly one of two subcases holds:

Footnote
1 : However, it may pose difficulties in estimating b by applying uniform convergence over a larger H ([2]).