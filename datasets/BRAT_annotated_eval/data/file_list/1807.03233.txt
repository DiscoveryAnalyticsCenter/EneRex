A New ECOC Algorithm for Multiclass Microarray Data Classification

Abstract
The classification of multi-class microarray datasets is a hard task because of the small samples size in each class and the heavy overlaps among classes. To effectively solve these problems, we propose novel Error Correcting Output Code (ECOC) algorithm by Enhance Class Separability related Data Complexity measures during encoding process, named as ECOCECS. In this algorithm, two nearest neighbor related DC measures are deployed to extract the intrinsic overlapping information from microarray data. Our ECOC algorithm aims to search an optimal class split scheme by minimizing these measures. The class splitting process ends when each class is separated from others, and then the class assignment scheme is mapped as a coding matrix. Experiments are carried out on five microarray datasets, and results demonstrate the effectiveness and robustness of our method in comparison with six state-of-art ECOC methods. In short, our work confirm the probability of applying DC to ECOC framework.

INTRODUCTION
DNA microarray is a widely deployed technology for cancer diagnosis, and it has gone from obscurity to being almost ubiquitous in biological and medical research. However, a challenge for researchers is raised due to the "large dimensions, small samples" problem embedded in microarray data analysis: the number of samples is far smaller than that of genes. Because of the severe data unbalanced problem among classes, multiclass microarray data is much harder compared with the binary class problem, and a lot of algorithms were proposed to mine key genes and obtain accurate cancer diagnosis  #b0 .A popular solution is to pick up importance features to reduce problem difficulty and to complete multi-class classification by Error Correcting Output Code (ECOC) method.As it provides high error correcting ability, it has been successfully applied in many fields, such as face recognition  #b1 , biological disease diagnosis  #b2  and text recognition  #b3 . In general, ECOC consists of two main phases: encoding phase and decoding phase. In the encoding phase, a given data set with R classes {c1, c2, …, cR}, is divided into L binary problems. These separation schemes are represented as a coding matrix M, where M ∈ {+1, −1} * . Each row in M defines a unique codeword for each class in R, and each column defines a partition of classes with +1, -1 as their classes membership. In the decoding phase, the outputs of L classifiers compose a code vector, {L1x, L2x, …, LRx} for an unknown sample x. This vector is compared with each codeword of M, and the class represented by the codeword with the highest similarity is selected as predicted class, shown as Fig.1(a).There are two types of ECOC algorithms: data-independent  #b4  and data-dependent. The difference between them is whether a coding matrix is generated based on data characteristics. Some data-dependent algorithms deploy different measures for data characteristics evaluation, and a typical one is mutual information (MI) in DECOC  #b2 . However, such measures require a large number of samples to estimate key parameters, so they unavoidably lead to the bias in microarray data due to the small-sample size, degrading their performance  #b5 .Instead, this paper aims to design a reliable data-dependent ECOC algorithm to tackle the high dimensional microarray data based on Data complexity (DC), which is a powerful tool to explore data distribution and relationships among characteristics. In  #b6 , some complexity measures are defined for binary classification, focusing on the geometrical complexity of the class boundary. Since the classification results are affected by the complexity in a data set,  #b7  used DC measures to determine the domain of competition among the classifiers. Other researchers also used different complexity measures to choose some promising classification methods  #b8  #b9  #b10 . In short, DC is helpful in a classification task.In this paper, we propose a new ECOC algorithm based on Enhancing Class Separability related DC (N2 and N3), named as ECOCECS. It aims to search optimal schemes to split classes into two groups by minimizing N2/N3 indices between groups, so as to improve dichotomizers' performances. Our experiments are based on five microarray datasets, and six widely used ECOC algorithms are employed for comparisons. Experimental results prove our ECOCECS is more accurate and stable.The structure of paper is as follows. Section 2 briefly introduces N2 and N3, and Section 3 presents the details of ECOCECS. Section 4 analyzes and discusses the experimental results, and Section 5 concludes this paper.

II. CLASS SEPARABILITY BASED DC MEASURES


A. Ratio of nearest neighbor distance (noted as N2)
Assume there are N samples in a data set. N2 measure is proposed to judge whether samples in the same class are close or not. The Euclidean distance is calculated from sample xi to its nearest neighbor inside the class (denoted as intraDis(xi)) and outside the class (denoted as interDis(xi)). Then N2 index is set as the average distance of intra-class nearest neighbor over the average distance of inter-class nearest neighbor, as shown in formula  #b0 . It changes in the range of [0, +∞). A low N2 index suggests that the samples of the same class lay closely, and a large one indicates a disperse distribution.N2 = ∑ ( ) ∑ ( )(1)

B. Loss of the nearest neighbor classifier (noted as N3)
N3 is defined as the loss of neighbor classifier on training data. It is calculated as the mismatching between the predicted label f( ) and true label for sample , as shown in the formulas  #b1  #b2 . In the consideration of the class-imbalance and small sample size problem in training data sets, leave-one-out method is used to get robust results.( , ) = 1, = ( ) 0, ≠ ( ) (2) 3 = ∑ ( , )(3)This measure shows the compactness of samples in a data set. The domain of N3 metric is in the range of [0,1], and a low index shows there is a large margin in the class boundary.

III. THE WORKFLOW OF ECOCECS
In our algorithm, a search algorithm is designed to exchange classes in groups iteratively, aiming to search an optimal class splitting scheme with high class separability. Because N2 and N3 are proposed on different principles, they are applied in our algorithm individually.Let the mean value of all samples (Cenk,i) in class ck represent the centroid of ck in Gi. Function Dis(a,b) represents the Euclidean distance between a and b. The class assignment scheme for a group is a column in a coding matrix, directly affects the generalization ability of our ECOC algorithm. The workflow of our algorithm can be summarized as: 1. split groups at random at the first step; 2. evaluates the complexity between based on N2 or N3 measures in current group; 3. search a better splitting scheme to lower the complexity by exchanging the most complex classes in two groups.N2 index measures the ratio of inner-class distance and inter-class distance. Based on this idea, formula (4) is designed to calculate the inner-group ( , , , , ≠ ) and inter-group ( , , , , k ≠ h, i ≠ j) distance for each class. For ck in Gi, a small , indicates that ck is far away from other classes in the same group and close the classes in another group. So the class with the minimum , is considered as the most complex class in the i-th group. By exchanging such classes between two groups, it is expected that the complexity would be lower.As for N3, the class with the longest distance from other classes in same group is treated as the most complex class. So formula  #b4  calculates the distance between each class pair in a group. Then the class far away from other classes in its group is to be exchanged. The detailed working process is given as Fig.2.  Calculate , on formula (4) or (5); 6: ai←argmax ( , );, = ∑ , , , ∀ ∈ , ∈ , ∑ , , , ∀ ∈ , ∈ , , ∈ [1,2], ≠(4), = ∑ ( , , , ) ∀ ∈ , ∈ , , ∈ [1,2](5)

7:
/ ← exchange a1 and a2 in G1/ G2; 8: I'← new DC index between and ; 9: if I' is smaller than I 10:Replace I, G1 and G2 with I', and ;11: go to step 3-8; 12: else 13:Add a new column to M based on assignment scheme in G1 and G2; 14: for i in range  #b0  #b1  15: if Gi contains one more classes 16:G ← and go to step 1; 17: else

18:
Return M; In the algorithm, G is divided into two groups G1 and G2 with equal size at random firstly. By labelling samples in G1 and G2 as +1 and -1, DC index I is calculated by formula (1) for N2 or formula (2-3) for N3. Then class complexity of each class is evaluated by formula (4) or  #b4 . The most complex classes a1 and a2 in G1 and G2 are exchanged to form two new groups, G1' and G2'. Then the new index I' for G1',G2' is calculated by formula (1) or  #b1 . If I' is lower than I, G1, G2 and I would be replaced. This exchange process ends when I can't be lower any more. Then the class assignment scheme of G1 and G2 is recorded as a new column, added to M. This process can be illustrated as a binary tree, and an example is shown in Fig.1(b). This algorithm runs iteratively until both G1 and G2 contain only one class. After this algorithm stops, a coding matrix is generated. As N2 and N3 exchange classes based on different principles, they would produce diverse coding matrices.

IV. EXPERIMENTS AND DISCUSSIONS


A. Experiment Settings
To validate the proposed algorithm, experiments are carried out on five multiclass datasets (as listed in Table 1). SVM and NativeBayes(NB) are used as base classifiers, and three filter feature selection methods ROC, T-test and Wilcoxon are deployed. In addition, One VS One (OVO), One VS All (OVA), Ordinal ECOC(Ordinal), DECOC, ECOCONE and ECOC-Forest(Forest) methods are used for comparison. The last three ECOC methods are provided by ECOC library  #b4 , and the rest are supported by MATLAB 2016 toolboxes. Default settings of all algorithms are adopted.   #b15  Fscore and accuracy are often used to evaluate and compare the performances of different algorithms. The original Fscore and accuracy are designed for binary problems. When applied in a multiclass problem, the average Fscore and accuracy among classes are used. That is, for the i-th binary problem, positive rate (Pi), negative rate (Ni), true positive (TPi), true negative (TNi), false positive (FPi) and false negative (FNi) are calculated by means of OVA. In this way, the i-th class is regard as the positive class, and others are labelled as the negative class. The final score is the average of all binary problems, as shown by formulas  #b5  #b6  #b7  #b8 . Here β is set to 1 to get balanced results. Accuracy = avg(∑ )(6) Precision = avg(∑ ) (7) Recall = avg(∑ ) (8) Fscore = avg(∑ * * * )(9)

B. Experiment results


1) the analysis of Local improvement algorithms
(a) (b) Fig. 3. the change of complexity using Wilcoxon and feature size is 80. Fig.3 describes the change of complexity indices over all datasets in a class adjustment process. When the original data sets appear high overlapping situation, especially the DLBCL dataset, the class exchange process is of higher probability to occur multiple times. As a result, for N2 based ECOCECS, the index drops to 0.55 after twice exchanges, only about 2/3 of the original. And the N3 value on DLBCL drops to 0 after an exchange. And the similar results obtained by N2 and N3 on other data sets, showing that our algorithm can effectively optimize the class assignment schemes.

2) Analysis of accuracy obtained by ECOC methods
Table2 lists the accuracies and Fscore values by different approaches with top 80 features. It is found that accuracies obtained by ECOCECS based schemes are usually higher compared to other ECOC methods. The results obtained by N2 based ECOCECS method are 11% higher than that by OVA and Forest-ECOC. On the Breast data set, the accuracy rate N2 algorithm is 49% higher than OVA, 30% higher than the traditional ECOC, 25% higher than ECOCONE and 54% more than Forest-ECOC. The results show that the algorithm with N2 could balance the precision and recall situation for dataset, performing more precisely. In addition, N2 based ECOCECS accomplishes the highest average datasets accuracy. At the same time, the results of Fscore are similar to those of the accuracy results, revealing that N2 based ECOCECS can produce more balanced results.Although the N2 based approach wins most cases, the N3 algorithm just wins on the Leukemia data, achieving close performance to other algorithms. However, it should be noted that only our algorithm and OVA, DECOC require R-1 base learners, and other algorithm need much more base learners. So the N3 based algorithm can obtain similar results by a much smaller ensemble scale. While N2 based algorithm is more robust and effective compared with other state-of-art ECOC approaches. Similar conclusions can be drawn from Table3.  Fig.4 shows that the accuracies obtained by eight ECOC methods change with varying feature size over Cancers data set using NB. It is found that when features size is larger than 50, most of ECOC algorithms reached 90% accuracy, depicting that almost all multi-class classification issues still require a mass of genes to ensure good accuracy. Among the results, our algorithm always maintains a lead position (97%-99%