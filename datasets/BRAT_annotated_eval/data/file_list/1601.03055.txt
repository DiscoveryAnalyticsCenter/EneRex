Subspace Clustering Based Tag Sharing for Inductive Tag Matrix Refinement with Complex Errors

Abstract
Annotating images with tags is useful for indexing and retrieving images. However, many available annotation data include missing or inaccurate annotations. In this paper, we propose an image annotation framework which sequentially performs tag completion and refinement. We utilize the subspace property of data via sparse subspace clustering for tag completion. Then we propose a novel matrix completion model for tag refinement, integrating visual correlation, semantic correlation and the novelly studied property of complex errors. The proposed method outperforms the state-of-the-art approaches on multiple benchmark datasets even when they contain certain levels of annotation noise.

INTRODUCTION
It is useful to annotate images with textual tags for the purpose of image indexing and retrieval. To annotate proper tags, one need to bridge the gap between low level visual features of an image and corresponding high level semantic information  #b15 . Since manual annotation is labor intensive, automatic annotation has aroused much attention. Many machine learning based approaches have been developed.Currently many image annotation data have been collected from crowdsourcing services  #b20  #b11 , providing large amount of data for training while being noisy due to annotation errors. Annotation errors are usually complex and mainly come in two forms: missing tags and inaccurate tags. Most image annotation approaches solely focus on one of those two, either trying to impute the missing tags (tag completion/tag assignment)  #b10  or correcting inaccurate tags (tag refinement)  #b21  #b7  #b15 . Other existing methods fail to model the complex errors properly. They either treat them in the same way  #b23 , ignoring the complex property of the * Corresponding author.Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.SIGIR '16, July 17-21, 2016 errors, or rigidly assign fixed weights to different kinds of errors  #b10 , having no adaptability when working on different datasets with different levels of annotation errors.In this paper, we propose a framework called Subspace clustering and Matrix completion with Complex errors (SMC). Since current tag refinement methods suffer from the extreme sparsity problem  #b19 , SMC performs tag completion and refinement sequentially. During tag completion, SMC tries to introduce many additional proper tags to images via exploring subspace property in the image collection. We then adapt the inductive matrix completion  #b12  model to perform the following tag refinement procedure, utilizing side information such as the correlation between visual features and their corresponding tags (visual correlation), correlation between the semantic information of tags (semantic correlation) and the complex errors.The main contributions of this paper include:• We perform tag completion and tag refinement sequentially, showing that tag refinement benefits from tag completion.• We formulate tag completion in a subspace clustering framework to tackle the extreme sparsity problem.• We novelly adapt the inductive matrix completion model for tag refinement, taking visual correlation, semantic correlation and our novelly studied complex errors property into consideration.

THE PROPOSED FRAMEWORK


Overview
We denote the observed tag matrix as O ∈ {0, 1} N i ×Nt , where each row corresponds to one image, each column corresponds to one textual tag, and Ni and Nt denote the number of images and tags, respectively. Oij takes value 1 only if image i is annotated with tag j and 0 otherwise.We are targeting at modifying the values in matrix O by matrix completion methods to perform image annotation. After the matrix completion procedure, if the value of Oij changes from nonzero (zero) to zero (nonzero), we say that the algorithm removes (adds) tag j from (to) image i. Methods based on matrix completion are robust and efficient since they only operate on the tag matrix, avoiding error propagation from image segmentation.However, in many cases O is so sparse that some columns have at most one known entries and some rows have no known entries at all, making existing methods not applicable  #b19 . In order to overcome such extreme sparsity, we first perform tag completion to make O denser, creating a better condition for the following tag refinement procedure. More specifically, we perform subspace clustering over images and share tags within subspaces.For tag refinement, existing methods usually depends heavily on image segmentation and visual feature extraction accuracies  #b0 . However, image segmentation and feature extraction procedures always contain a lot of noises, which affect the following annotation procedure severely. Meanwhile, recent matrix completion based methods  #b9  #b7  #b8  stand out due to their robustness and efficiency, since these algorithms avoid the image segmentation procedure.Our proposed framework is called Subspace clustering and Matrix completion with Complex errors (SMC), because it utilizes the subspace property of image collections (Section 2.2) and addresses the complex errors and side information in an inductive matrix completion model for tag refinement (Section 2.3).

Subspace Clustering for Tag Completion


Subspace Clustering
It is reasonable to assume that images belonging to different categories are approximately sampled from a mixture of several low-dimensional subspaces. The membership of the data points to the subspaces is unobserved, leading to the challenging problem of subspace clustering. Here the goal is to cluster data into k clusters with each cluster corresponding to a subspace.One of the state-of-the-art method is the sparse subspace clustering (SSC) model  #b4 . The idea behind SSC is to express a data point as a linear (or affine) combination of neighboring data points. The neighbors can be any other points in the data set. While every point is a combination of all other data points, SSC seeks for the sparsest representation among all the candidates by minimizing the number of nonzero coefficients  #b5 .We denote the set of images, represented as visual feature vectors, as V = [v1, v2, . . . , vn]. Assuming that they are drawn from a union of k subspaces. Each column of V can be represented by a linear combination of the bases in a "dictionary". SSC uses the matrix V itself as the dictionary while explicitly considering noise:min Z,E Z 1 + µ E 2 F ,(1)s.t. V = VZ ⊤ + E, diag(Z) = 0, Z1 = 1,(2)where Z ⊤ = [z1, z2, . . . , zn] is the coefficient matrix with each zi being the representation of vi and E is the error matrix. This problem can be solved efficiently using modern sparse optimization algorithms, such as linearized alternating direction methods  #b16 . Given a sparse representation for each data point, we can define the affinity matrix as A = |Z| + |Z ⊤ |. Subspaces are then obtained by applying spectral clustering to the Laplacian matrix of A [5].

Tag Sharing
We improve the search based neighbor voting algorithm proposed in  #b17  to share tags in each cluster separately. We rank all the tags for the cluster, taking tag frequency, tag co-occurrence and local frequency into consideration. The elements of tag matrix after tag sharing are no longer binary but take values in [0, 1], representing the confidence level between each image-tag pair.

Matrix Completion for Tag Refinement
The tag completion procedure makes the tag matrix much denser and thus avoids the extreme sparsity problem. Then we can refine the tag matrix. In our framework we novelly adapt the inductive matrix completion model (IMC)  #b12  for tag refinement, due to its scalability and capability of incorporating various kinds of side information.

Inductive Matrix Completion
Let vi ∈ R f i denote the fi-dimensional feature vector of image i and tj ∈ R ft denote the ft-dimensional feature vector of tag j. Let V ∈ R N i ×f i denote the feature matrix of Ni images, where the i-th row is the image feature vector v ⊤ i , and T ∈ R Nt×ft denote the feature matrix of Nt tags, where the i-th row is the tag feature t ⊤ i . For image annotation, we assume that the tag matrix can be approximated by applying visual feature vectors and tag feature vectors associated with its row and column entries onto an underlying low-rank matrix M, i.e. O ≈ VMT ⊤ , where M = PQ ⊤  #b12  and P ∈ R f i ×r and Q ∈ R r×ft are of rank r ≪ Ni, Nt. The goal is to solve the following problem:min P,Q loss(O, VPQ ⊤ T ⊤ ) + λ1(rank(PQ ⊤ )).(3)A common choice for the loss function is the squared loss. The low-rank constraint on PQ ⊤ makes (3) NP-hard. A standard relaxation is to use the trace norm, i.e. sum of singular values. Minimizing the trace-norm of M = PQ ⊤ is equivalent to minimizing 1 2 ( P 2 F + Q 2 F )  #b12 . The relaxed optimization problem we use in this work is therefore:min P,Q O − VPQ ⊤ T ⊤ 2 F + λ1 2 ( P 2 F + Q 2 F ).(4)

Visual Correlation
We want to get the refined tag matrixÔ = VPQ ⊤ T ⊤ from the original tag matrix O. Here we represent the ith row ofÔ asÔi, corresponding to the refined tag vector of image i. Thus we can measure the correlation between image i and image j in two ways: 1) similarity between image features vi and vj, 2) similarity between refined tag vectorŝ Oi andÔj . Since visually similar images often belong to similar themes and thus are annotated with similar tags, these two kinds of similarities should be correlated.Such visual correlation can be enforced by solving the following optimizationmin P,Q Nt i=1 Nt j=1 Ô i −Ôj 2 gij ,(5)where Ô i −Ôj 2 measures the similarity between tag vec-torsÔi andÔj and gij measures the similarity between visual features vi and vj. In this work, we adopt cosine similarity, i.e. gij = cos(vi, vj). The formulation forces tag vectors with large similarities also have large similarity in their corresponding visual features and vice versa.The formulation can be rewritten as min P,Q

Tr(ÔLvÔ
⊤ ) = min P,Q Tr(VPQ ⊤ T ⊤ LvTQP ⊤ V ⊤ ),(6)where Lv = diag(G1) − G is the Graph Laplacian  #b2  of the similarity matrix G = (gij).

Semantic Correlation
Similarly, we can also enforce semantic correlation between tags. Since each column of the matrixÔ represents the feature of a tag, we can measure the correlation between two tags using the similarity between their corresponding column vectors ofÔ. Meanwhile, semantic similarity between two tags can be measured using word vectors. These two kinds of similarities should be correlated as well.We can enforce the semantic correlation by solving the following optimization, in a similar form as (6): (7) where Ls = diag(H1)−H is the Graph Laplacian of the similarity matrix H = (hij ), with each element hij = cos(ti, tj ).min P,Q Tr(Ô ⊤ LsÔ) = min P,Q Tr(TQP ⊤ V ⊤ LsVPQ ⊤ T ⊤ ),

Features Vectors
We utilize DeCAF6  #b3  to extract 4, 096-dimensional visual features for each image, which have high level information. Meanwhile, we adopt pre-trained word embedding vectors (word2vec)  #b18  to construct 300-dimensional features for each tag, trying to capture semantic information.

Complex Errors
As we have mentioned, annotation errors come in two forms: missing tags and inaccurate tags. Since human beings are relatively reasonable, the user-provided tags are reasonably accurate to certain level  #b23 . Users might miss one or several proper tags among the few related tags, but may become less probable to add one or several inaccurate tags from the massive unrelated tag sets  #b11 . In other words, if an image is not originally annotated with a tag, it is more likely that they really have no relation at all. Thus the errors are mainly composed of inaccurate tags rather than missing tags. And we should pay more attention to denoise the inaccurate tags rather than completing the missing ones.To model the complex structure of errors, we improve the matrix completion model by putting less weights on the unannotated positions:min P,Q O − VPQ ⊤ T ⊤ 2 F − µ UΩ(O − VPQ ⊤ T ⊤ ) 2 F ,(8)where Ω represents the positions where the images are originally not annotated. U is a projection operator and µ acts as a weighting parameter which changes adaptively in different datasets according to their noise levels. Existing methods never model these two kinds of errors separately. They simply model the errors as Laplacian noise  #b23  or Gaussian noise  #b21 . To our knowledge, our model is the first to model the missing errors and inaccurate errors separately. The model can further adapt to different datasets according to their noise levels.

FINAL MODEL
Based on the components regarding low-rankness, visual correlation, semantic correlation and complex errors, we formulate the objective function as follows:min P,Q O − VPQ ⊤ T ⊤ 2 F − µ UΩ(O − VPQ ⊤ T ⊤ ) 2 F + λ1 2 ( P 2 F + Q 2 F )+ λ2[Tr(VPQ ⊤ T ⊤ LvTQP ⊤ V ⊤ +TQP ⊤ V ⊤ LsVPQ ⊤ T ⊤ )].By solving P and Q we can then construct the refined tag matrixÔ = VPQ ⊤ T ⊤ and use it for refined annotation.We set the regularization terms of visual correlation and semantic correlation with the same weight λ2 for simplicity. This simplification does not harm performance, as we find during preliminary experiments.This objective function is non-convex. To solve the optimization problem, we adapt the solver for low-rank empirical risk minimization for multi-label learning (LEML)  #b22 , which naturally fits for the settings of large-scale multi-label learning with missing labels. The solver uses alternating minimization (fix P and solve for Q and vice versa) to update the variables. When either P or Q is fixed, the resulting subproblem in one variable (Q or P) can be solved using iterative conjugate gradient procedure.

EXPERIMENTAL EVALUATION


Datasets and Experimental Setup
We evaluate our proposed SMC framework on two benchmark datasets: Labelme  #b20  and MIRFlickr-25K  #b11 . Table 1 demonstrates the detailed statistics. These two datasets, especially MIRFlickr-25K, are rather noisy, with a number of the tags being misspelled or meaningless. Hence, a preprocessing procedure is performed. We match each tag with entries in the Wikipedia thesaurus and only retain the tags in accordance with Wikipedia. We compare our method with the state-of-the-art methods, including matrix completion-based models (i.e. LRES  #b23 , TCMR  #b7 , RKML  #b8 ), search-based models (i.e. JEC  #b17 , TagProp  #b10 , and TagRelevance  #b14 ), mixture models (i.e. CMRM  #b13  and MBRM  #b6 ) and co-regularized learning model (FastTag  #b1 ). The tag refinement procedure by itself, denoted as SMC IMC, is also compared to verify the benefit from the tag completion procedure. We tune the parameters on the validation set of the two datasets separately for every method in comparison. Note that the weighting parameter µ we tune changes from 0.4 (Labelme) to 0.7 (MIRFlickr-25K), confirming that as the data become more and more noisy, we should pay more attention to the noisy tags and less on missing tags.We measure all the methods in terms of average preci-sion@N (AP @N ) and average recall @N (AR@N ). In the top N completed tags, precision@N is to measure the ratio of correct tags and recall @N is to measure the ratio of missing ground-truth tags, both averaged over all test images. Table 2 and Table 3 show performance comparisons on the two datasets, respectively.

Evaluation and Observation
We can observe that: 1) Generally, methods achieve better performance on Labelme, since tags in MIRFlickr-25K are more noisy. 2) Methods based on matrix completion, such as SMC, LRES and TCMR, usually achieve the best performances. 3) Our SMC framework shows increasing advantage to LRES as the data become more and more noisy, justifying our assumption and model on the noises. 4) SMC nearly  outperforms all the other algorithms in all cases. 5) Performance comparison between SMC and SMC IMC demonstrate the remarkable benefit of tag completion for tag refinement. 6) Performance on MIRFlickr-25K in some sense provides an evidence for the robustness of SMC.

CONCLUSION
In this work we present an effective framework for image annotation by performing tag completion and tag refinement sequentially. Our method first clusters images using sparse subspace clustering and shares tags using a neighbor voting algorithm, then refines tags by adapting inductive matrix completion while novelly utilizing visual and semantic information. Experiments show the effectiveness of our framework and suggest that tag refinement can benefit a lot from performing tag completion first.