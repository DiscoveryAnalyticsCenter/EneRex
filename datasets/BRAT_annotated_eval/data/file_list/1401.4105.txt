Learning 1 -based analysis and synthesis sparsity priors using bi-level optimization

Abstract
We consider the analysis operator and synthesis dictionary learning problems based on the the 1 regularized sparse representation model. We reveal the internal relations between the 1 -based analysis model and synthesis model. We then introduce an approach to learn both analysis operator and synthesis dictionary simultaneously by using a unified framework of bi-level optimization. Our aim is to learn a meaningful operator (dictionary) such that the minimum energy solution of the analysis (synthesis)-prior based model is as close as possible to the groundtruth. We solve the bi-level optimization problem using the implicit differentiation technique. Moreover, we demonstrate the effectiveness of our leaning approach by applying the learned analysis operator (dictionary) to the image denoising task and comparing its performance with state-of-the-art methods. Under this unified framework, we can compare the performance of the two types of priors.

Introduction
Maximum a Posteriori (MAP) inference under the Bayesian framework is a popular method for solving various inverse problems in image processing. The MAP estimator is equivalent to an energy minimization problem, which consists of a data fidelity term and a signal prior term (also known as regularization term). Roughly speaking, the priors fall into two main prior types. One is the analysisbased prior and the other is the synthesis-based one.Notation: In this paper our model presents a global prior over the entire image, in contrast to the common patch-based one. In order to distinguish between a patch and an image, we use the notation x ∈ R m to indicate a patch (patch size:√ m × √ m, m is odd), and u ∈ R M N to indicate an image (image size:M × N , with m M, m N ). We refer D ∈ R m×n and A ∈ R n×m with m ≤ n to the patch-based synthesis dictionary and analysis operator respectively. Furthermore, when the analysis operator A is applied to the entire image u, we use the common sliding-window fashion to compute the coefficients Ax for all M N patches in the 2-D image form of u. This result is equivalent to a multiplication of a sparse matrix A ∈ R (n×M N )×M N and u, i.e., Au. We can group A to n separable sparse matrices {A 1 , . . . , A n }, where A i ∈ R M N ×M N is associated with the i th row of A (A i ). If we consider A i as a 2-D filter ( √ m × √ m), we have: A i u is equivalent to the result of convolving image u with filter A i . Finally, we use A that is expanded from the patch-based analysis operator A, to denote the global analysis operator associated with an entire image.Patch based analysis and synthesis model: Under the framework of MAP, the patch-based analysis model is given as the following minimization problemx * = arg min x∈R m φ(Ax) + λ 2 x − f 2 2 , (1.1)where A is called analysis operator. The form of the penalty function φ depends on the prior utilized. For sparse representation, it can be · p p (p ∈ [0, 1]) or log(1 + |x|). The second type of prior is so-called synthesis prior. Basically, in the synthesis-based sparse representation model, a signal x is called sparse over a given dictionary D, when it can be approximated as a linear combination of a few atoms from dictionary D. This is formulated as following minimization problem using the MAP estimator. When we concentrate on the sparse prior, normally the penalty function φ is chose as · p p (p ∈ [0, 1]).x = Dα * ; α * = arg min α∈R n φ(α) + λ 2 Dα − f 2 2 . (1.2)Learning patch based analysis and synthesis prior: In order to pursue better performance, an intuitive possibility is to make a better choice for the analysis operator A and dictionary D based on training. Indeed, there exist several typical and successful training algorithms for over-complete dictionary learning: (i) the K-SVD algorithm  #b6  #b0  (ii) On-line dictionary learning algorithm  #b12  (ii) efficient sparse coding algorithms  #b10 . However, compared to the extensive study for the training of the synthesis dictionary, the analysis operator learning problem has received relatively much less attention in the past decade, although the analysis model is the counterpart to the celebrated synthesis sparse model. But fortunately, it has been gaining more and more attention these two years. Consequently, there appear different algorithms for analysis operator learning  #b13  #b16  #b18  #b20  #b19  #b8  #b14 . Among existing analysis operator learning algorithms, the learning approach proposed by Peyré and Fadili is very appealing since they consider this problem from a novel point of view. They interpret the action of analysis operator as convolution with some finite impulse response filters and they formulate the analysis operator learning task as a bi-level optimization problem  #b4  which is solved using a gradient descent algorithm.Contributions: Based on the investigation of existing dictionary and analysis operator learning algorithms, we find that (1) all the training approaches are based on patch priors; (2) the study of the later is immature since so far only few prior work has been tested with natural images  #b20  #b19  #b8 ; and (3) most analysis operator learning algorithms have to impose some non-convex constraints on the operator A; this therefore makes the corresponding optimization problems relatively complex and difficult to solve. Thus three questions arise: (1) can we formulate the image-based model using the patch priors? (2) is it possible to formulate the analysis operator learning problem in a relatively easy way? (3) can we compare two types of priors under an unified framework? We give answers to these questions in this paper.

Analysis operator and dictionary learning via bi-level optimization
From patch-based model to image-based one: In this paper, we concentrate on convex 1 sparse representation. In the case of analysis model, following the filter-based MRF model for image restoration, it is straightforward to extend the patch-based analysis model to the image-based one, which is given as:u * = arg min u E(u) = n i=1 A i u 1 + λ 2 u − f 2 2 = Au 1 + λ 2 u − f 2 2 , (2.1)where A is the global analysis operator constructed from the local patch-based analysis operator A, u and f are images (M × N ). However, if we want to extend the patch-based synthesis model to the image-based one, we find it not as easy as the analysis case. Considering the common strategy that averages over-lapping patches, we can make explicit use of this strategy of patch-averaging to reconstruct the recovered image, then we arrive at our image-based synthesis model{α * ij } = arg min αij ij α ij 1 + λ 2 1 m ij R T ij Dα ij − f 2 2 , (2.2) where the size of image f is M × N , the patch size is √ m × √ m, matrix R ij is an m × N p (N p = M × N ) matrix that extracts the (i, j)patch from the image, and α ij is a n × 1 vector. We explicitly average all the over-lapping patches by a factor m, because ij R T ij R ij = mI Np×Np (the number of patches is equal to the number of pixels using symmetrical boundary condition). Note that in our formulation, α ij is not independent any more, in contrast to their independence in  #b6 . If we stack all the α ij and R ij to a huge column vector α and a huge matrix R respectively, and construct a huge diagonal-block matrix D by using dictionary D, (2.2) can be rewritten asα * = arg min α α 1 + λ 2 1 m R T Dα − f 2 2 = α 1 + λ 2 Dα − f 2 2 , (2.3)where D = 1 m R T D. Now we can see the image-based model has the unified form with the patchbased one, which has a nice MAP interpretation. However, this formulation involves too many unknown variables (n × N p ), compared to N p unknown variables for the analysis model. This is a big drawback for our training scheme. we expect to formulate it by N p variables. Indeed, we succeed by considering its dual problem. We introduce a auxiliary variable u = Dα into the 2 norm, and use v to denote the Lagrange multiplier, by using definition of the convex conjugate function to the 1 norm  #b2 , we arrive atv * = arg min v δ(D T v) + 1 2λ v − λf 2 2 , u = f − v/λ,(2.4)where the function δ(D T v) denotes the indicator function of the interval [−1, 1]. After having a closer look at the connection between the primal variable α and the dual variable v, we find that v is exactly the additive noise itself, because the recovered image is given by Dα * with Dα * = u * = f − v * /λ. More interestingly, after expanding D T v = D T (Rv)/m, we find that this result is surprisingly equivalent to filter response result when applying the analysis operator 1 m D T to an image v. Then we draw the conclusion that the synthesis dictionary D can also be interpreted as an analysis operator A D = 1 m D T . If we use the notation A D to denote the global analysis operator as used in the aforementioned image-based analysis model, we can present the similarity between the 1 -based analysis and synthesis model, which is given asv * = arg min v δ(A D v) + 1 2λ v − λf 2 2 , u * = f − v * /λ, ( 1 -based synthesis model) u * = arg min u Au 1 + λ 2 u − f 2 2( 1 -based analysis model).

(2.5)
Bi-level framework for synthesis dictionary and analysis operator learning: Motivated by the work presented in  #b14 , and the successful training instance to learn optimized parameters of MRF model  #b17 , we propose our analysis operator (dictionary) learning approach based on the unified bi-level optimization framework. Equation (2.5) is so-called lower-level problem in our bi-level framework, and we need to define an upper-level problem, also known as loss function. Following the work of  #b17 , we use the differentiable loss functionL(u * ) = 1 2 u * − g 2 2 ,(2.6)where g is the ground-truth image and u * is the minimizer of energy function (2.5). Given S training samples {f k , g k } S k=1 , where g k and f k are the k th clean image and the corresponding noisy version respectively, our bi-level model aims to learn an meaningful analysis operator (dictionary) such that the overall loss function for all samples is as small as possible. Therefore, our learning model is formally formulated as the following unconstrained bi-level optimization problem (take analysis operator learning model for instance; the dictionary learning model is similar).   min A L(u * (A)) = S k=1 L k (u * k (A)) = S k=1 1 2 u * k (A) − g k 2 2 subject to u * k (A) = arg min u E(u, f k ; A) = n i=1 A i u 1 + λ 2 u − f k 2 2 .

(2.7)
Advantage of our model: The most appealing property of our approach is that it is not necessary to impose any constraint set over the analysis operator A. Our training model can avoid trivial solutions naturally, e.g., if A = 0, the optimal solution of the energy function of (2.7) is certainly u * k (A) = f k , which makes the loss function still large; thus this trivial solution is not acceptable since the target of our model is to minimize the loss function. Therefore, the learned operator A must contain some meaningful filters such that the minimizer of the lower-level problem is close to the ground-truth.Solving the bi-level problem using implicit differentiation: Following the work of  #b17 , we can compute the gradient of the loss function w.r.t the parameter A by using implicit differentiation. In order to employ the implicit differentiation rule, we need differentiable penalty functions. We have· 1,ε : φ(z) = z 2 + ε 2 δ ε : φ(z) = 1 2ε max(|z| − 1, 0) 2 . (2.8)In our training, we concentrate on mean-zero filters to keep consistent with the findings in the work  #b9 ; therefore, we express the filter A i as a linear combination of a set of basis filters{B 1 , . . . , B N B }, i.e., A i = N B j=1 θ ij B j .Then we obtain the derivatives of the loss function with respect to parameters θ ij , which is given as∇ θij L = S k=1 {− B T j φ (A i u * ) + A T i D i B j u * T ( n i=1 A T i D i A i + I) −1 (u * − g)} k , (2.9)where φ (A i u) is an N p × 1 vector obtained by applying function φ (z) element-wise to the vector A i u, and D i is an N p × N p diagonal matrix with each [D i ] n,n entry given by applying the function φ (z) element-wise to the vector A i u. In this formulation, we eliminate the parameter λ for simplicity, since it can be incorporated into the norm of the analysis operator A. As given by (2.9), we have collected all the necessary information to compute the required gradients, then we can employ the gradient descent based algorithms for optimization. In this paper, we make use of an efficient quasi-Newton's method, L-BFGS  #b11 .

Learning experiments and application results for image denoising
We conducted our training experiments using the training images from the BSDS300  #b1  image segmentation database. We used the whole 200 training images, and randomly sampled one 64 × 64 patch from each training image, giving us a total of 200 training samples. We then generated the noisy versions by adding Gaussian noise with standard deviation σ = 15. In our experiments, we learned an analysis operator A ∈ R 98×49 and synthesis dictionary D ∈ R 49×98 from the given training samples. In order to guarantee the property of mean-zero, each atom in A or D is expressed as the linear combination of the DCT-7 basis excluding the first filter with uniform entries.After we learned an meaningful operator A and dictionary D, we applied them to the image denoising problem based on the same 68 test images used in  #b15 . Tab. 1 presents the comparison of the average denoising results achieved by our 1 -based analysis and synthesis model with (i) one state-of-the-art denoising method BM3D  #b5  (ii) the K-SVD approach  #b6  and (iii) the total variation (TV)-based ROF denoising model  #b3 . We would like to point out that the TV based approach is the most commonly used 1 -based analysis operator; the K-SVD approach is a synthesis sparse representation model based on 0 optimization; BM3D is one current state-of-the-art denoising approach which is an image based, not generic prior based method, and is a specialized denoising algorithm. Fig. 1 

Conclusions and future work
From Tab. 1 and Fig. 1 we can draw the following conclusions: (i) the 1 -based analysis model is significantly superior to the 1 -based synthesis model which is coherent with the findings in the work  #b7 . We believe the essential reason lies in the ineffectual way the 1 -based synthesis model characterizes the natural images, since it tries to model the noise signal, not the natural image itself as aforementioned. This inferiority also appeared in the training. (ii) our analysis model is comparable with the 0 -based synthesis model K-SVD, as can be seen in Fig. 1. Compared with specialized methods for image denoising task such as BM3D, our 1 -based analysis model still can not compete. However, its denoising performance is always significantly better than the TV based approach.It is well known that the probability density function (PDF) of the response of zero mean linear filters on natural images has heavily tailed distribution  #b9 . Therefore, our future work will concentrate on non-convex penalty function such as |z| or log(1 + |z|). According to our preliminary experience about the analysis model using log(1 + |z|) as penalty function, it clearly outperforms the 0 -based synthesis model K-SVD, and has already been on par with BM3D. (We will present this result in our future work.) However, for the case of non-convex, since the Fenchel's duality we used in this paper is not available any more, how to handle the synthesis model becomes a problem. This will be the subject of our future work.