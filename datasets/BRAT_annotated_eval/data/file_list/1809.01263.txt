An Efficient Approach for Polyps Detection in Endoscopic Videos Based on Faster R-CNN

Abstract
Polyp has long been considered as one of the major etiologies to colorectal cancer which is a fatal disease around the world, thus early detection and recognition of polyps plays an crucial role in clinical routines. Accurate diagnoses of polyps through endoscopes operated by physicians becomes a chanllenging task not only due to the varying expertise of physicians, but also the inherent nature of endoscopic inspections. To facilitate this process, computer-aid techniques that emphasize on fully-conventional image processing and novel machine learning enhanced approaches have been dedicatedly designed for polyp detection in endoscopic videos or images. Among all proposed algorithms, deep learning based methods take the lead in terms of multiple metrics in evolutions for algorithmic performance. In this work, a highly effective model, namely the faster region-based convolutional neural network (Faster R-CNN) is implemented for polyp detection. In comparison with the reported results of the state-of-the-art approaches on polyps detection, extensive experiments demonstrate that the Faster R-CNN achieves very competing results, and it is an efficient approach for clinical practice.

I. INTRODUCTION
It is well known that the predecessor of colorectal cancer (CRC), also termed as colon cancer, is most likely to be a polyp. According to the statistics of American Cancer Society, colorectal carcinoma is the third most commonly diagnosed cancer and the second leading cause of death from cancers in the United States  #b0 . CRC is the fourth cause of cancer death worldwide with around 750,000 new cases diagnosed in 2012 alone  #b1 .Pathologically, neoplastic polyps may chronically turn into cancer, located hiddenly on colorectal wall unless filmed during colonoscopy, which is the main diagnostic procedure of doctors. Though this process may be intuitively achieved successfully, approximately 25% of polyps are missed  #b2 , which brings about potential risks to patients' lives. For early diagnoses and prevention of colon cancer, an urgent task for physicians and computer vision researchers is to find more reliable, accurate and even faster approaches for polyp detection. In response to the demands, well-designed grand challenges organized by Medical Image Computing and Computer Assisted Intervention (MICCAI) and International Symposium on Biomedical Imaging (ISBI), have attracted a lot of attention worldwide.Specifically, multiple factors affect either the process of manual inspection or computer-aided detection significantly. For the first and foremost, it is common that during the clinical practices, physicians usually operate conventional colonscope for hours to seek, observe, and diagnose polyps, when considering the heavy workload of physicians that leads to both mental and physical fatigue, even an experienced doctor would miss or wrongly diagnose benign polyps. Therefore, automatic computer-aid system is urgently in modern medicine communities  #b3 ,  #b4 .In regard to computer-aid methods, the factors are diverse. There are varieties of noises in the videos which can be classified as the specular highlights caused by illumination along with the non-Lambertian colorectal walls, the curving veins distributed around the polyps, the polyp like bulges on internal wall to lumen, blob-like matters such as bubbles that always being observed, and the insufficient illumination that shield all regions of interest (ROI). These noises may invalidate the state-of-the-art conventional and learning-based approaches  #b2 ,  #b5 ,  #b6 . Our experiments show that, in some rare cases, Faster R-CNN  #b7  may mistake some oval specular highlights for polyps.Another factor is the shape information. Polyps are not always appeared as regular oval lumps, furthermore, they can be various in their sizes from 3mm to more than 10mm or more variable due to projective transformation and distortions of imaging sensors. Conventional hand-crafted approaches and some fusion approaches often suffer from this factor in that they are initially designed according to the morphological features of polyp  #b0 ,  #b2 ,  #b6 ,  #b8 ,  #b9 .Bernal et al.  #b10  categorize off-the-shelf methods for polyp detection into three classes: hand-crafted, hybrid, and end-toend learning. Our work emphasizes on the deep learning solution to polyp detection, and provide evaluation of variations in parameters. Our contributions include:• To our best knowledge, this work provides the first evaluation for polyp detection using Faster-RCNN framework. In addition to reducing the false positive rate during the test phase whose goal is to lower the risks for misdiagnosis when taking the detector for clinical practice, our system provides a good trade-off between efficiency and accuracy. • We demonstrate a fine-tuned set of parameters for polyps detection in endoscopic videos that outperform many state-of-the-art methods. The testing results set a novel baseline for polyp detection. framework together with the trained parameters are available for the research community on the author's website.

II. RELATED WORKS
According to MICCAI 2015 challenge evaluations, fully CNN based methods with or without data augmentation outperform fusion methods and hand-crafted when considering the evaluation metrics in most cases: Recall, Precision, Fscores (e.g. CUMED, OUS in all videos and videos with only polyp frames). However, high false positive rate has been observed during the experiments  #b11  that a novel data augmentation technique -random view aggregation is implemented, while for pursuing the highest F-scores and remedying the deficiencies of 2D-CNNs, online and offline 3D-fully convolutional networks (FCNs) are integrated to acquire the final confidence map  #b12 . For 2D-CNNs, most of related works focus on no more than 5-convolutional layered deep CNNs such as AlexNet  #b13 , but a few  #b11  have experimented on deeper networks. It remains to be a key topic whether light weighted CNNs can achieve the same capacity as their very deep counterparts. Still, we believe that a trade-off between architectural complexity and runtime would contribute to the ideal design, which is the main reason that we choose VGG16  #b14 , once achieved 92.7% top-5 test accuracy in ImageNet dataset as the feature extractor.To the best of our knowledge, Faster-RCNN is the first detector so far that replaces hand-crafted ROI selection step with a network i.e., the regional proposal network (RPN) towards fully end-to-end fashion. Its structure is developed from previous R-CNN  #b15  and Fast-RCNN  #b16 . Recently, an improvement of Faster R-CNN, i.e., the Mask R-CNN  #b17  is proposed by extending a novel multi-task branch: mask subnetwork for segmentation purpose along with replacement of ROI Pooling layer by ROI Align layer. We apply Faster R-CNN without the sub-network for its redundancy in detecting polyps.Other novel end-to-end detectors such as You Only Look Once (YOLOv1)  #b18 , YOLOv2  #b19 , SSD  #b20 , so far, most of them have been implemented and tested on other public or private datasets such as COCO, ImageNet, etc. Although these approaches could fulfill realtime requirements (up to more than 24fps), the ROIs are randomly chosen without an end-to-end fashion, and the mAP is compromised in terms of polyp detection as reported in  #b21  that examines YOLOv1 on ASU-Mayo Clinic dataset  #b6 .

III. ARCHITECTURE OF FASTER R-CNN
A. Backbone Structure Fig. 2 illustrates the complete testing structure of this work. The backbone  #b17  computes high-level features of entire test frame such that the weights between ROIs are shared, which is different from previous R-CNN and patchwise OUS  #b10  methods. Faster R-CNN removes all subsequent layers of 512 feature maps conv5 3 whose shape is 50×37 for each. In reference to VGG16 and ZFnet, it is reported that the latter runs faster up to 17fps, while the former runs at 5fps  #b7 , on a K40 GPU. When comparing mAPs on PascalVOC 2007, ZFnet backbone achieves highest 59.9%, and VGG16 78.8%. VGG16 thus benefits for its deep feature extraction process besides its relatively high speed compared to CUMED  #b10  that runs at 5fps on a more advanced TitanX GPU for former CVC-ClinicDB (CVC-ClinicDB2015)  #b22 .

B. RPN and Head Networks
The conv5 3 is fed to two sibling branches -RPN and Head  #b17 . After performing 3×3 convolution, RPN constructs 9 anchors at each position on the resulted feature map, the anchors are designed according to 3 scales (small, medium, large) with 3 different ratios of 1:1, 1:2, 2:1. As a result, it outputs maximum 50×37×9×4=66600 positional coordinates of all 16650 potential proposals (for each proposed, the coordinates in the test image are the center (x a , y a ), width w a and height h a of the bounding box), and 50×37×9×2=33300 scores per proposal being the background or polyp. During the training, not all proposals are transformed to training samples, of which a limited number of refined proposals e.g. 2000, are selected by trimming invalided bounding boxes along the borders; proposals with intersection of union (IoU) between 0.3 and 0.7, and in the meantime, keep as many positive samples (IoU>0.7) as possible, and replenish with negative samples (IoU<0.3); and applying non-maximum suppression (NMS) to the scores S bg and S obj , as depicted in Fig. 2.During the testing process, we let RPN generate 150 top proposals further trimmed by NMS of the scores s obj and s bg , afterall, RPN is trained for valid regional proposals better than its counterpart -selective search. Refined candidates are then mapped to anchors on conv5 3. The Head network leverages on each anchor to yield the detection outcomes.As shown in Fig. 2, the blue arrow represents the ROI pooling process. All 150 anchors are resized to the same size, which is equivalent to a single-layered SPPnet  #b23 . This procedure is essential as it transforms different scaled feature map into the two following 4096 fix-length fully-connected layers, each is followed by a dropout layer with a probability of 0.5, which makes the softmax classifier applicable. In addition to regress bounding box of predicted ROIs (x, y, w, h), the Head output 2-class probabilities of the correspondent ROIs to be either background P bg or polyp P polyp . The image is best viewed in its colored version that the red arrow signifies the convolution-ReLU flow, green arrow the max pooling flow, purple arrow the 1×1-convolution or fully-connected flow, blue arrow the ROI pooling flow, and black arrow represents the normal datum flow. Noted that all operations of the same sized convolutional kernel is only labeled at the first appearance of that flow. The input frame is selected from new CVC-ClinicDB (CVC-ClinicDB2017), resizing to 800×600.

C. Loss Function
Either RPN or the Head loss functions  #b7  of Faster R-CNN consists of two parts i.e., the classification loss L cls and bounding box regression loss L reg . Suppose the ground truth of a proposal to be {x * , y * , w * , h * , P * i }, among which P * bg = 1 and P * polyp = 0 if the proposal is positive, and P * bg = 1 and P * polyp = 0 if negative. To alleviate the influence of scales during training, the coordinates are parameterized ast x = (x − x a )/w a , t y = (y − y a )/h a , t w = log(w/w a ), t h = log(h/h a ), t * x = (x * − x a )/w a , t * y = (y * − y a )/h a , t * w = log(w * /w a ), t h = log(h * /h a ),(1)and the general loss function is denoted asL({P bg , P polyp }, {t i }) = 1 N cls [L cls (P bg , P * bg )+ L cls (P polyp , P * polyp )] + λ 1 N reg i P * i L reg (t i , t * i ),(2)where N cls denotes the mini-batch size, N reg the number of all proposals from an image for training. Here the classification loss L cls (P i , P * i ) = −P * i log(P i ), where P poyp + P bg = 1, P poyp and P bg are outputs of softmax classifier, and the bounding box regression loss L reg (t i , t * i ) = R(t i − t * i ), in which R(·) is smooth L 1 function for Head loss denoted asR(x) = 0.5x 2 |x| < 1 |x| − 0.5 otherwise.(3)For joint training, the total loss is the sum of RPN and Head losses. while applying 4-step training, two losses are tuned alternately.

IV. IMPLEMENTATION DETAILS


A. Data Preparation
The framework is tested using the following public datasets tested during our experiments include: Only simple transformations are made to the raw images without augmentation. All training frames are resized to   • CVC-Clinic2015 (CVC15

B. Training
Instead of the 4-steps alternately training strategy to optimize RPN and Head losses, we test another approximately joint optimization (AJO) proposed by authors of  #b7  that takes a mini-batch as input and optimizes both losses at the same time. Nevertheless, there is no differential error increments for stochastic gradient descent (SGD) method at RoI pooling layer, the remedy is to propagate these increments backwards without processing. In contrast the 4-steps training methods, AJO has nearly the same test mAP on PascalVOC 2007 whereas faster during training (save up to 9 hours).The training datasets contains 11954 images in total. We train Faster R-CNN on a K40c GPU with default parameters except setting mini-batch size to 128, all batches are normalized by subtraction of fix mean values. Training took no more than 4 days for fine-tuned network without observation of overfitting. In addition, VGG16 is initialized by ImageNet weights. And after 70000 iterations, fully-trained network saw the convergence except for class loss, which indicates that the fully trained Faster-RCNN using AJO may fail to detect polyps.

C. Validation
Our polyp detection tasks include predicating whether a frame shows a polyp, and localizing the exact location of a polyp. To track training status, we utilize the rest two sequences of CVC-ClinicDB2017 training set as validation sets for evaluating the performance that contain 1178 frames, 910 of which contain a polyp. All evaluation metrics are consistent with MICCAI2017 sub-challenge except F-scores as shown in Tab. I. Noted that FN, TP are counted once per frame, and FP, FN multiple times per frame.Training sets of other datasets are considered as validation sets except for CVC-EndoSceneStill where the dataset has its own division up to 183 frames. 1, 25, 50, 100, 200, 300 regional proposals are tested respectively for each dataset.  

V. EXPERIMENTS


A. Detection
Tab. II show the fined-tuned results of 300 proposals which yields the best performance upon metrics whereas having the longest runtime. Typically, the detector runs at 17fps for 1 proposal which reaches the lower bound of realtime application, and 0.9fps for 300. Parameters are set as follows: Thresholds for RPN NMS, confidence of detection are 0.7 and 0.3, top 1,000,000 proposals before feeding to RPN NMS to ensure 100% detection rate, a higher confidence threshold 0.5 would drop the rate to 97.4%. It can be inferred from the detection results that the detection rate reaches a high level for CVC-Clinic2015, CVC-ColonDB and CVC-EndoSceneStill for the reason that each frame of these datasets contains at least one polyp. During the test of the experiments, due to the lower threshold set for confidence, the higher FN rate is observed during detection. Moreover, it is crucial to make a good trade-off between the performance and speed if an automatic detector is designed for real practice. We found on CVC-ColonDB that number of proposals influenced the detection rate greatly that the accuracy reduced from 97.3% for 300 proposals to 88.3% for 1 proposal. This trend is identical with that of CVC-Clinic2015 and CVC-EndoSceneStill.

B. Localization
Corresponding localization results are shown in Tab. III and IV. To compute MD, the Euclidean distance between the center of detected bounding box and that of ground truth is In comparison, as is manifested in Tab. IV, the outcomes indicate that Faster R-CNN achieves competitive performance compared to novel learning-based techniques, CUMED, ASU, and OUS  #b10  on videos with only polyp frames. Noted that these methods take one detection as TP if the detected center falls within the area of ground-truth mask, which is slightly different from MD metric. To be more specific, MD metric implemented here is more strict for it only considers the shortest side of the ground-truth box. During the experiments, we did not validate Faster R-CNN on the private ASU-Mayo Clinic dataset and the MICCAI2015 testing dataset due to their unavailability. However, the design of test set may differ from that of training set, this potential problem is alleviated by the various sets of polyps under different conditions from CVC15 training set and the similar sources of samples.

C. Fine-Tuning vs from Scratch
On small polyp datasets, we are interested in the resultant performances by training from scratch or fine-tuned. For fully trained Faster R-CNN, all weights are initialized by random sampling from Gaussian distribution with zero mean and a standard deviation of 0.01. Fine tuned network manifests high performance during the test as shown in Tab. II-III. The fullytrained network, on the other hand, requires a few more days for training, and it has been observed that the lower mAP of fully-trained network might due to the AJO strategy in that the anchors are more sensitive to the initialized weights and RPN fails to provide sufficient positive samples.The Faster R-CNN detector can detect largely occluded polyp and being robust to illumination changes as is depicted in Fig. 1a, also, noises as circular bubbles (Fig. 1b) are cor-rectly predicted by the detector, even in the case that there are other tissues except polyp, and the detector correctly localizes the polyp in frames. Another advantage is that very large polyps that may occupy whole receptive field are successfully detected.On the other aspect, although the detector is more liable to locate large polyps, it misses some very small polyps in the frames, as depicted in Fig. 3a, which accounts for the high FP rate in Tab. III, especially when predicting validation sequence 17 of CVC-Clinic2017. It should note that the detector learns the oval shape of polyp so firmly that it mistakes false areas (Fig. 3b-3d) as the real polyps, which causes high localization FP rate with respect to all datasets. In our future work, we would focus on solutions to these issues.

VI. CONCLUSION
Faster R-CNN has been a fully end-to-end approach for object detection tasks on public datasets of natural scenes. For polyp detection and localization in endoscopic videos, this work first applies Faster R-CNN with VGG16 as the backbone. Through extensive experimental evaluation, the proposed approach exhibits potentials for reaching the best performance on precision, as well as yields competitive results in other metrics. The high detection performance indicates that Faster R-CNN could help lower the risk of missing polyps during colonoscopy examination even if RPN predicts only 1 proposal per test. On the other side, Faster R-CNN shows high false-positive rate in frames with presence of polyp during localization tests, which needs to be further investigated and discussed.