Approximate Matrix Multiplication with Application to Linear Embeddings

Abstract
In this paper, we study the problem of approximately computing the product of two real matrices. In particular, we analyze a dimensionality-reduction-based approximation algorithm due to Sarlos [1], introducing the notion of nuclear rank as the ratio of the nuclear norm over the spectral norm. The presented bound has improved dependence with respect to the approximation error (as compared to previous approaches), whereas the subspace -on which we project the input matrices -has dimensions proportional to the maximum of their nuclear rank and it is independent of the input dimensions. In addition, we provide an application of this result to linear low-dimensional embeddings. Namely, we show that any Euclidean point-set with bounded nuclear rank is amenable to projection onto number of dimensions that is independent of the input dimensionality, while achieving additive error guarantees.

Introduction
Living in the era of Big Data, the excess of available information constitutes its manipulation and interpretation a strenuous task: In contrast to conventional wisdom where more data is a source of "simplicity" in statistical terms  #b1 , large datasets embody preprocessing tasks of high time-and space-complexity, jeopardizing any hope for data analysis within reasonable time and with low computational cost. Due to such difficulties, one might be interested in screening the data, even if no prior information is available. I.e., identifying a coreset such that most of the latent structure/information that we want to infer is maintaned within an error that we can control. Such approaches have been witnessed in a broad class of data analysis problems such as data matrix sparsification for accelerated spectral calculation  #b2 , feature selection (a.k.a. column subset selection) for better interpretation of the results  #b3  and low-complexity calculations  #b4 , feature extraction via dimensionality reduction techniques  #b5  #b6 , etc.In this work, we analyze a particular approximation algorithm of  #b0  for the task of matrix multiplication with respect to their spectral norm  #b7 . Within this context, we focus on approximately computing the product of two matrices when their intrinsic dimensionality is low, as expressed by the nuclear rank. Here, we define the nuclear rank as the ratio of the nuclear norm of a matrix over its spectral norm. We provide an elementary proof based on the randomized algorithm described in  #b0  #b7 . As a result, we further strengthen the performance of the proposed scheme in situations where the allowed approximation error is small: Using a weaker notion of intrinsic dimensionality (i.e., nuclear rank instead of stable rank; see the definitions later in text), the dependence on the approximation error ε is improved to O(1/ε 2 ) instead of O(1/ε 4 ), indicating that the proposed scheme scales better when ε decreases. Table 1 places our result into context with prior works. Table 1: Summary of results on ε-approximate matrix multiplication A · B with respect to spectral normnr (·) and sr (·) denote the nuclear and stable rank of a matrix, respectively.

Metric # of dimensions Reference
Rank O((r(A) + r(B))/ε 2 ) [9] Stable rank O((sr (A) + sr (B))/ε 4 ) [8]Nuclear norm O((nr (A) + nr (B))/ε 2 ) Theorem 1As an application, we use this result to design dimensionality reducing linear embeddings that operate on a given dataset and preserve Euclidean distances between data points. In general, it is well-known that simply rotating, scaling and translating in a random way is adequate for this task: According to JL lemma  #b9 , such linear mapping approximately preserves (i.e., within a ε-radius of points) the distances between all the pairs of points. While there are many random constructions that achieve this property with high probability, such schemes are oblivious to the geometry of the data set at hand, leaving space for further improvements.

Contributions:
The main contributions of this manuscript are the following:(i) We provide a novel analysis on the approximate matrix multiplication problem based on the notion of nuclear rank (See Theorem 1).(ii) We demonstrate an application of the approximate matrix multiplication bound to dimensionality reduction with relative guarantees.

Related Work
There is rich literature on the topic of linear dimensionality reduction and the techniques utilized for this purpose, mainly due to the wide range of applications it covers. Here, we highlight a few approaches that provide bounds depending on the input data: These bounds are usually stronger than the classic JL lemma, provided that the input points have low "geometric complexity", i.e., points that lie on a low dimensional subspace  #b0 , lie on a manifold  #b10  #b11 , etc. Here, we focus only on results that provide theoretical guarantees. For the special case where the input points lie on the unit sphere, there is a close connection between the Talagrand functional, denoted as γ2, and the required number of dimensions for point distance preservation  #b12 ,  #b13 . These results can be viewed as stronger bounds compared to the JL lemma. Baraniuk et al. provide bounds on the number of dimensions required by a random linear embedding to preserve Euclidean distances for the case of smooth manifolds  #b10 ; for improvements and a nice exposition on the topic, see  #b11 . Sarlos provided improved bounds for algorithms on large matrices with applications on matrix multiplications, linear regression and low rank matrix approximation  #b0 . On a similar flavor, the authors in  #b14  provide bounds for relative error approximations of points that lie on a surface. Several approaches exist for non-linear dimensionality reduction as well  #b15  #b16 . From a different perspective, Indyk and Naor consider the "doubling dimension" and other related measures of dimensionality for preserving nearest neighbor queries  #b17 .Recent developments in  #b18  describe deterministic constructions of linear embeddings in polynomial time via SemiDefinite Programming (SDP) relaxations, under the assumption that the data is known apriori and fixed. For the latest developments on this topic, we refer the reader to  #b19 .

Preliminaries
A scalar is denoted by an italic letter, e.g. a. A column vector is denoted by a bold lowercase letter, e.g. a ∈ R n whose i-th entry is ai. A matrix is denoted by a mathtype uppercase letter, e.g., A ∈ R n×d with (i, j)-th entry Ai,j . To denote the i-th row and j-th column of A, we use the ai and a j , respectively. Use A 2 = max x: x 2 =1 Ax 2 to denote the spectral norm of A andA F = i j A 2i,j to represent its Frobenius norm. In our analysis, we utilize the notion of nuclear norm. Given a matrix A ∈ R n×d with rank r(A), its nuclear norm A ⋆ is given byA ⋆ = r(A) i=1σi where σi is the i-th largest singular value; for multiple matrices, we also use ξi to denote a singular value. Using the nuclear norm, we define the nuclear rank of A as nr (A) := A ⋆ / A 2 . Moreover, we use the notion of stable rank, which is defined as sr (A) = A 2 F / A 2 2 . Throughout the paper, we will denote by G a t×d random matrix whose entries are independent Gaussian random variables with variance 1/t, i.e., Gij ∼ N (0, 1/t).

Main result
We consider the following problem: APPROXIMATE MATRIX MULTIPLICATION PROBLEM: Let X ∈ R n×d and Y ∈ R d×m be two arbitrary matrices, ε > 0 is an approximation parameter and 0 < δ < 1 the failure probability. We desire to construct sketched matrices X ∈ R n×t and Y ∈ R t×m where t ≪ d such that:X Y − XY 2 ≤ ε X 2 · Y 2 ,holds with probability at least 1 − δ.We now state the main theorem of the paper: Theorem 1. Fix 0 < ε, δ < 1 and assume arbitrary matrices X ∈ R n×d and Y ∈ R d×m . Set X = XG ⊤ and Y = GY. If t = Ω nr(X)+nr(Y)+log(log(1/ε))+log(1/δ)ε 2, then the following holds:P X Y − XY 2 ≤ ε X 2 · Y 2 ≥ 1 − δ,(1)We devote the rest of this section to prove Theorem 1. We recall a well-established result from the literature for Gaussian matrices; observe the lack of any upper bound on the error parameter. Lemma 1. Fix ζ > 0 and let X ∈ R n×d and Y ∈ R d×m . Set X = XG ⊤ and Y = GY. Then,  #b0  where c1 is the Hanson-Wright constant  #b20  and c2 = 18.P X Y − XY 2 > ζ X 2 Y 2 ≤ c r(X)+r(Y) 2 e −c 1 tζ·min(ζ,Proof. The proof is a corollary of the Hanson-Wright inequality [21, Theorem 1.1], combined with a dense net argument on the unit sphere defined by the union of the column and row span of Y and X, respectively.By homogeneity, we observe that, to prove XG ⊤ GY − XY 2 ≤ ε X 2 Y 2 is satisfied with some probability, it suffices to prove the same argument forX X 2 G ⊤ G Y Y 2 − X X 2 · Y Y 2 2 ≤ ε.Thus, without loss of generality, we can assume thatX 2 = Y 2 = 1.Let X = r(X) i=1 σiuiv ⊤ i be the singular value decomposition (SVD) of X, where σ1, . . . , σ r(X) denote the singular values of X and, u1, . . . , u r(X) ∈ R n and v1, . . . , v r(X) ∈ R d denote the left and right singular vectors, respectively. Similarly, we define theSVD of Y as Y = r(Y) j=1 ξjpj q ⊤ j . Define θ := c3 · nr(X)+nr(Y) ε 2where c3 > 1 is a constant and set t := θ + 8 ln(8/δ)/ε 2 + ln (⌈ln(e/ε)⌉) /(c1 · ε 2 ). Given θ, one can decompose X as X = X θ + X c θ where X θ represents the best rank-θ approximation of X and X c θ := X − X θ . Similarly, we can decompose Y = Y θ + Y c θ . Now, by the triangle inequality:XG ⊤ GY − XY 2 ≤ X θ G ⊤ GY θ − X θ Y θ 2 (2) + X c θ G ⊤ GY θfor both X c θ 2 ≤ ε and Y c θ 2 ≤ ε to be satisfied.Based on the definitions above and using triangle inequality on X θ G ⊤ G − I Y θ 2 , we obtain:X θ G ⊤ G − I Y θ 2 (i) = L l=1 X l θ G ⊤ G − I Y θ 2 (iv) ≤ L l=1 X l θ G ⊤ G − I Y θ 2 (i) = L l=1 X l θ G ⊤ G − I S s=1 Y s θ 2 (iv) ≤ S s=1 L l=1 X l θ G ⊤ G − I Y s θ 2 .(7)X θ G ⊤ G − I Y θ 2 ≤ S s=1 L l=1 ε + c2 r(X l θ ) + r(Y s θ ) c1t X l θ 2 Y s θ 2 = ε S s=1 L l=1 X l θ 2 Y s θ 2 + c2 c1t S s=1 L l=1 r(X l θ ) X l θ 2 Y s θ 2 + c2 c1t S s=1 L l=1 r(Y s θ ) X l θ 2 Y s θ 2 (iii) < ε S s=1 e −s+1 L l=1 e −l+1 + c2 c1t S s=1 e −s+1 L l=1 r(X l θ ) X l θ 2 + c2 c1t L l=1 e −l+1 S s=1 r(Y s θ ) Y s θ 2 ≤ ε ∞ s=1 e −s+1 ∞ l=1 e −l+1 + c2 c1t ∞ s=1 e −s+1 L l=1 r(X l θ ) X l θ 2 + c2 c1t ∞ l=1 e −l+1 S s=1 r(Y s θ ) Y s θ 2 = ε · e 2 (e − 1) 2 + c2 c1t · e e − 1 L l=1 r(X l θ ) X l θ 2 + S s=1 r(Y s θ ) Y s θ 2(10)For a pair (l, s) of positive integers, we define the following event, over the probability space defined by G:Ω(l, s) := X l θ G ⊤ G − I Y s θ 2 ≥ ε + c 2 r(X l θ ) + r(Y s θ ) c 1 εt X l θ 2 Y s θ 2 ,where c1, c2 > 0 are positive constants. According to Lemma 1, the above event holds with probability:P (Ω(l, s)) ≤ c r(X l θ )+r(Y s θ ) 2 · e −c 1 γ l,s t·min(γ l,s ,1)(8)where γ l,s = ε + c2r(X l θ )+r(Y s θ ) c 1 εt. A useful observation for  #b7  is given in the next lemma; the proof is provided in the appendix.

Lemma 2. Fix integer t > 0. The following inequality holds
P (Ω(l, s)) ≤ e −c 1 t·min(ε,ε 2 ) , ∀l, s.(9)By conditioning on the event S s=1 L l=1 Ω(l, s) c , we can upper bound (7) as in (10) where the infinite series ∞ i=1 e −i+1 = e e−1 is used in the last equality.To proceed, we observe the following for L l=1 r(X l θ ) X l θ 2 ; similar reasoning applies for S s=1 r(Y s θ ) Y s θ 2 . By construction in (iii):X l θ 2 < e −l+1 = e · e −l ≤ e · min i∈I l σi.Thus, it is obvious that:r(X l θ ) · X l θ 2 < e · r(X l θ ) · min i∈I l σi ≤ e · X l θ ⋆ ,since E ⋆ ≥ r(E) · minj σj(E), for any matrix E. To this end,L l=1 r(X l θ ) X l θ 2 < e L l=1 X l θ ⋆ = e X θ ⋆.Similarly, we have S s=1 r(Y s θ ) Y s θ 2 < e Y θ ⋆ and, therefore, (10) becomes:X θ G ⊤ G − I Y θ 2 < ε · e 2 (e − 1) 2 + c2 c1t · e e − 1 ( X ⋆ + Y ⋆ ) = ε · e 2 (e − 1) 2 + c2 c1t · e e − 1 (nr (X) + nr (Y)) ≤ 3ε(11)where the last equality is satisfied since t ≥ θ (c3 ≥ c 2 e c 1 (e−1) ). By the union bound and since 0 < ε < 1, (11) is violated with probability: where the last inequality is satisfied since t ≥ ln(2/δ)+ln(⌈ln(e/ε)⌉)c 1 ·ε 2 .The terms in (4) can be bounded as follows:X c θ Y θ 2 + X θ Y c θ 2 + X c θ Y c θ 2 ≤ X c θ 2 Y θ 2 + X θ 2 Y c θ 2 + X c θ 2 Y c θ 2 ≤ 2ε + ε 2 ≤ 3εwhere we used the fact that X θ 2 , Y θ 2 ≤ 1 and the inequalities X c θ 2 ≤ ε (similarly for Y c θ ). To bound the terms in (3), we provide the next Lemma; the proof is given in the appendix. Lemma 3. If t ≥ θ + 8 ln(8/δ) and 0 < ε < 1, then, with probability at least δ/2, the term appearing in (3) is at most 33ε.Applying the union bound on Lemma 3 and the complement of S s=1 L l=1 Ω(l, s) c , we conclude that the following inequality holds:XG ⊤ GY − XY 2 ≤ 39ε,with probability at least 1 − δ. By rescaling ε we obtain the required result.

Application to data-driven low-dimensional embedding
As an application of the result above, we consider the following question: PROBLEM: Given a collection n points in R d , forming a matrix A ∈ R n×d and an error parameter ε > 0, construct efficient and approximately accurate low-dimensional embedding G ∈ R t×d such that:G(ai − aj ) T 2 2 ±ε ≈ ai − aj 2 2 , ∀ai ∈ R 1×d , ai ∈ Rows(A).

The target dimension t is to heavily depend on the input data matrix A and be independent of the input dimensions.
Inspired by Theorem 1, the following theorem proposes a data-dependent randomized low-dimensional embedding G with the following guarantees: ∀i, j ∈ [n] G (ai − aj ) ⊤ 2 2 − ai − aj 2 2 ≤ 2ε A 2 2 ,with probability at least 1 − δ.To fully specify the algorithmic procedure followed in practice, only A ∈ R n×d and constants δ, ε ∈ (0, 1) are given as input and G ∈ R t×d is returned. Moreover, G guarantees to preserve the distances between the rows of A with additive error 2ε A 2 2 and with probability at least 1 − δ. The exact steps followed are given in Algorithm 1.

Algorithm 1 Data-Driven Random Linear Embedding
Input: A ∈ R n×d , ε, δ ∈ (0, 1)

1:
Compute (or approximate) A ⋆ and A 2 . Proof. By substituting X = Y ⊤ = A in Theorem 1, we have:P AG ⊤ GA ⊤ − AA ⊤ 2 ≤ ε A 2 2 ≥ 1 − δ.By the definition of the spectral norm, the condition above can be further written as the following maximization problem:max x:x∈R n , x 2 =1 x ⊤ AG ⊤ GA ⊤ − AA ⊤ x ≤ ε A 2 2 .Moreover, it is obvious that, if we restrict the search space x ∈ R n to x ∈ B n where:B n = y | y = 1 √ 2 (ei − ej) , ∀i, j ∈ [n] ,where ei is the standard basis vector with 1 in the i-th position, we further have:max x:x∈B n x ⊤ AG ⊤ GA ⊤ − AA ⊤ x ≤ max x:x∈R n , x 2 =1 x ⊤ AG ⊤ GA ⊤ − AA ⊤ x which leads to: P max x:x∈B n x ⊤ AG ⊤ GA ⊤ − AA ⊤ x ≤ ε A 2 2 ≥ 1 − δ.(12)Observe also thatx ⊤ AG ⊤ GA ⊤ − AA ⊤ x = GA ⊤ x 2 2 − A ⊤ x 2 2, which further transforms (12) as:P max x:x∈B n GA ⊤ x 2 2 − A ⊤ x 2 2 ≤ ε A 2 2 ≥ 1 − δ.(13)For any vector x ∈ B n , we observe thatx ⊤ AG ⊤ = 1/ √ 2 (ai − aj) G ⊤ while GA ⊤ x = 1/ √ 2G (ai − aj ) ⊤ . Similarly, x ⊤ A = 1/ √ 2 (ai − aj ) and A ⊤ x = 1/ √ 2 (ai − aj ) ⊤ .Thus, (13) becomes:P max i,j∈[d],i =j G (a i − a j ) ⊤ 2 2 − a i − a j 2 2 ≤ 2ε A 2 2 ≥ 1 − δ.(14)Since (14) is satisfied for the maximizing combination of i, j, we can safely remove the maximization to get:P G (ai − aj) ⊤ 2 2 − ai − aj 2 2 ≤ 2ε A 2 2 ≥ 1 − δ,which completes the proof.A complete set of experiments will be included in an extended version of the paper.

Conclusions
We present a novel analysis for a class of randomized and provably ε-accurate algorithms for the problem of matrix multiplication. As an application of this result, we show the utilization of the proposed scheme on data-driven low dimensional embeddings with additive error approximation, in the case where the data live on a subspace characterized by a small nuclear rank.An interesting question to pursue lies in the substitution of nuclear rank by stable rank: recent developments on this topic  #b7  show similar results using the latter metric as the intrinsic data dimension; a weaker assumption than the nuclear rank. However, the dependence on the approximation error is of the order O( 1 ε 4 ), as opposed to O( 1 ε 2 ) presented in this work. We hope this paper triggers future efforts to improve stable rank-based bounds with respect to error dependency.

Proof of Lemma 2
In all the cases below, observe γ l,s > ε by definition. Using Eqn. (9), we consider the following two cases:(i) γ l,s < 1: in this case, we have min(γ l,s , 1) ≥ min(ε, 1) = ε. Thus, (9) becomes: P (Ω(l, s)) ≤ c