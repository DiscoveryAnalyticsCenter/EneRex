Linear-time Learning on Distributions with Approximate Kernel Embeddings

Abstract
Many interesting machine learning problems are best posed by considering instances that are distributions, or sample sets drawn from distributions. Previous work devoted to machine learning tasks with distributional inputs has done so through pairwise kernel evaluations between pdfs (or sample sets). While such an approach is fine for smaller datasets, the computation of an N × N Gram matrix is prohibitive in large datasets. Recent scalable estimators that work over pdfs have done so only with kernels that use Euclidean metrics, like the L2 distance. However, there are a myriad of other useful metrics available, such as total variation, Hellinger distance, and the Jensen-Shannon divergence. This work develops the first random features for pdfs whose dot product approximates kernels using these non-Euclidean metrics, allowing estimators using such kernels to scale to large datasets by working in a primal space, without computing large Gram matrices. We provide an analysis of the approximation error in using our proposed random features and show empirically the quality of our approximation both in estimating a Gram matrix and in solving learning tasks in real-world and synthetic data.

Introduction
As machine learning matures, focus has shifted towards datasets with richer, more complex instances. For example, a great deal of effort has been devoted to learning functions on vectors of a large fixed dimension. While complex static vector instances are useful in a myriad of applications, many machine learning problems are more naturally posed by considering instances that are distributions, or sets drawn from distributions. Political scientists can learn a function from community demographics to vote percentages to understand who supports a candidate  #b5 . The mass of dark matter halos can be inferred from the velocity of galaxies in a cluster  #b26 . Expensive expectation propagation messages can be sped up by learning a "just-in-time" regression model  #b13 . All of these applications are aided by working directly over sets drawn from the distribution of interest, rather than having to develop a per-problem ad-hoc set of summary statistics. * These two authors contributed equally.Distributions are inherently infinite-dimensional objects, since in general they require an infinite number of parameters for their exact representation. Hence, it is not immediate how to extend traditional finite vector technique machine learning techniques to distributional instances. However, recent work has provided various approaches for dealing with distributional data in a nonparametric fashion. For example, regression from distributional covariates to real or distributional responses is possible via kernel smoothing  #b28  #b27 , and many learning tasks can be solved with RKHS approaches  #b25  #b29 . A major shortcoming of both approaches is that they require computing N kernel evaluations per prediction, where N is the number of training instances in a dataset. Often, this implies that one must compute a N × N Gram matrix of pairwise kernel evaluations. Such approaches fail to scale to datasets where the number of instances N is very large. Another shortcoming of these approaches is that they are often based on Euclidean metrics, either working over a linear kernel, or one based on the L 2 distance over distributions. While such kernels are useful in certain applications, better performance can sometimes be obtained by considering non-Euclidean based kernels. To this end,  #b29  use a kernel based on Rényi divergences; however, this kernel is not positive semi-definite (PSD), leading to even higher computational cost and other practical issues.This work addresses these major shortcomings by developing an embedding of random features for distributions. The dot product of the random features for two distributions will approximate kernels based on various distances between densities (see Figure 1). With this technique, we can approximate kernels based on total variation, Hellinger, and Jensen-Shannon divergences, among others. Since there is then no need to compute a Gram matrix, one will be able to use these kernels while still scaling to datasets with a K( , ) ≈ z( ) T z( ) Figure 1: We approximate kernels between densities p i , p j with random features of sample sets χ i iid ∼ p i , χ j iid ∼ p j . large number of instances using primal-space techniques. We provide an approximation bound for the embeddings, and demonstrate the efficacy of the embeddings on both realworld and synthetic data. To the best of our knowledge, this work provides the first non-discretized embedding for non-L 2 kernels for probability density functions.

Related Work
The two main lines of relevant research are the development of kernels on probability distributions and explicit approximate embeddings for scalable kernel learning.Learning on distributions In computer vision, the popular "bag of words" model  #b19  represents a distribution by quantizing it onto codewords (usually by running k-means on all, or many, of the input points from all sets), then compares those histograms with some kernel (often exponentiated χ 2 ).Another approach estimates a distance between distributions, often the L 2 distance or Kullback-Leibler (KL) divergence, parametrically  #b11  #b24  #b12  or nonparametrically  #b33  #b15 . The distance can then be used in kernel smoothing  #b28  #b27  or Mercer kernels  #b24  #b14  #b12  #b29 .These approaches can be powerful, but usually require computing an N × N matrix of kernel evaluations, which can be infeasible for large datasets. Using these distances in Mercer kernels faces an additional challenge, which is that the estimated Gram matrix may not be PSD, due to estimation error or because some divergences in fact do not induce a PSD kernel. In general this must be remedied by altering the Gram matrix a "nearby" PSD one. Typical approaches involve eigendecomposing the Gram matrix, which usually costs O(N 3 ) computation and also presents challenges for traditional inductive learning, where the test points are not known at training time  #b3 ).One way to alleviate the scaling problem is the Nyström extension  #b41 , in which some columns of the Gram matrix are used to estimate the remainder. In practice, one frequently must compute many columns, and methods to make the result PSD are known only for mildly-indefinite kernels  #b1 .Another approach is to represent a distribution by its mean RKHS embedding under some kernel k. The RKHS inner product is known as the mean map kernel (MMK), and the distance the maximum mean discrepancy (MMD)  #b8  #b25  #b35 . When k is the common RBF kernel, the MMK estimate is proportional to an L 2 inner product between Gaussian kernel density estimates.Approximate embeddings Recent interest in approximate kernel embeddings was spurred by the "random kitchen sink" (RKS) embedding  #b30 , which approximates shift-invariant kernels K on R by sampling their Fourier transform.A related line of work considers additive kernels, of the form K(x, y) = j=1 κ(x j , y j ), usually defined on R ≥0 (e.g. histograms).  #b23  construct an embedding for the intersection kernel j=1 min(x j , y j ) via step functions.  #b37  consider any homogeneous κ, so that κ(tx, ty) = t κ(x, y), which also allows them to embed histogram kernels such as the additive χ 2 kernel and Jensen-Shannon divergence. Their embedding uses the same fundamental result of  #b6  as ours; we expand to the continuous rather than the discrete case. Vempati et al. (2010) later apply RKS embeddings to obtain generalized RBF kernels (1).For embeddings of kernels on input spaces other than R , the RKS embedding extends naturally to locally compact abelian groups  #b20 .  #b27  embedded an estimate of the L 2 distance between continuous densities via orthonormal basis functions. An embedding for the base kernel k also gives a simple embedding for the mean map kernel  #b5  #b13  #b22  #b34 .

Embedding Information Theoretic Kernels
For a broad class of distributional distances d, including many common and useful information theoretic divergences, we consider generalized RBF kernels of the formK(p, q) = exp − 1 2σ 2 d 2 (p, q) .(1)We will construct features z(A(·)) such that K(p, q) ≈ z(A(p)) T z(A(q)) as follows: 1. We define a random function ψ such that d(p, q) ≈ ψ(p) − ψ(q) , where ψ(p) is a function from [0, 1] to R 2M . Thus the metric space of densities with distance d is approximately embedded into the metric space of 2Mdimensional L 2 functions. 2. We use orthonormal basis functions to approximately embed smooth L 2 functions into finite vectors in R |V | . Combined with the previous step, we obtain features A(p) ∈ R 2M |V | such that d is approximated by Euclidean distances between the A features. 3. We use the RKS embedding z(·) so that inner products between z(A(·)) features, in R D , approximate K(p, q). We can thus approximate the powerful kernel K without needing to compute an expensive N × N Gram matrix.

Homogeneous Density Distances (HDDs)
We consider kernels based on metrics which we term homogeneous density distances (HDDs):d 2 (p, q) = [0,1] κ(p(x), q(x)) dx,(2)where κ(x, y) : R + × R + → R + is a negative-type kernel, i.e. a squared Hilbertian metric, and κ(tx, ty) = tκ(x, y) for all t > 0. Table 1 shows a few important instances. Note that we assume the support of the distributions is contained Table 1: Squared HDDs. JS is Jensen-Shannon divergence; H is Hellinger distance; TV is total variation distance.within [0, 1] . Name κ(p(x), q(x)) dµ(λ) JS r∈{p,q} 1 2 r(x) log 2r(x) p(x)+q(x) dλ cosh(πλ)(1+λ 2 ) H 2 1 2 p(x) − q(x) 2 1 2 δ(λ = 0) dλ TV |p(x) − q(x)| 2 π 1 1+4λ 2 dλWe then use these distances in a generalized RBF kernel (1). d is a Hilbertian metric  #b6 , so K is positive definite  #b9 . Note we use the √ TV metric, even though TV is itself a metric.Embedding HDDs into L 2Fuglede (2005) shows that κ corresponds to a bounded measure µ(λ), as in Table 1, withκ(x, y) = R ≥0 |x 1 2 +iλ − y 1 2 +iλ | 2 dµ(λ). (3) Let Z = µ(R ≥0 ) and c λ = (− 1 2 + iλ)/( 1 2 + iλ); then κ(x, y) = E λ∼ µ Z |g λ (x) − g λ (y)| 2 where g λ (x) = √ Zc λ (x 1 2 +iλ − 1). We can approximate the expectation with an empirical mean. Let λ j iid ∼ µ Z for j ∈ {1, . . . , M }; then κ(x, y) ≈ 1 M M j=1 |g λj (x) − g λj (y)| 2 .Hence, using R, I to denote the real and imaginary parts:d 2 (p, q) = [0,1] κ(p(x), q(x)) dx = [0,1] E λ∼ µ Z |g λ (p(x)) − g λ (q(x))| 2 dx ≈ 1 M M j=1 [0,1] R(g λj (p(x))) − R(g λj (q(x))) 2 + I(g λj (p(x))) − I(g λj (q(x))) 2 dx = ψ(p) − ψ(q) 2 ,(4)where [ψ(p)](x) is given by 1 √ M p R λ1 (x), . . . , p R λ M (x), p I λ1 (x), . . . , p I λ M (x) , defining p R λj (x) = R(g λj (p(x))), p I λj (x) = I(g λj (p(x))). Hence, the HDD between densities p and q is approximately the L 2 distance from ψ(p) to ψ(q), where ψ maps a function f : [0, 1] → R to a vector-valued function ψ(f ) : [0, 1] → R 2M of λ functions. M can typically be quite small, since the kernel it approximates is one-dimensional.

Finite Embeddings of L 2
If densities p and q are smooth, then the L 2 metric between the p λ and q λ functions may be well approximated using projections to basis functions. Suppose that {ϕ i } i∈Z is an orthonormal basis for L 2 ([0, 1]); then we can construct an orthonormal basis for L 2 ([0, 1] ) by the tensor product:{ϕ α } α∈Z where ϕ α (x) = i=1 ϕ αi (x i ), x ∈ [0, 1] , ∀f ∈ L 2 ([0, 1] ), f (x) = α∈Z a α (f ) ϕ α (x)anda α (f ) = ϕ α , f = [0,1] ϕ α (t) f (t) dt ∈ R. Let V ⊂ Z be an appropriately chosen finite set of indices. If f, f ∈ L 2 ([0, 1] ) are smooth and a(f ) = (a α1 (f ), . . . , a α |V | (f )), then f −f 2 ≈ a(f )− a(f ) 2 .Thus we can approximate d 2 as the squared distance between finite vectors:d 2 (p, q) ≈ ψ(p) − ψ(q) 2 ≈ 1 M M j=1 a(p R λj ) − a(q R λj ) 2 + a(p I λj ) − a(q I λj ) 2 = A(p) − A(q) 2 (5) where A : L 2 ([0, 1] ) → R 2M |V | concatenates the a fea- tures for each λ function. That is, A(p) is given by 1 √ M a(p R λ1 ), . . . , a(p R λ M ), a(p I λ1 ), . . . , a(p I λ M ) . (6)We will discuss how to estimate a(p R λ ), a(p I λ ) shortly.

Embedding RBF Kernels into R D
The A features approximate the HDD (2) in R 2M |V | ; thus applying the RKS embedding  #b30  to the A features will approximate our generalized RBF kernel (1). The RKS embedding is 1 z :R m → R D such that for fixed {ω i } D/2 i=1 iid ∼ N (0, σ −2 I m ) and for each x, y ∈ R m : z(x) T z(y) ≈ exp − 1 2σ 2 x − y 2 , where z(x) = 2 D sin(ω T 1 x), cos(ω T 1 x), . . . .(7)Thus we can approximate the HDD kernel (1) as:K(p, q) = exp − 1 2σ 2 d 2 (p, q) ≈ exp − 1 2σ 2 A(p) − A(q) 2 ≈ z(A(p)) T z(A(q)).(8)

Finite Sample Estimates
Our final approximation for HDD kernels (8) depends on integrals of densities p and q. In practice, we are unlikely to directly observe an input density, but even given a pdf p, the integrals that make up the elements of A(p) are not readily computable. We thus first estimate the density asp, e.g. with kernel density estimation (KDE), and estimate A(p) as A(p).Recall that the elements of A(p) are:a α (p S λj ) = [0,1] ϕ α (t)p S λj (t) dt (9) where j ∈ {1, . . . , M }, S ∈ {R, I}, α ∈ V .In lower dimensions, we can approximate (9) with simple Monte Carlo numerical integration. Choosing{u i } ne i=1 iid ∼ Unif([0, 1] ): a α (p S λj ) = 1 n e ne i=1 ϕ α (u i )p S λj (u i ),(10)obtainingÂ(p). We note that in high dimensions, one may use any high-dimensional density estimation scheme (e.g. Lafferty, Liu, and Wasserman 2012) and estimate (9) with MCMC techniques (e.g. Hoffman and Gelman 2014).

Summary and Complexity
The algorithm for computing features{z(A(p i ))} N i=1 for a set of distributions {p i } N i=1 , given sample sets {χ i } N i=1 where χ i = {X (i) j ∈ [0, 1] } ni j=1 iid ∼ p i , is thus: 1. Draw M scalars λ j iid ∼ µ Z and D/2 vectors ω r iid ∼ N (0, σ −2 I 2M |V | ), in O(M |V | D) time.2. For each of the N input distributions i:(a) Compute a kernel density estimate from χ i ,p i (u j ) for each u j in (10), in O(n i n e ) time.(b) ComputeÂ(p i ) using a numerical integration estimate as in (10) 

Theory
We bound Pr K(p, q) − z(Â(p)) T z(Â(q)) ≥ ε for two fixed densities p and q by considering each source of error: kernel density estimation (ε KDE ); approximating µ(λ) with M samples (ε λ ); truncating the tails of the projection coefficients (ε tail ); Monte Carlo integration (ε int ); and the RKS embedding (ε RKS ).We need some smoothness assumptions on p and q: that they are members of a periodic Hölder class Σ per (β, L β ), that they are bounded below by ρ * and above by ρ * , and that their kernel density estimates are in Σ per (γ, L) with probability at least 1 − δ. We use a suitable form of kernel density estimation, to obtain a uniform error bound with a rate based on the function C −1  #b7 . We use the Fourier basis and choose V = {α ∈ Z | j=1 |α j | 2s ≤ t} for parameters 0 < s <γ, t > 0.Then, for any ε RKS + 1 σ k √ e (ε KDE + ε λ + ε tail + ε int ) ≤ ε, the probability of the error exceeding ε is at most:2 exp −Dε 2 RKS + 2 exp −M ε 4 λ /(8Z 2 ) + δ + 2C −1 ε 4 KDE n 2β/(2β+ ) 4 log n + 2M 1 − µ [0, u tail ) + 8M |V | exp   − 1 2 n e 1 + ε 2 int /(8 |V | Z) − 1 √ ρ * + 1 2   where u tail = max 0, ρ * t 8M L 2 4γ −4 s 4γ ε 2 tail − 1 4. The bound decreases when the function is smoother (larger β,γ; smaller L) or lower-dimensional ( ), or when we observe more samples (n). Using more projection coefficients (higher t or smaller s, giving higher |V |) improves the approximation but makes numerical integration more difficult. Likewise, taking more samples from µ (higher M ) improves that approximation, but increases the number of functions to be approximated and numerically integrated.For the proof and further details, see the appendix.

Numerical Experiments
Throughout these experiments we use M = 5, |V | = 10 (selected as rules of thumb; larger values did not improve performance), and use a validation set (10% of the training set) to choose bandwidths for KDE and the RBF kernel as well as model regularization parameters. Except in the scene classification experiments, the histogram methods used 10 bins per dimension; performance with other values was not better. The KL estimator used the fourth nearest neighbor. We evaluate RBF kernels based on various distances. First, we try our JS, Hellinger, and TV embeddings. We compare to L 2 kernels as in  #b27 : exp − 1 2σ 2 p − q 2 2 ≈ z( a(p)) T z( a(q)) (L2). We also try the MMD distance (Muandet et al. 2012) with approximate kernel embeddings: exp − 1 2σ 2 MMD(p, q) ≈ z (z(p)) T z (z(q)), wherez is the mean embeddingz(p) = 1 n n i=1 z(X i ) (MMD). We further compare to RKS with histogram JS embeddings (Vempati et al. 2010) (Hist JS); we also tried χ 2 embeddings, but their performance was quite similar. We finally try the full Gram matrix approach of  #b29  with the KL estimator of  #b40  in an RBF kernel (KL), as did  #b26 .

Gram Matrix Estimation
We first illustrate that our embedding, using the parameter selections as above, can approximate the Jensen-Shanon kernel well. We compare three different approaches to estimating K(p i , p j ) = exp(− 1 2σ 2 JS(p i , p j )). Each approach uses kernel density estimatesp i . The estimates are compared on a dataset of N = 50 random GMM distributions {p i } N i=1 and samples of size n = 2 500:χ i = {X (i) j ∈ [0, 1] 2 } n j=1iid ∼ p i . See the appendix for more details.The first approach approximates JS based on empirical estimates of entropies E logp i . The second approach estimates JS as the Euclidean distance of vectors of projection coefficients (5) : JS pc (p i , p j ) = Â (p i ) −Â(p j ) 2 . For these first two approaches we compute the pairwise kernel evaluations in the Gram matix as G ent ij = exp(− 1 2σ 2 JS ent (p i , p j )), and G pc ij = exp(− 1 2σ 2 JS pc (p i , p j )) using their respective approximations for JS. Lastly, we directly estimate the JS kernel with dot products of our random features (8): G rks ij = z(Â(p i )) T z(Â(p j )), with D = 7 000.  Figure 2 shows the N 2 true pairwise kernel values versus the aforementioned estimates. Quantitatively, the entropy method obtained a squared correlation to the true kernel value of R 2 ent = 0.981; using the A features with an exact kernel yielded R 2 pc = 0.974; adding RKS embeddings gave R 2 rks = 0.966. Thus our method's estimates are nearly as good as direct estimation via entropies, while allowing us to work in primal space and avoid N × N Gram matrices.

Estimating the Number of Mixture Components
We will now illustrate the efficacy of HDD random features in a regression task, following  #b27 : estimate the number of components from a mixture of truncated Gaussians. We generate the distributions as follows: Draw the number of components Y i for the ith distribution as Y i ∼ Unif{1, . . . , 10}. For each component select a mean µ(i) k ∼ Unif[−5, 5] 2 and covariance Σ (i) k = a (i) k A (i) k A (i)T k + B (i) k , where a ∼ Unif[1, 4], A (i) k (u, v) ∼ Unif[−1, 1], and B (i) k is a diagonal 2 × 2 matrix with B (i) k (u, u) ∼ Unif[0, 1].Then weight each component equally in the mixture. Given a sample χ i , we predict the number of components Y i . An example distribution and sample are shown in Figure 3; predicting the number of components is difficult even for humans.   Figure 4 presents results for predicting with ridge regression the number of mixture components Y i , given a varying number of sample sets χ i , with |χ i | ∈ {200, 800}; we use D = 5 000. The HDD-based kernels achieve substantially lower error than the L 2 and MMD kernels. They also outperform the histogram kernel, especially with |χ i | = 200, and the KL kernel. Note that fitting mixtures with EM and selecting a number of components using AIC  #b0  or BIC  #b31  performed much worse than regression; only AIC with |χ i | = 800 outperformed a constant predictor of 5.5. Linear versions of the L 2 and MMD kernels were also no better than the constant predictor.The HDD embeddings were more computationally expensive than the other embeddings, but much less expensive than the KL kernel, which grows at least quadratically in the number of distributions. Note that the histogram embeddings used an optimized C implementation  #b36 , as did the KL kernel 2 , while the HDD embeddings used a simple Matlab implementation.

Image Classification
As another example of the performance of our embeddings, we now attempt to classify images based on their distributions of pixel values. We took the "cat" and "dog" classes from the CIFAR-10 dataset  #b16 , and represented each 32 × 32 image by a set of triples (x, y, v), where x and y are the position of each pixel in the image and v the pixel value after converting to grayscale. The horizontal reflection of the image was also included, so each sample set χ i ⊂ R 3 had |χ i | = 2 048. This is certainly not the best representation for these images; rather, we wish to show that given this simple representation, our HDD kernels perform well relative to the other options.We used the same kernels as above in an SVM classifier from LIBLINEAR  #b4 , for the embeddings) or LIBSVM  #b2 , for the KL kernel), with D = 7 000. Figure 5 shows computation time and accuracy on the standard test set (of size 2K) with 2.5K, 5K, and 10K training images. Our JS and Hellinger embedding approximately match the histogram JS embedding in accuracy here, while our TV embedding beats histogram JS; all outperform L 2 and MMD. We could only run the KL kernel for the 2.5K training set size; its accuracy was comparable to the HDD and histogram embeddings, at far higher computational cost.

Scene Classification
Modern computer vision classification systems typically consist of a deep network with several convolutional and pooling layers to extract complex features of input images, followed by one or two fully-connected classification layers. The activations are of shape n × h × w, where n is the number of filters; each unit corresponds to an overlapping patch of the original image. We can thus treat the final pooled activations as a sample of size hw from an n-dimensional distribution, similarly to how  #b29  and  #b25  used SIFT features from image patches.  #b42  set accuracy records on several scene classification datasets with a particular ad-hoc method of extracting features from distributions (D3); we compare to our more principled alternatives.  We consider the Scene-15 dataset  #b18 , which contains 4 485 natural images in 15 location categories, and follow Wu, Gao, and Liu in extracting features from the last convolutional layer of the imagenet-vgg-verydeep-16 model  #b32 . We replace that layer's rectified linear activations with sigmoid squashing to [0, 1]. 3 hw ranges from 400 to 1 000. There are 512 filter dimensions; we concatenate featuresÂ(p i ) extracted from each independently.We train on the standard for this dataset of 100 images from each class (1500 total) and test on the remainder; Figure 6 shows results. We do not add any spatial information to the model; still, we match the best prior published performance of 91.59 ± 0.48, which trained on over 7 million external images  #b43 . Adding spatial information brought the D3 method slightly above 92% accuracy; their best hybrid method obtained 92.9%. Using these features, however, our methods match or beat MMD and substantially outperform D3, L 2 , and the histogram embeddings.

Discussion
This work presents the first nonlinear embedding of density functions for quickly computing HDD-based kernels, including kernels based on the popular total variation, Hellinger and Jensen-Shanon divergences. While such divergences have shown good empirical results in the comparison of densities, nonparametric uses of kernels with these divergences previously necessitated the computation of a large N × N Gram matrix, prohibiting their use in large datasets. Our embeddings allow one to work in a primal space while using information theoretic kernels. We analyze the approximation error of our embeddings, and illustrate their quality on several synthetic and real-world datasets. [0.05, 0.15] 2 ) and N t (m, s) is the distribution of a Gaussian truncated on [0, 1] 2 with mean parameter m and covariance matrix parameter s. We work over the sample setp i (x) = 1 5 5 j=1 N t (m ij , diag(s 2 ij )) where m ij iid ∼ Unif([0, 1] 2 ), s ij iid ∼ Unif({χ i } N i=1 , where χ i = {X (i) j ∈ [0, 1] 2 } n j=1iid ∼ p i , n = 2500, N = 50. We compare three different approaches to estimating K(p i , p j ) = exp(− 1 2σ 2 JS(p i , p j )). Each approach uses density estimatesp i , which are computed using kernel density estimation. The first approach is based on estimating JS using empirical estimates of entropies:JS(p i , p j ) = − 1 2 Ep i log 1 p i (x) − 1 2 Ep j log 1 p j (x) + E 1 2 pi+ 1 2 pj log 2 p i (x) + p j (x) ≈ − 1 2 n/2 m=1 log 1 p i (X (i) m ) − 1 2 n/2 m=1 log 1 p j (X (j) m ) + 1 2 n/2 m=1 log 2 p i (X (i) m ) +p j (X (i) m ) + 1 2 n/2 m=1 log 2 p i (X (j) m ) +p j (X (k) m ) = JS ent (p i , p j ),where density estimatesp i above are based on points {X (i) m } n m= n/2 +1 to avoid biasing the empirical means. The second approach estimates JS as the Euclidean distance of vectors of projection coefficients:JS(p i , p j ) ≈ Â (p i ) −Â(p j ) 2 = JSpc(p i , p j ),where here the density estimatesp i are based on the entire set of points χ i . We build a Gram matrix for each of these approaches by setting G ent ij = exp(− 1 2σ 2 JS ent (p i , p j )) and G pc ij = exp(− 1 2σ 2 JSpc(p i , p j )). Lastly, we directly estimate the JS kernel with random features:G rks ij = z(Â(p i )) T z(Â(p j )).We compare the effectiveness of each approach by computing the R 2 score of the estimates produced versus a true JS kernel value computed through numerically integrating the true densities (see Figure 7 and Table 2). The RBF values estimated with our random features produce estimates that are nearly as good as directly estimating JS divergences through entropies, whilst allowing us to work over a primal space and thus avoid computing a N × N Gram matrix for learning tasks. 

Proofs
We will now prove the bound on the error probability of our embedding Pr K(p, q) − z(Â(p)) T z(Â(q)) ≥ ε for fixed densities p and q.Setup We will need a few assumptions on the densities: 1. p and q are bounded above and below: forx ∈ [0, 1] , 0 < ρ * ≤ p(x), q(x) ≤ ρ * < ∞.2. p, q ∈ Σ(β, L β ) for some β, L β > 0. Σ(β, L) refers to the Hölder class of functions f whose partial derivatives up to order β are continuous and whose rth partial derivatives, where r is a multi-index of order β , satisfy|D r f (x) − D r f (y)| ≤ L x − y β .Here β is the greatest integer strictly less than β.3. p, q are periodic.These are fairly standard smoothness assumptions in the nonparametric estimation literature. Let γ = min(β, 1). If β > 1, then p, q ∈ Σ(1, Lγ ) for some Lγ ; otherwise, clearly p, q ∈ Σ(β, L β ). Then, from assumption 3, p, q ∈ Σper(γ, Lγ ), the periodic Hölder class. We'll need this to establish the Sobolev ellipsoid containing p and q.We will use kernel density estimation with a bounded, continuous kernel so that the bound of  #b7  applies, with bandwidth h n −1/(2β+ ) log n, and truncating density estimates to [ρ * , ρ * ].We also use the Fourier basis ϕα = exp 2iπα T x , and define V as the set of indices α s.t. j=1 |α j | 2s ≤ t for parameters 0 < s ≤ 1, t > 0 to be discussed later.Decomposition Let rσ(∆) = exp −∆ 2 /(2σ 2 ) . Then K(p, q) − z(Â(p)) T z(Â(q)) ≤ K(p, q) − rσ k Â (p) −Â(q) + rσ k Â (p) −Â(q) − z(Â(p)) T z(Â(q)) .The latter term was bounded by  #b30 . For the former, note that rσ is 1 σ √ e -Lipschitz, so the first term is at most 1 σ k √ e d(p, q) − Â (p) −Â(q) . Breaking this up with the triangle inequality:d(p, q) − Â (p) −Â(q) ≤ |d(p, q) − d(p,q)| + |d(p,q) − ψ(p) − ψ(q) | + | ψ(p) − ψ(q) − A(p) − A(q) | + A(p) − A(q) − Â (p) −Â(q) . (11)Estimation error Recall that d is a metric, so the reverse triangle inequality allows us to address the first term with |d(p, q) − d(p,q)| ≤ d(p,p) + d(q,q).For d 2 the total variation, squared Hellinger, or Jensen-Shannon HDDs, we have that d 2 (p,q) ≤ TV(p,p)  #b21 . Moreover, as the distributions are supported on [0,1] , TV(p,p) = 1 2 p −p 1 ≤ 1 2 p −p ∞ . It is a consequence of  #b7  that, for any δ > 0, Pr p −p ∞ > √ C δ log n n β/(2β+ ) < δ for some C δ depending on the kernel. Thus Pr (|d(p, q) − d(p,q)| ≥ ε) < 2C −1 ε 4 n 2β/(2β+ ) 4 log n , where C C −1 (x) = x. λ approximation The second term of (11), the approximation error due to sampling λs, admits a simple Hoeffding bound. Note that p R λ −q R λ 2 + p I λ −q I λ 2 , viewed as a random variable in λ only, has expectation d 2 (p,q) and is bounded by [0, 4Z] (where Z = R ≥0 dµ(λ)): write it as Z |p(x) 1 2 +iλ −q(x) 1 2 +iλ | 2 dx, expand the square, and use p(x)q(x)dx ≤ 1 (via Cauchy-Schwarz). For nonnegative random variables X and Y , Final bound Combining the bounds for the decomposition (11) with the pointwise rate for RKS features, we get:Pr K(p, q) − z(Â(p) T z(Â(q)) ≥ ε ≤ 2 exp −Dε 2 RKS + 2C −1 ε 4 KDE n 2β/(2β+ ) 4 log n + 2 exp −M ε 4 λ /(8Z 2 )+ δ + 2M 1 − µ 0, max 0, ρ * tε 2 tail 8M L 2 4γ − 4 s 4γ − 1 4 + 8M |V | exp   − 1 2 ne   1 + ε 2 int /(8 |V | Z) − 1 √ ρ * + 1   2   (14)for any ε RKS + 1 σ k √ e (ε KDE + ε λ + ε tail + ε int ) ≤ ε.

Footnote
1 : There are two versions of the embedding in common use, but this one is preferred.
2 : https://github.com/dougalsutherland/skl-groups/
3 : We used piecewise-linear weights before the sigmoid function such that 0 maps to 0.5, the 90th percentile of the positive observations maps to 0.9, and the 10th percentile of the negative observations to 0.1, for each filter.