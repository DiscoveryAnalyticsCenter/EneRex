Towards Instance Optimal Bounds for Best Arm Identification

Abstract
In the classical best arm identification (Best-1-Arm) problem, we are given n stochastic bandit arms, each associated with a reward distribution with an unknown mean. Upon each play of an arm, we can get a reward sampled i.i.d. from its reward distribution. We would like to identify the arm with the largest mean with probability at least 1 − δ, using as few samples as possible. The problem has a long history and understanding its sample complexity has attracted significant attention since the last decade. However, the optimal sample complexity of the problem is still unknown. Recently, Chen and Li (2016) made an interesting conjecture, called gap-entropy conjecture, concerning the instance optimal sample complexity of Best-1-Arm. Given a Best-1-Arm instance I (i.e., a set of arms), let µ [i] denote the ith largest mean and samples in expectation for any instance I. For the lower bound, we show that for any Gaussian Best-1-Arm instance with gaps of the form 2 −k , any δ-correct monotone algorithm requires at least Ω H(I) · ln δ −1 + Ent(I) samples in expectation. Here, a monotone algorithm is one which uses no more samples (in expectation) on I ′ than on I, if I ′ is a sub-instance of I obtained by removing some sub-optimal arms.


n i=2 ∆ −2[i] denotes the complexity of the instance. The gap-entropy conjecture states that for any instance I, Ω H(I) · ln δ −1 + Ent(I) is an instance lower bound, where Ent(I) is an entropy-like term determined by the gaps, and there is a δ-correct algorithm for Best-1-Arm with sample complexity O H(I) · ln δ −1 + Ent(I) + ∆ −2[2] ln ln ∆ −1 [2] . We note that Θ ∆ −2[2] ln ln ∆ −1 [2] is necessary and sufficient to solve the two-arm instance with the best and second best arms. If the conjecture is true, we would have a complete understanding of the instance-wise sample complexity of Best-1-Arm (up to constant factors).In this paper, we make significant progress towards a complete resolution of the gap-entropy conjecture. For the upper bound, we provide a highly nontrivial algorithm which requires

Introduction
The stochastic multi-armed bandit is one of the most popular and well-studied models for capturing the exploration-exploitation tradeoffs in many application domains. There is a huge body c 2017 L. Chen, J. Li & M. Qiao. of literature on numerous bandit models from several fields including stochastic control, statistics, operation research, machine learning and theoretical computer science. The basic stochastic multi-armed bandit model consists of n stochastic arms with unknown distributions. One can adaptively take samples from the arms and make decision depending on the objective. Popular objectives include maximizing the cumulative sum of rewards, or minimizing the cumulative regret (see e.g., Cesa-Bianchi and Lugosi (2006); Bubeck et al. (2012)).In this paper, we study another classical multi-armed bandit model, called pure exploration model, where the decision-maker first performs a pure-exploration phase by sampling from the arms, and then identifies an optimal (or nearly optimal) arm, which serves as the exploitation phase. The model is motivated by many application domains such as medical trials Robbins (1985); Audibert and Bubeck (2010) (2015); Carpentier and Locatelli (2016); Garivier and Kaufmann (2016). Now, we formally define the problem and set up some notations.Definition 1.1 Best-1-Arm: We are given a set of n arms {A 1 , . . . , A n }. Arm A i has a reward distribution D i with an unknown mean µ i ∈ [0, 1]. We assume that all reward distributions are Gaussian distributions with unit variance. Upon each play of A i , we get a reward sampled i.i.d. from D i . Our goal is to identify the arm with the largest mean using as few samples as possible. We assume here that the largest mean is strictly larger than the second largest (i.e., µ [1] > µ [2] ) to ensure the uniqueness of the solution, where µ [i] denotes the ith largest mean.Remark 1.2 Some previous algorithms for Best-1-Arm take a sequence (instead of a set) of n arms as input. In this case, we may simply assume that the algorithm randomly permutes the sequence at the beginning. Thus the algorithm will have the same behaviour on two different orderings of the same set of arms. Remark 1.3 For the upper bound, everything proved in this paper also holds if the distributions are 1-sub-Gaussian, which is a standard assumption in the bandit literature. On the lower bound side, we need to assume that the distributions are from some family parametrized by the means and satisfy certain properties. See Remark D.4. Otherwise, it is possible to distinguish two distributions using 1 sample even if their means are very close. We cannot hope for a nontrivial lower bound in such generality.The Best-1-Arm problem for Gaussian arms was first formulated in Bechhofer (1954). Most early works on Best-1-Arm did not analyze the sample complexity of the algorithms (they proved their algorithms are δ-correct though). The early advances are summarized in the monograph Bechhofer et al. (1968).For the past two decades, significant research efforts have been devoted to understanding the optimal sample complexity of the Best-1-Arm problem. On the lower bound side, Mannor and Tsitsiklis (2004) proved that any δ-correct algorithm for Best-1-Arm takes Ω( n i=2 ∆ −2 [i] ln δ −1 ) samples in expectation. In fact, their result is an instance-wise lower bound (see Definition 1.6). Kaufmann et al. (2015) also provided an Ω( n i=2 ∆ −2 [i] ln δ −1 ) lower bound for Best-1-Arm, which improved the constant factor in Mannor and Tsitsiklis (2004). Garivier and Kaufmann (2016) focused on the asymptotic sample complexity of Best-1-Arm as the confidence level δ approaches zero (treating the gaps as fixed), and obtained a complete resolution of this case (even for the leading constant). 1 Chen and Li (2015) showed that for each n there exists a Best-1-Arm instance with n arms that require Ω n i=2 ∆ −2[i] ln ln n samples, which further refines the lower bound. The algorithms for Best-1-Arm have also been significantly improved in the last two decades Even-Dar et al. by Chen and Li (2015). There is still a gap between the best known upper and lower bound.To understand the sample complexity of Best-1-Arm, it is important to study a special case, which we term as SIGN-ξ. The problem can be viewed as a special case of Best-1-Arm where there are only two arms, and we know the mean of one arm. SIGN-ξ will play a very important role in our lower bound proof.Definition 1.4 SIGN-ξ: ξ is a fixed constant. We are given a single arm with unknown mean µ = ξ. The goal is to decide whether µ > ξ or µ < ξ. Here, the gap of the problem is defined to be ∆ = |µ − ξ|. Again, we assume that the distribution of the arm is a Gaussian distribution with unit variance.In this paper, we are interested in algorithms (either for Best-1-Arm or for SIGN-ξ) that can identify the correct answer with probability at least 1 − δ. This is often called the fixed confidence setting in the bandit literature.Definition 1.5 For any δ ∈ (0, 1), we say that an algorithm A for  is δcorrect, if on any  instance, A returns the correct answer with probability at least 1 − δ.

Almost Instance-wise Optimality Conjecture
It is easy to see that no function f (n, δ) (only depending on n and δ) can serve as an upper bound of the sample complexity of Best-1-Arm (with n arms and confidence level 1 − δ). Instead, the sample complexity depends on the gaps. Intuitively, the smaller the gaps are, the harder the instance is (i.e., more samples are required). Since the gaps completely determine an instance (for Gaussian arms with unit variance, up to shifting), we use ∆ [i] 's as the parameters to measure the sample complexity. Now, we formally define the notion of instance-wise lower bounds and instance optimality.For algorithm A and instance I, we use T A (I) to denote the expected number of samples taken by A on instance I. Definition 1.6 (Instance-wise Lower Bound)For a Best-1-Arm instance I and a confidence level δ, we define the instance-wise lower bound of I as L(I, δ) := inf A:A is δ-correct for Best-1-ArmT A (I).We say a Best-1-Arm algorithm A is instance optimal, if it is δ-correct, and for every instance I, T A (I) = O(L(I, δ)). Now, we consider the Best-1-Arm problem from the perspective of instance optimality. Unfortunately, even for the two-arm case, no instance optimal algorithm may exist. In fact, Farrell (1964) showed that for any δ-correct algorithm A for SIGN-ξ, we must have lim inf ∆→0 T A (I) ∆ −2 ln ln ∆ −1 = Ω(1).This implies that any δ-correct algorithm requires ∆ −2 ln ln ∆ −1 samples in the worst case. Hence, the upper bound of ∆ −2 ln ln ∆ −1 for SIGN-ξ is generally not improvable. However, for a particular SIGN-ξ instance I ∆ with gap ∆, there is an δ-correct algorithm that only needs O(∆ −2 ln δ −1 ) samples for this instance, implying L(I ∆ , δ) = Θ(∆ −2 ln δ −1 ). See Chen and Li (2015) for details.Despite the above fact, Chen and Li (2016) conjectured that the two-arm case is the only obstruction toward an instance optimal algorithm. Moreover, based on some evidence from the previous work Chen and Li (2015), they provided an explicit formula and conjecture that L(I, δ) can be expressed by the formula. Interestingly, the formula involves an entropy term (similar entropy terms also appear in Afshani et al. (2009) for completely different problems). In order to state Chen and Li's conjecture formally, we define the entropy term first. Definition 1.7 Given a Best-1-Arm instance I and k ∈ N, letG k = {i ∈ [2, n] | 2 −(k+1) < ∆ [i] ≤ 2 −k }, H k = i∈G k ∆ −2 [i] , and p k = H k / j H j .We can view {p k } as a discrete probability distribution. We define the following quantity as the gap entropy of instance I: for any instance I and δ < 0.01. And we say such an algorithm is almost instance-wise optimal for Best-1-Arm. Moreover, L(I, δ) = Θ n i=2 ∆ −2 [i] · ln δ −1 + Ent(I) .Ent(I) = k∈N:G k =∅ p k ln p −1 k . 2Remark 1.10 As we mentioned before, the term ∆ −2 ln ln ∆ −1 is sufficient and necessary for distinguishing the best and the second best arm, even though it is not an instance-optimal bound. The gap entropy conjecture states that modulo this additive term, we can obtain an instance optimal algorithm. Hence, the resolution of the conjecture would provide a complete understanding of the sample complexity of  

Our Results
In this paper, we make significant progress toward the resolution of the gap-entropy conjecture. On the upper bound side, we provide an algorithm that almost matches the conjecture.Theorem 1.11 There is a δ-correct algorithm for Best-1-Arm with expected sample complexityO n i=2 ∆ −2 [i] · ln δ −1 + Ent(I) + ∆ −2 [2] ln ln ∆ −1 [2] · polylog(n, δ −1 ) .Our algorithm matches the main term n i=2 ∆ −2 [i] · ln δ −1 + Ent(I) in Conjecture 1.9. For the additive term (which is typically small), we lose a polylog(n, δ −1 ) factor. In particular, for those instances where the additive term is polylog(n, δ −1 ) times smaller than the main term, our algorithm is optimal.On the lower bound side, despite that we are not able to completely solve the lower bound, we do obtain a rather strong bound. We need to introduce some notations first. We say an instance is discrete, if the gaps of all the sub-optimal arms are of the form 2 −k for some positive integer k. We say an instance I ′ is a sub-instance of an instance I, if I ′ can be obtained by deleting some sub-optimal arms from I. Formally, we have the following theorem.Theorem 1.12 For any discrete instance I, confidence level δ < 0.01, and any δ-correct algorithm A for Best-1-Arm, there exists a sub-instance I ′ of I such thatT A (I ′ ) ≥ c · n i=2 ∆ −2 [i] · ln δ −1 + Ent(I) , where c is a universal constant.We say an algorithm is monotone, if T A (I ′ ) ≤ T A (I) for every I ′ and I such that I ′ is a subinstance of I. Then we immediately have the following corollary.Corollary 1.13 For any discrete instance I, and confidence level δ < 0.01, for any monotone δ-correct algorithm A for Best-1-Arm, we have thatT A (I) ≥ c · n i=2 ∆ −2 [i] · ln δ −1 + Ent(I) , where c is a universal constant.We remark that all previous algorithms for Best-1-Arm have monotone sample complexity bounds. The above corollary also implies that if an algorithm has a monotone sample complexity bound, then the bound must be Ω n i=2 ∆ −2[i] · ln δ −1 + Ent(I) on all discrete instances.

Related Work
SIGN-ξ and A/B testing. In the A/B testing problem, we are asked to decide which arm between the two given arms has the larger mean. A/B testing is in fact equivalent to the SIGN-ξ problem. It is easy to reduce SIGN-ξ to A/B testing by constructing a fictitious arm with mean ξ. For the other direction, given an instance of A/B testing, we may define an arm as the difference between the two given arms and the problem reduces to SIGN-ξ where ξ = 0. In particular, our refined lower bound for SIGN-ξ stated in Lemma 4.1 also holds for A/B testing. Kaufmann et al. (2015); Garivier and Kaufmann (2016) studied the limiting behavior of the sample complexity of A/B testing as the confidence level δ approaches to zero. In contrast, we focus on the case that both δ and the gap ∆ tend to zero, so that the complexity term due to not knowing the gap in advance will not be dominated by the ln δ −1 term.Best-k-Arm. The Best-k-Arm problem, in which we are required to identify the k arms with the k largest means, is a natural extension of Best-1-Arm. Best-k-Arm has been extensively studied in the past few years Kalyanakrishnan and Stone (2010) 2017), and most results for Best-k-Arm are generalizations of those for Best-1-Arm. As in the case of Best-1-Arm, the sample complexity bounds of Best-k-Arm depend on the gap parameters of the arms, yet the gap of an arm is typically defined as the distance from its mean to either µ [k+1] or µ [k] (depending on whether the arm is among the best k arms or not) in the context of Best-k-Arm problem. The Combinatorial Pure Exploration problem, which further generalizes the cardinality constraint in Best-k-Arm (i.e., to choose exactly k arms) to general combinatorial constraints, was also studied Chen et al. (2014,2016); Gabillon et al. (2016).PAC learning. The sample complexity of Best-1-Arm and Best-k-Arm in the probably approximately correct (PAC) setting has also been well studied in the past two decades. For Best-1-Arm, the tight worst-case sample complexity bound was obtained by Even-Dar et al. 

Preliminaries
Throughout the paper, I denotes an instance of Best-1-Arm (i.e., I is a set of arms). The arm with the largest mean in I is called the optimal arm, while all other arms are sub-optimal. We assume that every instance has a unique optimal arm. A i denotes the arm in I with the i-th largest mean, unless stated otherwise. The mean of an arm A is denoted by µ A , and we use µ [i] as a shorthand notation for µ A i (i.e., the i-th largest mean in an instance). Define ∆ A = µ [1] − µ A as the gap of arm A, and let ∆ [i] = ∆ A i denote the gap of arm A i . We assume that ∆ [2] > 0 to ensure the optimal arm is unique.We partition the sub-optimal arms into different groups based on their gaps. For each k ∈ N, group G k is defined asA i : ∆ [i] ∈ 2 −(k+1) , 2 −k . For brevity, let G ≥k and G ≤k denoted ∞ i=k G i and k i=1 G i respectively. The complexity of arm A i is defined as ∆ −2 [i], while the complexity of instance I is denoted by H(I) = n i=2 ∆ −2 [i] (or simply H, if the instance is clear from the context). Moreover, H k = A∈G k ∆ −2 A denotes the total complexity of the arms in group G k .(H k ) ∞ k=1 naturally defines a probability distribution on N, where the probability of k is given by p k = H k /H. The gap-entropy of the instance I is then denoted byEnt(I) = k p k ln p −1 k .Here and in the following, we adopt the convention that 0 ln 0 −1 = 0.

A Sketch of the Lower Bound


A Comparison with Previous Lower Bound Techniques
We briefly discuss the novelty of our new lower bound technique, and argue why the previous techniques are not sufficient to obtain our result. To obtain a lower bound on the sample complexity of Best-1-Arm, all the previous work Mannor and Tsitsiklis (2004);Chen et al. (2014);Kaufmann et al. (2015); Garivier and Kaufmann (2016) are based on creating two similar instances with different answers, and then applying the change of distribution method (originally developed in Kaufmann et al. (2015)) to argue that a certain number of samples are necessary to distinguish such two instances. The idea was further refined by Garivier and Kaufmann (2016). They formulated a max-min game between the algorithm and some instances (with different answers than the given instance) created by an adversary. The value of the game at equilibrium would be a lower bound of the samples one requires to distinguish the current instance and several worst adversary instances. However, we notice that even in the two-arm case, one cannot prove the Ω(∆ −2 ln ln ∆ −1 ) lower bound by considering only one max-min game to distinguish the current instance from other instance. Roughly speaking, the ln ln ∆ −1 factor is due to not knowing the actual gap ∆, and any lower bound that can bring out the ln ln ∆ −1 factor should reflect the union bound paid for the uncertainty of the instance. In fact, for the Best-1-Arm problem with n arms, the gap entropy Ent(I) term exists for a similar reason (not knowing the gaps). Hence, any lower bound proof for Best-1-Arm that can bring out the Ent(I) term necessarily has to consider the uncertainty of current instance as well (in fact, the random permutation of all arms is the kind of uncertainty we need for the new lower bound). In our actual lower bound proof, we first obtain a very tight understanding of the SIGN-ξ problem (Lemma 4.1). 3 Then, we provide an elegant reduction from SIGN-ξ to Best-1-Arm, by embedding the SIGN-ξ problem to a collection of Best-1-Arm instances.

Proof of Theorem 1.12
Following the approach in Chen and Li (2015), we establish the lower bound by a reduction from SIGN-ξ to discrete Best-1-Arm instances, together with a more refined lower bound for SIGNξ stated in the following lemma.Lemma 4.1 Suppose δ ∈ (0, 0.04), m ∈ N and A is a δ-correct algorithm for SIGN-ξ. P is a probability distribution on {2 −1 , 2 −2 , . . . , 2 −m } defined by P (2 −k ) = p k . Ent(P ) denotes the Shannon entropy of distribution P . Let T A (µ) denote the expected number of samples taken by A when it runs on an arm with distribution N (µ, 1) and ξ = 0. Define α k = T A (2 −k )/4 k . Then, m k=1 p k α k = Ω(Ent(P ) + ln δ −1 ). (1964) is not sufficient for our purpose.

Farrell's lower bound Farrell
It is well known that to distinguish the normal distribution N (2 −k , 1) from N (−2 −k , 1), Ω(4 k ) samples are required. Thus, α k = T A (2 −k )/4 k denotes the ratio between the expected number of samples taken by A and the corresponding lower bound, which measures the "loss" due to not knowing the gap in advance. Then Lemma 4.1 can be interpreted as follows: when the gap is drawn from a distribution P , the expected loss is lower bounded by the sum of the entropy of P and ln δ −1 . We defer the proof of Lemma 4.1 to Appendix D. Now we prove Theorem 1.12 by applying Lemma 4.1 and an elegant reduction from SIGN-ξ to Best-1-Arm. Proof [Proof of Theorem 1.12] Let c 0 be the hidden constant in the big-Ω in Lemma 4.1, i.e., m k=1 p k α k ≥ c 0 · (Ent(P ) + ln δ −1 ).We claim that Theorem 1.12 holds for constant c = 0.25c 0 .Suppose towards a contradiction that A is a δ-correct (for some δ < 0.01) algorithm for Best-1-Arm and I = {A 1 , A 2 , . . . , A n } is a discrete instance, while for all sub-instance I ′ of I,T A (I ′ ) < c · H(I)(Ent(I) + ln δ −1 ).Recall that H(I) and Ent(I) denote the complexity and entropy of instance I, respectively.Construct a distribution of SIGN-ξ instances. Let n k be the number of arms in I with gap 2 −k , and m be the greatest integer such that n m > 0. Since I is discrete, the complexity of instance I is given byH(I) = m k=1 4 k n k .Let p k = 4 k n k /H(I). Then (p k ) m k=1 defines a distribution P on {2 −1 , 2 −2 , . . . , 2 −m }. Moreover, the Shannon entropy of distribution P is exactly the entropy of instance I, i.e., Ent(P ) = Ent(I).Our goal is to construct an algorithm for SIGN-ξ that violates Lemma 4.1 on distribution P .A family of sub-instances of I. Let U = {k ∈ [m] : n k > 0} be the set of "types" of arms that are present in I. We consider the following family of instances obtained from I. For S ⊆ U , define I S as the instance obtained from I by removing exactly one arm of gap 2 −k for each k ∈ S. Note that I S is a sub-instance of I.Let S denote U \ S, the complement of set S relative to U . For S ⊆ U and k ∈ S, let τ S k denote the expected number of samples taken on all the n k arms with gap 2 −k when A runs on I S . Define α S k = 4 −k τ S k /n k . We note that 4 k α S k is the expected number of samples taken on every arm with gap 2 −k in instance I S . 4 We have the following inequality:S⊆U k∈S 4 k n k α S k = S⊆U k∈S τ S k ≤ S⊆U T A (I S ) < c · 2 |U | H(I)(Ent(I) + ln δ −1 ).(1)The second step holds because the lefthand side only counts part of the samples taken by A. The last step follows from our assumption and the fact that I S is a sub-instance of I.

4.
Recall that a Best-1-Arm algorithm is defined on a set of arms, so the arms with identical means in the instance cannot be distinguished by A. See Remark 1.2 for details.Construct algorithm A new from A. Now we define an algorithm A new for SIGN-ξ with ξ = 0.Given an arm A, we first choose a set S ⊆ U uniformly at random from all subsets of U . Recall that µ [1] denotes the mean of the optimal arm in I. A new runs the following four algorithms A 1 through A 4 in parallel:1. Algorithm A 1 simulates A on I S ∪ {µ [1] + A}. 2. Algorithm A 2 simulates A on I S ∪ {µ [1] + A}. 3. Algorithm A 3 simulates A on I S ∪ {µ [1] − A}. 4. Algorithm A 4 simulates A on I S ∪ {µ [1] − A}.More precisely, when one of the four algorithms requires a new sample fromµ [1] + A (or µ [1] − A),we draw a sample x from arm A, feed µ [1] + x to A 1 and A 2 , and then feed µ [1] − x to A 3 and A 4 . Note that the samples taken by the four algorithms are the same up to negation and shifting.A new terminates as soon as one of the four algorithms terminates. If one of A 1 and A 2 identifies µ [1] + A as the optimal arm, or one of A 3 and A 4 identifies an arm other than µ [1] − A as the optimal arm, A new outputs "µ A > 0"; otherwise it outputs "µ A < 0".Clearly, A new is correct if all of A 1 through A 4 are correct, which happens with probability at least 1 − 4δ. Note that since 4δ < 0.04, the condition of Lemma 4.1 is satisfied.Upper bound the sample complexity of A new . The crucial observation is that when µ A = −2 −k and k ∈ S, A 1 effectively simulates the execution of A on I S\{k} . In fact, since all arms are Gaussian distributions with unit variance, the arm µ [1] + A is the same as an arm with gap 2 −k in the original Best-1-Arm instance. Recall that the number of samples taken on each of the arms with gap 2 −k in instance I S\{k} is 4 k α S\{k} k . Therefore, the expected number of samples taken on A is upper bounded by 4 k α S\{k} k . 5 Likewise, when µ A = −2 −k and k ∈ S, A 2 is equivalent to the execution of A on I S\{k} , and thus the expected number of samples on A is less than or equal to 4 k α S\{k} k . Analogous claims hold for the case µ A = +2 −k and algorithms A 3 and A 4 as well. It remains to compute the expected loss of A new on distribution P and derive a contradiction to Lemma 4.1. It follows from a simple calculation thatm k=1 p k α k ≤ k∈U p k · 1 2 |U |   S⊆U :k∈S α S\{k} k + S⊆U :k∈S α S\{k} k   = 1 2 |U |−1 k∈U S⊆U :k∈S p k α S\{k} k = 1 2 |U |−1 S⊆U k∈S 4 k n k H(I) · α S k ≤ 2 |U | 2 |U |−1 · c · (Ent(I) + ln δ −1 ) < c 0 (Ent(P ) + ln(4δ) −1 ).5. Recall that if A1 terminates after taking T samples from µ [1] + A, the number of samples taken by A new on A is also T (rather than 4T ).The first step follows from our discussion on algorithm A new . The third step renames the variables and rearranges the summation. The last line applies (1). This leads to a contradiction to Lemma 4.1 and thus finishes the proof.

Warmup: Best-1-Arm with Known Complexity
To illustrate the idea of our algorithm for Best-1-Arm, we consider the following simplified yet still non-trivial version of Best-1-Arm: the complexity of the instance,H(I) = n i=2 ∆ −2 [i], is given, yet the means of the arms are still unknown.

Building Blocks
We introduce some subroutines that are used throughout our algorithm.Uniform sampling. The first building block is a uniform sampling procedure, Unif-Sampl(S, ε, δ), which takes 2ε −2 ln(2/δ) samples from each arm in set S. Letμ A be the empirical mean of arm A (i.e., the average of all sampled values from A). It obtains an ε-approximation of the mean of each arm with probability 1 − δ. The following fact directly follows by the Chernoff bound.Fact 5.1 Unif-Sampl(S, ε, δ) takes O(|S|ε −2 ln δ −1 ) samples. For each arm A ∈ S, we have Pr [|μ A − µ A | ≤ ε] ≥ 1 − δ.We say that a call to procedure Unif-Sampl(S, ε, δ) returns correctly, if |μ A − µ A | ≤ ε holds for every arm A ∈ S. Fact 5.1 implies that when |S| = 1, the probability of returning correctly is at least 1 − δ.Median elimination. Even-Dar et al. (2002) introduced the Median Elimination algorithm for the PAC version of Best-1-Arm. Med-Elim(S, ε, δ) returns an arm in S with mean at most ε away from the largest mean. Let µ [1] (S) denote the largest mean among all arms in S. The performance guarantees of Med-Elim is formally stated in the next fact.Fact 5.2 Med-Elim(S, ε, δ) takes O(|S|ε −2 ln δ −1 ) samples. Let A be the arm returned by Med-Elim.

Then
Pr[µ A ≥ µ [1] (S) − ε] ≥ 1 − δ.We say that Med-Elim(S, ε, δ) returns correctly, if it holds thatµ A ≥ µ [1] (S) − ε.Fraction test. Procedure Frac-Test(S, c low , c high , θ low , θ high , δ) decides whether a sufficiently large fraction (compared to thresholds θ low and θ high ) of arms in S have small means (compared to thresholds c low and c high ). The procedure randomly samples a certain number of arms from S and estimates their means using Unif-Sampl. Then it compares the fraction of arms with small means to the thresholds and returns an answer accordingly. The detailed implementation of Frac-Test is relegated to Appendix A, where we also prove the following fact.Fact 5.3 Frac-Test(S, c low , c high , θ low , θ high , δ) takes O (ε −2 ln δ −1 ) · (∆ −2 ln ∆ −1 ) samples, where ε = c high − c low and ∆ = θ high − θ low . With probability 1 − δ, the following two claims hold simultaneously:• If Frac-Test returns True, |{A ∈ S : µ A < c high }| > θ low |S|.• If Frac-Test returns False, |{A ∈ S : µ A < c low }| < θ high |S|.We say that a call to procedure Frac-Test returns correctly, if both the two claims above hold; otherwise the call fails.Elimination. Finally, procedure Elimination(S, d low , d high , δ) eliminates the arms with means smaller than threshold d low from S. More precisely, the procedure guarantees that at most a 0.1 fraction of arms in the result have means smaller than d low . On the other hand, for each arm with mean greater than d high , with high probability it is not eliminated. We postpone the pseudocode of procedure Elimination and the proof of the following fact to Appendix A.Fact 5.4 Elimination(S, d low , d high , δ) takes O(|S|ε −2 ln δ −1 ) samples in expectation, where ε = d high − d low . Let S ′ denote the set returned by Elimination(S, d low , d high , δ). Then with probability at least 1 − δ/2, |{A ∈ S ′ : µ A < d low }| ≤ 0.1|S ′ |.

Moreover, for each arm
A ∈ S with µ A ≥ d high , we have Pr[A ∈ S ′ ] ≥ 1 − δ/2. Proof Let ε = d high − d low .To bound the number of samples taken by Elimination, we note that the number of samples taken in the r-th iteration is dominated by that taken by Unif-Sampl, O(|S r |ε −2 ln δ −1 r ). It suffices to show that |S r | decays exponentially (in expectation); a direct summation over all r proves the sample complexity bound.We fix a particular round r. Suppose Frac-Test returns correctly (which happens with probability at least 1 − δ r ) and the algorithm does not terminate at round r. Then by Fact 5.3, it holds that|{A ∈ S r : µ A < d mid }| > 0.05|S r |.For each A ∈ S r with µ A < d mid , it holds with probability 1 − δ r thatµ A < µ A + (d high − d mid )/2 < d mid + (d high − d mid )/2 = (d mid + d high )/2.Note that δ r = δ/(10 · 2 r ) ≤ 0.1. Thus, at most a 0.1 fraction of arms in {A ∈ S r : µ A < d mid } would remain in S r+1 in expectation. It follows that conditioning on the correctness of Frac-Test at round r, the expectation of |S r+1 | is upper bounded by0.05|S r | · δ r + 0.95|S r | ≤ 0.05|S r |/10 + 0.95|S r | = 0.955|S r |.Moreover, even if Frac-Test returns incorrectly, which happens with probability at most 0.1, we still have |S r+1 | ≤ |S r |. Therefore,E[|S r+1 |] ≤ 0.9 · 0.955E[|S r |] + 0.1E[|S r |] < 0.96E[|S r |]. A simple induction yields E[|S r |] ≤ 0.96 r−1 |S|. Then the sample complexity of Elimination is upper bounded by ∞ r=1 E[|S r |]ε −2 ln δ −1 r = O |S|ε −2 ∞ r=1 0.96 r−1 (ln δ −1 + r) = O |S|ε −2 ln δ −1 .Then we proceed to the proof of the second claim. Let E denote the event that all calls to procedure Frac-Test returns correctly. By Fact 5.3 and a union bound,Pr [E A ] ≥ 1 − ∞ r=1 δ r ≥ 1 − δ/2.Conditioning on event E, if the algorithm terminates and returns S r at round r, Fact 5.3 implies that|{A ∈ S r : µ A < d low }| < 0.1|S r |.This proves the second claim.Finally, fix an arm A ∈ S with µ A > d high . Define E A as the event that every call to Frac-Test returns correctly in the algorithm, and |μ A − µ A | < (d high − d mid )/2 in every round. By Facts 5.1 and 5.3,Pr [E A ] ≥ 1 − ∞ r=1 2δ r ≥ 1 − δ/2.Then in each round r, it holds conditioning on E A thatµ A ≥ µ A − (d high − d mid )/2 > d high − (d high − d mid )/2 = (d mid + d high )/2.Thus, with probability 1 − δ/2, A is never removed from S r .

Algorithm
Now we present our algorithm for the special case that the complexity of the instance is known in advance. The Known-Complexity algorithm takes as its input a Best-1-Arm instance I, the complexity H of the instance, as well as a confidence level δ. The algorithm proceeds in rounds, and maintains a sequence {S r } of arm sets, each of which denotes the set of arms that are still considered as candidate answers at the beginning of round r.Roughly speaking, the algorithm eliminates the arms with Ω(ε r ) gaps at the r-th round, if they constitute a large fraction of the remaining arms. Here ε r = 2 −r is the accuracy parameter that we use in round r. To this end, Known-Complexity first calls procedures Med-Elim and Unif-Sampl to obtainμ ar , which is an estimation of the largest mean among all arms in S r up to an O(ε r ) error. After that, Frac-Test is called to determine whether a large proportion of arms in S r have Ω(ε r ) gaps. If so, Frac-Test returns True, and then Known-Complexity calls the Elimination procedure with carefully chosen parameters to remove suboptimal arms from S r .The following two lemmas imply that there is a δ-correct algorithm for Best-1-Arm that matches the instance-wise lower bound up to an O ∆ −2[2] ln ln ∆ −1 [2] additive term. 6 6. Lemma 5.6 only bounds the number of samples conditioning on an event that happens with probability 1 − δ, so the algorithm may take arbitrarily many samples when the event does not occur. However, Known-Complexity can be transformed to a δ-correct algorithm with the same (unconditional) sample complexity bound, using the "parallel simulation" technique in the proof of Theorem 1.11 in Appendix C.Algorithm 1: Known-Complexity(I, H, δ) Input: Instance I with complexity H and risk δ. Output: The best arm.S 1 ← I;Ĥ ← 4096H; for r = 1 to ∞ do if |S r | = 1 then return the only arm in S r ; ; ε r ← 2 −r ; δ r ← δ/(10r 2 ); a r ← Med-Elim(S r , 0.125ε r , 0.01); µ ar ← Unif-Sampl({a r }, 0.125ε r , δ r ); if Frac-Test(S r ,μ ar − 1.75ε r ,μ ar − 1.125ε r , 0.3, 0.5, δ r ) then δ ′ r ← |S r |ε −2 r /Ĥ δ; S r+1 ← Elimination(S r ,μ ar − 0.75ε r ,μ ar − 0.625ε r , δ ′ r ); else S r+1 ← S r ; endLemma 5.5 For any Best-1-Arm instance I and δ ∈ (0, 0.01), Known-Complexity(I, H(I), δ) returns the optimal arm in I with probability at least 1 − δ.Lemma 5.6 For any Best-1-Arm instance I and δ ∈ (0, 0.01), conditioning on an event that happens with probability 1 − δ, Known-Complexity(I, H(I), δ) takesO H(I) · (ln δ −1 + Ent(I)) + ∆ −2 [2] ln ln ∆ −1 [2]samples in expectation.

Observations
We state a few key observations on Known-Complexity, which will be used throughout the analysis. The proofs are exactly identical to those of Observations A.3 through A.5 in Appendix A. The following observation bounds the value ofμ ar at round r, assuming the correctness of Unif-Sampl and Med-Elim.Observation 5.7 If Unif-Sampl returns correctly at round r,μ ar ≤ µ [1] (S r ) + 0.125ε r . Here µ [1] (S r ) denotes the largest mean of arms in S r . If both Unif-Sampl and Med-Elim return correctly,µ ar ≥ µ [1] (S r ) − 0.25ε r .The following two observations bound the thresholds used in Frac-Test and Elimination by applying Observation 5.7.Observation 5.8 At round r, let c low r =μ ar − 1.75ε r and c high r =μ ar − 1.125ε r denote the two thresholds used in Frac-Test. If Unif-Sampl returns correctly, c highr ≤ µ [1] (S r ) − ε r . If both Med-Elim and Unif-Sampl return correctly, c low r ≥ µ [1] (S r ) − 2ε r .Observation 5.9 Let d low r =μ ar − 0.75ε r and d high r =μ ar − 0.625ε r denote the two thresholds used in Elimination. If Unif-Sampl returns correctly, d highr ≤ µ [1] (S r ) − 0.5ε r . If both Med-Elim and Unif-Sampl return correctly, d low r ≥ µ [1] (S r ) − ε r .

Correctness
We define E as the event that all calls to procedures Unif-Sampl, Frac-Test, and Elimination return correctly. We will prove in the following that Known-Complexity returns the correct answer with probability 1 conditioning on E, and Pr [E] ≥ 1 − δ. Note that Lemma 5.5 directly follows from these two claims.Event E implies correctness. It suffices to show that conditioning on E, Known-Complexity never removes the best arm, and the algorithm eventually terminates. Suppose that A 1 ∈ S r . Observation 5.9 guarantees that at round r, the upper threshold used by Elimination is smaller than or equal to µ [1] (S r ) − 0.5ε r < µ [1] . By Fact 5.4, the correctness of Elimination guarantees thatA 1 ∈ S r+1 .It remains to prove that Known-Complexity terminates conditioning on E. Define r max := max Gr =∅ r. Suppose r * is the smallest integer greater than r max such that Med-Elim returns correctly at round r * . 7 By Observation 5.9, the lower threshold in Elimination is greater than or equal to µ [1] − ε r * . The correctness of Elimination implies that|S r * +1 |−1 = |S r * +1 ∩G ≤rmax | ≤ |S r * +1 ∩G <r * | = |{A ∈ S r * +1 : µ A < µ [1] −ε r * }| < 0.1|S r * +1 |.It follows that |S r * +1 | = 1. Therefore, the algorithm terminates either before or at round r * + 1.E happens with high probability. We first note that at round r, the probability that either Unif-Sampl or Frac-Test fails (i.e., returns incorrectly) is at most 2δ r . By a union bound, the probability that at least one call to Unif-Sampl or Frac-Test returns incorrectly is upper bounded by∞ r=1 2δ r = ∞ r=1 δ 5r 2 < δ/2.It remains to bound the probability that Elimination fails at some round, yet procedures Unif-Sampl and Frac-Test are always correct. Define P (r, S r ) as the probability that, given the value of S r at the beginning of round r, at least one call to Elimination returns incorrectly in round r or later, yet Unif-Sampl and Frac-Test always return correctly. We prove by induction that for any S r that contains the optimal arm A 1 ,P (r, S r ) ≤ δ H 128C(r, S r ) + 16M (r, S r )ε −2 r ,(2)where M (r, S r ) := |S r ∩ G ≤r−2 | andC(r, S r ) := ∞ i=r−1 |S r ∩ G i | i+1 j=r ε −2 j + rmax+1 i=r ε −2 i .The details of the induction are postponed to Appendix E.7. Med-Elim returns correctly with probability at least 0.99 in each round, so r * is well-defined with probability 1.Observe that M (1, I) = 0 andC(1, I) = ∞ i=0 |S r ∩ G i | i+1 j=1 4 j + rmax+1 i=1 4 i ≤ 16 3 ∞ i=0 |S r ∩ G i |4 i + 4 rmax ≤ 16 3   ∞ i=0 A∈Sr∩G i ∆ −2 A + ∆ −2 [2]   ≤ 32 3 H(I).Therefore we conclude thatPr [E] ≥ 1 − P (1, S 1 ) − δ 2 ≥ 1 − δ H 128C(1, I) + 16M (1, I)ε −2 1 − δ 2 ≥ 1 − 128 · δ 4096H · 32H 3 − δ 2 ≥ 1 − δ,which completes the proof of correctness. Here the first step applies a union bound. The second step follows from inequality (2), and the third step plugs in C(1, I) ≤ 32H(I)/3 andĤ = 4096H.

Sample Complexity
As in the proof of Lemma 5.5, we define E as the event that all calls to procedures Unif-Sampl, Frac-Test, and Elimination return correctly. We prove that Known-Complexity takesO H(I)(ln δ −1 + Ent(I)) + ∆ −2 [2] ln ln ∆ −1 [2]samples in expectation conditioning on E.Samples taken by Unif-Sampl and Frac-Test. By Facts 5.1 and 5.3, procedures Unif-Sampl and Frac-Test take O ε −2 r ln δ −1 r = O ε −2 r (ln δ −1 + ln r) samples in total at round r. In the proof of correctness, we showed that conditioning on E, the algorithm does not terminate before or at round k (for k ≥ r max + 1) implies that Med-Elim fails between round r max + 1 and round k − 1, which happens with probability at most 0.01 k−rmax−1 . Thus for k ≥ r max + 1, the expected number of samples taken by Unif-Sampl and Frac-Test at round k is upper bounded byO 0.01 k−rmax−1 · ε −2 k (ln δ −1 + ln k) .Summing over all k = 1, 2, . . . yields the following upper bound:rmax k=1 ε −2 k (ln δ −1 + ln k) + ∞ k=rmax+1 0.01 k−rmax−1 · ε −2 k (ln δ −1 + ln k) =O 4 rmax (ln δ −1 + ln r max ) = O ∆ −2 [2] ln δ −1 + ln ln ∆ −1 [2].Here the first step holds since the first summation is dominated by the last term (k = r max ), while the second one is dominated by the first term (k = r max + 1). The second step follows from the observation that r max = max Gr =∅ r = log 2 ∆ −1 [2] .

Samples taken by Med-Elim and Elimination. By Facts 5.2 and 5.4, Med-Elim and Elimination
(if called) take O(|S r |ε −2 r ) + O(|S r |ε −2 r ln(1/δ ′ r )) = O |S r |ε −2 r ln δ −1 + ln H |S r |ε −2 r samples in total at round r.We upper bound the number of samples by a charging argument. For each round i, define r i as the largest integer r such that |G ≥r | ≥ 0.5|S i |. 8 Then we defineT i,j =      0, j < r i , ε −2 i ln δ −1 + ln H |G j |ε −2 i , j ≥ r ias the number of samples that each arm in G j is charged at round i.We prove in Appendix E that for any i, j |G j |T i,j is an upper bound on the number of samples taken by Med-Elim and Elimination at the i-th round. Moreover, the expected number of samples that each arm in group G j is charged is upper bounded byi E[T i,j ] = O ε −2 j ln δ −1 + ln H |G j |ε −2 j . Note that H k = A∈G k ∆ −2 A = Θ(|G k |ε −2 k ). Therefore, Med-Elim and Elimination take O   i,j |G j |E[T i,j ]   = O   j |G j |ε −2 j ln δ −1 + ln H |G j |ε −2 j   = O   j H j ln δ −1 + ln H H j   = O H(I) ln δ −1 + Ent(I)samples in expectation conditioning on E.In total, algorithm Known-Complexity takesO ∆ −2 [2] ln δ −1 + ln ln ∆ −1 [2] + O H(I) ln δ −1 + Ent(I) =O H(I) ln δ −1 + Ent(I) + ∆ −2 [2] ln ln ∆ −1 [2]samples in expectation conditioning on E. This proves Lemma 5.6.

Discussion
In the Known-Complexity algorithm, knowing the complexity H in advance is crucial to the efficient allocation of confidence levels (δ ′ r 's) to different calls of Elimination. When H is unknown, 8. Note that |G ≥0 | = n − 1 ≥ 0.5|Si| and |G ≥r | = 0 < 0.5|Si| for sufficiently large r, so ri is well-defined.our approach is to run an elimination procedure similar to Known-Complexity with a guess of H. The major difficulty is that when our guess is much smaller than the actual complexity, the total confidence that we allocate will eventually exceed the total confidence δ. Thus, we cannot assume in our analysis that all calls to the Elimination procedure are correct. We present our Complexity-Guessing algorithm for the Best-1-Arm problem in Appendix A.

References
Peyman Afshani, Jérémy Barbay, and Timothy M Chan. Instance-optimal geometric algorithms. 

Organization of the Appendix
The appendix contains the proofs of our main results. In Section A, we present our algorithm for Best-1-Arm along with a few useful observations. In Section B and Section C, we prove the correctness and the sample complexity of our algorithm, thus proving Theorem 1.11. We present the complete proof of Theorem 1.12 in Section D. Finally, Section E contains the complete proofs of Lemma 5.5 and Lemma 5.6.

Appendix A. Upper Bound


A.1. Building Blocks
We start by presenting the missing implementation and performance guarantees of our subroutines Frac-Test and Elimination.Fraction test. Recall that on input (S, c low , c high , θ low , θ high , δ), procedure Frac-Test decides whether a sufficiently large fraction (with respect to θ low and θ high ) of arms in S have means smaller than the thresholds c low and c high . The pseudocode of Frac-Test is shown below.Algorithm 2: Frac-Test(S, c low , c high , θ low , θ high , δ)Input: An arm set S, thresholds c low , c high , θ low , θ high , and confidence level δ.ε ← c high − c low ; ∆ ← θ high − θ low ; m ← (∆/6) −2 ln(2/δ); cnt ← 0; for i = 1 to m do Pick A ∈ S uniformly at random; µ A ← Unif-Sampl({A}, ε/2, ∆/6); ifμ A < (c low + c high )/2 then cnt ← cnt + 1; end if cnt/m > (θ low + θ high )/2 then return True; elsereturn False; Now we prove Fact 5.3.Fact 5.3 (restated) Frac-Test(S, c low , c high , θ low , θ high , δ) takes O((ε −2 ln δ −1 ) · (∆ −2 ln ∆ −1 )) samples, where ε = c high − c low and ∆ = θ high − θ low .With probability 1 − δ, the following two claims hold simultaneously:• If Frac-Test returns True, |{A ∈ S : µ A < c high }| > θ low |S|. • If Frac-Test returns False, |{A ∈ S : µ A < c low }| < θ high |S|.Proof The first claim directly follows from Fact 5.1 andm · O(ε −2 ln ∆ −1 ) = O((ε −2 ln δ −1 ) · (∆ −2 ln ∆ −1 )).It remains to prove the contrapositive of the second claim: |{A ∈ S : µ A < c low }| ≥ θ high |S| implies Frac-Test returns True, and |{A ∈ S : µ A < c high }| ≤ θ low |S| implies Frac-Test returns False.Suppose |{A ∈ S : µ A < c low }| ≥ θ high |S|. Then in each iteration of the for-loop, it holds that µ A < c low with probability at least θ high . Conditioning on µ A < c low , by Fact 5.1 we havêµ A ≤ µ A + ε/2 < c low + ε/2 = (c low + c high )/2with probability at least 1 − ∆/6. Thus, the expected increment of counter cnt is lower bounded byθ high (1 − ∆/6) ≥ θ high − ∆/6.Thus, cnt/m is the mean of m i.i.d. Bernoulli random variables with means greater than or equal to θ high − ∆/6. By the Chernoff bound, it holds with probability 1 − δ/2 thatcnt/m ≥ θ high − ∆/6 − ∆/6 > (θ low + θ high )/2.An analogous argument proves cnt/m < (θ low + θ high )/2 with probability 1 − δ/2, given |{A ∈ S : µ A < c high }| ≤ θ low |S|. This completes the proof.Elimination. We implement procedure Elimination by repeatedly calling Frac-Test to determine whether a large fraction of the remaining arms have means smaller than the thresholds. If so, we uniformly sample the arms, and eliminate those with low empirical means.Algorithm 3: Elimination(S, d low , d high , δ)Input: An arm set S, thresholds d low , d high , and confidence level δ. Output: Arm set after the elimination.S 1 ← S; d mid ← (d low + d high )/2; for r = 1 to +∞ do δ r ← δ/(10 · 2 r ); if Frac-Test(S r , d low , d mid , 0.05, 0.1, δ r ) then µ ← Unif-Sampl(S r , (d high − d mid )/2, δ r ); S r+1 ← A ∈ S r :μ A > (d mid + d high )/2 ; else return S r ; endWe prove Fact 5.4 in the following.Fact 5.4 (restated) Elimination(S, d low , d high , δ) takes O(|S|ε −2 ln δ −1 ) samples in expectation, where ε = d high − d low . Let S ′ be the set returned by Elimination(S, d low , d high , δ). Then we have Pr[|{A ∈ S ′ : µ A < d low }| ≤ 0.1|S ′ |] ≥ 1 − δ/2.

A.2. Overview
As shown in Section 5, we can solve Best-1-Arm usingO H · (Ent + ln δ −1 ) + ∆ −2 [2] ln ln ∆ −1 [2]samples, if we know in advance the complexity of the instance, i.e., H = n i=2 ∆ −2 [i] . The value of H is essential for allocating appropriate confidence levels to different calls of Elimination and achieving the near-optimal sample complexity. When H is unknown, our strategy is to guess its value. The major difficulty with our approach is that when our guess,Ĥ, is much smaller than the actual complexity H, the total confidence that we allocate will exceed the total confidence δ. To prevent this from happening, we maintain the total confidence that we have allocated so far, and terminate the algorithm as soon as the sum exceeds δ. 9 After that, we try a guess that is a hundred times larger. As we will see later, the most challenging part of the analysis is to ensure that our algorithm does not return an incorrect answer whenĤ is too small.We also keep track of the number of samples that have been taken so far. Roughly speaking, when the number exceeds 100Ĥ , we also terminate the algorithm and try the next guess ofĤ. This simplifies the analysis by ensuring that the number of samples we take for each guess grows exponentially, and thus it suffices to bound the number of samples taken on the last guess.

A.3. Algorithm
Algorithm Entropy-Elimination takes an instance of Best-1-Arm, a confidence δ and a guess of complexityĤ t = 100 t . It either returns an optimal arm (i.e., "accept"Ĥ t ) or reports an error indicating that the givenĤ t is much smaller than the actual complexity (i.e., "reject"Ĥ t ).Throughout the algorithm, we maintain S r , H r and T r for each round r. S r denotes the collection of arms that are still under consideration at the beginning of round r. We say that an arm is removed (or eliminated) at round r, if it is in S r \ S r+1 . Roughly speaking, H r is an estimate of the total complexity of arms in group G 1 , G 2 , . . . , G r . When this quantity exceeds our guessĤ t , Entropy-Elimination directly rejects (i.e., returns an error). T r is an upper bound on the number of samples taken by Med-Elim and Elimination 10 before round r. As mentioned before, we also terminate the algorithm when T r exceeds 100Ĥ t . Intuitively, this prevents Entropy-Elimination from taking too many samples on small guesses of H, which gives rise to an inferior sample complexity.In each round of Entropy-Elimination, we first call Med-Elim to obtain a near-optimal arm a r . Then we use Unif-Sampl to estimate the mean of a r , denoted byμ ar . After that, we call Frac-Test with appropriate parameters to find out whether a considerable fraction of arms in S r have gaps larger than ε r . If so, we call procedure Elimination and update the value of H r+1 accordingly. Note that we set the thresholds {θ r } of Frac-Test such that the intervals [θ r−1 , θ r ] are disjoint. In particular, this property is essential for proving Lemma B.6 in the analysis of the correctness of the algorithm.Our algorithm for Best-1-Arm guesses the complexity of the instance and invokes Entropy-Elimination to check whether the guess is reasonable. If Entropy-Elimination reports an error, 9. For ease of analysis, we actually use δ 2 instead of δ in the algorithm. 10. As we will see later, the analysis of the sample complexity of Med-Elim and Elimination are different from the other two procedures.Algorithm 4: Entropy-Elimination(I, δ,Ĥ t )Input: Instance I, confidence δ and a guess of complexityĤ t = 100 t . Output: The best arm, or an error indicating the guess is wrong.S 1 ← I; H 1 ← 0; T 1 ← 0; θ 0 ← 0.3; c ← log 4 100; for r = 1 to ∞ do if |S r | = 1 then return the only arm in S r ; ε r ← 2 −r ; δ r ← δ/(50r 2 t 2 ); δ ′ r ← (4|S r |ε −2 r /Ĥ)δ 2 ; T r+1 ← T r + |S r |ε −2 r ln |S r |ε −2 r δ/Ĥ t −1 ;if (H r + 4|S r |ε −2 r ≥Ĥ t ) or (T r+1 ≥ 100Ĥ t ) then return error; a r ← Med-Elim(S r , 0.125ε r , 0.01); µ ar ← Unif-Sampl({a r }, 0.125ε r , δ r ); θ r ← θ r−1 + (ct − r) −2 /10; if Frac-Test(S r ,μ ar − 1.75ε r ,μ ar − 1.125ε r , δ r , θ r−1 , θ r ) thenH r+1 ← H r + 4|S r |ε −2 r ; S r+1 ← Elimination(S r ,μ ar − 0.75ε r ,μ ar − 0.625ε r , δ ′ r ); else S r+1 ← S r ; H r+1 ← H r ; endwe try a guess that is a hundred times larger. Otherwise, we return the arm chosen by Entropy-Elimination.Algorithm 5: Complexity-Guessing Input: Instance I and confidence δ. Output: The best arm. for t = 1 to ∞ dô H t ← 100 t ; Call Entropy-Elimination(I, δ,Ĥ t );if Entropy-Elimination does not return an error then return the arm returned by Entropy-Elimination; end

A.4. Observations
We start with a few simple observations on Entropy-Elimination that will be used throughout the analysis.We first note that Entropy-Elimination lasts O(t) rounds on guessĤ t , and our definition of θ r ensures that all θ r are in [0.3, 0.5].Observation A.1 The for-loop in Entropy-Elimination(I, δ,Ĥ t ) is executed at most ct times, where c = log 4 100.Proof When r ≥ ct − 1, H r + 4|S r |ε −2 r ≥ 4 · 4 ct−1 =Ĥ t . Thus Entropy-Elimination rejects at the if-statement.Observation A.2 For all t ≥ 1 and 1 ≤ r ≤ ct − 1, 0.3 ≤ θ r−1 ≤ θ r ≤ 0.5.Proof Clearly θ r ≥ θ 0 = 0.3. Moreover,θ r = θ 0 + r k=1 (ct − k) −2 /10 ≤ 0.3 + 1 10 ∞ k=1 k −2 ≤ 0.5.The following observation bounds the value ofμ ar at round r, conditioning on the correctness of Unif-Sampl and Med-Elim. Proof By definition, µ ar ≤ µ [1] (S r ). When Unif-Sampl({a r }, 0.125ε r , δ r ) returns correctly, it holds thatμ ar ≤ µ ar + 0.125ε r ≤ µ [1] + 0.125ε r .When both Med-Elim and Unif-Sampl are correct, µ ar ≥ µ [1] (S r ) − 0.125ε r , and thuŝµ ar ≥ µ ar − 0.125ε r ≥ µ [1] (S r ) − 0.25ε r .The following two observations bound the thresholds used in Frac-Test and Elimination by applying Observation A.3. ≥ µ [1] (S r ) − 2ε r . Proof Observation A.3 implies that when Unif-Sampl is correct, c high r ≤ µ [1] (S r ) + 0.125ε r − 1.125ε r = µ [1] (S r ) − ε rand when both Med-Elim and Unif-Sampl return correctly, We start with a high-level overview of the proof of our algorithm's correctness. We first define a good event on which we condition in the rest of the analysis. Let E 1 be the event that in a particular run of Complexity-Guessing, all calls of procedure Unif-Sampl and Frac-Test return correctly. Recall that δ r , the confidence of Unif-Sampl and Frac-Test, is set to be δ/(50r 2 t 2 ) in the r-th round of iteration t. By a union bound,c low r ≥ µ [1] (S r ) − 0.25ε r − 1.75ε r = µ [1] (S r ) − 2ε r .Pr[E 1 ] ≥ 1 − 2 ∞ t=1 ∞ r=1 δ/(50t 2 r 2 ) = 1 − 2δ(π 2 /6) 2 /50 ≥ 1 − δ/3.The δ-correctness of our algorithm is guaranteed by the following two lemmas. The first lemma states that Entropy-Elimination accepts a guessĤ t and returns correctly with high probability when H t is sufficiently large. The second lemma guarantees that Entropy-Elimination rejects a guessĤ t whenĤ t is significantly smaller than H, the actual complexity. More precisely, we define the following two thresholds: t max = ⌊log 100 H⌋ − 2 and t ′ max = log 100 H(Ent + ln δ −1 )δ −1 + 2. The precise statements of the two lemmas are shown below.Lemma B.1 With probability 1 − δ/3 conditioning on event E 1 , Complexity-Guessing halts before or at iteration t ′ max and it never returns a sub-optimal arm between iteration t max + 1 and t ′ max .Lemma B.2 With probability 1 − δ/3 conditioning on event E 1 , Complexity-Guessing never returns a sub-optimal arm in the first t max iterations.Lemma B.1 and Lemma B.2 directly imply the following theorem.Theorem B.3 Complexity-Guessing is a δ-correct algorithm for Best-1-Arm.Proof Recall that Pr[E 1 ] ≥ 1 − δ/3. It follows directly from Lemma B.1 and Lemma B.2 that with probability 1 − δ, Entropy-Elimination accepts at least one ofĤ 1 ,Ĥ 2 , . . . ,Ĥ t ′ max . Moreover, when Entropy-Elimination accepts, it returns the optimal arm. Therefore, Complexity-Guessing is δ-correct.

B.2. Useful Lemmas
To analyze our algorithm, it is essential to bound the probability that a specific guessĤ t gets rejected by Entropy-Elimination. We hope that this probability is high whenĤ t is small (compared to the true complexity H), while it is reasonably low whenĤ t is large enough.It turns out to be useful to consider the following procedure P obtained from Entropy-Elimination by removing the if-statement that checks whether H r + 4|S r |ε −2 r ≥Ĥ t and T r+1 ≥ 100Ĥ t . In other words, the modified procedure P never rejects, regardless the value ofĤ t . Note that r, the number of rounds, may exceed ct in P, which leads to invalid values of θ r . In this case, we simply assume that the thresholds used in Frac-Test are 0.3 and 0.5 respectively, and the following analysis still works. Define random variable H ∞ and T ∞ to be the final estimation of the complexity and the number of samples at the end of P. More precisely, if P terminates at round r * , then H ∞ and T ∞ are defined as H r * and T r * , respectively. Note that there is a natural mapping from an execution of P to an execution of Entropy-Elimination. In particular, if both H ∞ <Ĥ t and T ∞ < 100Ĥ t hold in an execution of procedure P, then Entropy-Elimination accepts in the corresponding run. Therefore, we may upper bound the probability of rejection by establishing upper bounds of H ∞ and T ∞ . The following two lemmas bound the expectation of H ∞ and T ∞ conditioning on the event that Elimination always returns correctly.

Lemma B.4 E[H ∞ |all Elimination return correctly] ≤ 256H.


Lemma B.5 SupposeĤ t ≥ H. E[T ∞ |all Elimination return correctly] ≤ 16(H(Ent + ln δ −1 + ln(Ĥ t /H))).
Note that it is crucial for the two lemmas above that all Elimination are correct. The following lemma gives an upper bound on the probability that some call of Elimination returns incorrectly. Lemmas B.4 through B.6 together can be used to upper bound the probability of rejecting a guesŝ H. In the statement of Lemma B.6, we abuse the notation a little bit by assuming A 1 ∈ G ∞ and ∆ −2[1] = +∞.Lemma B.6 Suppose that s ∈ {2, 3, . . . , n} and r * ∈ N ∪ {∞} satisfy A s−1 ∈ G r * . When Entropy-Elimination runs on parameterĤ t < ∆ −2 [s−1] , the probability that there exists a call of procedure Elimination that returns incorrectly before round r * is upper bounded by 3000s n i=s ∆ −2[i] δ 2 /Ĥ t .The proofs of the three lemmas above are shown below. Proof [Proof of Lemma B.4] In the following analysis, we always implicitly condition on the event that all Elimination return correctly. Define H(r, S) as the expectation of H ∞ −H r at the beginning of the r-th round of Entropy-Elimination, when the current set of arms is S r = S. Let r max denotelog 2 ∆ −1 [2] . Define C(r, S) = ∞ i=r−1 |S ∩ G i | i+1 j=r ε −2 j + rmax+1 i=r ε −2 iand M (r, S) = |S ∩ G ≤r−2 |. We prove by induction on r that H(r, S) ≤ 128C(r, S) + 16M (r, S)ε −2 r .We start with the base case at round r max + 2. Recall that c low r and d low r denote the lower threshold of Frac-Test and Elimination in round r respectively. For all r ≥ r max + 2, if Med-Elim returns correctly at round r (which happens with probability 0.99), according to Observation A.4 and Observation A.5, we haved low r ≥ c low r ≥ µ [1] − 2ε r ≥ µ [1] − 2 −(rmax+1) ≥ µ [2] .Since Frac-Test returns correctly (contioning on E 1 ) and|{A ∈ S r : µ A ≤ c low r }| ≥ |{A ∈ S r : µ A ≤ µ [2] }| = |S r | − 1 ≥ 0.5|S r | ≥ θ r |S r |(the last step applies Observation A.2), Frac-Test must return True and Elimination will be called.Since we assume that all calls of Elimination return correctly, we have|S r+1 | − 1 = |{A ∈ S r+1 : µ A ≤ µ [2] }| ≤ |{A ∈ S r+1 : µ A ≤ d low r }| ≤ 0.1|S r+1 |,which guarantees that S r+1 only contains the optimal arm and the algorithm will return correctly in the next round. Let r 0 denote the first round after round r max + 2 (inclusive) in which Med-Elim returns correctly. Then according to the discussion above, we have Pr[r 0 = r] ≤ 0.01 r−rmax−2 for all r ≥ r max + 2. Thus it follows from a direct summation on possible values of r 0 thatH(r max + 2, S) ≤ ∞ r=rmax+2 Pr[r 0 = r] · 4|S|ε −2 r ≤ ∞ r=rmax+2 4|S|ε −2 r 0.01 r−rmax−2 ≤ 8|S|ε −2 rmax+2 ≤ 16M (r max + 2, S)ε −2 rmax+2 ,which proves the base case. Before proving the induction step, we note the following fact: for r = 1, 2, . . . , r max + 1,C(r, S) − C(r + 1, S) = ∞ i=r−1 |S ∩ G i | i+1 j=r ε −2 j − ∞ i=r |S ∩ G i | i+1 j=r+1 ε −2 j + ε −2 r = ∞ i=r−1 |S ∩ G i |ε −2 r + ε −2 r = (|S ∩ G ≥r−1 | + 1)ε −2 r .(4)Suppose inequality (3) holds for r + 1. Consider the following three cases of the execution of Entropy-Elimination in round r. Let N cur = |S ∩ G r−1 | and N big = |S ∩ G ≥r |. For brevity, let N sma denote M (r, S) in the following. We have N sma + N cur + N big = |S| − 1. Note that S r+1 is the set of arms that survive round r. Case 1: Med-Elim returns correctly and Frac-Test returns True.According to the induction hypothesis, the expectation of H ∞ − H r in this case can be bounded by:H(r + 1, S r+1 ) + 4|S|ε −2 r ≤128C(r + 1, S r+1 ) + 16M (r + 1, S r+1 )ε −2 r+1 + 4|S|ε −2 r ≤128C(r + 1, S) + 16[(N sma + N cur )/10] · (4ε −2 r ) + 4|S|ε −2 r =128[C(r, S) − (N cur + N big + 1)ε −2 r ] + (6.4N sma + 6.4N cur + 4|S|)ε −2 r =128C(r, S) + (10.4N sma − 117.6N cur − 124N big − 124)ε −2 r ≤128C(r, S) + 10.4N sma ε −2 r .Here the third line follows from the fact that S r+1 ⊆ S and C(r+1, S) is monotone in S. Moreover, the correctness of the Elimination procedure implies that M (r + 1, S r+1 ) ≤ (N sma + N cur )/10. The fourth line applies identity (4). Case 2: Med-Elim returns correctly and Frac-Test returns False.Since Frac-Test is always correct (conditioning on E 1 ) and it returns False, Fact 5.3, Observation A.2 and Observation A.4 together imply N sma ≤ θ r |S| ≤ |S|/2. Thus N sma ≤ |S| − N sma = N cur + N big + 1. As Elimination is not called in this round, the expectation of H ∞ − H r in this case can be bounded byH(r + 1, S) ≤128C(r + 1, S) + 16M (r + 1, S)ε −2 r+1 ≤128[C(r, S) − (N cur + N big + 1)ε −2 r ] + 64(N sma + N cur )ε −2 r =128C(r, S) + (64N sma − 64N cur − 128N big − 128)ε −2 r ≤ 128C(r, S).Here the last step follows from 64N sma −64N cur −128N big −128 ≤ 64(N sma −N cur −N big −1) ≤ 0. Case 3: Med-Elim returns incorrectly. In this case, the worst scenario happens when we add 4|S|ε −2 r to the complexity H r , but no arms are eliminated. Then the expectation of H ∞ − H r in this case can be bounded byH(r + 1, S) + 4|S|ε −2 r ≤128C(r + 1, S) + 16M (r + 1, S)ε −2 r+1 + 4|S|ε −2 r ≤128[C(r, S) − (N cur + N big + 1)ε −2 r ] + [64(N sma + N cur ) + 4|S|]ε −2 r =128C(r, S) + (68N sma − 60N cur − 124N big − 124)ε −2 r ≤ 128C(r, S) + 68N sma ε −2 r .Recall that Case 3 happens with probability at most 0.01. Thus we have: The induction is completed. Note that (3) directly implies our bound:E [H ∞ |all Elimination return correctly] =H(1, S) ≤ 128C(1, S) + 16M (1, S) =128 ∞ i=0 |S ∩ G i | ·   i+1 j=0 4 j   ≤256 ∞ i=0 4 i+1 |S ∩ G i | ≤256 ∞ i=0 A∈S∩G i ∆ −2 A = 256H.Then we prove Lemma B.5, which is restated below.Lemma B.5. (restated) SupposeĤ t ≥ H. E[T ∞ |all Elimination return correctly] ≤ 16(H(Ent + ln δ −1 + ln(Ĥ t /H))). Proof [Proof of Lemma B.5] Recall that T ∞ is the sum of |S r |ε −2 r ln |S r |ε −2 r H t δ −1 = |S r |ε −2 r ln H |S r |ε −2 r + ln δ −1 + lnĤ t H(5)for all round r. T ∞ serves as an upper bound on the expected number of samples taken by Med-Elim and Elimination (up to a constant factor). Before the technical proof, we discuss the intuition of our analysis. In order to bound T ∞ , we attribute each term in (5) to a specific subset of arms. For simplicity, we assume for now that this term is just |S r |ε −2 r = 4 r |S r |. Roughly speaking, we "charge" a cost of ε −2 r = 4 r to each arm in group G ≥r . We expect that |G ≥r | is at least a constant times |S r |, so that the number of samples (i.e., 4 r |S r |) can be covered by the total charges. Then the analysis reduces to calculating the total cost that each arm is charged. Fix an arm A ∈ G r ′ for some r ′ . As described above, A is charged 4 r in round r (1 ≤ r ≤ r ′ ), and thus the total charge is bounded by 4 r ′ , which is the actual complexity of A. Now we start the formal proof. Consider the execution of procedure P onĤ t . We define a collection of random variables {T i,j : i, j ≥ 1}, where T i,j corresponds to the cost we charge each arm in G j at round i. For each i, let r i denote the largest integer such that |G ≥r i | ≥ 0.5|S i |. Note that such an r i always exists, as |G ≥1 | = |S 1 | ≥ 0.5|S i | and |G ≥r | = 0 for sufficiently large r. We define T i,j asT i,j =    0, j < r i , ε −2 i ln H |G j |ε −2 i + ln δ −1 + lnĤ t H , j ≥ r i .Note that this slightly differs from the proof idea described above: T i,j might be positive when i > j (i.e., we may not always charge G ≥i in round i). In fact, the charging argument described in the proof idea works only if, ideally, all calls of Med-Elim are correct. Since actually some Med-Elim may return incorrectly, we have to slightly modify the charging method. Nevertheless, we will show that this difference only incurs a reasonably small cost in expectation.We first claim thatT ∞ ≤ 2 i,j |G j | · T i,j .(6)In other words, the total cost we charge is indeed an upper bound on T ∞ . Note that the contribution of round i to T ∞ is |S i |ε −2 i ln(H/(|S r |ε −2 r )) + ln δ −1 + ln(Ĥ t /H) , while its contribution to the right-hand side of (6) is2 j |G j | · T i,j = 2 j |G j | · ε −2 i ln(H/(|G j |ε −2 i )) + ln δ −1 + ln(Ĥ t /H) ≥ 2|G ≥r i | · ε −2 i ln(H/(|S r |ε −2 r )) + ln δ −1 + ln(Ĥ t /H) ≥ |S i |ε −2 i ln(H/(|S r |ε −2 r )) + ln δ −1 + ln(Ĥ t /H) .Then identity (6) directly follows from a summation on i.Then we bound the expectation of each T i,j . When i ≤ j, we have the trivial boundE[T i,j ] ≤ ε −2 i ln H |G j |ε −2 i + ln δ −1 + lnĤ t H .When i > j, we bound the probability that T i,j > 0. By definition, T i,j > 0 if and only if r i ≤ j, where r i is the largest integer that satisfies |G ≥r i | ≥ 0.5|S i |. It follows that T i,j > 0 only if |G ≥j+1 | < 0.5|S i |.Observe that in order to have |G ≥j+1 | < 0.5|S i |, Med-Elim must return incorrectly between round j + 1 and round i − 1. In fact, suppose towards a contradiction that Med-Elim is correct in round k ∈ [j + 1, i − 1]. Then we have|G ≥j+1 | ≥ |G ≥k | ≥ |S k+1 ∩ G ≥k | > 0.5|S k+1 | ≥ 0.5|S i |,a contradiction. Here the third step is due to the fact that when Elimination returns correctly at round k, the fraction of arms in S k+1 with gap greater than 2 −k is less than 0.1.Note that for each specific round, the probability that Med-Elim returns incorrectly is at most 0.01. Thus, the probability that T i,j > 0 for i > j is upper bounded by 0.01 i−j−1 . Therefore,E[T i,j ] ≤ 0.01 i−j−1 ε −2 i ln H |G j |ε −2 i + ln δ −1 + lnĤ t H .It remains to sum up the upper bounds of E[T i,j ] to yield our bound of E[T ∞ ].E[T ∞ ] ≤ 2 i,j |G j | · E[T i,j ] = 2 i≤j |G j | · E[T i,j ] + 2 i>j |G j | · E[T i,j ].Here the first part can be bounded by2 i≤j |G j | · E[T i,j ] ≤ 2 j j i=1 |G j | · 4 i ln H |G j |4 i + ln δ −1 + lnĤ t H ≤ 4 j |G j | · 4 j ln H |G j |4 j + ln δ −1 + lnĤ t H ≤ 4 j H j ln H H j /4 + H j ln δ −1 + H j lnĤ t H ≤ 8H Ent + ln δ −1 + lnĤ t H .The second part can be bounded similarly.2 i>j |G j | · E[T i,j ] ≤ 2 j ∞ i=j+1 0.01 i−j−1 |G j | · 4 i ln H |G j |4 i + ln δ −1 + lnĤ t H ≤ 4 j |G j | · 4 j ln H |G j |4 j + ln δ −1 + lnĤ t H ≤ 4 j H j ln H H j /4 + H j ln δ −1 + H j lnĤ t H ≤ 8H Ent + ln δ −1 + lnĤ t H .In fact, the crucial observation for both the two inequalities above is that the summation decreases exponentially as i becomes farther away from j. The lemma directly follows.Finally, we prove Lemma B.6, which is restated below. Recall that we abuse the notation a little bit by assuming A 1 ∈ G ∞ and ∆ −2[1] = +∞.Lemma B.6. (restated) Suppose that s ∈ {2, 3, . . . , n} and r * ∈ N ∪ {∞} satisfy A s−1 ∈ G r * . When Entropy-Elimination runs on parameterĤ t < ∆ −2 [s−1] , the probability that there exists a call of procedure Elimination that returns incorrectly before round r * is upper bounded by3000s n i=s ∆ −2 [i] δ 2 /Ĥ t .Proof [Proof of Lemma B.6] Recall that A s−1 ∈ G r * . Suppose A s ∈ G r ′ . Suppose that we are at the beginning of round r of Entropy-Elimination and the subset of arms that have not been removed is S r = S. Moreover, we assume that the optimal arm, A 1 , is still in S r . Let P (r, S) denote the probability that some call of procedure Elimination returns incorrectly in round r, r + 1, . . . , r * − 1.As in the proof of Lemma B.4, we bound P (r, S) by induction using the potential function method. DefineC(r, S) = r ′ i=r−1 |S ∩ G i | i+1 j=r ε −2 j + (s − 1) r ′ +2 j=r ε −2 jand M (r, S) = |S ∩ G ≤r−2 |. Then it holds that for 1 ≤ r ≤ r ′ + 1,C(r, S) − C(r + 1, S) = r ′ i=r−1 |S ∩ G i |ε −2 r + (s − 1)ε −2 r ≥ (|S ∩ G ≥r−1 | + 1)ε −2 r .We prove by induction thatP (r, S) ≤ 128C(r, S) + 16M (r, S)ε −2 r δ 2 /Ĥ.(7)We first prove the base case at round r ′ + 2. If r ′ + 2 ≥ r * , the bound holds trivially. Otherwise, we consider the ratioα = |S r ′ +2 ∩ {A s , A s+1 , . . . , A n }|/|S r ′ +2 |,which is the fraction of arms at round r ′ + 2 that are strictly worse than A s−1 . Let r 0 be the first round after r ′ + 2 (inclusive) in which Med-Elim returns correctly. If Frac-Test returns False in round r 0 , according to Fact 5.3 and the correctness of Frac-Test conditioning on event E 1 , we have α ≤ θ r 0 . Consequently, in each of the following rounds (say, round r > r 0 ), Frac-Test always returns False since α ≤ θ r 0 ≤ θ r−1 , and Elimination will never be called before round r * . Note that it is crucial that the threshold interval of Frac-Test in diffrent rounds are disjoint. For the other case, suppose Frac-Test returns True and we call Elimination in round r 0 . Then after that, assuming Elimination returns correctly, the fraction of arms worse than A s−1 will be smaller than 0.1. It also follows that Elimination will never be called after round r 0 . Therefore, Elimination is called at most once between round r ′ + 2 and r * − 1, and it can only be called at round r 0 . Note that for r ≥ r ′ + 2, Pr[r 0 = r] ≤ 0.01 r−r ′ −2 . A direct summation on all possible values of r 0 yieldsP (r ′ + 2, S) ≤ r * −1 r=r ′ +2 Pr[r 0 = r] · δ ′ r = r * −1 r=r ′ +2 0.01 r−r ′ −2 · 4|S|ε −2 r δ 2 /Ĥ ≤ 4|S|ε −2 r ′ +2 δ 2 /Ĥ ∞ k=0 0.01 k 4 k ≤ 5|S|ε −2 r ′ +2 δ 2 /Ĥ.Note that C(r ′ +2, S) = (s−1)ε −2 r ′ +2 , M (r ′ +2, S) = |S ∩G ≤r ′ | and |S| ≤ |S ∩G ≤r ′ |+(s−1). ThusP (r ′ + 2, S) ≤ 5(|S ∩ G ≤r ′ | + s − 1)ε −2 r ′ +2 δ 2 /Ĥ ≤ 128C(r ′ + 2, S) + 16M (r ′ + 2, S)ε −2 r ′ +2 δ 2 /Ĥ,which proves the base case.Then we proceed to the induction step. Again, we consider whether Med-Elim returns correctly and whether Frac-Test returns True. Let N cur = |S ∩ G r−1 | and N big = |S ∩ G ≥r |. Again, we denote M (r, S) by N sma for brevity. Note that S r+1 is the set of arms that survive round r.Case 1: Med-Elim returns correctly and Frac-Test returns True. In this case, Elimination is called with confidence level δ ′ r . Then the conditional probability that some Elimination returns incorrectly in this case is bounded byP (r + 1, S r+1 ) + δ ′ r ≤ 128C(r + 1, S r+1 ) + 16M (r + 1, S r+1 )ε −2 r+1 + 4|S|ε −2 r δ 2 /Ĥ ≤ 128C(r + 1, S) + 64(N sma + N cur )ε −2 r /10 + 4|S|ε −2 r δ 2 /Ĥ = 128C(r, S) − 128(N cur + N big + s − 1)ε −2 r + (6.4N sma + 6.4N cur + 4|S|)ε −2 r δ 2 /Ĥ ≤[128C(r, S) + 10.4M (r, S)ε −2 r ]δ 2 /Ĥ.Case 2: Med-Elim returns correctly and Frac-Test returns False.Since Frac-Test returns False, according to Fact 5.3 and Observation A.4, we have N sma ≤ |S| − N sma = N cur + N big + 1. Then the conditional probability in this case is bounded byP (r + 1, S) ≤ [128C(r + 1, S) + 16M (r + 1, S)ε −2 r+1 ]δ 2 /Ĥ ≤ [128C(r, S) − 128(N cur + N big + s − 1)ε −2 r + 64(N sma + N cur )ε −2 r ]δ 2 /Ĥ ≤ [128C(r, S) + (64N sma − 64N cur − 128N big − 128(s − 1))ε −2 r ]δ 2 /Ĥ ≤ 128C(r, S)ε −2 r δ 2 /Ĥ.Here the last step follows from64N sma − 64N cur − 128N big − 128(s − 1) ≤ 64(N sma − N cur − N big − 1) ≤ 0.Case 3: Med-Elim returns incorrectly. In this case, the worst scenario is that we call Elimination with confidence δ ′ r ≤ 4|S|ε −2 r δ 2 /Ĥ, yet no arms are removed. So the conditional probability in this case is bounded by The induction is completed. It follows from (7) thatP (r + 1, S) + 4|S|ε −2 r δ 2 /Ĥ ≤ 128C(r + 1, S) + 16M (r + 1, S)ε −2 r+1 + 4|S|ε −2 r δ 2 /Ĥ ≤[128C(r, S) − 128(N cur + N big + s − 1)ε −2 r + 64(N sma + N cur )ε −2 r + 4(N sma + N cur + N big + 1)ε −2 r ]δ 2 /Ĥ ≤[128C(r, S) + (68N sma − 60N cur − 124N big − 124)ε −2 r ]δ 2 /Ĥ ≤ 128C(r, S) + 68M (r, S)ε −2 r δ 2 /Ĥ.P (1, S) ≤ 128   r ′ i=0 |G i | i+1 j=1 ε −2 j + (s − 1) r ′ +2 j=1 ε −2 j   δ 2 /Ĥ ≤ 128 (4/3) r ′ i=0 |G i |4 i+1 + (4/3)(s − 1)4 r ′ +2 δ 2 /Ĥ ≤ 128 (16/3) n i=s ∆ −2 [i] + (64/3)(s − 1)4 r ′ δ 2 /Ĥ ≤ 3000s n i=s ∆ −2 [i]δ 2 /Ĥ.

B.3. Proof of Lemma B.1
Recall that t max = ⌊log 100 H⌋ − 2 and t ′ max = ⌈log 100 [H(Ent + ln δ −1 )δ −1 ]⌉ + 2. We restate and prove Lemma B.1 in the following.Lemma B.1. (restated) With probability 1 − δ/3 conditioning on event E 1 , Complexity-Guessing halts before or at iteration t ′ max and it never returns a sub-optimal arm between iteration t max + 1 and t ′ max .The high-level idea of the proof is to construct three other "good events" E 2 , E 3 and E 4 . We show that each event happens with high probability conditioning on E 1 . Moreover, events E 1 through E 4 together imply the desired event.Proof Recall that t max = ⌊log 100 H⌋ − 2 and t ′ max = ⌈log 100 [H(Ent + ln δ −1 )δ −1 ]⌉ + 2. Let E 2 denote the following event: for all t such that t ≥ t max + 1 andĤ t < 100 3 H, Entropy-Elimination either rejects or outputs the optimal arm. SinceĤ tmax+1 = 100 tmax+1 ≥ H/10000, there are at most log 100 [100 3 H/(H/10000)] + 1 = 6 different values of suchĤ t . For eachĤ t , the probability of returning a sub-optimal arm is bounded by the probability that the optimal arm is deleted, which is in turn upper bounded by δ 2 as a corollary of Lemma B.9 proved in the following section.Thus, by a union bound,Pr[E 2 |E 1 ] ≥ 1 − 6δ 2 .Let E 3 be the event that for allĤ t such that t ≤ t ′ max andĤ t ≥ 100 3 H (or equivalently, ⌈log 100 H⌉ + 3 ≤ t ≤ t ′ max ), Entropy-Elimination never returns an incorrect answer. In fact, in order for Entropy-Elimination to return incorrectly, some call of Elimination must be wrong. Thus we may apply Lemma B.6 to bound the probability of E 3 . Specifically, we apply Lemma B.6 with s = 2. Then we havePr[E 3 |E 1 ] ≥ 1 − t ′ max t=⌈log 100 H⌉+3 3000s n i=s ∆ −2 [i] δ 2 H t ≥ 1 − ∞ t=⌈log 100 H⌉+3 6000H 100 t δ 2 ≥ 1 − ∞ k=3 6000 100 k δ 2 ≥ 1 − δ 2 /100.Here the third step is due to the simple fact that 100 ⌈log 100 H⌉ ≥ H. Finally, let E 4 denote the event that when Entropy-Elimination runs onĤ t ′ max , no Elimination is wrong and the algorithm finally accepts. In order to bound the probability of the last event, we simply apply Markov inequality based on Lemma B.4 and Lemma B.5. Let E 0 be the event that no Elimination is wrong when Entropy-Elimination runs onĤ t ′ max . Then we havePr[E 4 |E 1 ] ≥ Pr[E 0 |E 1 ] − E[H ∞ |E 0 ] H t ′ max − E[T ∞ |E 0 ] 100Ĥ t ′ max ≥ 1 − δ 2 − 256H 100 2 H(Ent + ln δ −1 )δ −2 − 16H Ent + ln δ −1 + ln(Ĥ t ′ max /H) 100 3 H(Ent + ln δ −1 )δ −2 ≥ 1 − δ 2 − 256 100 2 δ 2 − 16 Ent + 3 ln δ −1 + ln(100 2 (Ent + ln δ −1 )) 100 3 (Ent + ln δ −1 ) δ 2 ≥ 1 − 2δ 2 .Note that conditioning on events E 1 through E 4 , Entropy-Elimination never outputs an incorrect answer between iteration t max + 1 and t ′ max . Moreover, our algorithm terminates before or at iteration t ′ max . The lemma directly follows from a union bound and the observation that for all δ ∈ (0, 0.01), 6δ 2 + δ 2 /100 + 2δ 2 ≤ δ/3.

Remark B.7
The last part of the proof implies a more general fact: for fixedĤ t , Entropy-Elimination accepts with probability at least1 − δ 2 − 256Ĥ H t − 16H(Ent + ln δ −1 + ln(Ĥ t /H)) 100Ĥ t .

B.4. Mis-deletion of Arms
We prove Lemma B.2 in the following. Again, our analysis in this subsection conditions on event E 1 , which guarantees that all calls of Frac-Test and Unif-Sampl in Entropy-Elimination are correct. The high-level idea of the proof is to show that a large proportion of arms will not be accidentally removed before they have contributed a considerable amount to the total complexity. Formally, we define the mis-deletion of arms as follows.Definition B.8 An arm A ∈ G r is mis-deleted in a particular run of Entropy-Elimination, if A is deleted before or at round r − 1. In particular, the optimal arm is mis-deleted if it is deleted in any round.The following lemma bounds the probability that a certain collection of arms are all mis-deleted. Lemma B.9 For a fixed collection of k arms, the probability that all of them are mis-deleted is at most δ 2k .Proof Let S = {A 1 , A 2 , . . . , A k } be a fixed set of k arms. (Here we temporarily drop the convention that A i denotes the arm with the i-th largest mean.) For each A i , let E bad i denote the event that A i is mis-deleted, and let r i denote the group that contains A i (i.e., A i ∈ G r i ). By definition,µ A i ≥ µ [1] − ε r i .We start by proving the following fact: suppose Elimination is called with confidence level δ ′ r in round r. Then the probability that all arms in S are mis-deleted in round r simultaneously is bounded by δ ′ k r . We assume that r < r i for all i = 1, 2, . . . , k. Otherwise, if r ≥ r i for some i, then A i cannot be mis-deleted in round r, since the definition of mis-deletion requires that r < r i . To analyze the behaviour of Elimination, we recall that each run of Elimination consists of several stages. (Here we use the term "stage" for an iteration of Elimination, while the term for Entropy-Elimination is "round".) In each stage, procedure Unif-Sampl is called at line 6 to estimate the means of the arms that have not been eliminated. Let r bad i denote the stage in which A i gets deleted. Recall that d high r is the upper threshold used in Elimination in round r. According to Observation A.5, d highr ≤ µ [1] (S r ) − 0.5ε r = µ [1] (S r ) − 2 −(r+1) ≤ µ [1] − ε r i ≤ µ A i .Here the third step follows from our assumption that r < r i . In order for Elimination to eliminate an arm A i with mean greater than d high in stage r bad i , the Unif-Sampl subroutine must return an incorrect estimation for A i (i.e., |μ A i − µ A i | > (d high − d mid )/2), which happens with probability at most δ ′ r / 10 · 2 r bad i . Since the samples taken on different arms are independent, the events that Unif-Sampl returns incorrect estimates for different arms are also independent, and it follows that the probability that each arm A i is removed at stage r bad i is bounded by k i=1 δ ′ r / 10 · 2 r bad i . Therefore, the probability that all the k arms in S are mis-deleted in Elimination is upper bounded by∞ r bad 1 =1 ∞ r bad 2 =1 · · · ∞ r bad k =1 k i=1 δ ′ r /(10 · 2 r bad i ) = k i=1   ∞ r bad i =1 δ ′ r / 10 · 2 r bad i   ≤ k i=1 δ ′ r = δ ′ k r .Then we start with the proof of the lemma. Suppose that we are at the beginning of round r. m arms among S are still in S r , while the sum of confidence levels allocated in the previous rounds is δ ′ (i.e., δ ′ = r−1 i=1 δ ′ i ). Let P (r, δ ′ , m) denote the probability that all the m remaining arms are mis-deleted in the future. We prove by induction thatP (r, δ ′ , m) ≤ (δ 2 − δ ′ ) m .(8)Recall that the number of rounds that Entropy-Elimination lasts is bounded by ct according to Observation A.1. Thus when r = ⌈ct⌉ + 1, we have P (r, δ ′ , m) = 0. Observe that δ ′ never exceeds δ 2 according to the behaviour of Entropy-Elimination. Therefore (8) holds for the base case. Now we proceed to the induction step. If Elimination is not called in round r, by induction hypothesis we haveP (r, δ ′ , m) ≤ P (r + 1, δ ′ , m) ≤ (δ 2 − δ ′ ) m ,which proves inequality (8). If, on the other hand, Elimination is called with confidence δ ′ r . We observe that by the claim we proved above, the probability that exactly j arms among the m arms are mis-deleted is at most m j δ ′ j r . Thus by a simple summation,P (r, δ ′ , m) ≤ m j=0 m j δ ′ j r ·P (r +1, δ ′ +δ ′ r , m−j) ≤ m j=0 m j δ ′ j r (δ 2 −δ ′ −δ ′ r ) m−j = (δ 2 −δ ′ ) m ,which completes the induction step. Finally, the lemma directly follows from (8) by plugging in r = 1, δ ′ = 0 and m = k.Remark B.10 Let E bad i denote the event that A i is mis-deleted. Note that although the events {E bad i } are not independent, we can still obtain an exponential bound (i.e., δ 2k ) on the probability that k such events happen simultaneously. We call such events quasi-independent to reflect this property. Formally, a collection of n events {E i } n i=1 are δ-quasi-independent, if for all 1 ≤ k ≤ n and 1 ≤ a 1 < a 2 < · · · < a k ≤ n, we havePr[E a 1 ∩ E a 2 ∩ · · · ∩ E a k ] ≤ δ k .Then the collection of events {E bad i } are δ 2 -quasi-independent.The following lemma proves a generalized Chernoff bound for quasi-independent events.Lemma B.11 Suppose v 1 , v 2 , . . . , v n > 0. {Y i } n i=1 is a collection of random variables, where the support of Y i is {0, v i }. Moreover, the collection of events {Y i = v i } are δ-quasi-independent. Let (S 1 , S 2 , . . . , S m ) be a partition of {1, 2, . . . , n} such that j∈S i v j ≤ 1 for all i. Define X i = j∈S i Y j . Let X = 1 m m i=1 X i and p = δ m n i=1 v i .Then for all q ∈ (p, 1),Pr[X ≥ q] ≤ e −mD(q||p) , where D(x||y) = x ln(x/y) + (1 − x) ln[(1 − x)/(1 − y)]is the relative entropy function.Proof Let p i = δ j∈S i v j . Then p = 1 m m i=1 p i . For t > 0, we have Pr[X ≥ q] = Pr[e tmX ≥ e tmq ] ≤ E[e tmX ] e tmq .To bound E[e tmX ], we consider a collection of independent random variablesỸ 1 ,Ỹ 2 , . . . ,Ỹ n defined by Pr[Ỹ i = v i ] = δ and Pr[Ỹ i = 0] = 1 − δ. DefineX i = j∈S iỸ j for i = 1, 2, . . . , m, andX = 1 m m i=1X i . Note that each term in the Taylor expansion of e tmX can be written as α l i=1 Y n l , where l ≥ 0, (n 1 , n 2 , . . . , n l ) ∈ {1, 2, . . . , n} l , and α = t l /(l!) > 0. The corresponding term in e tmX is then α l i=1Ỹ n l . Let U = |{n i : i ∈ {1, 2, . . . , l}}| denote the set of distinct numbers among n 1 , n 2 , . . . , n l . We haveE l i=1 Y n l = Pr[Y i = v i for all i ∈ U ] · l i=1 v n l ≤ δ |U | l i=1 v n l = E l i=1Ỹ n l .Summing over all terms in the expansion yieldsE e tmX ≤ E e tmX = m i=1 E e tX i .Here the last step holds since {X i } are independent. Note that sinceX i ∈ [0, 1], it follows from Jensen's inequality thatE e tX i ≤ E e tX i + 1 −X i = p i e t + 1 − p i . Then E e tmX ≤ m i=1 (p i e t + 1 − p i ) ≤ (pe t + 1 − p) m .Recall that p = 1 m m i=1 p i . Here the last step follows from Jensen's inequality and the concavity of ln(e t x + 1 − x) for t > 0.By setting t = ln q(1−p) p(1−q) , we havePr[X ≥ q] ≤ E[e tmX ] e tmq ≤ pe t + 1 − p e tq m = e −mD(q||p) .The following lemma states that if a collection of arms with a considerable amount of total complexity are not mis-deleted, Entropy-Elimination rejectsĤ.Lemma B.12 S is a set of sub-optimal arms with complexity H(S) >Ĥ. Let r * = max A∈S log 2 ∆ −1 A . If in a particular run of Entropy-Elimination, no arm in S is mis-deleted and there exists an arm A * outside S with µ A * ≥ max A∈S µ A such that A * is not deleted in the first r * − 1 rounds, thenĤ is rejected in that run.Proof Suppose S = {A 1 , A 2 , . . . , A k } and A i ∈ G r i . Without loss of generality, µ A 1 ≤ µ A 2 ≤ · · · ≤ µ A k . By definition of r * , we have r * = max 1≤i≤k r i = r k . According to our assumption, both A k and A * are not deleted in the first r * − 1 rounds. Thus Entropy-Elimination does not accept in the first r * rounds.Suppose for contradiction thatĤ is not rejected by Entropy-Elimination in a particular run.Define R = {r ∈ [1, r * − 1] : Elimination is called in round r}. Let N 1 = {i ∈ [k] : ∃r ∈ R, r ≥ r i } and N 2 = [k] \ N 1 . For each i ∈ N 1 , since A i is not mis-deleted, A i ∈ S r i . Define r ′ i = min{r ∈ R : r ≥ r i } as the first round after r i (inclusive) in which Elimination is called. It follows that A i ∈ S r ′ i . At round r ′ i of Entropy-Elimination, H r ′ i +1 is set to H r ′ i + 4|S r ′ i |ε −2 r ′ i .Therefore we can "charge" A i a cost of 4ε −2r ′ i = ε −2 r ′ i +1 .It follows that H r * is at least the total cost that arms in N 1 are charged, i∈N 1 ε −2 r ′ i +1 . For each i ∈ N 2 , we have A i ∈ S r i and S r i = S r * . Thus it holds that |S r * | ≥ |N 2 |. When the if-statement in Entropy-Elimination is checked in round r * , we haveH r * + 4|S r * |ε −2 r * ≥ i∈N 1 ε −2 r ′ i +1 + N 2 ε −2 r * +1 ≥ k i=1 ε −2 r i +1 ≥ k i=1 ∆ −2 A i = H(S) >Ĥ.Here the second step follows from r ′ i ≥ r i and r * ≥ r i , while the third step follows from ∆ A i ≥ 2 −(r i +1) . Therefore Entropy-Elimination rejects in round r * , a contradiction.

B.5. Proof of Lemma B.2
Lemma B.2 is restated below. Recall that t max = ⌊log 100 H⌋ − 2.Lemma B.2. (restated) With probability 1 − δ/3 conditioning on event E 1 , Complexity-Guessing never returns a sub-optimal arm in the first t max iterations.The high-level idea of the proof is simple. For eachĤ t , we identify a collection of near-optimal "crucial arms". By Lemma B.9, the probability that all "crucial arms" are mis-deleted is small, thus we may assume that at least one crucial arm survives. This crucial arm serves as A * in Lemma B.12. Then according to Lemma B.12, in order for Entropy-Elimination to acceptĤ t , it must mis-delete a collection of "non-crucial" arms with a significant fraction of complexity. The probability of this event can also be bounded by using the generalized Chernoff bound proved in Lemma B.11.The major technical difficulty is the choice of "crucial arms". We deal the following three cases separately: (1)Ĥ t is greater than ∆ −2[2] , the complexity of the arm with the second largest mean; (2) H t is between ∆ −2[s] and ∆ −2 [s−1] for some 3 ≤ s ≤ n; and (3)Ĥ t is smaller than ∆ −2 [n] . We bound the probability that the lemma is violated in each case, and sum them up using a union bound.

Proof [Proof of Lemma B.2]
Case 1: ∆ −2 [2] ≤Ĥ t ≤Ĥ tmax . We first deal with the case thatĤ t is relatively large. We partition the sequence of sub-optimal arms A 2 , A 3 , . . . , A n into contiguous blocks B 1 , B 2 , . . . , B m such that the total complexity in each block B i , denoted by H(B i ) = A∈B i ∆ −2 A , is between ∆ −2 [2] and 3∆ −2 [2]. To construct such a partition, we append arms to the current block one by one from A 2 to A n . When the complexity of the current block exceeds ∆ −2[2] , we start with another block. Clearly, the complexity of each resulting block is upper bounded by 2∆ −2 [2] . Note that the last block may have a complexity less than ∆ −2 [2] . In that case, we simply merge it into the second last block. As a result, the total complexity of every block is in∆ −2 [2] , 3∆ −2 [2] . It follows that H ∈ m∆ −2 [2] , 3m∆ −2 [2] . For brevity, let B ≤i denote B 1 ∪B 2 ∪· · ·∪B i and B <i = B ≤i−1 . Since H(B 1 ) = ∆ −2 [2] ≤Ĥ t < H = H(B ≤m ), there exists a unique integer k ∈ [2, m] that satisfies H(B <k ) ≤Ĥ t < H(B ≤k ). Then we haveĤ t ∈ (k − 1)∆ −2 [2] , 3k∆ −2 [2]. Since B ≤k contains at least k arms, it follows from Lemma B.9 that with probability 1 − δ 2k , at least one arm in B ≤k is not mis-deleted. Recall that by Lemma B.12, Entropy-Elimination acceptsĤ t only if either of the following two events happens: (a) no arm in B ≤k survives, which happens with probability δ 2k ; (b) a collection of arms among B >k with total complexity of at least H(B >k ) −Ĥ are mis-deleted.Fori = 2, 3, . . . , n, define v i = ∆ −2 [i] /(3∆ −2 [2] ) and Y i = v i · I[A i is mis-deleted]. For i = 1, 2, . . . , m, X i is defined as X i = A j ∈B i Y j = A∈B i ∆ −2 A 3∆ −2 [2] · I[A is mis-deleted].In other words, X i is the total complexity of the arms in block B i that are mis-deleted, divided by a constant 3∆ −2[2] . Recall that H(B i ) ≤ 3∆ −2 [2], so X i is between 0 and 1. LetX = 1 m m i=1 X i = 1 3m∆ −2 [2] n i=2 ∆ −2 [i] · I[A i is mis-deleted]denote the mean of these random variables. Since the events of mis-deletion of arms are δ 2 -quasiindependent, we may apply Lemma B.11. Note thatp = δ 2 m n i=2 v i = δ 2 m n i=2 ∆ −2 [i] 3∆ −2 [2] = Hδ 2 3m∆ −2 [2] ≤ δ 2 .Here the last step applies H ≤ 3m∆ −2 [2] . On the other hand, conditioning on event (b) (i.e., a collection of arms with total complexity H(B >k ) −Ĥ are mis-deleted), we haveX = 1 3m∆ −2 [2] n i=2 ∆ −2 [i] · I[A i is mis-deleted] ≥ H(B >k ) −Ĥ 3m∆ −2 [2] ≥ (m − k)∆ −2 [2] − 3k∆ −2 [2] 3m∆ −2 [2] ≥ m − 4k 3m ≥ m − 12m/10000 3m ≥ 1 6 .Here the third step follows from H( ≤ (6δ 2 ) m/6 · (6/5) 5m/6 ≤ δ m/6 .B >k ) ≥ (m − k)∆ −2 [2] andĤ ≤ 3k∆ −2[Recall that D(x||y) stands for the relative entropy function. The last step follows from 6δ ·(6/5) 5 ≤ 1.Therefore, Pr Entropy-Elimination acceptsĤ t ≤ δ 2k + δ m/6 .It remains to apply a union bound to (9) for all values ofĤ t in ∆ −2 [2] ,Ĥ tmax . Recall that k ≥ 2, and the ratio between different guessesĤ t is at least 100. It follows that the values of k are distinct for different values ofĤ t , and thus the sum of the first term, δ 2k , can be bounded by∞ k=2 δ 2k = δ 4 1 − δ 2 ≤ 2δ 4 .For the second term, we note that the number of guessesĤ t between ∆ −2[2] andĤ tmax is at mostt max − log 100 ∆ −2 [2] + 1 ≤ log 100 H − 2 − log 100 ∆ −2 [2] + 1 = log 100 H ∆ −2 [2]− 1 ≤ log 100 (3m) − 1.In particular, if m < 100 2 /3, noĤ t will fall into [∆ −2 [2] , t max ]. Thus we focus on the nontrivial case m ≥ 100 2 /3. Then the sum of the second term δ m/6 can be bounded by δ m/6 · (log 100 (3m) − 1) ≤ δ 100 2 /18 , since δ m/6 · (log 100 (3m) − 1) decreases on [100 2 /3, +∞) for δ ∈ (0, 0.01). Finally, we havePr Entropy-Elimination acceptsĤ t for someĤ t ∈ [∆ −2 [2] , H tmax ] ≤ 2δ 4 + δ 100 2 /18 ≤ 3δ 4 . Case 2: ∆ −2 [s] ≤Ĥ < ∆ −2 [s−1]for some 3 ≤ s ≤ n. In this case,Ĥ is between the complexity of A s−1 and A s . Our goal is to prove an upper bound of δ Ω(s) on the probability of returning a sub-optimal arm for each specific s. Summing over all s yields a bound on the total probability. Our analysis depends on the ratio betweenĤ and n i=s ∆ −2 [i] , the complexity of arms that are worse than A s . Intuitively, whenĤ is greater than the sum (Case 2-1), the contribution of the arms worse than A s to the complexity is negligible. Thus we have to rely on the fact that the s − 1 arms with the largest means will not be mis-deleted simultaneously with high probability. On the other hand, whenĤ is significantly smaller than the sum (Case 2-2), we may apply the same analysis as in Case 1. Finally, if the value ofĤ is between the two cases (Case 2-3), it suffices to prove a relatively loose bound, since the number of possible values is small.Case 2-1:Ĥ > 300000s n i=s ∆ −2 [i]. In this case, our guessĤ is significantly larger than the total complexity of A s , A s+1 , . . . , A n , yetĤ is smaller than the complexity of any one among the remaining arms. Thus intuitively, in order to rejectĤ, Entropy-Elimination should not mis-delete all the first s − 1 arms. More formally, we have the following fact: in order for Entropy-Elimination to return a sub-optimal arm, it must delete A 1 along with at least s − 3 arms among A 2 , A 3 , . . . , A s−1 before round r * , where r * is the group that contains A s−1 . In fact, since 4ε −2 r * = 4 r * +1 ≥ ∆ −2 [s−1] ≥Ĥ t , Entropy-Elimination terminates before or at round r * . If A 1 is not deleted before round r * , Entropy-Elimination can only return A 1 as the optimal arm, which is correct. If less than s−3 arms among A 2 , A 3 , . . . , A s−1 are deleted before round r * , for example A i and A j are not deleted (2 ≤ i < j ≤ s − 1), then both of them are contained in S r * . It follows that Entropy-Elimination does not return before round r * .We first bound the probability that A 1 is deleted before round r * . In order for this to happen, some Elimination must return incorrectly. By Lemma B.6, the probability of this event is upper bounded by3000s n i=s ∆ −2 [i] δ 2 /Ĥ t .In fact, we have a more general fact: the probability that a fixed set of k arms among {A 2 , A 3 , . . . , A s−1 } together with A 1 are deleted before round r * is bounded by3000s n i=s ∆ −2 [i] δ 2 /Ĥ t · δ 2k = 3000s n i=s ∆ −2 [i] δ 2(k+1) /Ĥ t .The proof follows from combining the two inductions in the proof of Lemma B.6 and Lemma B.9, and we omit it here. Since {A 2 , A 3 , . . . , A s−1 } contains s − 2 subsets of size s − 3, the probability that Entropy-Elimination returns an incorrect answer on a particular guessĤ t is at most(s − 2) · 3000s n i=s ∆ −2 [i] δ 2(s−2) /Ĥ t .It remains to apply a union bound on allĤ t that fall into this case. Recall thatĤ t > 300000s n i=s ∆ −2[i]andĤ t grows exponentially in t at a rate of 100. Thus the total probability is upper bounded by∞ k=0 3000s n i=s ∆ −2 [i] δ 2(s−2) (s − 2) 100 k · 300000s n i=s ∆ −2 [i] = ∞ k=0 δ 2(s−2) (s − 2) 100 k+1 = 1 99 δ 2(s−2) (s − 2).Case 2-2:Ĥ < n i=s ∆ −2 [i] /(78s). In this case, we apply the technique in the proof of Case 1. We partition the sequence A s , A s+1 , . . . , A n into m consecutive blocks B 1 , B 2 , . . . , B m such that H(B i ) ∈ ∆ −2 [s] , 3∆ −2 [s] . Let B ≤i denote B 1 ∪ B 2 ∪ · · · ∪ B i . Since H(B 1 ) = ∆ −2 [s] ≤Ĥ < n i=s ∆ −2 [i] /(78s) < H(B ≤m ), there exists a unique integer k ∈ [2, m] such that H(B <k ) ≤Ĥ < H(B ≤k ). It follows thatĤ ∈ (k − 1)∆ −2 [s] , 3k∆ −2 [s]. By Lemma B.12, in order for Entropy-Elimination to acceptĤ, one of the following two events happens: (a) Entropy-Elimination mis-deletes all arms in B ≤k ∪ {A 1 , A 2 , . . . , A s−1 }; (b) the total complexity of mis-deleted arms among B >k is greater than H(B >k ) −Ĥ. Since B ≤k contains at least k arms, by Lemma B.9, the probability of event (a) is bounded by δ 2(s+k−1) .Again, we bound the probability of event (b) using the generalized Chernoff bound in Lemma B.11. For each i = s, s + 1, . . . , n,define v i = ∆ −2 [i] /(3∆ −2 [s] ) and Y i = v i · I[A i is mis-deleted]. Define random variables {X i : i ∈ {1, 2, . . . , m}} as X i = A j ∈B i Y j = 1 3∆ −2 [s] A∈B i ∆ −2 A · I[A is mis-deleted].SinceH(B i ) ≤ 3∆ −2 [s], X i is between 0 and 1. LetX = 1 m m i=1 X i = 1 3m∆ −2 [s] n i=s ∆ −2 [i] · I[A i is mis-deleted]denote the mean of these random variables. Since the events {Y i = v i } are δ 2 -quasi-independent, we may apply Lemma B.11. We havep = δ 2 m n i=s v i = H(B ≤m )δ 2 3m∆ −2 [s] ≤ δ 2 .Here the last step applies H(B ≤m ) ≤ 3m∆ −2 [s] . On the other hand, conditioning on event (b) (i.e., a collection of arms in B >k with total complexity H(B >k ) −Ĥ are mis-deleted), we haveX = 1 3m∆ −2 [s] n i=s ∆ −2 [i] · I[A i is mis-deleted] ≥ H(B >k ) −Ĥ 3m∆ −2 [s] ≥ (m − k)∆ −2 [s] − 3k∆ −2 [s] 3m∆ −2 [s] ≥ m − 4k 3m ≥ m − 4m/(26s) 3m ≥ 1 6 .Here the third step follows fromH(B >k ) ≥ (m − k)∆ −2 [s] andĤ ≤ 3k∆ −2 [s] . The last line holds since k∆ −2 [s] ≤Ĥ ≤ H(B ≤m )/(78s) ≤ m∆ −2 [s] /(26s), which implies k ≤ m/(26s). By Lemma B.11, we have Pr[X ≥ 1/6] ≤ δ m/6 , and thus the probability that Entropy-Elimination return an incorrect answer onĤ t is bounded by δ 2(s+k−1) + δ m/6 . It remains to apply a union bound on all valus ofĤ t that fall into this case. Since k ≥ 2 and the values of k are distinct, the sum of the first term is bounded by In particular, if m < 26s, noĤ t will fall into this case. So in the following we focus on the nontrivial case that m ≥ 26s. Since The sum of the second term is at most δ m/6 (log 100 [m/(26s)] + 1) ≤ δ 13s/3 ≤ δ 2s+2 .∞ k=2 δ 2(s+k−1) = δ 2s+2 1 − δ 2 ≤ 2δ 2s+2 .Here the first step follows from the fact that δ m/6 (log 100 [m/(26s)] + 1) decreases on [26s, +∞) for all δ ∈ (0, 0.01) and s ≥ 3. The second step follows from s ≤ 3. Therefore, the total probability that Entropy-Elimination returns incorrectly in this sub-case is bounded by2δ 2s+2 + δ 2s+2 ≤ 3δ 2s+2 . Case 2-3:Ĥ ∈ [ n i=s ∆ −2 [i] /(78s), 300000s n i=s ∆ −2 [i]]. In this case, we simply bound the probability of returning an incorrect answer by the probability that at least s − 2 arms in {A 1 , A 2 , . . . , A s−1 } are mis-deleted, which is in turn bounded by (s − 1)δ 2(s−2) according to Lemma B.9. As in the argument of Case 2-1, suppose that two arms A i and A j (1 ≤ i < j ≤ s − 1) are not mis-deleted. Let r * be the group that contain A s−1 . Then both A i and A j are contained in S r * . However, as 4ε −2 r * = 4 r * +1 ≥ ∆ −2 [s−1] ≥Ĥ t , Entropy-Elimination will reject in round r * , which implies that Entropy-Elimination will never return a sub-optimal arm.Note that at most log 100 300000s 1/(78s) + 1 ≤ 2 log 100 s + 5 = log 10 s + 5 different values ofĤ fall into this case. Therefore, the total probability is bounded by δ 2(s−2) (log 10 s + 5)(s − 1).Combining Case 2-1 through Case 2-3 yields the following bound: the probability that Entropy-Elimination outputs an incorrect answer for some 3 ≤ s ≤ n andĤ ∈ [∆ −2[s] , ∆ −2 [s−1] ) is at most n s=3 1 99 δ 2(s−2) (s − 2) + 3δ 2s+2 + δ 2(s−2) (log 10 s + 5)(s − 1) = n s=3 δ 2(s−2) s − 2 99 + 3δ 6 + (log 10 s + 5)(s − 1)≤ ∞ s=3δ 2(s−2) (log 10 s + 6)(s − 1) ≤δ 2 ∞ s=3 0.01 2(s−3) (log 10 s + 6)(s − 1) ≤ 20δ 2 .Case 3:Ĥ t < ∆ −2 [n] .Finally, we turn to the case thatĤ is smaller than ∆ −2 [n] . In this case, Complexity-Guessing always rejects. Suppose A n ∈ G r * . Then in the first r * − 1 rounds of Entropy-Elimination, Frac-Test always returns False. Thus no elimination is done before round r * . SinceĤ t < ∆ −2[n] ≤ 4ε −2 r * , Entropy-Elimination directly rejects when checking the if-statement at round r * . Case 1 through Case 3 together directly imply the lemma, as 3δ 4 + 20δ 2 < δ/3 for all δ ∈ (0, 0.01).

Appendix C. Analysis of Sample Complexity
Recall that E 1 is the event that all calls of Frac-Test and Unif-Sampl in Entropy-Elimination return correctly. We bound the sample complexity of our algorithm using the following two lemmas. O(∆ −2 [2] ln ln ∆ −1 [2] polylog(n, δ −1 )).The two lemmas above directly imply the following theorem. conditioning on an event which happens with probability at least 1 − δ. However, to prove Theorem 1.11, we need a δ-correct algorithm with the desired sample complexity in expectation (not conditioning on another event). In the following, we prove Theorem 1.11 using a parallel simulation trick developed in Chen and Li (2015). Proof [Proof of Theorem 1.11] Given an instance I of Best-1-Arm and a confidence level δ, we define a collection of algorithms {A k : k ∈ N}, where A k simulates Complexity-Guessing on instance I and confidence level δ k = δ/2 k . Then we construct the following algorithm A:• A runs in iterations. In iteration t, for each k such that 2 k−1 divides t, A simulates A k until A k requires a sample from some arm A. A draws a sample from A, feeds it to A k , and continue simulating A k until it requires another sample. After that, A temporarily suspends A k .• When some algorithm A k terminates, A also terminates and returns the same answer.We first note that if all algorithms in {A k } are correct, A eventually returns the correct answer. Recall that A k is a δ/2 k -correct algorithm for Best-1-Arm. Thus by a simple union bound, A is correct with probability 1 − ∞ k=1 δ/2 k = 1 − δ, thus proving that A is δ-correct. It remains to bound the sample complexity of A. According to Theorem C.3, there exist constants C and m, along with a collection of events {E k }, such that for each k, Pr[E k ] ≥ 1 − δ k , and the expected number of samples taken by A k conditioning on E k is at mostC H · (ln δ −1 k + Ent) + ∆ −2 [2] ln ln ∆ −1 [2] (ln m n + ln m δ −1 k ) ≤C H · (k ln δ + Ent) + ∆ −2 [2] ln ln ∆ −1 [2] (ln m n + (k ln δ −1 ) m ) ≤k m · T (I).Here T (I) denotes C H · (ln δ −1 + Ent) + ∆ −2[2] ln ln ∆ −1 [2] (ln m n + ln m δ −1 ) , the desired sample complexity. The first step follows from the fact that ln δ −1 k = ln δ −1 + k ≤ k ln δ −1 for δ < 0.01.Since different algorithms in {A k } take independent samples, the events {E k } are independent. Define random variable σ as the minimum number such that event E σ happens. Then it follows thatPr[σ = k] ≤ Pr[E 1 ∩ E 2 ∩ · · · ∩ E k−1 ] ≤ k−1 i=1 δ i ≤ 0.01 k−1 .Let T k denote the number of samples taken by A k if it is allowed to run indefinitely (i.e., A does not terminate). Conditioning on σ = k, we have E[T k ] ≤ k m · T (I). Moreover, A terminates before or at iteration 2 k−1 k m · T (I). It follows that the number of samples taken by A is bounded by∞ i=1 ⌊2 k−1 k m · T (I)/2 i−1 ⌋ ≤ 2 k−1 k m · T (I) ∞ i=1 2 −(i−1) ≤ 2 k k m · T (I).

Thus the expected sample complexity of A is bounded by
∞ k=1 Pr[σ = k] · 2 k k m · T (I) ≤ ∞ k=1 0.01 k−1 · 2 k k m · T (I) ≤100T (I) ∞ k=1 0.02 k k m = O(T (I)).Therefore, A is a δ-correct algorithm for Best-1-Arm with expected sample complexity of O(H · (ln δ −1 + Ent) + ∆ −2[2] ln ln ∆ −1 [2] polylog(n, δ −1 )).We conclude the section with the proofs of Lemmas C.1 and C.2.Proof [Proof of Lemma C.1] Suppose that Complexity-Guessing terminates after iteration t 0 . According to Entropy-Elimination, for each 1 ≤ t ≤ t 0 , the algorithm takes O(Ĥ t ) = O(100 t ) samples in Med-Elim and Elimination when it runs onĤ t . As 100 t grows exponentially in t, it suffices to bound the expectation of the last term, namely 100 t 0 . Let t * = ⌈log 100 H(Ent + ln δ −1 ) + 3⌉. We first show that when t ≥ t * , Entropy-Elimination acceptsĤ t with constant probability. According to Remark B.7, the probability that Entropy-Elimination rejectsĤ t is upper bounded by256Ĥ H t + H(Ent + ln δ −1 + ln(Ĥ t /H)) 100Ĥ t ≤ 256H 100 3 H +Ĥ t /20 100Ĥ t ≤ 1/200.The first step follows from the following two observations. First, asĤ t ≥Ĥ t * ≥ 100 3 H(Ent + ln δ −1 ), we have H(Ent + ln δ −1 ) ≤ 100 −3Ĥ t . Second, sinceĤ t /H ≥ 100 3 and x ≥ 100 ln x holds for all x ≥ 10 6 , we have H ln(Ĥ t /H) ≤ H · 1 100 (Ĥ t /H) =Ĥ t /100.Therefore, the probability that t 0 equals t * + k is bounded by 200 −k for all k ≥ 1. It follows from a simple summation on all possible t 0 thatE 100 t 0 = ∞ t=1 100 t Pr[t 0 = t] ≤ t * t=1 100 t · 1 + ∞ k=1 100 t * +k · 200 −k = O 100 t * = O H(Ent + ln δ −1 ) .Proof [Proof of Lemma C.2] When Entropy-Elimination runs on guessĤ t , Unif-Sampl takes O(ε −2 r ln δ −1 r ) samples in the r-th round, while the number of samples taken by Frac-Test isO ε −2 r ln δ −1 r (θ r − θ r−1 ) −2 ln(θ r − θ r−1 ) −1 = O ε −2 r ln δ −1 r (θ r − θ r−1 ) −3 .As the second term dominates the first, we focus on the complexity of Frac-Test in the following analysis.Recall that ε r = 2 −r , δ r = δ/(50r 2 t 2 ) ≥ δ 2 /(r 2 t 2 ) and θ r − θ r−1 = (ct − r) −2 /10. For each t, suppose r ranges from 1 to r max , then the complexity at iteration t is bounded byrmax r=1 ε −2 r ln δ −1 r (θ r − θ r−1 ) −3 ≤2 rmax r=14 r (ln δ −1 + ln r + ln t)[(ct − r) −2 /10] −3 ≤2000 rmax r=1 4 r (ln δ −1 + ln r + ln t)(ct − r) 6 =O(4 rmax (ln δ −1 + ln t)(ct − r max ) 6 )The last step follows from the observation that the last term dominates the summation, and the fact ln r max = O(ln t) due to Observation A.1. Let random variable t 0 denote the last t in the execution of Complexity-Guessing. As in the proof of Lemma C.1, we define t * = ⌈log 100 H(Ent + ln δ −1 ) + 3⌉. We have also shown that Pr[t ≥ t 0 + k] ≤ 200 −k for all k ≥ 1. Thus, the expected complexity incurred after iteration t * can be bounded by the complexity at iteration t * .When t < log 100 ∆ −2 [2] , it follows from r max ≤ ct − 1 that the complexity is O(4 ct (ln δ −1 + ln t)) = O(100 t (ln δ −1 + ln t)).Summing over t = 1, 2, . . . , log 100 ∆ −2[2] yields log 100 ∆ −2 [2] t=1 100 t (ln δ −1 + ln t) = O(∆ −2 [2] (ln δ −1 + ln ln ∆ −1 [2] )).Clearly this term is bounded by the desired complexity.When 2] . Note that in fact the algorithm may not always terminate before or at round r max . However, since the probability that the algorithm lasts r max + k rounds is bounded by O(100 −k ), the contribution of those rounds to total complexity is also dominated. Thus we have log 100 ∆ −2 [2] ≤ t ≤ t * , we choose r max = log 2 ∆ −1 [2] = log 4 ∆ −2 [t 0 t=log 100 ∆ −2 [2] O(4 rmax (ln δ −1 + ln t)(ct − r max ) 6 ) = t 0 t=log 100 ∆ −2 [2] O(∆ −2 [2] (ln δ −1 + ln t)(ct − log 4 ∆ −2 [2] ) 6 ) =O t * − log 100 ∆ −2 [2] · O ∆ −2 [2] (ln δ −1 + ln t * )(ct * − log 4 ∆ −2 [2] ) 6 =O ∆ −2 [2] (ln δ −1 + ln t * )(ct * − log 4 ∆ −2 [2] ) 7 =O ∆ −

Appendix D. Lower Bound
In this section, we prove Lemma 4.1. We restate it here for convenience.Lemma 4.1. (restated) Suppose δ ∈ (0, 0.04), m ∈ N and A is a δ-correct algorithm for SIGN-ξ. P is a probability distribution on {2 −1 , 2 −2 , . . . , 2 −m } defined by P (2 −k ) = p k . Ent(P ) denotes the Shannon entropy of distribution P . Let T A (µ) denote the expected number of samples taken by A when it runs on an arm with distribution N (µ, 1) and ξ = 0. Define α k = T A (2 −k )/4 k . Then, m k=1 p k α k = Ω(Ent(P ) + ln δ −1 ).

D.1. Change of Distribution
We introduce a lemma that is essential for proving the lower bound for SIGN-ξ in Lemma 4.1, which is a special case of (Kaufmann et al., 2015, Lemma 1). In the following, KL stands for the Kullback-Leibler divergence, while D(x||y) = x ln(x/y)+(1−x) ln[(1−x)/(1−y)] is the relative entropy function.Lemma D.1 (Change of Distribution) Let A be an algorithm for SIGN-ξ. Let A and A ′ be two instances of SIGN-ξ (i.e., two arms We start with an overview of our proof of Lemma 4.1. For each k, we consider the number of samples taken by Algorithm A when it runs on an arm with mean 2 −k . We first show that with high probability, this number is between Ω(4 k ) and O(4 k α k ). Then we apply Lemma D.1 to show that the same event happens with probability at least e −α k when the input is an arm with mean zero.Since the probability of an event is at most 1, we would like to bound the sum of e −α k by 1, yet the problem is that the events for different k may not be disjoint. To avoid this difficulty, we carefully select a collection of disjoint events denoted by S. We bound m k=1 e −dα k (for appropriate constant d) by k∈S e −α k based on the way we construct S. After that, we use the "change of distribution" argument (Lemma D.1) to bound k∈S e −α k by 1. As a result, we have the following inequality for appropriate constant M , which is reminiscent of Kraft's inequality in coding theory. Once we obtain (10), the desired bound directly follows from a simple calculation.Proof [Proof of Lemma 4.1] Fix m ∈ N. Recall that all arms are normal distributions with a standard deviation of 1 and ξ is always equal to zero. 4 k α k is the expected number of samples taken by A on an arm A with distribution N (2 −k , 1). It is well-known that to distinguish N (2 −k , 1) from N (−2 −k , 1) with confidence level δ, Ω(4 k ln δ −1 ) samples are required in expectation. Therefore, we have α k = Ω(ln δ −1 ) for all k. It follows that m k=1 p k α k = Ω(ln δ −1 ). It remains to prove that m k=1 p k α k = Ω(Ent(P )) for all 0.04-correct algorithms. For each µ ∈ R, let Pr µ and E µ denote the probability and expectation when A runs on an arm with mean µ (i.e., N (µ, 1)). Define random variable τ A as the number of samples taken by A. Let c = 1/64. Let E k denote the event that A outputs "µ > 0" and τ A ∈ [4 k c, 16 · 4 k α k ]. The following lemma gives a lower bound of Pr 0 [E k ].

Lemma D.2
Pr0 [E k ] ≥ 1 4 e −α k .Our second step is to choose a collection of disjoint events from {E k : 1 ≤ k ≤ m}. We have the following lemma. Here the first two steps follow from Lemma D.3 and Lemma D.2, respectively. The last step follows from the fact that {E k : k ∈ S} is a disjoint collection of events.Finally, for a distribution P on {2 −1 , 2 −2 , . . . , 2 −m } defined by P (2 −k ) = p k , we consider the following optimization problem with variables α 1 , α 2 , . . . , α m : minimize m k=1 p k α k subject to m k=1 e −dα k ≤ 4MThe method of Lagrange multipliers yields that the minimum value is obtained when m k=1 e −dα k = 4M and e −dα k is proportional to p k . It follows that α k = − 1 d ln(4M p k ) and consequently Note that d and M are constants independent of m, distribution P and algorithm A. This completes the proof.

D.3. Proofs of Lemma D.2 and Lemma D.3
Finally, we prove the two technical lemmas. Proof [Proof of Lemma D.2] Recall that our goal is to lower bound Pr 0 [E k ]. We first show that Pr 2 −k [E k ] ≥ 1/2 and then prove the desired lower bound by applying change of distribution. Recall that E k = (A outputs µ > 0) ∧ (τ A ∈ [4 k c, 16 · 4 k α k ]). We havePr 2 −k [E k ] ≥ Pr 2 −k [A outputs µ > 0] − Pr 2 −k τ A > 16 · 4 k α k − Pr 2 −k τ A < 4 k c ≥ 1 − 0.04 − 1/16 − Pr 2 −k τ A < 4 k c ≥ 0.8 − Pr 2 −k τ A < 4 k c .Here the second step follows from Markov's inequality and the fact that E 2 −k [τ A ] = 4 k α k .It remains to show that Pr 2 −k τ A < 4 k c ≤ 0.3. Suppose towards a contradiction this does not hold. Then we consider the algorithm A ′ that simulates A in the following way: if A terminates within 4 k c samples, A ′ outputs the same answer; otherwise A ′ outputs nothing. Let Pr A ′ ,µ denote the probability when A ′ runs on an arm of mean µ. Moreover, let E bad k denote the event that the output is "µ > 0". Then we havePr A ′ ,2 −k [E bad k ] = Pr 2 −k E bad k ∧ τ A < 4 k c ≥ Pr 2 −k τ A < 4 k c − 0.04 > 0.26.On the other hand, when we run A ′ on an arm with mean −2 −k , we havePr A ′ ,−2 −k E bad k ≤ Pr −2 −k E bad k ≤ 0.04.Since A ′ never takes more than 4 k c samples, it follows from Lemma D.1 that 2c = 4 k c · KL(N (2 −k , 1), N (−2 −k , 1))≥ E A ′ ,2 −k [τ A ′ ] · KL(N (2 −k , 1), N (−2 −k , 1)) ≥ D Pr A ′ ,2 −k [E bad k ] Pr A ′ ,−2 −k [E bad k ]≥ D(0.26||0.04) ≥ 0.2, which leads to a contradiction as c = 1/64. In the following, we lower bound Pr 0 [E k ] using change of distribution. Note that D Pr2 −k [E k ] Pr 0[E k ] ≤ 4 k α k · KL(N (2 −k , 1), N (0, 1)) ≤ 4 k α k · 1 2 2 −k 2 = α k /2.Let θ k = e −α k /4. We have D(1/2||θ k ) = 1 2 ln 1 4θ k (1 − θ k ) ≥ 1 2 ln 1 4θ k = α k /2.Since we have shown Pr 2 −k [E k ] ≥ 1/2, the two inequalities above implyPr 0 [E k ] ≥ θ k = 1 4 e −α k .Proof [Proof of Lemma D.3] We map each event E k to an interval I k = [log 4 (4 k c) + 3, log 4 (16 · 4 k α k ) + 3] = [k, k + log 4 α k + 5].By construction, two events E i and E j are disjoint if and only if their corresponding intervals, I i and I j , are disjoint. We construct a subset of {1, 2, . . . , m} using the following greedy algorithm:• Sort (1, 2, . . . , m) into a list (l 1 , l 2 , . . . , l m ) such that α l 1 ≤ α l 2 ≤ · · · ≤ α lm .• While the list is not empty, we add the first element x in the list into set S. Let S x = {y : y is in the current list, and I x ∩ I y = ∅}. We remove all elements in S x from the list.Note that the way we construct S ensures that {E k : k ∈ S} is indeed a disjoint collection of events, which proves the first part of the lemma. Moreover, {S k : k ∈ S} is a partition of {1, 2, . . . , m}. Thus we have According to our construction of S, for all j ∈ S k we have α j ≥ α k . For each integer l ≥ ⌊log 4 α k ⌋, we consider the values of j such that log 4 α j ∈ [l, l + 1). Recall that the interval corresponding to event E k is I k = [k, k + log 4 α k + 5]. In order for I j to intersect I k , we must have j ∈ [k − log 4 α j − 5, k + log 4 α k + 5]. Since log 4 α j < l + 1, j must be contained in [k − l − 6, k + log 4 α k + 5], and thus there are at most (log 4 α k + l + 12) such values of j.Recall that since I k = [k, k + log 4 α k + 5] is nonempty, we have α k ≥ 4 −5 . In the following calculation, we assume for simplicity that α k ≥ 1 for all k, since it can be easily verified that the contribution of the terms with α k < 1 (i.e., l = −5, −4, . . . , −1) is a constant, and thus can be The first step rearranges the summation based on the value of l. The second step follows from the observation that S k contains at most log 4 α k + l + 12 values of j corresponding to each l. The fourth step holds since both summations decrease double-exponentially, and thus can be bounded by their respective first terms. Then we find a sufficiently large constant M (which depends on d) to cover the hidden constant in the big-O notation. Finally, the last step holds for sufficiently large d. In fact, we first choose d according to the last step, and then find the appropriate constant M . Clearly the choice of M and d is independent of the value of m and the algorithm A.Remark D.4 Recall that all distributions are assumed to be Gaussian distributions with a fixed variance of 1. In fact, our proof of Lemma 4.1 only uses the following property: the KL-divergence between two distributions with mean µ 1 and µ 2 is Θ((µ 1 − µ 2 ) 2 ). Note that this property is indeed essential to the "change of distribution" argument in the proof of Lemma D.2.In general, suppose U is a set of real numbers and D = {D µ : µ ∈ U } is a family of distributions with the following two properties: (1) the mean of distribution D µ is µ; (2) KL(D µ 1 , D µ 2 ) ≤ C(µ 1 − µ 2 ) 2 for fixed constant C > 0. Then Lemma 4.1 also holds for distributions from D.For instance, suppose D = {B(1, µ) : µ ∈ [1/2 − ε, 1/2 + ε]}, where ε ∈ (0, 1/2) is a constant and B(1, µ) denotes the Bernoulli distribution with mean µ. Since KL (B(1, p), B(1, q)) ≤ (p − q) 2 q(1 − q) ≤ (p − q) 2 1/4 − ε 2 , distribution family D satisfies the condition above with C = 4 1 − 4ε 2 . It follows that Lemma 4.1 also holds for Bernoulli distributions with means sufficiently away from 0 and 1.

Appendix E. Missing Proofs in Section 5
In this section, we present the technical details in the proofs of Lemma 5.5 and Lemma 5.6. These are essentially identical to the proofs of Lemmas B.4 and B.5, which either use a potential function or apply a charging argument.