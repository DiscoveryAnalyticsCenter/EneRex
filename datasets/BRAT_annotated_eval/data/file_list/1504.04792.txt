Visual Recognition Using Directional Distribution Distance

Abstract
In computer vision, an entity such as an image or video is often represented as a set of instance vectors, which can be SIFT, motion, or deep learning feature vectors extracted from different parts of that entity. Thus, it is essential to design efficient and effective methods to compare two sets of instance vectors. Existing methods such as FV, VLAD or Super Vectors have achieved excellent results. However, this paper shows that these methods are designed based on a generative perspective, and a discriminative method can be more effective in categorizing images or videos. The proposed D3 (discriminative distribution distance) method effectively compares two sets as two distributions, and proposes a directional total variation distance (DTVD) to measure how separated are they. Furthermore, a robust classifier-based method is proposed to estimate DTVD robustly. The D3 method is evaluated in action and image recognition tasks and has achieved excellent accuracy and speed. D3 also has a synergy with FV. The combination of D3 and FV has advantages over D3, FV, and VLAD.

Introduction
In visual recognition, an entity (object or video) is usually represented as a set of instance vectors. Each instance vector is extracted using part of the entity (e.g., a local window extracted from an image or a time-space subvolume extracted from a video). Various features have emerged as the state-of-the-art to extract instance vectors at different stages of recognition research, such as dense SIFT features  #b20 , dense CENTRIST features  #b28  or CNN features for images  #b12 , or (improved) dense trajectory features  #b27  or CNN features for videos  #b6 . Although originally CNN (or other deep learning methods) integrates visual representation and classification into one system  #b18  #b14 , recent works have shown that if multiple (a set of) CNN features are extracted from entities and classify images or videos based on these sets, higher accuracies can be ob-tained  #b7  #b31  #b2  #b30 .Because most existing learning algorithms assume that an entity is represented as a vector instead of a set of vectors, we need to find a suitable visual representation that encodes the set of instance vectors into one single vector. It is desirable that the representation will capture useful (i.e., discriminative) information from the set. Thus, comparing one entity (a set of instance vectors) to another can be divided into two steps: first represent the sets as two vectors, then find a suitable distance metric to compare the vectors. One useful variant is to compare one entity to a set of entities (e.g., all training images, corresponding to a bigger union set by gathering the instance vectors in every image), which is often used too.Since the 2 distance (or correspondingly linear SVM) is very efficient and has shown great accuracy in the second step, an effective visual representation that turns a set of instance vectors into one single vector (i.e., the first step) has been very important in visual recognition research efforts. Many representations have been proposed, for example, • Fisher Vector (FV) and VLAD. FV  #b24  is based on the idea of Fisher kernel in machine learning  #b10 . It models the distribution of instance vectors in training entities using a Gaussian Mixture Model (GMM). Then, one training or testing entity is modeled generatively, by a vector which describes how the GMM can be modified to generate the instance vectors inside that entity. A GMM with K components has three sets of parameters (w i , µ i , σ i ), 1 ≤ i ≤ K. VLAD  #b11 , another popular visual representation, can be regarded as a special case of FV, by using only the µ parameters. The classic bag-of-visual-words (BOVW)  #b3  representation is also a special case of FV, using the w parameters. • Super-Vector Instead of modeling the instance vectors as distributions, the Super-Vector  #b34  represents a set of instance vectors based on how they can be reconstructed from dictionary items. Super-Vector aims at reducing the reconstruction error, which is also from a generative perspective. The output of Super-Vector has two parts, which are conceptually related to the w and µ parame-ters in Fisher Vectors. Both threads of methods have shown excellent accuracy in the literature. However, they both focus on modeling how one entity or one distribution is generated. Given the fact that the task in hand is recognition, we argue that we need to pay more attention to how two entities or two distributions are separated. In other words, we need a visual representation that pays more attention to the discriminative side. We naturally expect that such a representation would be suitable for visual recognition tasks, whose objective is to properly separate entities belonging to different categories.In this paper, we propose a discriminative distribution distance (D3) representation that converts a set of instance vectors into a vector representation. D3 explicitly considers two distributions: a density X which is estimated from the training set as a reference model, and one entity forms another distribution Y . D3 then uses the distribution distance between X and Y as a discriminative representation for the entity Y . Technically, D3 has the following contributions.• We propose a direction total variation distance (DTVD) to measure the distance between X and Y , which contains more discriminative information than classic distribution distances by considering directions; • Directly calculating DTVD is unstable and problematic because Y may be non-Gaussian and only contains few items. We propose to estimate DTVD in a discriminative manner, by calculating robust classification errors when we try to classify every dimension of X from Y ; • We also show that D3 and FV are complementary to each other. By combining D3 and FV, we can achieve an accuracy higher than D3, FV, and VLAD. We will start by explaining closely related methods, then proposing the directional distribution distance, its robust estimation, and the entire D3 pipeline in Sec. 2. Sec. 3 presents empirical results, and Sec. 4 concludes this paper.

Discriminative Distribution Distance
In this section, we propose a discriminative distribution distance (abbreviated as D3) to compare two sets of observations, which leads to an efficient and effective visual representation.

Distribution distance: generative vs. discriminative
Given two objects X and Y , each of which is represented as an unordered set of instance vectors, i.e., X = {x 1 , . . . , x n X }, Y = {y 1 , . . . , y n Y }, we are interested in finding d(X, Y ), the distance (or dissimilarity) between them. This task is frequently encountered in compute vision. For example, an image or a video is usually represented as a set of feature vectors extracted from various im-age patches or supervoxels.In the Fisher Vector (FV) representation, a large set of instance vectors are extracted from training images or videos. We treat this set as X and a Gaussian Mixture Model (GMM) p X with parameters λ = (w k , µ k , Σ k ) K k=1 is estimated from X. When a test image or video Y is presented, we extract its instance vectors and treat it as Y . The FV representation considers X and Y as generated from two underlying distributions p X and p Y , and encodes Y as a vector f . This is a generative model and each component in f describes how each parameter in λ should be modified such that p X can be modified to fit the data Y properly.Specifically, the probability that y i is generated by the k-th Gaussian isγ i (k) = p(k|y i , λ) = 1 Z w k p k (y i |λ) ,(1)where Z is a normalization constant, p k is the k-th Gaussian component with weight w k , and parameters (µ k , Σ k ). In FV, the GMM covariances Σ k are assumed to be diagonal, whose diagonal entries form a vector σ k . The trends of parameter changing (gradients) that modify p X to fit Y is then (for all 1 ≤ k ≤ K)  #b24  f w k = 1 √ w k n Y i=1 (γ i (k) − w k ) ,(2)f µ k = 1 √ w k n Y i=1 γ i (k) y i − µ k σ k ,(3)f σ k = 1 √ 2w k n Y i=1 γ i (k) (y i − µ k ) 2 (σ k ) 2 − 1 ,(4)which correspond to the w, µ, σ parameters for p X , respectively. The image or video Y is then represented by a feature vector f , which concatenates f w k , f µ k , and f σ k for all 1 ≤ k ≤ K. We want to emphasize two observations based on the FV representation.• The gradient vector f is formed under the generative assumption that Y can be modeled by p X if we are allowed to modify the parameter set λ. Since what we are really interested in is how far is X from Y , we believe that a discriminative distance between X and Y is a better option. That is, in this paper we will treat X and Y as sampled from two different distributions p X and p Y , and find their distribution distance to encode the image or video Y; • Since diagonal Σ k are used, after the soft assignment probabilities γ i (k) are calculated, each dimension of f is generated independent of any other dimension. Thus, in finding a suitable representation for Y , we only need to consider each dimension individually. The problem is then: given two sets of scalar values X = {x 1 , x 2 , . . . , } and Y = {y 1 , y 2 , . . .} (sampled from 1-d distribution p X and p Y , respectively), how do we properly compute d(p X , p Y )? As a final note in this section, the VLAD and super vector representation can be interpreted as special cases of FV, while VLAD uses the f µ k components of f , and super vectors use both f w k and f µ k . It is also a common practice to use only f µ k and f σ k in FV implementations.

Directional Total Variation Distance
We need to be more discriminative. Thus, we propose to explicitly consider two distributions X and Y , where X is a density of instance vectors estimated from the training set, and Y is from one (training or testing) entity. A representation of Y that encodes the distance between X and Y will contain useful discriminative information about Y .A widely used distance that compares two distributions is the total variation distance, which is independent of the distributions' parameterizations. Let ν 1 and ν 2 be two probability measures on a measurable space (O, B), the total variation distance is defined asd T V (ν X , ν Y ) sup A∈B |ν X (A) − ν Y (A)| .(5)While this definition is rather formal, d T V has a more intuitive equation for commonly used continuous distributions by the Scheffe's Lemma  #b4 . For example, for twonormal distributions with p.d.f. X ∼ N (µ X , σ 2 X ) and Y ∼ N (µ Y , σ 2 Y ), d T V (p X , p Y ) = 1 2 u |p X (u) − p Y (u)| du .(6)As illustrated in Fig. 1a, it is half the summed area of the red and green regions, which clearly indicates how two distributions are separated from each other. The classic total variation distance (Eq. 6), however, is missing one most important information that captures the key difference between p X and p Y , as shown in Fig. 1b. In Fig. 1b, p 1 and p 2 are symmetric with respect to the mean of p, thus we have d T V (p, p 1 ) = d T V (p, p 2 ), in spite of the fact that p 1 and p 2 are far apart. The missing of directional information is responsible for this drawback. Thus, we propose a directional total variation distance (DTVD) asd DT V (p X , p Y ) = sign(µ Y − µ X ) × d T V (p X , p Y ) . (7)DTVD is a signed distance. In Fig. 1b, we will (correctly) have d DT V (p, p 1 ) = −d DT V (p, p 2 ), which clearly signifies the difference between p 1 and p 2 . Obviously the distance function d DT V is not a metric, because it is neither non-negative, nor symmetric.  Figure 1. Illustration of the total variation distance. 1a illustrates dT V for two Gaussians, and 1b reveals that direction is essential.0.5 p2(x) p(x) p1(x) (b)

Robust estimation of the DTVD
For two Gaussians p X and p Y , their p.d.f. will have two intersections if σ X = σ Y . For example, in Fig. 1a the second intersection is in the far right end of the x-axis. A closed-form solution to calculate d T V based on both intersections is available  #b4 . However, this closed-form solution leads to serious performance drop when used in visual recognition in our experiments. We conjecture that two reasons have caused this issue: • The distributions are not necessarily normal. As shown in Fig. 2, the typical example of p X (in Fig. 2a) is generated from many training instances, its shape resembles that of a Gaussian, but has a shaper peak. p Y , which is generated from a single image, deviates from a normal distribution; • Since the set Y (which is extracted from a single image or video, cf. Fig. 2b) usually contains small number of instance vectors, this fact leads to unstable estimation of its distribution parameters, and hence unstable d DT V (p X , p Y ). Thus, we need a more robust way to estimate the distribution distance.Our key insight again arises from the discriminative perspective. It is obvious that the total variation distance d T V is equivalent to one minus the Bayes error of a binary classification problem, where the two classes have equal prior and follow p X and p Y , respectively. Thus, we can estimate d T V (hence d DT V ) by robustly estimating the classification error between the two sets of examples X and Y . Note that this task is easy since X and Y only contain scalar examples.We adopt the minimax probability machine (MPM)  #b16  to estimate the classification error. MPM is robust because it minimizes the maximum probability of misclassification, hence the name minimax. Given examples X with mean µ X and covariance Σ X and examples Y with µ Y and Σ Y , the classifier boundary a T x − b = 0 is determined by the MPM problemκ −1 min a a T Σ X a + a T Σ Y a(8)s.t. a T (µ X − µ Y ) = 1 ,(9)and b = a T µ X − κ a T Σ X a .(10)Eq. 9 is a second order cone problem (SOCP) that can be solved by an iterative algorithm. However, since we are dealing with scalar examples that (assumed to) follow normal distributions, it has a closed form solution. Note thatX ∼ N (µ X , σ 2 X ) and Y ∼ N (µ Y , σ 2 Y ), we can immedi- ately get the following boundary a x − b = 0, where a = 1 µ X − µ Y , b = a × µ X σ Y + µ Y σ X σ X + σ Y ,(11)κ = |µ X − µ Y | σ X + σ Y .(12)That is, the two 1-d distributions p X and p Y are classified at the threshold valueT = µ X σ Y + µ Y σ X σ X + σ Y .(13)If we re-use Fig. 1a and (approximately) assume the red, blue, and green areas intersect atT = µ X σ Y +µ Y σ X σ X +σ Y, which is guaranteed to reside in between µ X and µ Y . Then, the area of the blue region is:Area = 1 − Φ T − µ X σ X + Φ T − µ Y σ Y .(14)whereΦ(x) = 1 √ 2π x −∞ e −t 2 /2 dt(15)is the cumulative distribution function (c.d.f.) of a standard normal distribution N (0, 1). And, we haved DT V (p X , p Y ) = 2 − 2Area = 4Φ µ Y − µ X σ X + σ Y − 2 ,(16)making use of the fact thatT − µ X σ X = µ Y − µ X σ X + σ Y = − T − µ Y σ Y , and the property of Φ that Φ(−x) = 1 − Φ(x).Two points are worth mentioning about Eq. 16. • Although our derivation and Fig. 1a is assuming µ X < µ Y , it is easy to derive that when µ X ≥ µ Y , Eq. 16 still holds. And, it always have the same sign as µ Y − µ X . Hence, Eq. 16 computes d DT V instead of d T V , and its range is [−2 2]. • In practice we use the error function. The error function is defined aserf(x) = 1 √ π x −x e −t 2 dt ,(17)and it satisfies thatΦ(x) = 1 2 1 + erf x √ 2 .(18)Thus, we haved DT V (p X , p Y ) = 2 erf µ Y − µ X √ 2(σ X + σ Y ) .(19)The error function erf is built-in and efficient in most major programming languages, which facilitates the calculation of d DT V using Eq. 19.We also want to note there has been research to model the discriminative distance between two sets of instance vectors. In  #b22 , non-parametric kernels are estimated from two Algorithm 1 Visual representation using D3 1: Input: An image or video Y = {y 1 , y 2 , . . .}; and, a dictionary (visual code book) with size K and cluster mean µ k and standard deviations σ k (1 ≤ k ≤ K) 2: for i = 1, 2, . . . , K do 3:Y = {y j |y j ∈ Y, arg min 1≤k≤K y j − µ k = i} 4:Compute the mean and standard deviation vectors of the set Y , denote as µ and σ , respectively 5:f i = erf µ −µ i √ 2(σ +σi) .Note that the erf function is applied to every component of the µ −µ i √ 2(σ +σi) vector individually  $b6 :f i = f i f i 7: end for 8: f = [f T 1 f T 2 . . . f T K ] 9: f = f f 10: Output:The new representation f ∈ R d×K sets, and use the Hellinger's distance or the Rényi-α divergence to measure the distance between two distributions. This method, however, suffers from one major limitation. Non-parametric kernel estimation is very time consuming, which took 3.3 days in a subset of the Scene 15 dataset, a fact that renders it impractical for large problems. As a direct comparison, D3 only requires less than 2 minutes.

The pipeline using d DT V for visual recognition
We assume that an image or video Y is represented as a bag of instance feature vectors Y = {y 1 , y 2 , . . .}, where each y i ∈ R d . The instance vectors are usually extracted as dense SIFT vectors or deep learning (CNN) features for images, or dense trajectory features or deep learning features for videos, or other representations that use a set of vectors to represent an entity.The pipeline to use d DT V to generate image or video representation follows two steps.• Dictionary generation. For simplicity and computational efficiency, we collect a large set of instance vectors from the training set, and then use the k-means algorithm to generate a dictionary that partitions the space of instance vectors into K regions. We compute the mean and standard deviation of the instance vectors inside cluster k as µ k and σ k for all 1 ≤ k ≤ K. Values in the standard deviation vector σ k is computed for every dimension independently; • Visual representation. Given an image or video Y, we use Algorithm 1 to convert it to a vector representation.Note that since we normalize every f i in Algorithm 1, the constant factor ('2') in Eq. 19 is not necessary and is thus omitted.In Algorithm 1, we use the k-means algorithm to gen-erate a visual codebook, and an instance vector is hardassigned to one visual code word. A GMM model can also be used as a soft codebook, similar to what is performed in FV. However, a GMM has higher costs in generating both the dictionary and visual representation. Thus, we use kmeans to generate a codebook in D3. Then, D3 and VLAD have very similar frameworks, and it is interesting to compare D3 with both VLAD and FV.

Efficiency and hybrid representation
Since the error function implementation is efficient, the computational cost of D3 is roughly the same as that of VLAD, which is much more efficient than the FV method. The evaluation in  #b21  showed that the time for VLAD is only less than 5% of that of FV. Thus, a visual representation using D3 is efficient to compute.It is also worth noting that although D3 and FV both used first-and second-order statistics of an image or video Y and compare these statistics with those computed from the training set, they use these statistics in very different ways. Thus, different information (discriminative vs. generative) are extracted by D3 and FV. By computing the D3 and FV representation separately and then concatenate them together to form a hybrid one, we can get higher recognition accuracy than both D3 and FV, as will be shown in Sec. 3. Suppose we form a dK dimensional D3 vector and a dK dimensional FV vector, the hybrid representation will be 2dK dimensional. However, its computational time will be only roughly half of that of forming a 2dK dimensional FV representation.A final note is about higher order VLAD. VLAD only uses first-order statistics (mean) of the set of instance vectors. In  #b21 , higher-order statistics (variance and skewness) are added to effectively improve VLAD. Since this method will triple the number of dimensions of VLAD (with the same K) and its accuracy is not as high as FV, we will not empirically compare D3 with this method in this paper. However, because D3 does not specify how a codebook is generated, the supervised codebook generation method of  #b21  can be adopted to further improve D3 in the future.

Experimental Results
To compare the representations fairly, we compare them using the same number of dimensions. For example, the following setups will be compared to each other.• D3 (or VLAD) with K 1 = 256 visual words; the representation has dK 1 dimensions; • FV with K 2 = 128 components (2dK 2 = dK 1 ); • A mixture of D3 and FV with K 3 = 128 in D3 and K 4 = 64 in FV (dK 3 + 2dK 4 = 2dK 2 = dK 1 ). We will use D3's K size to indicate the size of all the above setups (i.e., K = 256 in this example). 

Why use D3?
We first study the properties of the proposed D3 representation, and shed some lights on why it is an effective way to encode the distance between two sets of dense SIFT instance vectors.Using the training images of the Scene 15 dataset  #b17  and dense SIFT features (with step size 4), we compare the per-dimensional discriminative power of these two representations (D3 and VLAD).Suppose X is the D3 or VLAD representation of a set of images with corresponding image labels l, whose i-th dimension form a vector x :i . It is natural to measure the discriminative power of the i-th dimension by computing the mutual information between x :i and l, i.e., MI(x :i , l)  #b32 . We use the 2-bit method in  #b32  to quantize x :i and compute the mutual information. The distribution of all dimension's MI values are shown in Fig. 3. Fig. 3a shows the MI's quantile values. For example, when the x-axis is 0.5, the D3 and VLAD curve has value 4.8292 and 4.7966, meaning that the median of D3's MI value is above the of VLAD's by 0.0326. Similarly, when the y-axis is 4.8282, the D3 and VLAD curves has corresponding quantile (x-axis) values 0.5 and 0.3888, meaning that 50% of D3's MI value is above 0.4282, but only 38.88% of VLAD's dimensions reaches this discriminative power. Since the D3 (red solid) curve is almost consistently above the VLAD (dashed black) curve, D3's dimensions have higher discriminative power than VLAD's. Fig. 3b shows the frequencies of dimensions that have the highest MI values. Although VLAD has a few dimensions that have higher MI values than D3, D3 obviously have many more discriminative dimensions. The total number of dimensions with MI values > 5.25 are 2020 and 1544 for D3 and VLAD, respectively, a 30.8% advantage for D3. Since every single dimension is too weak to classify the image, it is more important to have many dimensions with good discriminative powers than having few only slightly more discriminative ones.

Action recognition results
We first show experimental results for action recognition. A set of improved trajectory features (ITF)  #b27  are extracted and then converted to D3, VLAD, FV, and two hybrid representations (D3+FV and VLAD+FV). The default parameters are used to extract ITF features.We experimented on three datasets: UCF 101  #b26 , HMDB 51  #b15  and Youtube  #b19 . For UCF 101, the three splits of train and test videos in  #b13  are used and we report the average accuracy. This dataset has 13320 videos and 101 action categories. The HMDB 51 dataset has 51 actions in 6766 clips. We use the original (not stabilized) videos and follow  #b15  to report average accuracy of its 3 predefined splits of training / testing videos. Youtube is a small scale dataset with 11 action types. There are 25 groups in each action category and 4 videos are used in each group. Following the original protocol, we report the average of the 25-fold leave one group cross validation accuracy rates. Results on these datasets are reported in Table 1. We summarize the experimental results into the following observations.D3 is better than VLAD in almost all cases. In the 12 comparisons between D3 and VLAD, D3 wins in 11 cases. D3 often has a margin even if it use half of number of dimensions of VLAD (e.g., D3 K = 128 vs. VLAD K = 256). It shows that the D3 representation is effective in capturing useful information for classification.D3 bridges the gap between VLAD and FV. In practice we often see that FV has higher accuracy than VLAD, but also much higher computational costs. D3 has roughly the same speed as VLAD, but its accuracy is close to that of FV. Compared to VLAD whose accuracies are usually 2-3% lower than FV, D3 has much closer accuracy rates to FV. On average, D3 is 1% worse than FV. On the Youtube dataset D3 is better than FV (91.55% vs. 91.00%). Given the computational benefits of D3, it can act as an attractive alternative for FV.The hybrid D3 / FV representation (nearly) consistently outperforms all other methods. We show that the hybrid methods are the best performers in Table 1. The D3+FV representation is especially effective: it is the winner in 8 out of 9 cases. With K = 128 in the Youtube set being the only exception, D3+FV consistently beats other methods, Table 1. Action recognition accuracy (%) comparisons. Note that the results in one column are compared with the same number of dimensions in the representations. For example, the column K=256 means that K = 256 for D3 and VLAD, K = 128 for FV, and in the hybrid representation, K = 128 for D3 or VLAD combined with K = 64 for FV. Note that K = 64 results for the hybrid representation is not presented. including FV and VLAD+FV. Two points are worth pointing out. First, the success of D3+FV shows that the information encoded in D3 and FV, although both used first and second order statistics of the two distributions, are complementary to each other. The hybrid of these two outperforms both D3 and FV. Since the running time of D3+FV is only roughly half of that of FV, D3+FV is attractive in both speed and accuracy. Second, VLAD+FV is obviously inferior to D3+FV. Its accuracy is very similar to that of FV, but lower than D3+FV in most cases.

Image recognition results
Now we test how D3 (and the comparison methods) work with instance vectors that are extracted by state-ofthe-art deep learning methods. To extract instance vectors, we use the DSP (deep spatial pyramid) method  #b0 , which spatially integrates deep fully convolutional networks. A set of instance vectors are efficiently extracted, each of which corresponds to a spatial region (i.e., receptive field) in the original image. The CNN model we use is imagenet-vggverydeep-16 in  #b25  till the last convolutional layer, and the input image is resized such that its shortest edge is no smaller than 314 pixels, and its longest edge is no larger than 1120 pixels. Six spatial regions are used, corresponding to the level 1 and 0 regions in  #b28 .  #b0  finds that FV or VLAD usually achieves optimal performance with very small K sizes in DSP. Hence, we test K ∈ {4, 8}.The following image datasets are used. • Scene 15  #b17 . It contains 15 categories of scene images. We use 100 training images per category, the rest are for testing. • MIT indoor 67  #b23 . It has 15620 images in 67 indoor scene types. We use the train/test split provided in  #b23 . • Caltech 101  #b5 . It consists of 9K images in 101 object categories plus a background category. We train on 30 and test on 50 images per category. • Caltech 256  #b8 . It is a superset of Caltech 101, with 31K images, and 256 object plus 1 background categories. We train on 60 images per category, the rest for testing.• SUN 397  #b29 . It is a large scale scene recognition dataset, with 397 categories and at least 100 images per category. We use the first 3 train/test splits of  #b29 . Except for the indoor and SUN datasets, we run 3 random train/test splits in each dataset. Average accuracy rates on these datasets are reported in Table 2. As shown by the standard deviation numbers in Table 2, the deep learning instance vectors are stable and the standard deviations are small in most cases. Thus, we tested with 3 random train/test splits instead of more (e.g., 5 or 10).D3 and D3+FV have shown excellent results when combining with instance vectors extracted by deep nets. We have the following key observations from Table 2, which mostly coincides well with the observations concerning action recognition in Table 1. The last row in Table 2 shows the current state-of-the-art recognition accuracy in the literature, which are achieved by various systems that depend on deep learning using the same evaluation protocol.D3 is slightly better than FV. D3 is better than FV in 3 datasets (Scene 15, indoor 67 and SUN 397), but worse than FV in the two Caltech datasets. It is worth noting that D3's accuracy is higher than that of FV by a larger margin in indoor 67 (1-2%) and SUN 397 (1.5-2.2%), while FV is only higher than D3 by 0.3-0.7% in the Caltech 101 and 256 datasets. Another important observation is that the win/loss are consistent among the train/test splits. In other words, if D3 wins (loses) in one dataset, it wins (loses) consistently in all three splits.  $b1  Thus, the CNN instance vectors lead to stable comparison results, and we believe 3 train/test splits are enough to compare these algorithms.VLAD is better than both D3 and FV, but D3 bridges the gap between VLAD and FV. Although FV usually outperforms VLAD in image classification and retrieval using dense SIFT features and in the action recognition results of Table 1, a reversed trend is shown in Table 2 using CNN instance vectors. VLAD is almost consistently better than FV, up to 3.2% higher in the SUN 397 dataset. The accuracy of D3, however, is much closer to that of VLAD than FV's accuracy. D3 is usually 0.3%-0.6% lower than VLAD, with only two cases up to 1.1% (K = 8 in Caltech 256 and SUN   #b33  77.56  #b7  93.42±0.50  #b9  77.61±0.12  #b1  53.86±0.21  #b33  397).The hybrid methods are all effective, and D3+FV is the overall winning method. The second part of Table 2 presents results of hybrid methods. Beyond D3+FV and VLAD+FV, we also add the results of D3+VLAD, because VLAD is the winner in the first part of Table 2. Excluding the MIT indoor 67 dataset, obviously all hybrid methods have higher accuracy rates than every individual method. Among the hybrid methods, D3+FV is the overall winner again. It has the highest accuracy in 6 cases, while VLAD+FV and D3+VLAD has only one each. When comparing D3+FV with D3, FV or VLAD in detail, this hybrid method has higher accuracy than any single method in all train/test splits in all 36 comparisons (4 datasets excluding the indoor 67 dataset × 3 individual representations × 3 train/test splits). The MIT indoor 67 dataset is a special case, where VLAD is better than all other methods. We are not yet clear what characteristic of this dataset makes it particularly suitable for VLAD.The fact that D3 is in general inferior to VLAD in this setup also indicates that CNN instance vectors have different characteristics than the dense SIFT vectors (cf. Sec. 3.1), for which VLAD is inferior to D3. This might be caused by the fact that D3 and VLAD used very small K values (K = 4 or 8) with CNN instance vectors, compared to K ≥ 64 in Sec. 3.1. Hence, both methods have much fewer number of dimensions now, and a few VLAD dimensions with highest discriminative powers may lead to better performance than D3. We will leave a careful, more detailed analysis of this observation to future work.Significantly higher accuracy than state-of-the-art, especially in those difficult datasets. DSP  #b0  (with D3 or other individual representation methods) is a strong baseline, which already outperforms previous state-of-the-art in the literature (shown in the last row of Table 2). The hybrid method D3+FV leads to even better performance, e.g., its accuracy is 7.2% higher than  #b1  for the Caltech 256 dataset, 2 and 7.6% higher than the place deep model of  #b33  for SUN 397.  $b2   #b25  reported an average recall rate of 86.2% for Caltech 256. DSP's average recall is 89.12% and D3+FV is 90.25% (K = 4).

Discussions
Overall, the proposed D3 representation method has the following properties: • D3 is discriminative, efficient, and stable. D3 is not the individual representation method that leads to the highest accuracy. FV is the best in our action recognition experiments with ITF instance vectors, while VLAD is the best in our image categorization experiments using CNN features. It is, however, the most stable one. It is only slightly worse than FV in action recognition and slightly worse than VLAD in image categorization. Although VLAD is outperformed by FV by a large margin in action recognition (Table 1) and vice versa for image categorization (Table 2), D3 has stably achieved high accuracy rates. D3 is also as efficient as VLAD, and is much faster than the FV method; • D3+FV is the overall winning method. Using the same number of dimensions for all individual and hybrid methods, D3+VLAD has shown the best performance, which indicates that the information encoded by D3 and FV form a synergy. Since the FV part of D3 only uses half the number of Gaussian components than that in individual FV, D3+FV is still more efficient than FV alone. In short, D3 and D3+FV are effective and efficient in encoding entities that are represented as sets of instance vectors.

Conclusions and Future Work
We proposed the Discriminative Distribution Distance (D3) method to encode an entity (which comprises of a set of instance vectors) into a vector representation. Unlike existing methods such as FV, VLAD or Super Vectors that are designed from a generative perspective, D3 is based on discriminative ideas. We proposed to use directional distances to measure how two distributions (sets of vectors) are different with each other, and proposed to use the robust MPM classifier to robustly estimate this distance.These discriminative design choices lead to excellent classification accuracy of the proposed D3 representation, which are verified by extensive experiments on action and image categorization datasets. D3 is also efficient, and the hybrid D3+FV representation has achieved the best results among compared individual and hybrid methods.In the same spirit as D3, we plan to combine D3 and FV in a principled way, which will add discriminative perspectives to FV and will further reduce the computational cost of the hybrid representation using D3+FV. We will further study how the benefits of VLAD can be utilized (e.g., when CNN instance vectors are used).

Footnote
1 : Detailed per-split accuracy numbers are omitted.