Tractable Learning and Inference for Large-Scale Probabilistic Boolean Networks

Abstract
Probabilistic Boolean Networks (PBNs) have been previously proposed so as to gain insights into complex dynamical systems. However, identification of large networks and of the underlying discrete Markov Chain which describes their temporal evolution, still remains a challenge. In this paper, we introduce an equivalent representation for the PBN, the Stochastic Conjunctive Normal Form (SCNF), which paves the way to a scalable learning algorithm and helps predict longrun dynamic behavior of large-scale systems. Moreover, SCNF allows its efficient sampling so as to statistically infer multistep transition probabilities which can provide knowledge on the activity levels of individual nodes in the long run. Index Terms-Probabilistic Boolean Network (PBN), Markov Chain, dynamical system, network identification, statistical inference, learning systems.

I. INTRODUCTION
M ANY complex systems that have recently received intense research attention by the scientific community fall in the broad category of dynamical systems  #b0 -  #b5 . A dynamical system is typically governed by rules that describe the time dependence of a set of variables. Nonlinear (Linear) dynamical systems which are, in general, described by a system of nonlinear (linear) differential equations, can often be reconstructed and analyzed using a qualitative or semi-qualitative estimate of the behavior of its state variables. In situations where the exact values of the states are not required, estimating the behavior of such systems using binary logical models can be extremely fast when compared to learning and simulating complicated systems of differential equations using numerical methods.Boolean Networks (BNs), originally proposed by Kauffman  #b6 ,  #b7 , constitute a very-well studied qualitative modeling framework, and has been used for multifarious applications such as genetic regulatory networks  #b6 ,  #b8 , neural networks  #b9 ,  #b10 , BN robots  #b11 , econometrics  #b12  among others. The Probabilistic Boolean Network (PBN) paradigm was introduced by Shmulevich  #b13  as a semiqualitative extension of BN for an alternative representation of gene regulatory networks  #b14 ; it combines rule-based modeling with uncertainty principles. PBNs have been deployed in a spectrum of applications similar to those of BNs  #b15 ,  #b16 .For the past decade, BNs and PBNs have been the object of extensive studies. Past theoretical studies focus Ms. Apostolopoulou is with the Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, 15213 (e-mail: iapos-tol@andrew.cmu.edu).Ms. Marculescu is with the department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, 15213 (e-mail: dianam@andrew.cmu.edu). on learning  #b17 -  #b19 , steady-state distribution analysis  #b20 , relations to Dynamic Bayesian networks  #b21 ,  #b22 , and Markov Chains  #b23 ,  #b13 ,  #b24 . Recent research efforts have been primarily focused on control  #b25 -  #b33 , synchronization  #b34 -  #b37 , steady state analysis  #b38 ,  #b39  and network identification  #b40 -  #b44 . Identification of the long-run behavior of dynamical systems is of utter importance since it usually conveys domain implications. For instance, the characterization of the network's long-run dynamics plays a crucial role in treatment of various human cancers such as breast cancer, and leukemia  #b45 -  #b47 . Furthermore, it is possible to control certain nodes in a network, such that the whole system can evolve according to a desirable probability distribution  #b48 . However, automated system reconstruction faces main challenges. These hindrances arise mostly from the exponential growth of possible model configurations and the limited observations under changing initial conditions.Our contributions revolve around model learning and dynamics inference for PBNs under an alternative representation. This formulation leads to an Approximate Maximum Likelihood Estimation (AMLE) method which renders the tackling of both problems at a large scale feasible. To the best of our knowledge, no prior research has managed to address successfully the reconstruction of the state evolution of general PBNs in the order of 1000 nodes (or equivalently a Markov Chain with 2 1000 states), as opposed to other exact likelihood  #b49  and information-theoretic approaches  #b50  which can handle only deterministic and small-scale dynamics. Thus, our method provides a new framework for prediction of temporal dynamics generated from large networks, a problem crucial to modeling gene regulation, cell signaling, and other complex mechanisms.

II. RELATED WORK
As already been pointed out in prior work, the dynamical behavior of a PBN can be described by Markov Chain theory and thus, tools developed for that can be applied to the analysis of PBNs. A Maximum Likelihood Estimation (MLE) approach for estimating the transition probability matrix of the Markov Chain (and the associated PBN) is presented in  #b23 , along with certain theoretical guarantees. However, as expected, straightforward transition probabilistic representations require the estimation of 2 N × (2 N − 1) probabilities, where N is the number of nodes in the network. Therefore, such representations demand an unrealistic amount of data which hinders their adoption in real-world scenarios. Other studies  #b18  also use Markov chains for solving the problem of predicting the system dynamics. However, the state probability of individual nodes is represented as a linear combination of N 2 × 2 (in the case of binary logic) transition probability matrices, which pertain to the influence of only one node on the dynamics of the targeted node. Hence, while the number of parameters that have to be estimated is reduced to O(N 2 ), the problem can still be considered quite complex for large N . Moreover, the equivalent PBN can be learned as a 2 N × 2 N transition probability matrix, from which it is hard to extract the underlying logic rules. This size can be prohibitively large for computing multistep transition probabilities and for steady-state analysis which can provide valuable information for developing intervention-oriented approaches  #b51 ,  #b52 .Other work  #b19  expresses the transition probability matrix as a sum of Boolean Network matrices and estimates the selection probabilities of the rules, while assuming that the transition probability matrix and the logic portion of the PBN are known. The learning procedure in  #b17  attempts to learn both the logic portion (in the form of a truth table) and the parametric portion (in the form of switching, selection, and perturbation probabilities). However, the amount of temporal data needed for estimating the parametric part, which is crucial for dynamics recovery, is huge. Indeed, only results for network connectivity are reported, and only for network sizes of up to 7 variables. Moreover, a maximum, much smaller than N number of different interacting nodes/variables (node in-degree) in the Boolean functions known a priori is assumed, while, as already mentioned, the tabular representation of the learned PBN is unsuitable for further analysis. The approach described in  #b53  relies on prior domain knowledge in terms of the biological pathways of the network that has to be learned as a PBN, and it can cope with only up to 7 nodes. Similarly, the software tools described in  #b55 ,  #b56  require prior knowledge on the possible interactions between the nodes. The work in  #b41  and  #b40  offers sample complexity guarantees for PBNs and threshold PBNs respectively, both of which only pertain to the discovery of the logic part. Moreover, conditions on the number of the constituent boolean functions (which come only in the form of pairs or triplets), the logic structure of the rules (only AND/OR boolean functions), and fixed node in-degrees are necessary for the derivation of these results.The main contributions of this article include: 1) We propose the SCNF (Stochastic Conjunctive Normal Form) network as an alternative representation for PBNs. 2) We suggest a scalable and accurate learning algorithm, which manages to recover both the logic and the parametric portion of the underlying PBN from a sufficiently small number of observed system transitions, without making any prior assumptions on the structure of the logic formulas that have to be learned, and without using any prior domain knowledge. 3) We show that the SCNF model is amenable to efficient stochastic simulation, and can therefore be used to infer approximately up to 100-step system transition probabilities.The formal definitions of BNs and PBNs are given in Section III. In Section IV, we present our model definition and show its equivalence to PBNs. In Section V, we provide our reconstruction algorithm. In Section VI, we present experimental results which demonstrate accurate dynamic prediction of new temporal trajectories and efficient transition probabilities estimation. Finally, Section VII provides the reader with examples which illustrate the SCNF definition and learning.

III. PRELIMINARIES
A. Boolean Networks (BNs) Definition 1. A Boolean Network (BN) is a directed network with N binary-valued nodes V = {x 1 , x 2 , . . . , x N }. Each node x i has N (i ) parent nodes V (i ) = x (i ) 1 , x (i ) 2 , . . . , x (i ) N (i ) , where x (i ) j ∈ V . Let s t (x) ∈ {F(i ) = f (i ) x (i ) 1 , x (i ) 2 , . . . , x (i ) N (i ) . The state of the whole network at time step t is represented by the vector S t = s (1) t , s (2) t , . . . , s (N ) t ∈ {F al se, Tr ue} N . Let F = f (1) , f (2) , .. . , f (N ) be the network function. The states of all nodes are updated synchronously (at the same time).Then the dynamics of the BN is given byS t +1 = F(S t ), where s (i ) t +1 = f (i ) s t x (i ) 1 , . . . , s t x (i ) N (i ) for i = 1, 2, . . . , N . We denote the BN by G(V, F).Note that the network function is homogeneous in time, meaning that it is time invariant. Therefore, we can drop the time quantifier and the dynamics equation can be further simplified to S = F(S) (with S representing the next state of the system and S the current state). The initial state (or initial condition) S 0 and the network function F fully determine the evolution of the BN: S 0 → S 1 → · · · → S t → · · · →.

B. Probabilistic Boolean Networks (PBNs)
A Probabilistic Boolean Network (PBN) is an extension of a BN: Definition 2. A Probabilistic Boolean Network (PBN) is a directed network with N binary-valued nodes V = {x 1 , x 2 , . . . , x N }. Each node x i has N (i ) parent nodes V (i ) = x (i ) 1 , x (i ) 2 , . . . , x (i ) N (i ) . Let s t (x) ∈ F(i ) = p (i ) 1 , p (i ) 2 , . . . , p (i ) M (i ) ∈ [0, 1] M (i ) , such that M (i ) j =1 p (i ) j = 1, and p (i ) j is the probability that the function f (i ) j will be selected. Each f (i ) j has N (i ) j variables, f (i ) j : {F al se, Tr ue} N (i ) j → {F al se, Tr ue} such that it satisfies: f (i ) j = f (i ) j x (i ) j ,1 , x (i ) j ,2 , . . . , x (i ) j ,N (i ) j . If V (i ) j is the set of variables of the rule f (i ) j , i.e, V (i ) j = x (i ) j ,1 , x (i ) j ,2 , . . . , x (i ) j ,N (i ) j ,it holds that the set of the N (i ) parents of node x i will beV (i ) = M (i ) j =1 V (i ) j , with V (i ) ⊆ V .The state of the whole network at time step t is represented by the vector S t =  #b0  , f  #b1  , . . . , f (N ) be the network function, P = p  #b0  , . . . , p (N ) andF = (F, P).s (1) t , s (2) t , . . . , s (N ) t ∈ {F al se, Tr ue} N . Let F = fThe states of all nodes are updated synchronously (at the same time) and independently. Then the dynamics of the PBN is given by Note that the categorical distributions are mutually independent, time invariant and independent of the past. Given the above definition, there are M = N i =1 M (i ) constituent networks. The j -th network is described byS t +1 =F(S t ), where s (i ) t +1 = f (i ) j s t x (i ) j ,1 , . . . , s t x (i ) j ,N (i )F j = f (1) j (1) , f (2) j (2) , . . . , f (N ) j (N ) , with j (i ) ∈ {1, 2, .. . , M (i ) } denoting the boolean function selected for node i in the Boolean Network j , and is selected with probability p (F j ) = N i =1 p (i ) j (i ) .Since the selections of boolean rules at time t occur simultaneously, independently of the other nodes and of the states in the past, S t −1 , S t −2 , . . . , a PBN generates a discretetime, homogeneous, 2 N -state Markov chain, which can be fully characterized by a transition probability matrix P P P ∈ R 2 N ×2 N , where the entry P P P(µ,λ), with λ, µ ∈ {0, 1, . . . , 2 N − 1} represents the probability of moving from state µ to state λ , by considering λ,µ ∈ {F al se, Tr ue} N as the boolean representation of the integers λ, µ. The matrix P P P can be factorized asP = M j =1 p(F j )A A A j , where A A A j ∈ R 2 N ×2 N is the deterministic transition matrix of the Boolean Network F j .This decomposition yields O(M N 2 2N ) complexity for the computation of P P P  #b57 . A graphical representation of P P P for a PBN is the state transition diagram. The reader may refer to Figure 6 for a numerical example of a state transition diagram while the formal definition is given below: pointing from a state µ ∈ V s to its successor state λ ∈ V s . Its weight is the probability of moving from µ to λ, as dictated by F and P of the PBN.

C. Problem Statement
Assume that we observe D, a list of R boolean time series of potentially varying lengths n i , for i = 1, 2, . . . , R:D = S 1 t 1 0 , S 1 t 1 1 , . . . , S 1 t 1 n 1 , . . . , S R t R 0 , S R t R 1 , . . . , S R t R n R ,(1)where S r t r k ∈ {F al se, Tr ue} N . D can be transformed to a list (with repetitions of elements) of ordered pairs (S, S ) ∈ L which represent the transitions from the previous state S to the next state S of the dynamical system:L R r =1 S r t r k , S r t r k+1 n r −1 k=0 .(The reader may refer to Example B in Section IX for a numerical example of the structures D and L. The goal is to infer the logical dynamical equations of the system from the observed data D, which best explain its behavior and are capable of predicting its evolution under different initial states of its nodes. This reverse engineering process relies on the acquisition of sufficient data for the construction of an accurate model. However, it is not always feasible to capture sufficient data and it can also sometimes be very expensive.In this article, we propose the Stochastic Conjunctive Normal Form Network (SCNFN) which is equivalent to a PBN but effectively intertwines logic rules and probabilities. The SCNFN learning results in a statistical process which can be viewed as a "logic" regression problem; it estimates through boolean relationships the entries of the transition probability matrix (dependent variable) while a boolean representation is used for the independent variables (the states µ,λ). This process entails a significantly smaller number of parameters that have to be estimated. Therefore, in contrast to the methodologies introduced in prior work, SCNFs can be learned for large networks, from small training datasets.

IV. PROPOSED MODEL
In this section, we introduce the Stochastic Conjunctive Normal Form Network (SCNFN) (Subsection IV.A) and show that it is equivalent to the Probabilistic Boolean Network (PBN) (Subsection IV.B). In the SCNF network, the rule that corresponds to each node (a SCNF formula) consists of a conjunction (logical AND) of multiple disjunctions (logical OR) of boolean variables. Stochasticity is induced at the level of each separate disjunction which is associated with a probability of being activated (evaluated), and in which one literal is actually a Bernoulli random variable. 

A. Model Definition
Definition 4. A Stochastic Conjunctive Normal Form Net- work (SCNFN) is a directed network with N binary- valued nodes V = {x 1 , x 2 , . . . , x N }. Each node x i has N (i ) parent nodes V (i ) = x (i ) 1 , x (i ) 2 , . . . , x (i ) N (i ) . Let s t (x) ∈ F al se, Tr ue be the state of node x at time t . Define s (i ) t s t (x iΨ (i ) = Ψ (i ) x (i ) 1 , x (i ) 2 , . . . , x (i ) N (i ) ; p (i ) with p (i ) = [p (i ) 1 , p (i ) 2 , . . . , p (i ) M (i ) ] ∈ [0, 1] N and V (i ) = x (i ) 1 , x (i ) 2 , . . . , x (i ) N (i ) ⊆ V , the sets of the N (i ) parents of node x i . Ψ (i ) x (i ) 1 , x (i ) 2 , . . . , x (i ) N (i ) ; p (i ) = ψ (i ) 1 x (i ) 1,1 , x (i ) 1,2 , . . . , x (i ) 1,N (i ) 1 ; p (i ) 1 ∧ψ (i ) 2 x (i ) 2,1 , x (i ) 2,2 , . . . , x (i ) 2,N (i ) 2 ; p (i ) 2 . . . ∧ψ (i ) M (i ) x (i ) M (i ) ,1 , x (i ) M (i ) ,2 , . . . , x (i ) M (i ) ,N (i ) M (i ) ; p (i ) M (i ) .(3)LetV (i ) j = x (i ) j ,1 , x (i ) j ,2 , . . . , x (i ) j ,N (i ) j be the set of the N (i ) j vari- ables of the clauseψ (i ) j . Then V (i ) = M (i ) j =1 V (i ) j , and V (i ) ⊆ V . Each clauseψ (i )j in the conjunction is defined as follows:ψ (i ) j x (i ) j ,1 , x (i ) j ,2 , . . . , x (i ) j ,N (i ) j ; p (i ) j = (4) ψ (i ) j x (i ) j ,1 , x (i ) j ,2 , . . . , x (i ) j ,N (i ) j ∨ ¬α (i ) j ,(5)where it holds that: and independently. Then, the dynamics after one system transition of the SCNFN is given by:ψ (i ) j x (i ) j ,1 , x (i ) j ,2 , . . . , x (i ) j ,N (i ) j = l x (i ) j ,1 ∨ · · · ∨ l x (i ) j ,N (i ) j ,(6)l x (i ) j ,k ∈ x (i ) j ,k , ¬x (i ) j ,k ,(7)α (i ) j ∼ B er noul l i p (i ) j ,(8)S t +1 =Ψ(S t ), s (i ) t +1 =Ψ (i ) (S t ), Ψ (i ) (S t ) =ψ (i ) 1 (S t ) ∧ψ (i ) 2 (S t ) ∧ · · · ∧ψ (i ) M (i ) (S t ), ψ (i ) j (S t ) = l s t x (i ) j ,1 ∨ l s t x (i ) j ,2 ∨ . . . l s t x (i ) j ,N (i ) j , l s t x (i ) j ,k =      s t x (i ) j ,k if l x (i ) j ,k = x (i ) j ,k ¬s t x (i ) j ,k if l x (i ) j ,k = ¬x (i ) j ,k ,(9)for i = 1, 2, . . . , N , j = 1, 2, . . . , M (i ) and k = 1, 2, . . . , N (i ) j , where ¬ refers to the logical negation. Similarly, the dynamics after k system transitions is represented as S t +k =Ψ k (S t ), where the operatorΨ k corresponds to k repetitions of the update rules in Equation  $b9 . We denote the SCNFN by G(V,Ψ).By definition, the presence of the Bernoulli random literal α (i ) j in the disjunctionψ (i ) j implies that, with probability 1−p (i ) j ,ψ (i ) j does not contribute to the logical value ofΨ (i ) . That is because:ψ (i ) j S t ; p (i ) j = Tr ue w.p. 1 − p (i ) j ψ (i ) j (S t ) w.p. p (i ) j ,(10)where "w.p." stands for "with probability". We will use upper case Greek lettersΘ (i ) ,Φ (i ) ,Ψ (i ) to represent the SCNF related to node i , and lower case Greeklettersθ (i ) j ,φ (i ) j ,ψ (i ) jto represent the j th-stochastic disjunction in the SCNF rule of node i . Finally, l (i ) j ,k refers to the k-th literal in the jth disjunction in the SCNF rule of node i . Without loss of generality, the quantifier k of literal l (i ) j ,k refers to its lexicographic order within the disjunctionψ (i ) j , such that¬x j ≺ x j , and ¬x j ≺ ¬x j , ¬x j ≺ x j , x j ≺ ¬x j , x j ≺ x j if j < j , while the quantifier j of the disjunctionψ (i ) jrefers to its lexicographic order in the SCNF ruleΨ (i ) . Note that in the rest of the paper we may equivalently represent a conjunctionΘ as a set of disjunctions, and a disjunctionθ as a set of literals. Therefore, Θ is the number of disjunctions in the SCNF ruleΘ and θ is the number of literals (excluding the Bernoulli random variable) in the stochastic disjunctionθ. Finally, we may drop the arguments in a stochastic disjunction or a SCNF which correspond to its variables, or the Bernoulli parameter in case they do not contribute to the understanding of the concepts elaborated.

B. Equivalence of SCNFN and PBN
We now prove that any SCNFN can be converted into an equivalent PBN and vice-versa. This is expected since they both represent a discrete-time homogeneous Markov Chain. This equivalence can also be viewed as the stochastic extension of the conversion of any propositional formula to conjunctive normal form (CNF).

Proposition 1. Every SCNFN G(V,Ψ) can be converted to a PBN G(V ,F).
Proof: Clearly, V = V . Recall thatF = (F, P). Fix node i . We describe the conversion of the SCNF ruleΨ (i ) to the vector f (i ) = F(i ) and the corresponding selection probabilities p (i ) = P(i ). First, assume thatΨ (i ) = Φ (i ) ∧Θ (i ) is the SCNF rule of node i decomposed in the deterministic portion Φ (i ) and the stochastic portionΘ (i ) , such that each disjunction φ (i ) j ∈ Φ (i ) is deterministic (i.e., p (i ) j = 1.0 for j = 1, 2, . . . , Φ (i ) ). By definition of the activation of the disjunctions inΘ (i ) , each rule f (i ) j = f (i ) ( j ) will correspond to the logical AND of Φ (i ) and an element of the power set P Θ (i ) (which contains all possible subsets of the stochastic disjunctions inΘ (i ) ). Let β j be the j -th element in P Θ (i ) for j = 1, 2, . . . , 2 Θ (i ) (by assuming lexicographic order of the disjunctions in β, β and β < β if |β| < |β |).Then, the logic rule f (i ) j for j = 1, 2, . . . , 2Θ (i ), is:f (i ) j = Φ (i ) ∧ B j θ (i ) 1 ∧ · · · ∧ B j θ (i ) |Θ (i ) | , B j θ (i ) z = θ (i ) z ifθ (i ) z ∈ β j Tr ue ifθ (i ) z ∉ β j ,(11)for z = 1, 2, . . . , Θ (i ) , while the corresponding selection probability p (i ) j ∈ p (i ) ( j ) will be:p (i ) j = |Θ (i ) | z=1 p (i ) z I θ (i ) z ∈β j 1 − p (i ) z I θ (i ) z ∉β j ,(12)where I the numerical indicator function which returns 1 if the condition in its argument is Tr ue, otherwise it returns 0. The above formulation constitutes a PBN.

Proposition 2. Every PBN G(V,F) can be converted to a SCNFN G(V ,Ψ).
Proof: Clearly V = V . Recall thatF = (F, P). Fix node i . We will describe the conversion of the vector f (i ) = F(i ) which contains all the M (i ) logic rules which regulate the dynamics of node i and the corresponding selection probabilitiesp (i ) = P(i ) to the equivalent SCNF ruleΨ (i ) =Ψ(i ). The SCNFΨ (i ) consists of M (i ) = 2 N stochastic disjunctions ψ (i ) j such that:Ψ (i ) =ψ (i ) 0 ∧ψ (i ) 1 ∧ · · · ∧ψ (i ) 2 N −1 .(13)Each disjunction:ψ (i ) j (x 1 , . . . , x N ; q (i ) j ) = ψ (i ) j (x 1 , . . . , x N ; q (i ) j ) ∨ α (i ) j ,(14)for j = 0, 1, . . . , 2 N −1, corresponds to each possible combination of the available literals L = {x i , ¬x i } N i =1 and can become F al se for exactly one network state. Therefore,ψ (i ) 0 = (¬x 1 ∨ · · · ∨ ¬x 2 N −1 ∨ ¬x 2 N ), ψ (i ) 1 = (¬x 1 ∨ · · · ∨ ¬x 2 N −1 ∨ x 2 N ), ψ (i ) 2 = (¬x 1 ∨ · · · ∨ x 2 N −1 ∨ ¬x 2 N ), ψ (i ) 3 = (¬x 1 ∨ · · · ∨ x 2 N −1 ∨ x 2 N ), . . . ψ (i ) 2 N −1 = (x 1 ∨ · · · ∨ x 2 N −1 ∨ x 2 N ).(15)In order to find the parameter q (i ) j of the stochastic disjunctionψ (i ) j , we should first find the unique stateλ (i )j ∈ {F al se, Tr ue} N , which can yield F al se when the disjunction, and therefore the full CNF, is evaluated. Note that q (i ) j should be equal to the probability that the SCNF Ψ (i ) will be evaluated as F al se for the state λ (i ) j , because the rest of the disjunctions will always evaluate to Tr ue (no matter what the outcome of their associated Bernoulli variables is) and, therefore, they have no effect on the evaluation of the system for the state λ (i ) j . Subsequently, we compute the probability that the PBN will be evaluated as Tr ue for the state λ (i ) j , given the constituent Boolean rules f (i ) j and their corresponding selection probabilities p (i ) j . Therefore, the parameter of each Bernoulli variable q (i ) j of the j -th stochastic disjunction, can be described by the formula:q (i ) j = 1 − M (i ) j =1 p (i ) j I f (i ) j λ (i ) j ,(16)where I is the indicator function (which returns 1 if the condition of its argument is Tr ue).

V. LEARNING OF THE SCNF NETWORK


A. Overview of the Approach
We now describe the general idea and the main components involved in learning a SCNF network. The algorithm greedily, and not optimally, maximizes the likelihood of the time series used in training. Finding the optimal solution is a problem of combinatorial complexity, a fact which prevents any exact algorithm to be applicable to large-scale structures. The reconstruction methodology consists of two parts:1) learning the logical interactions between the nodes in the system (Algorithm 3 and Algorithm 5). 2) learning the parameters of the Bernoulli random variables associated with the disjunctions discovered in the previous step (Algorithm 4). 

Algorithm 1 SCNFN-Learn


14: ReturnΨ
Each iteration of Algorithm 1 learns the SCNF clause of a node i (Algorithm 1, Line 11). Given the list L (Equation 2), the reduced list L (i ) , which holds pairs of the previous system state and node i's next state is formed, as follows:L (i ) S, s (i ) : S, S ∈ L .(17)In the preprocessing step of Algorithm 2, the structure L (i ) is parsed and the sets S (i ) F , S (i ) T , S (i ) C are formed. The set S (i )Similarly, the set S   10:Φ (i ) V (i ) d ← C N F − Log i cLear n S (i ) F , S (i ) T ∪ S (i ) C , L 11:Θ (i ) V (i ) s ← C N F − Log i cLear n S (i ) C , S (i ) T , L 12:p (i ) ← C N F − P ar amet er Lear n L (i ) , S (i ) C ,Θ (i ) 13:Ψ (i ) V (i ) d ∪ V (i ) s ; p (i ) ← Φ (i ) V (i ) d ∧Θ (i ) V (i ) s ; p (i ) 14: ReturnΨ (i )The learning of the SCNF formula (Algorithm 2) involves three steps (Lines 10, 11, 12 in Algorithm 2). Initially, the deterministic logic portion Φ should be treated as positive byΘ (i ) as well so that the concatenatedΨ (i ) = Φ (i ) ∧Θ (i ) is deterministically evaluated as True. Once the learning of the logic parts has finished, the algorithm proceeds to learning the parametric part of Θ (i ) (Line 12 in Algorithm 2). Finally, the CNF Φ (i ) and the SCNFΘ (i ) are merged into the full SCNF ruleΨ (i ) of node i (Line 13 in Algorithm 2).

B. Logic Learning
In this subsection, we will delve into the details of learning the logic rules (Algorithms 3, 5). Algorithm 3 returns a CNF rule which is F al se for all the states in H F (Line 6.i) and Tr ue for all the states in H T (Line 6.ii). This implies that each disjunction in Φ should be Tr ue for all states in H T and that for each state in H F , there should be at least one disjunction in Φ which is F al se. Each loop iteration in Algorithm 3 adds a new disjunction φ k in the currently formed conjunction Φ (Line 11). The function Di s j unc t i on −Lear n is responsible for returning a disjunction which satisfies two conditions: i) it evaluates as Tr ue all the states passed in its first argument ii) it evaluates as F al se at least one state passed in its second argument. In this way, it is guaranteed that the loop in Lines 10-15 of Algorithm 3 will not perpetually add disjunctions and therefore the condition in Line 15 will finally become Tr ue. Consequently, the completeness of Algorithm 3 is guaranteed. In case a transition in H F is evaluated as 12: H F ← H F − {S : S ∈ H F , φ k (S) = F al se} 13: Φ ← Φ ∧ φ k 14: k ← k +Ψ (i ) F (λ) ψ (i ) j :ψ (i ) j ∈Ψ (i ) ,ψ (i ) j λ = F al se .(21)Note that the omission of p (i ) j in the arguments ofψ (i ) j above implies that we do care about the value ofψ (i ) j in case α (i ) j = Tr ue.Let P (i ) F λ represent the probability that the previous state λ of the system will turn the node i to F al se in the next time step. Formally,P (i ) F (λ) P r ob Ψ (i ) λ;p (i ) = F al se .(22)It can be computed by using the inclusion-exclusion principle  #b58  on the activation of at least one disjunction iñ Ψ (i ) F :P (i ) F λ = |Ψ (i ) F (λ)| m=1 (−1) (m+1) ∀Θ jm ∈ Ψ (i ) F (λ) m m k=1 p (i ) j m (k) . (23)In the above equation, S m is the set of all subsets of the set S of cardinality m.Θ j m for j m = 1, 2, . . . ,Ψ (i ) F (λ) m, denotes the j m -th element/SCNF inΨ (i ) F (λ) m(by assuming lexicographic order of the SCNFs as before) such that:Θ j m =ψ (i ) j m (1) ∧ψ (i ) j m (2) ∧ · · · ∧ψ (i ) j m (m) ,(24)withψ (i ) j m (k) ∈Ψ (i ) F (λ) denoting the k-th stochastic disjunction in the j m -th SCNF assuming a lexicographic order of the disjunctions as before.We should also define N (i ) F (λ), which is the number of times the system state λ drives node i to a low state in the list L (i ) :   #b26  In order to derive the above formula, we may consider each N (i )L i L (i ) ; p (i ) = ∀λ∈S (i ) C P (i ) F λ N (i ) F (λ) 1−P (i ) F λ N (i ) T (λ)We can now formulate the optimization problem to find the estimate of p (i ) which maximizes the likelihood of the transitions in the training set L (i ) (or equivalently minimizes the negative log-likelihood):min p (i ) P (i ) F (λ) ∀λ∈S (i ) C − ∀λ∈S (i ) C i (λ),(29)where:i (λ) = N (i ) F (λ) log P (i ) F λ + N (i ) T (λ) log 1 − P (i ) F λ ,(30)subject to:0 ≤ p (i ) ≤ 1, p (i ) ∈ R |Θ (i ) |(31)∀λ ∈ S (i ) C : P (i ) F λ = |Ψ (i ) F (λ)| m=1 (−1) (m+1) ∀Θ jm ∈ Ψ (i ) F (λ) m m k=1 p (i ) j m (k) .(32)

Algorithm 4 CNF-ParameterLearn


13:
Form P (i ) F λ (Equation 23). 14:Form ε i (λ) (Equation 36).  $b15 :ε 2 i ← ε 2 i + ε 2 i (λ)

16:
Compute N (i ) 

19
:i ← i + i (λ)

20:


EndFor


21:
Form optimization problem OP (Equations 33-36).  $b22 :p (i ) , P (i ) F λ ∀λ∈S (i ) C ← conSol ve(OP ) 1

23: Return p (i )
However, it is very likely that the feasible region of the optimization problem as defined in Equations 29-32 is the empty set due to the constraint between the optimization variables P F Ψ (i ) , λ and p (i ) in Equation  $b32 . By relaxing this constraint, the above problem can be converted as follows (Equations 33-36):min p (i ) P (i ) F (λ) ∀λ∈S (i ) C − ∀λ∈S (i ) C i (λ) + λ ∀λ∈S (i ) C ε 2 i (λ),(33)where:λ ≥ 0 i (λ) = N (i ) F (λ) log P (i ) F λ + N (i ) T (λ) log 1 − P (i ) F λ ,(34)subject to: 1 the routine conSol ve refers to TOMLAB's conSolve solver for general, constrained, nonlinear optimization problems.0 ≤ p (i ) ≤ 1, p (i ) ∈ R |Θ (i ) | ,(35)∀λ ∈ S (i ) C :ε i (λ) = P (i ) F λ − |Ψ (i ) F (λ)| m=1 (−1) (m+1) ∀Θ jm ∈ Ψ (i ) F (λ) m m k=1 p (i ) j m (k) .(36)The aforementioned problem is not convex, therefore it has to be solved for a varying number of regularization parameters λ in order to avoid local optima. A similar relaxation of probability constraints was used in  #b59 . Our implementation uses the TOMLAB Base Module "conSolve" solver, which is standard for solving of general, constrained, nonlinear optimization problems.

D. Comments and Future Extensions
We should mention that the core of our methodology lies on the fact that the structure learning guides the parameter learning through the disjunctions learned at this step. This way, our heuristic can circumvent the combinatorial nature of the joint structure/parameter learning problem. We should remark here that only the number of different combinations of literals which can participate in the SCNF of a node is O(2 2 N )  #b18 . Of course, the structure learning during the first two steps (Algorithm 2, Lines 10, 11) is not optimal, in the sense that a different combination of disjunctions and the corresponding learned parameters could achieve larger value in Equation  $b27 . However, our simulation results verify that the solution discovered, albeit not optimal, can still achieve very good accuracy. Consequently, the small accuracy loss is very well compensated by the low computational cost (compared to the complexity of learning the globally optimal PBN which maximizes the likelihood of the timeseries).The learning algorithm can be inherently extended to incorporate prior knowledge on network structure, which as suggested in  #b60 ,  #b43  can improve the accuracy of the learning. This can be accomplished by modifying the set of the available literals L so that it only includes literals related to nodes with which it is known that a node interacts. A Temporal Boolean Network  #b61 , in which the next state of the system can depend on many states in the past, can also be learned by augmenting the set L so that it includes literals for the temporal delays.Finally, the likelihood in Equation  $b27  can be modified to express more complex dynamics which can be drawn from asynchronous PBNs or time-dependent transition probabilities, so that the parameters which describe these dependencies are also learned.

VI. EXPERIMENTAL RESULTS
In this section, we test the predictive capability of the learning algorithm presented. Before proceeding to the details of the experiments, we would like to point out that the state of the art method  #b17  manages to reconstruct the PBN of a 7-node network from an unrealistically large amount of training data. More specifically, predictive capacity emerges after training with a single temporal sequence of 20000 transitions which is much larger than the data complexity required for the reconstruction of the 10-node network with our method. Moreover, a very small maximum node indegree k = 4 is assumed, which seriously restricts the class of PBNs, especially for large N , that can be reconstructed while no full-dynamics prediction is reported. Therefore, we did not replicate their results. The library in  #b62  purports that it supports learning of a PBN, but in fact no parameters are estimated. If more than one Boolean Network satisfies the time series data, then the candidate Boolean networks are uniformly selected. This approach yields predictive capacity slightly better than chance, therefore we omitted the presentation of these results in this article. As before, a maximum node in-degree is assumed. Their implementation failed to terminate within a reasonable amount of time for k = 5 and N = 100. Since then, to the best of our knowledge, no research has been able to handle the full (both its structural and parametric part) reconstruction of a general PBN when no prior knowledge is given. On the other hand, in our experiments our method can reconstruct the 2 1000 Markov Chain generated by a 1000-node network within hours for the case with the smallest amount of training data and after roughly 2.5 days for the case with the largest amount of data.

A. Experimental Setup
In subsequent sections, we evaluate the learning and inference methodologies for a 10-node, 100-node and 1000node model. The true PBNs were generated in the following manner. We used the "generateRandomNKNetwork" function provided by the BoolNet R-tool  #b62  to randomly generate two deterministic Boolean Networks in a Disjunctive Normal Form (DNF) representation. The deterministic BNs have homogeneous topology (the number of the neighboring nodes is independent and follows a Poisson distribution with mean 4) for each case of network size, and uniform linkage (the edges between the nodes are drawn uniformly at random). The behavior of all the nodes in the resulting network is governed by two Boolean rules, which are dictated by the constituent Boolean networks. Afterwards, we randomly merged the BNs into a PBN by assigning a selection probability to each rule. The selection probability among the rules of the stochastic nodes follows a uniform distribution in [0,1].

B. Recovery of Time Homogeneous Discrete Markov Chain Dynamics
In order to obtain the time series required for the training (model learning) and testing (accuracy evaluation) of the models, we simulated the true models for a different number of initial conditions, draws for each initial state and time steps for each model size case. Table I summarizes all the cases of training sets that were considered. The final model was learned by applying 5-fold cross-validation  #b63 , where each fold contains 1/5 of the total number of the training time series. The 10-node, 100-node, and 1000-node models were evaluated on R = 10 4 different initial conditions. For each one of them M = 400 stochastic simulations were performed. Note that for the case of the 10-node network, this number of initial conditions implies exhaustive testing (>2 10 ). In all cases, the testing time series contain 1000 time points. The initial conditions of the generated (both training and testing) time series were randomly drawn.In order to assess the similarity between the true and the learned model, we considered the absolute difference error between the parameters P (i ) T λ r , k of the true and the learned modelP (i ) T λ r , k , where λ r is the vector with the r -th randomly drawn initial conditions and P (i ) T λ r , k the probability that the state of the node i will be Tr ue after k steps when the system is initially at the state λ r :P (i ) T λ r , k P r ob µ (i ) = Tr ue, µ = Ψ k (λ r ) ,(37)δ (i ) r (k) = P (i ) T λ r , k −P (i ) T λ r , k .(38)Subsequently, the absolute difference was averaged across all the nodes and all the different initial conditions:δ r (k) = N i =1 δ (i ) r (k) N ,δ(k) = R r =1δ r (k) R .(39)We also considered the standard deviation of the absolute error across the different initial conditions in order to evaluate the dependence of the performance of the model on the initial conditions of the system:σ(k) = R r =1 δ r (k) −δ(k) 2 R .(40)We are also reporting the performance of individual nodes by considering the standard deviation of the absolute error given the initial condition and then by averaging the standard deviations across all the initial conditions:σ r (k) = N i =1 δ (i ) r (k) −δ r (k) 2 N ,σ(k) = R r =1 σ r (k) R .(41)The parameters P (i ) T λ r , k andP (i ) T λ r , k were estimated as follows:P (i ) T λ r , k = M j =1 I s j k (x i |λ r ) = Tr ue M ,(42)P (i ) T λ r , k = M j =1 I ŝ j k (x i |λ r ) = Tr ue M ,(43)where s j k (x i |λ r ) andŝ j k (x i |λ r ) are the states of node i at time k for λ r initial system state for the j -th stochastic run of the true and learned model, respectively. Note that even for the true model it is computationally intractable to obtain a closed-form value of P (i ) T λ r , k for large k.The quantity N ×δ(k) can be interpreted as the average number of wrongly predicted node states at time k, regardless the initial state. Alternatively, other difference metrics such as the Kullback Leibler divergence and the Hellinger distance have been investigated. Here, we only report the absolute difference given its direct and intuitive interpretation. Also note that only the initial state is given to the network (and that the predicted states are used for the prediction of the next state). Figures 1, 2, 3 depict the predictive capacity achieved by the learned models. The reader may refer to Table I, for the description of the coloring schemes in these plots which have to do with the various sizes of the training sets generated for the learning. The dark line refers to the unavoidable error, which occurs during the simulation of the true model, due to the finite M for the estimation of P (i ) T λ r , k in Equation  $b42 . These estimated parameters were obtained from a second round of stochastic simulations of the real model. As it can be concluded, the SCNF has managed to capture, almost perfectly, the dynamical behavior of the true PBN for N = 10 and N = 100 (the black and green lines in Figures 1.a, 2.a almost overlap) with similarly low standard deviation of the misprediction rate for the randomized initial conditions used for the testing (black and green lines in Figures 1.b, 2.b). The standard deviation of the predictability of individual nodes almost fits the performance of the true model (Figures 1.c, 2.c). Especially in the case of the 10node network which is learned with only 4 or 8 time steps (blue and pink line), the resulting formula for many nodes is simply a deterministic CNF formula. This fact is responsible for the high variance in the performance of individual nodes (1.c) and in the performance of the model for different initial conditions (1.a, 1.b).In the 1000-node case, the best average number of mispredicted nodes in all time points is roughly 0.07×1000 (green line in Figure 3.a) compared to the performance of the true model, which is 0.03 × 1000 (black line in Figure  3.a). However, as it is corroborated by the blue and pink lines in Figures 3.a, 3.b, 3.c, a network learned with only 125 single-step temporal sequences can still exhibit high accuracy.We also proceeded to the reconstruction of a 10 4 -node network which achieved error roughly 0.9 for R = 100. However, due to limited computational resources we were unable to test its performance for larger R, therefore we omitted these results.

C. Transition Probabilities Estimation
We now present the results on the networks learned in order to evaluate the computational cost and convergence of the sampling scheme described above (Equation  $b43 ). In all cases of network sizes N = 10, 100, 1000, we inferred 2-step and 100-step transition probabilities (parameters)P (i )T λ r , k , for λ r ∈ {F al se, Tr ue} N , i = 1, 2, . . . , N , k = 2, 100.Note that this implies also the inference of all the 2 N , kstep transition probabilities of moving from the state λ r to the state µ = {F al se, Tr ue} N , i.e P (i ) µ|λ r , k , where:     since it holds that: Figure 6: The probability transition diagram of Example 1.P µ|λ r , k P r ob Ψ k (λ r ) = µ ,(44)P µ|λ r , k = N i =1 P (i ) T λ r , k µ (i )therefore we omitted these results. Finally, it should be emphasized that the inference procedure is inherently parallel since the disjunctions are independently activated and the logical value can be directly computed if at least one disjunction is evaluated as F al se. However, the inference time in Figure 5 refers to the sequential execution time so that it can better illustrate the computational demands of the sampling method.

VII. ILLUSTRATIVE EXAMPLES


A. A simple PBN as a SCNFN
Consider a PBN which is given by the following rules:f (A) = f (A) 1 = x 2 ∨ ¬x 1 ∧ x 2 , p (A) 1 = 0.6 f (A) 2 = x 1 ∨ x 2 , p (A) 2 = 0.4, f (B ) = x 2 ¬x 1 p 2 1 = 1.0, f (C ) = f (C ) 1 = x 1 ∧ x 3 , p (C ) 1 = 0.8 f (C ) 2 = ¬x 3 , p (C ) 2 = 0.2.(49)The computation of the transition probabilities of each node i is demonstrated in Table II. Subsequently, the probability transition diagram can be computed, as illustrated in Figure 6. The rules of the nodes x 1 , x 2 , x 3 , written in the form of SCNF and according to Table II are:Ψ (1) = (x 1 ∨ x 2 ∨ x 3 ) ∧ (x 1 ∨ x 2 ∨ ¬x 3 ∨ α (1) 1 ) ∧ (¬x 1 ∨ x 2 ∨ x 3 ∨ α (1) 2 ) ∧ (¬x 1 ∨ x 2 ∨ ¬x 3 ∨ α (1) 3 ), q (1) 1 = 0.4, q (1) 2 = 0.6, q (1) 3 = 0.6,(50)Ψ (2) = (x 1 ∨ x 2 ∨ x 3 ) ∧ (x 1 ∨ x 2 ∨¬x 3 ) ∧ (¬x 1 ∨ x 2 ∨ x 3 ) ∧ (¬x 1 ∨ x 2 ∨ ¬x 3 ) ∧ (¬x 1 ∨ ¬x 2 ∨ x 3 ) ∧ (¬x 1 ∨ ¬x 2 ∨ ¬x 3 ),(51)Ψ (3) = (x 1 ∨ x 2 ∨ x 3 ∨ α (3) 1 ) ∧ (x 1 ∨ x 2 ∨ ¬x 3 ∨ α (3) 2 ) ∧ (x 1 ∨ ¬x 2 ∨ x 3 ∨ α (3) 3 ) ∧ (x 1 ∨ ¬x 2 ∨ ¬x 3 ∨ α (3) 4 ) ∧ (¬x 1 ∨ x 2 ∨ x 3 ∨ α (3) 5 ) ∧ (¬x 1 ∨ ¬x 2 ∨ ¬x 3 ∨ α (3) 6 ), q (3) 1 = 0.8, q (3) 2 = 0.2, q (3) 3 = 0.8, q (3) 4 = 0.2, q (3) 5 = 0.2, q (3) 6 = 0.2.(52)We will now describe the conversion for the node x 1 . The other two SCNF rules can be derived in a similar fashion. The first disjunction x 1 ∨ x 2 ∨ x 3 is deterministic and always evaluated since for the state λ = [F, F, F ] both of the rules yield F al se. The disjunctions x 1 ∨ ¬x 2 ∨ x 3 , x 1 ∨ ¬x 2 ∨ ¬x 3 , ¬x 1 ∨ ¬x 2 ∨ x 3 , and ¬x 1 ∨ ¬x 2 ∨ ¬x 3 are omitted since both of the rules are evaluated as Tr ue for the states [0, 1, 0], [0, 1, 1],  #b0  #b0 0], and  #b0  #b0  #b0  accordingly. The parameter of the disjunction x 1 ∨ x 2 ∨ ¬x 3 is 0.4 since the next state of x 1 when the current state is [0, 0, 1] is Tr ue with probability 0.6 (see the second row of Table  II). Observe that the rest of the stochastic disjunctions will always be Tr ue for this state, regardless of the outcome of their associated Bernoulli variables. Afterwards, the next state of x 1 should be F al se with probability 0.6 and for current state  #b0 0,0]. This negative transition is satisfied by the disjunction ¬x 1 ∨ x 2 ∨ x 3 . Finally, the disjunction ¬x 1 ∨x 2 ∨¬x 3 , carries the information that when the current state is [1, 0, 1], x 1 goes to F al se with probability 0.4.

B. Learning a simple SCNF formula from a single time series
Consider a set D which consists of a single time series with 10 transitions provided in Table IV S (A) F = {λ 1 , λ 3 , λ 5 , λ 6 }, S (A) T = {λ 2 , λ 4 , λ 7 , λ 8 }, S (A) C = {λ 0 }.(54)Note that, the transition λ 0 is a conflict, since at time 1 and 11, the node A has state F al se when the system state is λ 0 , while at time 9, the node A has state Tr ue. Table V provides the scores of the literals for each step of the learning process. We will provide the details for the computation of the score of ¬B in the first step.  Table III. Note that λ 0 is initially in H T and once the reconstruction of the stochastic part starts at Step 4, it is moved to H F . After the inclusion of the first literal ¬B , the states λ 0 , λ 7 , λ 8 are removed from H T since the disjunction will be evaluated as Tr ue due to the presence of ¬B . On the other hand H F remains the same since ¬B does not become Tr ue for any transition contained in it. After the inclusion of the literal ¬A, all the transitions in H T are evaluated as Tr ue and the algorithm proceeds to the formation of the next disjunction. The disjunction (¬B ∨ ¬A) yields F al se Table II: Derivation of the transition probabilities for each node in the Example 1.  x 1 x 2 x 3 x 2 ∨ ¬x 1 ∧ x 3 x 1 ∨ x 2 P r ob(x 1 = T ) x 2 ∧ ¬x 1 P r ob(x 2 = T ) x 1 ∨ x 3 ¬x 3 P r ob(x 3 = T ) F F F F F 0.0 F 0.0 F T 0.2 F F T T F 0.6 F 0.0 T F 0.8 F T F T T 1.0 T 1.0 F T 0.2 F T T T T 1.0 T 1.0 T F 0.8 T F F F T 0.4 F 0.0 T T 1.0 T F T F T 0.4 F 0.0 T F 0.8 T T F T T 1.0 F 0.0 T T 1.0 T T T T T 1.0 F 0.0 T F 0.8λ 8 } 5 (¬B ∨ ¬A) ∧ (E ∨ ¬B ) ∧ (¬A {λ 0 } {λ 8 } 6 (¬B ∨ ¬A) ∧ (E ∨ ¬B ) ∧ (¬A ∨ ¬E ∨ α (A) 1 ) 7 (¬B ∨ ¬A) ∧ (E ∨ ¬B ) ∧ (¬A ∨ ¬E ∨ α (A) 1 ), p (A) 1 = 0.67C , S (A) F , S (A) T . S t A B C D E F G H I J S 0 λ 0 T F T T T F T T T F S 1 λ 1 F T T T F F F F F T S 2 λ 2 F T F T T F F F F T S 3 λ 3 T T F F T T F F F F S 4 λ 4 F T F F T T F F F F S 5 λ 5 T T F F T T F F F T S 6 λ 6 F T F F F T F F F F S 7 λ 7 F F T T T F T T T T S 8 λ 0 T F T T T F T T T F S 9 λ 8 T F T T F F T T T F S 10 λ 0 T F T T T F T T T F S 11 λ 9 F F T T F F T T T Tfor the states λ 3 , λ 5 , therefore the next disjunction should give F al se only for the states λ 1 , λ 6 . The learning of the deterministic logic part finishes when H F becomes empty. Steps 5 and 6 pertain to the learning of the stochastic logic part, which proceeds in the same way once the new sets H T and H F are formed.Step 7 in Table III, refers to the learning of the parametric part of the SCNF model. The relevant quantities are:N (A) F (λ 0 ) = 2, N (A) T (λ 0 ) = 1, Ψ (A) F (λ 0 ) = ¬A ∨ ¬E ∨ α (A) 1 , P (A) F λ 0 = p (A) 1 , ε A (λ 0 ) = P (A) F λ 0 − p (A) 1 = 0, A (λ 0 ) = 2 * l og p (A) 1 + 1 * l og 1 − p (A) 1 , p (A) 1 = 0.67.

VIII. CONCLUSION
We have introduced a novel modeling framework of Probabilistic Boolean Networks, namely the Stochastic Conjunctive Normal Form Network. We proved that both PBN and SCNFN can represent the same class of boolean relationships between the nodes of a network. The adoption of PBNs by a wide spectrum of scientific domains has stimulated an intense research effort on network identification in recent years. However, the current approaches face one or more of the following limitations: 1) Prior knowledge of the network structure and/or the node interactions is assumed so that the parametric portion of the PBN; whose contribution is critical if the dynamics generated by the PBN have to be predicted, can be estimated from observed time series data. 2) Only the candidate boolean rules which regulate the behavior of a node are learned, while the estimation of the corresponding selection probabilities is ignored. 3) Very strong assumptions are made on the structure of the boolean relationships. In most of the approaches, a maximum number k of nodes which participate in the boolean functions of the network is assumed a priori known, while for the learning to be computationally manageable by the method k has to be very small, usually 4 or 5 even for middle-sized networks (i.e 100 nodes). Additional restrictions can pertain to the complexity of the functions, i.e, the nodes can interact only through AND or OR operations. 4) Approaches which attempt to deal with both the parametric and logic part of the PBN, fail to model in a "compressed" way the transition probability matrix of the PBN and rely on the estimation of a large number of parameters. Therefore, they have unrealistic sample and runtime requirements. 5) All of the methods do not report prediction accuracy for large (roughly more than 10 node) networks.The SCNFN is a compact (in terms of the involved parameters) representation of the PBN which enables an efficient, in terms of both sample and runtime demands, learning algorithm. The learning procedure circumvents all of the above limitations by greedily identifying the node interactions and approximately maximizing the likelihood of the observed temporal sequences. Subsequently, the SCNFN turns to be an efficient modeling structure in the sense that it can be efficiently sampled/simulated in order to obtain long-run transition probabilities of the generated Markov Chain.

APPENDIX DISJUNCTION LEARNING
Each recursive call of Algorithm 5 inserts a new literal l * in the currently formed disjunction φ (see Lines 25 and 30). The inclusion of a new literal l results in turning to Tr ue some transitions in H T (this is desirable, since it reduces the number of recursive calls required for the new disjunction to satisfy the condition i in Line 7 which is guaranteed by Line 24) and in H F (this is not desirable, since it increases the probability that the condition ii in Line 7 which is guaranteed by Line 21 is not satisfied, as well as the number of disjunctions that have to be included in the SCNF clause until the condition in Line 15, Algorithm 3 is satisfied). Therefore, for each candidate literal we compute a positive, see Line 13 (negative, see Line 14) score according to the number of positive (negative) transitions that will be satisfied (invalidated) if the literal is included in the disjunction, and a normalized total score, see Line 15. The literal with the maximum normalized score is selected (Line 17). In Lines 18-20, it is scrutinized whether all available literals have non-positive score (no transition in H T will be removed after its inclusion). Note that it suffices to check only the positive score of the literal with the largest normalized score to verify this condition. If this is the case, the recursion has to stop since the current combination of literals in φ will result in an invalid disjunction (some transitions in H T may not be satisfied). Similarly, the current branch of the recursion leads to an invalid disjunction, if after the inclusion of the new literal all the transitions in H F are evaluated as Tr ue by the φ ∨ l * . If this is the case, the algorithm should continue with finding the next best literal (Line 22) by excluding l * from the set of the available literals which can be used for the formation of the remaining disjunction. In case the inclusion of l still preserves the validity of the disjunction φ ∨ l * , H T (H F ) are reduced in Line 28 (Line 29) and the recursion proceeds by removing both l * and ¬l * from the set of the available literals. If the condition in Line 31 is F al se, a valid disjunction is discovered and returned (Line 33), otherwise the algorithm backtracks (Line 32) after H T and H F are restored and l * is removed from the set of the available literals that will be considered for inclusion in the next routine call. In the worst case, the algorithm will perform exhaustive search in the space of all possible disjunctions until it discovers a valid formula. However, due to the heuristic score of literals as defined in Line 15, the number of backtracks becomes negligible and it does not affect the computational performance of the algorithm.

Footnote
1 : − P (i ) T λ r , k 1−µ (i ),(45)