Random deep neural networks are biased towards simple functions

Abstract
We prove that the binary classifiers of bit strings generated by random wide deep neural networks with ReLU activation function are biased towards simple functions. The simplicity is captured by the following two properties. For any given input bit string, the average Hamming distance of the closest input bit string with a different classification is at least n/(2π ln n), where n is the length of the string. Moreover, if the bits of the initial string are flipped randomly, the average number of flips required to change the classification grows linearly with n. These results are confirmed by numerical experiments on deep neural networks with two hidden layers, and settle the conjecture stating that random deep neural networks are biased towards simple functions. This conjecture was proposed and numerically explored in [Valle Pérez et al., ICLR 2019] to explain the unreasonably good generalization properties of deep learning algorithms. The probability distribution of the functions generated by random deep neural networks is a good choice for the prior probability distribution in the PAC-Bayesian generalization bounds. Our results constitute a fundamental step forward in the characterization of this distribution, therefore contributing to the understanding of the generalization properties of deep learning algorithms.

Introduction
The field of deep learning provides a broad family of algorithms to fit an unknown target function via a deep neural network and is having an enormous success in the fields of computer vision, machine learning and artificial intelligence  #b0  #b1  #b2  #b3  #b4 . The input of a deep learning algorithm is a training set, which is a set of inputs of the target function together with the corresponding outputs. The goal of the learning algorithm is to determine the parameters of the deep neural network that best reproduces the training set.Deep learning algorithms generalize well when trained on real-world data  #b5 : the deep neural networks that they generate usually reproduce the target function even for inputs that are not part of the training set and do not suffer from over-fitting even if the number of parameters of the network is larger than the number of elements of the training set  #b6  #b7  #b8  #b9 . A thorough theoretical understanding of this unreasonable effectiveness is still lacking. The bounds to the generalization error of learning algorithms are proven in the probably approximately correct (PAC) learning framework  #b10 . Most of these bounds depend on complexity measures such as the Vapnik-Chervonenkis dimension  #b11  #b12  or the Rademacher complexity  #b13  #b14  which are based on the worst-case analysis and are not sufficient to explain the observed effectiveness since they become void when the number of parameters is larger than the number of training samples  #b9  #b15  #b16  #b17  #b18  #b19  #b20 . A complementary approach is provided by the PAC-Bayesian generalization bounds  #b18  #b21  #b22  #b23  #b24 , which apply to nondeterministic learning algorithms. These bounds depend on the Kullback-Leibler divergence  #b25  between the probability 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. distribution of the function generated by the learning algorithm given the training set and an arbitrary prior probability distribution that is not allowed to depend on the training set: the smaller the divergence, the better the generalization properties of the algorithm. Making the right choice for the prior distribution is fundamental to obtain a nontrivial generalization bound.A good choice for the prior distribution is the probability distribution of the functions generated by deep neural networks with randomly initialized weights  #b26 . Understanding this distribution is therefore necessary to understand the generalization properties of deep learning algorithms. PAC-Bayesian generalization bounds with this prior distribution led to the proposal that the unreasonable effectiveness of deep learning algorithms arises from the fact that the functions generated by a random deep neural network are biased towards simple functions  #b26  #b27  #b28 . Since real-world functions are usually simple  #b29  #b30 , among all the functions that are compatible with a training set made of real-world data, the simple ones are more likely to be close to the target function. The conjectured bias towards simple functions has been numerically explored in  #b26 , which considered binary classifications of bit strings and showed that binary classifiers with a small Lempel-Ziv complexity  #b31  are more likely to be generated by a random deep neural network than binary classifiers with a large Lempel-Ziv complexity. However, a rigorous proof of this bias is still lacking.

Our contribution
We prove that random deep neural networks are biased towards simple functions, in the sense that a typical function generated is insensitive to large changes in the input. We consider random deep neural networks with Rectified Linear Unit (ReLU) activation function and weights and biases drawn from independent Gaussian probability distributions, and we employ such networks to implement binary classifiers of bit strings. Our main results are the following:• We prove that for n 1, where n is the length of the string, for any given input bit string the average Hamming distance of the closest bit string with a different classification is at least n/(2π ln n) (Theorem 1), where the Hamming distance between two bit strings is the number of different bits.• We prove that, if the bits of the initial string are randomly flipped, the average number of bit flips required to change the classification grows linearly with n (Theorem 2). From a heuristic argument, we find that the average required number of bit flips is at least n/4 (subsection 3.3), and simulations on deep neural networks with two hidden layers indicate a scaling of approximately n/3.By contrast, for a random binary classifier drawn from the uniform distribution over all the possible binary classifiers of strings of n 1 bits, the average Hamming distance of the closest bit string with a different classification is one, and the average number of random bit flips required to change the classification is two. Therefore, our result identifies a fundamental qualitative difference between a typical binary classifier generated by a random deep neural network and a uniformly random binary classifier.The result proves that the binary classifiers generated by random deep neural networks are simple and identifies the classifiers that are likely to be generated as the ones with the property that a large number of bits need to be flipped in order to change the classification. While all the classifiers with this property have a low Kolmogorov complexity 1 , the converse is not true. For example, the parity function has a small Kolmogorov complexity, but it is sufficient to flip just one bit of the input to change the classification, hence our result implies that it occurs with a probability exponentially small in n. Similarly, our results explain why  #b26  found that the look-up tables for the functions generated by random deep networks are typically highly compressible using the LZW algorithm  #b34 , which identifies statistical regularities, but not all functions with highly compressible look-up tables are likely to be generated.The proofs of Theorems 1 and 2 are based on the approximation of random deep neural networks as Gaussian processes, which becomes exact in the limit of infinite width  #b35  #b36  #b37  #b38  #b39  #b40  #b41  #b42  #b43  #b44  #b45  #b46 . The crucial property of random deep neural networks captured by this approximation is that the outputs generated by inputs whose Hamming distance grows sub-linearly with n become perfectly correlated in the limit n → ∞. These strong correlations are the reason why a large number of input bits need to be flipped in order to change the classification. The proof of Theorem 2 also exploits the theory of stochastic processes, and in particular the Kolmogorov continuity theorem  #b47 . We stress that for activation functions other than the ReLU, the scaling with n of both the Hamming distance of the closest bit string with a different classification and the number of random bit flips necessary to change the classification remain the same. However, the prefactor can change and can be exponentially small in the number of hidden layers.We validate all the theoretical results with numerical experiments on deep neural networks with ReLU activation function and two hidden layers. The experiments confirm the scalings Θ( n/ ln n) and Θ(n) for the Hamming distance of the closest string with a different classification and for the average random flips required to change the classification, respectively. The theoretical pre-factor 1/ √ 2π for the closest string with a different classification is confirmed within an extremely small error of 1.5%. The heuristic argument that pre-factor for the random flips is greater than 1/4 is confirmed by numerics which indicate that the pre-factor is approximately 0.33. Moreover, we explore the Hamming distance to the closest bit string with a different classification on deep neural networks trained on the MNIST database  #b48  of hand-written digits. The experiments show that the scaling Θ( n/ ln n) survives after the training of the network and that the distance of a training or test picture from the closest classification boundary is strongly correlated with its classification accuracy, i.e., the correctly classified pictures are further from the boundary than the incorrectly classified ones.

Further related works
The properties of deep neural networks with randomly initialized weights have been the subject of intensive studies  #b37  #b38  #b39  #b40  #b41  #b49  #b50  #b51 . The relation between generalization and simplicity for Boolean function was explored in  #b52 , where the authors provide numerical evidence that the generalization error is correlated with a complexity measure that they define. Ref.  #b9  explores the generalization properties of deep neural networks trained on partially random data, and finds that the generalization error correlates with the amount of randomness in the data. Based on this result, Ref.  #b27  #b53  proposed that the stochastic gradient descent employed to train the network is more likely to find the simpler functions that match the training set rather than the more complex ones. However, further studies  #b28  suggested that stochastic gradient descent is not sufficient to justify the observed generalization. The idea of a bias towards simple patterns has been applied to learning theory through the concepts of minimum description length  #b54 , Blumer algorithms  #b55  #b56  and universal induction  #b33 . Ref.  #b57  proved that the generalization error grows with the Kolmogorov complexity of the target function if the learning algorithm returns the function that has the lowest Kolmogorov complexity among all the functions compatible with the training set. The relation between generalization and complexity has been further investigated in  #b29  #b58 . The complexity of the functions generated by a deep neural networks has also been studied from the perspective of the number of linear regions  #b59  #b60  #b61  and of the curvature of the classification boundaries  #b40 . We note that the results proved here -viz., that the functions generated by random deep networks are insensitive to large changes in their inputsimplies that such functions should be simple with respect to all the measures of complexity above, but the converse is not true: not all simple functions are likely to be generated by random deep networks.

Setup and Gaussian process approximation
We consider a feed-forward deep neural network with L hidden layers, activation function τ , input in R n and output in R. The most common choice for τ is the ReLU activation function τ (x) = max(0, x). We stress that Theorems 1 and 2 do not rely on this assumption and hold for any activation function. For any x ∈ R n and l = 2, . . . , L + 1, the network is recursively defined byφ (1) (x) = W (1) x + b (1) , φ (l) (x) = W (l) τ φ (l−1) (x) + b (l) ,(1)whereφ (l) (x), b (l) ∈ R n l , W (l)is an n l × n l−1 real matrix, n 0 = n and n L+1 = 1. We put for simplicity φ = φ (L+1) , and we define ψ(x) = sign (φ(x)) for any x ∈ R n . The function ψ is a binary classifier on the set of the strings of n bits identified with the set {−1, 1} n ⊂ R n , where the classification of the string x ∈ {−1, 1} n is ψ(x) ∈ {−1, 1}. We choose this representation of the bit strings since any x ∈ {−1, 1} n has x 2 = n, and the covariance of the Gaussian process approximating the deep neural network has a significantly simpler expression if all the inputs have the same norm. Moreover, having the inputs lying on a sphere is a common assumption in the machine learning literature  #b62 .We draw each entry of each W (l) and of each b (l) from independent Gaussian distributions with zero mean and variances σ 2 w /n l−1 and σ 2 b , respectively. We employ the Gaussian process approximation of  #b40  #b41 , which consists in assuming that for any l and any x, y ∈ R n , the joint probability distribution of φ (l) (x) and φ (l) (y) is Gaussian, and φ(l) i (x) is independent from φ (l) j (y) for any i = j.This approximation is exact for l = 1 and holds for any l in the limit n 1 , . . . , n L → ∞  #b38 . Indeed, φ(l) i (x) is the sum of b (l)i , which has a Gaussian distribution, with the n l−1 terms {W(l) ij τ (φ (l−1) j (x))} n l−1 j=1 which are iid from the inductive hypothesis. Therefore if n l−1 1, from the central limit theorem φ (l)i (x) has a Gaussian distribution. We notice that for finite width, the outputs of the intermediate layers have a sub-Weibull distribution  #b63 . Our experiments in section 4 show agreement with the Gaussian approximation starting from n 100.In the Gaussian process approximation, for any x, y with x 2 = y 2 = n, the joint probability distribution of φ(x) and φ(y) is Gaussian with zero mean and covariance that depends on x, y and n only through x · y/n:E (φ(x)) = 0 , E (φ(x) φ(y)) = Q F x·y n , x 2 = y 2 = n .(2)Analogously, φ(x) is a Gaussian process with zero average and covariance given by the kernel K(x, y) = Q F x·y n . Here Q > 0 is a suitable constant and F : [−1, 1] → R is a suitable function that depend on τ , L, σ w and σ b , but not on n, x nor y. We have introduced the constant Q because it will be useful to have F satisfy F (1) = 1. We provide the expression of Q and F in terms of τ , L, σ w and σ b in Appendix A, where we also prove that for the ReLU activation function t ≤ F (t) ≤ 1.The correlations between outputs of the network generated by close inputs are captured by the behavior of F (t) for t → 1. If F (t) stays close to 1 as t departs from 1, then the outputs generated by close inputs are almost perfectly correlated and have the same classification with probability close to one. On the contrary, if F (t) drops quickly, the correlations decay and there is a nonzero probability that close inputs have different classifications. In Appendix A we prove that for the ReLU activation function we have 0 < F (1) ≤ 1 and for t → 1,F (t) = 1 − F (1) (1 − t) + O (1 − t) 3 2 ,(3)implying strong short-distance correlations.3 Theoretical results

Closest bit string with a different classification
To confirm experimentally the findings of Theorem 1, Hamming distances to the closest bit string with a different classification were calculated for randomly generated neural networks with parameters sampled from normal distributions (see subsection 4.4). This distance was calculated using a greedy search algorithm (Figure 1a). In this algorithm, the search for a differently classified bit string progressed in steps, where in each step, the most significant bit was flipped. This bit corresponded to the one that produced the largest change towards zero in the value of the output neuron when flipped. To ensure that this algorithm accurately calculated Hamming distances, we compared the results of the greedy search algorithm to those from an exact search which exhaustively searched all bit strings at specified Hamming distances for smaller networks where this exact search method was computationally feasible. Comparisons between the two algorithms in Table ?? show that outcomes from the greedy search algorithm were consistent with those from the exact search algorithm. The results from the greedy search method confirm the n/ ln n scaling of the average Hamming distance starting from n 100. The value of the pre-factor 1/ √ 2π is also confirmed with the high precision of 1.5%. Figure 1b empirically validates the linear relationship between the value of the output neuron |φ(x)| and the Hamming distance to bit strings with different classification h * n (x) expressed by  #b6 . This linear relationship was consistent with all neural networks empirically tested in our analysis. Intuitively, |φ(x)| is an indication of the confidence in classification. The linear relationship shown here implies that as the value of |φ(x)| grows, the confidence of the classification of an input strengthens, increasing the distance from that input to boundaries of different classifications. Figure 2 confirms the findings of Theorem 2, namely that the expected number of random bit flips required to reach a bit string with a different classification scales linearly with the number of input neurons. The pre-factor found by simulation is 0.33, slightly above the lower bound of 0.25 estimated from the heuristic argument. Our results show that, though the Hamming distance to the nearest classification boundary scales on average at a rate of n/ ln n, the distance to a random boundary scales linearly and more rapidly. 

Random bit flips


Heuristic argument
For a better understanding of Theorems 1 and 2, we provide a simple heuristic argument to their validity. The crucial observation is that, if one bit of the input is flipped, the change in φ is Θ(1/ √ n). Indeed, let x, y ∈ {−1, 1} n with h(x, y) = 1. From (2), φ(y) − φ(x) is a Gaussian random variable with zero average and varianceE (φ(y) − φ(x)) 2 = 2Q 1 − F 1 − 2 n 4QF (1)/n .(10)For any i, at the i-th step of the sequence of bit strings of subsection 3.2, φ changes by the Gaussian random variable φ(x (i) ) − φ(x (i+1) ), which from (10)  with zero mean and variance 4h Q F (1)/n. Recalling that E(φ(x (0) ) 2 ) = Q and that F (1) ≤ 1 for the ReLU activation function, approximately h ≈ n/(4F (1)) ≥ n/4 steps are needed in order to flip the sign of φ and hence the classification.Let us now consider the problem of the closest bit string with a different classification from a given bit string x. For any bit string y at Hamming distance one from x, φ(y) − φ(x) is a Gaussian random variable with zero mean and variance 4Q F (1)/n. We assume that these random variables are independent, and recall that the minimum among n iid normal Gaussian random variables scales as √ 2 ln n  #b64 . There are n bit strings y at Hamming distance one from x, therefore the minimum over these y of φ(y) − φ(x) is approximately − 8Q F (1) ln n/n. This is the maximum amount by which we can decrease φ flipping one bit of the input. Iterating the procedure, the maximum amount by which we can decrease φ flipping h bits is h 8Q F (1) ln n/n. Since E(φ(x (0) ) 2 ) = Q, the minimum number of bit flips required to flip the sign of φ is approximately h ≈ n/(8F (1) ln n) ≥ n/(8 ln n), where the last inequality holds for the ReLU activation function. The pre-factor 1/ √ 8 0.354 obtained with the heuristic proof above is very close to the exact pre-factor 1/ √ 2π 0.399 obtained with the formal proof in (8).

Experiments


Analysis of MNIST data
Our theoretical results hold for random, untrained deep neural networks. It is an interesting question whether trained deep neural networks exhibit similar properties for the Hamming distances to classification boundaries. Clearly some trained networks will not: a network that has been trained to return as output the final bit of the input string has Hamming distance one to the nearest classification boundary. For networks that are trained to classify noisy data, however, we expect the trained networks to exhibit relatively large Hamming distances to the nearest classification boundary. Moreover, if a 'typical' network can perform the noisy classification task, then we expect training to guide the weights to a nearby typical network that does the job, for the simple reason that networks that exhibit Θ( n/ ln n) distance to the nearest boundary and an average distance of Θ(n) to a boundary under random bit flips have much higher prior probabilities than atypical networks.To determine if our results hold for models trained on real-world data, we trained 2-layer fullyconnected neural networks to categorize whether hand-drawn digits taken from the MNIST database  #b65  are even or odd. Images of hand drawn digits were converted from their 2-dimensional format (28 by 28 pixels) into a 1-dimensional vector of 784 binary inputs. The starting 8 bit pixel values were converted to binary format by determining whether the pixel value was above or below a threshold of 25. Networks were trained to determine whether the hand-drawn digit was odd or even. All networks followed the design described in subsection 4.4. 400 Networks were trained for 20 epochs using the Adam optimizer  #b66 ; average test set accuracy of 98.8% was achieved.For these trained networks, Hamming distances to the nearest bit string with a different classification were calculated using the greedy search method outlined in subsection 4.1. These Hamming distances were evaluated for three types of bit strings: bit strings taken from the training set, bit strings taken from the test set, and randomly sampled bit strings where each bit has equal probability of 0 and 1. For the randomly sampled bit strings, the average minimum Hamming distance to a differently classified bit string is very close to the expected theoretical value of n/(2π ln n) (Figure 3a). By contrast, for bit strings taken from the test and training set, the minimum Hamming distances to a classification boundary were on average much higher than that for random bits, as should be expected: training increases the distance from the data points to the boundary of their respective classification regions and makes the network more robust to errors when classifying real-world data compared with classifying random bit strings.Furthermore, even for trained networks, a linear relationship is still observed between the absolute value of the output neuron (prior to normalization by a sigmoid activation) and the average Hamming distance to the nearest differently classified bit string (Figure 3b). Here, the slope of the linear relationship is larger for test and training set data, consistent with the expectation that training should extend the Hamming distance to classification boundaries for patterns of data found in the training set.Finally, we have explored the correlation between the distance of a training or test picture from the closest classification boundary with its classification accuracy. Figure 4 shows that the incorrectly classified pictures tend to be significantly closer to the classification boundary than the correctly classified ones: the average distances are 1.42 and 10.61, respectively, for the training set, and 2.30 and 10.47, respectively, for the test set. Therefore, our results show that the distance to the closest classification boundary is empirically correlated with the classification accuracy and with the generalization properties of the deep neural network.

Experimental apparatus and structure of neural networks
Weights for all neural networks are initialized according to a normal distribution with zero mean and variance equal to 2/n in , where n in is the number of input units in the weight tensor. No bias term is included in the neural networks. All networks consist of two fully connected hidden layers, each with n neurons (equal to number of input neurons) and activation function set to the commonly used Rectified Linear Unit (ReLU). All networks contain a single output neuron with no activation function. In the notation of section 2, this choice corresponds to σ 2 w = 2, σ 2 b = 0, n 0 = n 1 = n 2 = n and n 3 = 1, and implies F (1) = 1. Simulations were run using the python package Keras with a backend of TensorFlow  #b67 . 

Conclusions
We have proved that the binary classifiers of strings of n 1 bits generated by wide random deep neural networks with ReLU activation function are simple. The simplicity is captured by the following two properties. First, for any given input bit string the average Hamming distance of the closest input bit string with a different classification is at least n/(2π ln n). Second, if the bits of the original string are randomly flipped, the average number of bit flips needed to change the classification is at least n/4. For activation functions other than the ReLU both scalings remain the same, but the prefactor can change and can be exponentially small in the number of hidden layers.The striking consequence of our result is that the binary classifiers of strings of n 1 bits generated by a random deep neural network lie with very high probability in a subset which is an exponentially small fraction of all the possible binary classifiers. Indeed, for a uniformly random binary classifier, the average Hamming distance of the closest input bit string with a different classification is one, and the average number of bit flips required to change the classification is two. Our result constitutes a fundamental step forward in the characterization of the probability distribution of the functions generated by random deep neural networks, which is employed as prior distribution in the PAC-Bayesian generalization bounds. Therefore, our result can contribute to the understanding of the generalization properties of deep learning algorithms.Our analysis of the MNIST data suggests that, for certain types of problems, the property that many bits need to be flipped in order to change the classification survives after training the network. Both our theoretical results and our experiments are completely consistent to the empirical findings in the context of adversarial perturbations  #b68  #b69  #b70  #b71  #b72  #b73 , where the existence of inputs that are close to a correctly classified input but have the wrong classification is explored. As expected, our results show that as the size of the input grows, the average number of bits needed to be flipped to change the classification increases in absolute terms but decreases as a percentage of the total number of bits. An extension of our theoretical results to trained deep neural networks would provide a fundamental robustness result of deep neural networks with respect to adversarial perturbations, and will be the subject of future work.Moreover, our experiments on MNIST show that the distance of a picture to the closest classification boundary is correlated with its classification accuracy and thus with the generalization properties of deep neural networks, and confirm that exploring the properties of this distance is a promising route towards proving the unreasonably good generalization properties of deep neural networks.Finally, the simplicity bias proven in this paper might shed new light on the unexpected empirical property of deep learning algorithms that the optimization over the network parameters does not suffer from bad local minima, despite the huge number of parameters and the non-convexity of the function to be optimized  #b74  #b75  #b76  #b77  #b78 .

A Setup and Gaussian process approximation
We consider a feed-forward deep neural network with L hidden layers, activation function τ , input in R n and output in R. The network is recursively defined by (1) of section 2.We draw each entry of each W (l) and of each b (l) from independent Gaussian distributions with zero mean and variances σ 2 w /n l−1 and σ 2 b , respectively. This implies for any x, y ∈ R nE φ (l) (x) = 0 , E φ (l) i (x) φ (l) j (y) = δ ij G l (x, y) .(11)We determine the covariance function G l in the Gaussian process approximation of  #b40  #b41 , which consists in assuming that for any l and any x, y ∈ R n , the joint probability distribution of φ (l) (x) and φ (l) (y) is Gaussian.We start with the diagonal elements G l (x, x), which depend on x and n only through x 2 /n  #b40 .Since any x ∈ {−1, 1} n has x 2 = n, we put by simplicity for any x ∈ R n with x 2 = nG l (x, x) = Q l .(12)The constants Q l can be computed from the recursive relation  #b40  Q 1 = σ 2 w + σ 2 b , Q l = σ 2 w ∞ −∞ τ Q l−1 z 2 e − z 2 2 dz √ 2π + σ 2 b .(13)We now consider the off-diagonal elements of G l . For x 2 = y 2 = n, the correlation coefficientsC l (x, y) = G l (x, y) Q l(14)depend on x, y and n only through the combination x · y/n  #b40 . We can therefore putC l (x, y) = F l x · y n , x 2 = y 2 = n .(15)The functions F l : [−1, 1] → R satisfy by definition F l (1) = 1 and can be computed from the recursive relation  #b40  F 1 (t) = σ 2 w t + σ 2 b σ 2 w + σ 2 b , Q l F l (t) = σ 2 w R 2 τ Q l−1 z τ Q l−1 F l−1 (t)z + 1 − F l−1 (t) 2 w e − z 2 +w 2 2 dz dw 2π + σ 2 b .(16)Defining F = F L+1 and Q = Q L+1 , the covariance of the function φ generated by the deep neural network is of the form given by  #b1 .For the ReLU activation function,  #b15  simplifies to  #b79  F l (t) =Q l−1 σ 2 w Ψ (F l−1 (t)) + 2σ 2 b Q l−1 σ 2 w + 2σ 2 b ,(17)whereΨ(t) = √ 1 − t 2 + (π − arccos t) t π .(18)The function Ψ satisfies for t → 1Ψ(t) = t + O (1 − t) 3 2 .(19)Proposition 1. For the ReLU activation function, t ≤ F (t) ≤ 1 for any −1 ≤ t ≤ 1.Proof. We prove by induction that t ≤ F l (t) ≤ 1. From (16), the claim is true for l = 1. Let us assume the claim for l − 1. We haveΨ (t) = 1 − arccos t π ≥ 0 ,(20)hence Ψ is increasing. We also have Ψ (t) ≤ 1 and Ψ(1) = 1, hence Ψ(t) ≥ t. Finally, we have from  #b16  and from the inductive hypothesisF l (t) ≥ Ψ (F l−1 (t)) ≥ Ψ(t) ≥ t ,(21)and the claim for l follows.Proposition 2 (short-distance correlations). For the ReLU activation function,F (t) = 1 − F (1) (1 − t) + O (1 − t) 3 2(22)for t → 1, where F (1) is determined by the recursive relationF 1 (1) = σ 2 w σ 2 w + σ 2 b , F l (1) = Q l−1 σ 2 w Q l−1 σ 2 w + 2σ 2 b F l−1 (1) , F (1) = F L+1 (1)(23)and satisfies 0 < F (1) ≤ 1.Proof. The recursive relation  #b22  follows taking the derivative of (17) in t = 1. Eq. (23) implies 0 < F l (1) ≤ 1 for any l, hence 0 < F (1) ≤ 1.The claim in  #b21  follows if we prove by induction thatF l (t) = 1 − F l (1) (1 − t) + O (1 − t) 3 2(24)for any l. The claim is true for l = 1. Let us assume by induction (24) for l − 1. We have from (19)Ψ (F l−1 (t)) = 1 − F l−1 (1) (1 − t) + O (1 − t) 3 2 ,(25)and the claim (24) for l follows from (17) and (23). B Proof of Theorem 1 Let x, y ∈ {−1, 1} n with h(x, y) = h n . From (2) we get E(φ(x) φ(y)) = Q F (1 − 2hn n ), then E φ(y) φ(x) = Q z = F 1 − 2hn n Q z , Var φ(y) φ(x) = Q z = 1 − F 1 − 2hn n 2 Q,(26)whereΦ(t) = t −∞ e − s 2 2 ds √ 2π(28)and we have used  #b21 . Using that ln Φ(−t) = − t 2 2 − 1 2 ln(2πt 2 ) + O( 1 t 2 ) for t → ∞ we get ln P n (a, z) = − n z 2 8F (1)h n + O n hn = − z 2 √ n ln n 8F (1)a + O 4 √ n ln n .We have N n (a, z) = n h n P n (a, z) .Using that ln k! = k + 1 2 ln k − k + O(1) for k → ∞ we get ln n h n = h n ln n h n + 1 − 1 2 ln h n + O(1) = a 2 √ n ln n + a 2 n ln n ln ln n a 2 + O(ln n) ,  #b30  and the claim follows. Let ϕ : [0, 1] → R be a random function with a Gaussian probability distribution such that for any s, t ∈ [0, 1] E (ϕ(t)) = 0 , E (ϕ(s) ϕ(t)) = F (1 − 2 |s − t|) .From (32), for any s, t ∈ [0, 1], ϕ(s) − ϕ(t) is a Gaussian random variable with zero average and variance E (ϕ(s) − ϕ(t)) 2 = 2 − 2F (1 − 2 |s − t|) .Recalling that F (1) = 1, there exists > 0 such that for any 0 ≤ u ≤ 2 we have 1 − F (1 − u) ≤ (F (1) + 1)u. Hence, if |s − t| ≤ we haveE (ϕ(s) − ϕ(t)) 4 = 12 (1 − F (1 − 2 |s − t|)) 2 ≤ 48 (F (1) + 1) 2 |s − t| 2 .(34)Then, the Kolmogorov continuity theorem  #b47  implies that with probability one the function ϕ is continuous. Let t(ϕ) be the minimum 0 ≤ t ≤ 1 such that ϕ(t) = 0:t(ϕ) = min {inf {0 ≤ t ≤ 1 : ϕ(t) = 0} , 1} .(35)Since with probability one ϕ is continuous and ϕ(0) = 0, we have ϕ(t) = 0 in a neighborhood of 0, hence t(ϕ) > 0 with probability one. Therefore, the expectation value of t(ϕ) is strictly positive: t 0 = E(t(ϕ)) > 0.From (2), for any i, j = 0, . . . , n we have E(φ(x (i) )) = 0 andE φ x (i) φ x (j) = Q F 1 − 2|i−j| n .(36)Comparing with (32) we get that {φ(x (i) )} n i=0 have the same probability distribution as { √ Q ϕ( i n )} n i=0 . From the definition of t(ϕ), for any 1 ≤ i < n t(ϕ), ϕ( i n ) has the same sign as ϕ(0). Therefore, h n ≥ n t 0 , and the claim follows. Table 1 shows Hamming distances of random bit strings to the nearest differently classified bit string measured using a heuristic greedy search algorithm and an exact search algorithm. Resulting breakdowns for the two algorithms are consistent across all network input sizes tested. For each algorithm and network input size, Hamming distances to nearest differently classified bit strings from a random bit string were evaluated 1000 times with each evaluation performed on a randomly created neural network. 

D Experiments on random deep neural networks


Footnote
1 : The Kolmogorov complexity of a function is the length of the shortest program that implements the function on a Turing machine[26,33,34].