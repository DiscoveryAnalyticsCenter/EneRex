High throughput quantitative metallography for complex microstructures using deep learning: A case study in ultrahigh carbon steel

Abstract
We apply a deep convolutional neural network segmentation model to enable novel automated microstructure segmentation applications for complex microstructures typically evaluated manually and subjectively. We explore two microstructure segmentation tasks in an openly-available ultrahigh carbon steel microstructure dataset [1, 2]: segmenting cementite particles in the spheroidized matrix, and segmenting larger fields of view featuring grain boundary carbide, spheroidized particle matrix, particle-free grain boundary denuded zone, and Widmanstätten cementite. We also demonstrate how to combine these data-driven microstructure segmentation models to obtain empirical cementite particle size and denuded zone width distributions from more complex micrographs containing multiple microconstituents. The full annotated dataset is available on materialsdata.nist.gov [3] .

Introduction
Quantitative microstructure analysis is central to materials engineering and design. Traditionally this entails careful measurements of volume fractions, size distributions, and shape descriptors for familiar microstructural features such as grains and second-phase particles. These quantities are connected to theoretical and/or empirical models for materials properties, e.g. grain boundary  #b3  or particle  #b4  strengthening mechanisms. Contemporary microstructure segmentation methods rely on specialized image processing pipelines that often require expert tuning for application to a particular microstructure system. Furthermore, the microstructures accessible to quantitative analysis are limited by the use of segmentation algorithms that rely on low-level image features (intensity and connectivity constraints). In this work, we apply deep learning methods for image segmentation to complex microstructure data, with the goal of extending the reach of quantitative analysis to microstructure systems that are currently evaluated subjectively or through laborious manual annotation.Since 2012, deep learning methods  #b5  have dominated many computer vision applications 1 , including object recognition and detection, scene summarization, semantic segmentation, and depth map prediction. The success of deep learning is often attributed to the ability of convolutional neural networks (CNNs) to learn to effectively represent the hierarchical structure of visual data, composing low-level image features (edges, color gradients) into higher level features corresponding to abstract qualities of the image subject (e.g. object parts). Recently, materials scientists have begun exploring a limited set of applications of contemporary computer vision techniques for flexible and generic microstructure representation.  #b7  and  #b8  explore these techniques in the context of microstructure classification.  #b9  and  #b10  use pretrained CNN representations to study relationships between processing conditions and microstructure via dimensionality-reduction and visualization techniques.  #b11  use a CNN segmentation model to identify constituent phases in steel microstructures.In this report, we train a pixelwise CNN  #b12  to segment microstructures at a high level of abstraction, and investigate the potential for this technique to enable quantitative microstructure analyses that conventionally would require a large amount of hands-on image processing. We evaluate the feasibility of this approach on a subset of the openly available Utrahigh Carbon Steel (UHCS) microstructure dataset  #b0  #b13  #b14 . CNNs can distinguish between the four principal microconstituents in this heat-treated UHCS: proeutectoid cementite network, fields of spheroidite particles, the ferritic matrix in the particle-free denuded zone near the network, and Widmanstätten laths. We also train a network to segment individual spheroidite particles, and briefly explore automated microstructure metrology techniques enabled by this kind of powerful segmentation model. Our training data and annotations for both microstructure segmentation tasks will be publicly available through the NIST materials resource registry  #b2 .Our primary contributions are:• Establishing two novel microstructure segmentation benchmark datasets• Connecting microstructure science to the deep semantic segmentation literature• Exploring novel means of expanding contemporary quantitative microstructure measurement techniques to more complex structuresFor microstructure scientists, CNN-based microstructure segmentation tools require an initial investment in annotation and training, but can enable longer-term or larger-scale research and characterization efforts. This tradeoff is particularly attractive for its potential to enable microstructure-based material qualification by making it easier/cheaper to obtain statistical data on high-level microstructure features known to mediate critical engineering properties of materials (e.g. particle size distributions; denuded zone widths, and particle coarsening kinetics). In industrial settings where reliance on semi-automated segmentation techniques is common, the barrier to entry is even lower because the training data has already been collected. CNNbased microstructure segmentation tools also offer a path forward to highthroughput microstructure quantification techniques for accelerated alloy design and processing optimization, where acquisition and analysis of highquality microstructure data is often a limiting factor.

Methods


Segmentation model
Recently a variety of deep CNN architectures have been developed for dense pixel-level tasks  #b15 , such as semantic segmentation  #b16 , edge detection, depth map, and surface normal prediction  #b17 . Conceptually, a modern deep CNN computes a highly nonlinear function through a layerwise composition of convolution, activation, and pooling (i.e. downsampling) functions, the parameters of which are learned from large annotated datasets by some variant of stochastic gradient descent  #b5  #b6 . Classification CNNs reduce an input image to a single latent feature vector, where CNNs designed for pixel-level tasks produce a latent representation for every pixel of the input image. This is typically accomplished by upsampling the intermediate feature maps via a fixed bilinear interpolation  #b12  #b18  or a learned deconvolution operation  #b19 . In the latter class of networks, popular architectures include SegNet  #b16 , Bayesian SegNet  #b20 , U-Net  #b21  with heavy data augmentation, and fully-convolutional DenseNets  #b22 . In particular, U-Net  #b21  was designed for application to medical image segmentation tasks with small dataset sizes, relying on strong data augmentation to achieve good performance.

PixelNet architecture
The PixelNet  #b12  architecture is illustrated schematically in Figure 1. Pix-elNet applies bilinear interpolation to intermediate feature maps to form hypercolumn features h(x) = [conv 1 (x), conv 2 (x), . . . conv 5 (x)], which represent each pixel in the input image with information drawn from multiple scales. A non-linear classifier implemented as a multi-layer perceptron (MLP, i.e. a traditional artificial neural network (ANN)) maps the hypercolumn features to the corresponding pixel-level target. Instead of computing dense high-dimensional feature maps at the input resolution as in other popular pixel prediction networks, at training time PixelNet performs a sparse upsampling to efficiently obtain hypercolumn features only for a small sample of the input pixels.2 This is attractive for quickly training segmentation networks from scratch with small training sets because it reduces the memory footprint during training and makes training a non-linear predictor with high-dimensional latent representations feasible  #b12 .The feature extraction portion of our PixelNet variant uses the VGG-16 architecture  #b23  used by the original PixelNet  #b12 ; this architecture consists of 13 convolution layers and two fully-connected layers7. The MLP layers in our Pix-elNet variant consist of 1024 neurons with rectified linear (ReLU) activations  #b24  (ReLU (y i ) = max(0, y i ) followed by batch normalization  #b25 . Following the original PixelNet implementation, our hypercolumn features consist of the highest convolution feature map within each block of the VGG architecture ({1 2 ,2 2 ,3 3 ,4 3 ,5 3 ,7}), converting layer 7 to a 7 × 7 convolution filter as in  #b19  and  #b12 . We apply batch normalization  #b25  to each VGG-16 feature map before upsampling via bilinear interpolation, immediately after the ReLU activations.  

Training details
We initialize the feature extraction portion of our networks with a pre-trained VGG-16  #b23  network trained on the ImageNet  #b26  classification dataset. We train the pixel classification layers from scratch, randomly sampling initial weights from Gaussian distributions with zero mean and standard deviation σ = 2/c  #b27 , where c is the dimensionality of the input to the layer. To prevent overfitting, we use a combination of batch normalization  #b25 , Dropout regularization  #b28 , weight decay regularization  #b29  , and data augmentation. We set the weight decay strength to 0.0005 and apply Dropout regularization with a rate of 10% after the final MLP layer. Training images are subjected to local histogram equalization to mitigate differences in overall brightness across different samples and datasets. The training input and label images are augmented with random rotations in the range [0, 2π), horizontal and vertical mirror symmetry, scaling in the range  #b0  #b1 , and a ± 5% random intensity shift. Rotated versions of the training input and label images are computed with mirror boundary conditions, with bilinear interpolation for the input images and nearest-neighbor interpolation for the (discrete) label images. We train the networks with the AdamW optimizer  #b29  #b30  with the recommended default parameters. First we fix the parameters in the feature extraction portion of the network and train the pixel classification layers with an initial learning rate of 10 -3 for 20 epochs (125 gradient updates). Each gradient update is computed from a random sample of 2048 pixels each from 4 augmented training images. We then fine-tune the entire CNN for 125 additional gradient updates using AdamW with an initial learning rate of 10 -5 .Our dataset has a heavy class imbalance (e.g. Widmanstätten cementite only accounts for ∼ 3% of pixels), so we compare a model trained using the standard cross-entropy classification loss with another trained using the focal loss  #b31 , which is designed for unbalanced datasets. The focal loss extends the cross-entropy loss function CrossEntropy(p t ) = − log(p t ), where p t (p, y) = p ify = 1 1 − p ify = 0 (1)with ground truth y and predicted class probability p = P (y = 1). The focal loss adds a modulating factor (1 − p t ) γ to emphasize examples about which the classifier is less confident during training, and a scaling parameter α to account for class imbalance:F ocalLoss(p t ) = −α t (1 − p t ) γ log(p t )(2)We follow the recommendation of  #b31  in setting the focusing parameter γ = 2 and setting the class imbalance parameters α t proportionally to the inverse frequency of each class.

Dataset
The semantic microstructure segmentation dataset consists of 24 manually annotated  $b3  micrographs from the open UHCS dataset  #b0  #b1 ; examples are shown in Figure 2 and in the online supplemental materials. These 645 × 484 pixel micrographs focus on the characteristic features of heat-treated UHCS: the proeutectoid cementite network and the associated denuded zone, and spheroidized and Widmanstätten cementite. Multiple heat treatment conditions and magnifications are represented in the semantic microstructure segmentation dataset.The particle segmentation dataset consists of 24 micrographs collected at a single magnification in support of the particle coarsening analysis reported in  #b14 . Particle annotations were obtained through a partially-automated edge-based segmentation workflow  #b14 . A thresholded blur smooths contrast in the matrix surrounding particles before application of the Canny edge detector  #b33 . The particle outlines are filled in, and spurious edges (e.g. at grain boundaries) are removed by a 2px median filter. The final particle segmentations are verified and retouched manually where the contrast is insufficient for the Canny detector to identify particle edges. Particles intersecting the edge of the image are removed from the annotations to reduce bias in the estimated particle size distributions.

Performance evaluation
Because our set of annotated images is small (24 annotated micrographs total), we use cross-validation to estimate the generalization performance of the PixelNet architecture on our two microstructure segmentation tasks. We use a 6-fold cross-validation scheme  #b34 : each dataset is split into six validation sets of four micrographs each, and six PixelNet models are trained on each of the complementary training sets. The quantitative performance metrics reported in Tables 1 and 4 are averages over each validation image in the 6 validation sets; uncertainties are standard errors computed over the six validation images  #b34 .We report several standard evaluation metrics for semantic segmentation tasks: pixel accuracy (AC), precision, recall, and region intersection over union (IU) for individual microconstituents. For each of these metrics, a higher score indicates better performance.Precision is the fraction of instances predicted to have class c that are correct:P recision(c) = iŷ i = c and y i = c iŷ i = c(3)whereŷ i indicates the predicted class label for each pixel i, and y i indicates the corresponding ground truth class label. Equivalently, precision is the ratio of true positives to total (true and false) positives, which decreases when the model overpredicts the number of member pixels in a class.Recall is the fraction of instances with ground truth class c that are predicted to have class c:Recall(c) = iŷ i = c and y i = c i y i = c(4)Equivalently, recall is the ratio of true positives to the total number of pixels in a class, which decreases when the method underpredicts the member pixels in a class. Since the overall accuracy is defined as the number of true positives divided by the total number of pixels, it is straightforward to show that the classwise average recall or precision equals the overall accuracy.The intersection over union metric IU (c) for class c (also referred to as the Jaccard metric) is the ratio of correctly predicted pixels of class c (true positives) to the union of pixels with either ground truth or predicted class c (true and false positives plus false negatives):IU (c) = iŷ i = c and y i = c iŷ i = c or y i = c(5)For the spheroidite particle segmentation task, we also report performance metrics comparing particle size distributions (PSDs) obtained from the model predictions with those obtained from the ground truth annotations (as reported in  #b14 ). We use the two-sample Kolmogorov-Smirnov (KS) test  #b35  to compare each pair of predicted and ground truth PSDs. The KS score reported in Table 4 is the fraction of micrographs where the KS test indicates that the predicted particle size distribution is consistent with the ground truth particle size distribution (i.e. the fraction of micrographs where we fail to reject (at the 95% confidence level) the null hypothesis that the distributions are equivalent).

Computing denuded zone widths
Given a microconstituent prediction map, we quantify the width of the denuded zone by computing the minimum distance to the network phase for each pixel on the matrix-particle interface. In practice, we compute a map of Euclidean distance to the network phase, and select the measurements at the denuded zone interface.To obtain the denuded zone interface, we apply a series of image processing techniques to clean up the microconstituent prediction map, so that only the matrix predictions associated with the diffusion-limited denuded zone adjacent to the proeutectoid cementite network remain. A morphological filling operation removes any matrix pixels within the network. Matrix regions that are not connected to the network are identified by application of a morphological closing to matrix phase: any matrix segments that do not intersect the network phase after the morphological operation are removed. Finally, we remove any matrix predictions that are closer to a widmanstatten region than to a network region, and subsequently remove the widmanstatten regions. The region boundaries on the cleaned up label image (shown in Figure  5) include only the interface of the proeutectoid cementite network phase (indicated in blue) and the diffuse interface of the denuded zone (indicated in yellow). Microconstituent predictions using the focal loss function and the crossentropy loss function are compared in Figure 2 (i-l) and (m-p) respectively. The predictions show reasonable correspondence with the annotations, despite nontrivial differences in features, such as particle size and appearance that arise from differences in heat treatment and magnification. Intensity variations and polishing damage evident in the input images have little impact on the predictive capability of the models. One notable exception is the cluster of spurious network predictions associated with the damaged areas in the lower left of Figure 2 c. Both models do a good job respecting the edges of the network carbide phase, with a few exceptions where the network is very fine or the contrast between network carbide and metal matrix is poor (see supplemental Figures S1.1 d and S1.5 d). Predicted boundaries between spheroidite particles and the denuded zone have little noise and tend to be smoother than in the annotations. The Widmanstätten predictions show the highest amount of noise, especially where the Widmanstätten lath are fine or are beginning to break up. The focal loss also tends to surround Widmanstätten cementite with wider swaths of the metallic matrix compared to the annotations. In addition to the low area fraction of Widmanstätten cementite, one potential contributing factor for these failure modes is labeling bias where the microstructure is ambiguous even to the human expert. For example, some areas with a low density of spheroidite particles are labeled by the model as metallic matrix where the annotation has made no such distinction. This phenomenon is evident in the lower half of Figure 2 i, where the model correctly identifies large patches of bare metal in the neighborhood of some large grain boundary cementite particles (refer to supplementary Figure S1.13 a for a more detail).

Results and Discussion


Semantic microconstituent segmentation
The cross-entropy model segmentation maps (Figure 2 (m-p)) tend to be more consistent with the annotations for the majority microconstituents. However, each model errs from the annotations in distinct ways. In general, the focal loss model seems to emphasize constituent contiguity, while the cross-entropy model tries to resolve fine features. For example, compared with the annotations in Figure 2 f, the focal loss model (Figure 2 j) more liberally identifies the very fine lath structures in the bottom right corner of the frame, consolidating them into a single patch, while the cross-entropy model ( Figure  2 n) produces a noisier map that attempts to track finer-grained details of the lath structure but loses some area fraction. One conclusion from these results is that while both models give acceptable results, neither is necessarily the best that can be achieved. For a given segmentation problem, the user must select model parameters and loss functions to achieve the desired quantitative and qualitative performance.   Table 1 shows the average validation set performance with standard errors for the focal loss model. The model obtains 86.5 ± 1.6 % overall accuracy (AC, equivalent to the average of the classwise recall or precision) in reproducing the pixel-level annotations. The model is consistently good at identifying spheroidite and network regions. The less prevalent microconstituents (matrix and Widmanstätten) are not as well captured, and show higher variation between images. For these microconstituents, the recall score is better than the precision score, meaning that the CNN tends to mistake other classes for matrix and Widmanstätten more than it tends to miss genuine matrix and Widmanstätten pixels. This effect is demonstrated on the fine Widmanstätten lath in the lower right portion of Figure 2 j, where the model fills in the fine spacing between Widmanstätten lath in its prediction. The low proportion of Widmanstätten pixels in the dataset enhances this effect. In the case of the matrix class, the difference in recall and precision scores is partly due to the overprediction of metallic matrix in areas containing a low density of spheroidite particles, as discussed in reference to Figure 2 i.In contrast, the spheroidite and network classes have slightly higher precision compared with their recall scores. The standard error for the network scores is large, and is therefore likely accounted for by the small number of gross errors discussed in supplemental Figures S1.1 d and S1.5 d.Finally, the small difference in precision and recall score for the spheroidite class is likely also due to the overprediction of the metal matrix in regions with low particle density. Table 2 shows the same performance metrics for the cross-entropy model trained with revised training hyperparameters. The crossentropy shows clearly superior overall numerical performance, including a nearly ten point bump in overall IU score. While most of the per-microconstituent scores are higher for the crossentropy model, the recall score for Widmanstätten cementite is consistently depressed due to underprediction. We also briefly experimented with the popular U-Net  #b21  architecture, but found that this architecture slightly underperformed compared with Pixelnet. These results indicate that various CNN architectures and training schemes can achieve reasonable results, so the user can select an approach based on desired outcomes. Because of its excellent performance in the spheroidite segmentation task (reported in the next section), we present results only from the Pixelnet model trained with the focal loss function throughout the rest of this manuscript. These quantitative metrics are useful for interpreting the strengths and weaknesses of a particular CNN model, but they do not necessarily directly quantify the quality of the predicted segmentation maps due to inherent subjectivity and bias in the labeling process. Even a single human annotator will not be able to consistently label an entire dataset, especially for ambiguous higher-level microconstituents such as the spheroidite class. For example, the annotator must decide how closely to track cementite particles when tracing out the edge of the denuded zone. In some cases, it is unclear whether a carbide should be labeled as grain boundary cementite or as a piece of Widmanstätten lath. Furthermore, the low resolution of the input images relative to some of the finer features of interest also places a practical upper bound on these numerical performance scores, especially for microconstituents with large interfacial areas like the Widmanstätten lath. Many of the Widmanstätten lath in this dataset are just a few pixels wide, which can lead large shifts in numerical scores for what a human might consider a minor difference in labeling (e.g. dilating or eroding the Widmanstätten lath by one pixel). Figure 3: (a-d) Validation set predictions for the spheroidite particle segmentation task, along with (e-h) corresponding derived particle size distributions for the particle predictions (blue) and annotations (green). Scale bars indicate 5µm. Figure 3 shows some validation results for the individual particle segmentation task, with numerical performance reported in Tables 3 and 4; additional examples are included in the online supplemental materials. Particle predictions are overlaid in red on the input micrographs (a-d). The second row (e-h) shows the empirical particle size distributions for both particle predictions and annotations, as well as the results of the two-sample Kolmogorov-Smirnov hypothesis test for distribution equivalence. Predictions for larger particles relative to the image frame (Figures 3 b and c) are consistently good, even where contrast gradients across particles and non-trivial background structure challenge thresholding and edge-based segmentation methods. The primary failure mode of the particle segmentation model is underprediction of very small particles, particularly in Figure 3 a and d. The vast majority of the fine particles in Figure 3 are missing entirely, and many are only partially labeled by the CNN with just one or two foreground pixels. These particles are typically one to five pixels in size, suggesting that higher-or multi-resolution inputs are necessary for general microstructure segmentation CNNs. However, the CNN does avoid spuriously labeling the small segments of Widmanstätten in Figure 3 as particles.

Spheroidite particle segmentation
The PixelNet model performs slightly better than Otsu's thresholding method  #b36  on all metrics. One source of bias in these performance measurements are missing particles in the annotations, either from the removal of particles intersecting the image border, or from failure of the semi-automated annotation method itself. An additional source of bias stems from the ap-    #b37  to split conjoined particles in the annotations; watershed segmentation is not presently applied to the particle predictions, increasing the relative rate of larger particles. Despite good numerical performance on the particle segmentation task, the KS test suggests we reject the null hypothesis that the predicted and ground truth particle size distributions are equivalent for all but one of the 24 validation micrographs (shown in 3 b). The difficulty in detecting small particles explains the discrepancies between empirical particle size distributions that contribute to the KS score. For the two validation micrographs in Figure  3 containing fine particles, the particle size histograms and prediction maps show that the model often entirely misses particles with radii smaller than 5px. Many of these missing~5px particles are partially labeled in the CNN predictions, leading to a severe overrepresentation of single-pixel particles, especially in Figure 3 h.

Quantitative analysis of higher-order features
High-quality automated segmentation techniques for complex microstructure constituents expand the scope of conventional quantitative microstructure analysis by reducing the manual labor required to obtain statistically meaningful amounts of data. In our UHCS case study, the CNN segmentation model allows us to collect volume and shape statistics for the proeutectoid carbide network, spheroidite particles, and Widmanstätten lath directly from SEM micrographs with no manual intervention. Additionally, the microconstituent prediction maps enable automated acquisition of interesting microstructural Figure 4: (a-d) Micrographs with (e-h) validation set microconstituent predictions and (i-l) derived particle size distributions obtained by applying the particle segmentation CNN to the semantic microstructure segmentation dataset. Scale bars indicate 10µm.statistics that were previously intractable, such as particle size distributions conditioned on spatial relationships with other microstructure features, or denuded zone widths  #b14 .Combining the two microstructure segmentation models allows us to filter out irrelevant microstructure features in order to estimate particle size distributions. Figure 4 shows combined microstructure predictions from both the abstract microstructure model and the particle model, using the same color scheme as Figures 2 and 3. We run the input image through separately-trained particle segmentation CNN and microconstituent CNN, suppressing particle predictions (red) outside of the predicted spheroidite regions (yellow). With an appropriate number of images, one could also compute particle size distributions spatially conditioned on other microstructure features (e.g. distance from the network phase), which could help lead to insights into operative microstructure evolution mechanisms (particle coarsening vs precipitation). The resolution of these input micrographs is insufficient to yield quantitatively accurate particle size distributions, especially with the underprediction of small particles discussed in Section 3.2, as evident in Figures 4 b and c. However, higher quality input and training micrographs will mitigate this effect. Figure 5: (a-d) Validation set microconstituent predictions with (e-h) corresponding denuded zone width distributions. The network interface is shown in blue and the particle matrix interface is shown in yellow. Scale bars indicate 10µm Figure 5 shows the predicted network and denuded zone boundaries for four validation images with corresponding computed denuded zone width distributions. The denuded zone width distributions are calculated by aggregating the minimum distance to the network interface for each pixel on the denuded zone boundary, as described in detail in Section 2.4. Generally, these empirical denuded zone widths are reasonable, but some care is required to interpret them. Specifically, the denuded zone width distributions in The initial investment of micrograph annotation and training a CNN makes sense where a statistical number of samples must be characterized in the context of alloy and processing optimization studies, and in the context of microstructure and process validation or verification. Microconstituent annotation accounted for a substantial portion of the time we spent on this project. After an initial learning curve, a typical micrograph in our dataset cost between 20 and 30 minutes to annotate. In contrast, a commodity GPU performs Pixelnet-based segmentation at a rate of approximately one micrograph per second, after an initial training period of two to four hours depending on training hyperparameters. Success in a practical microstructure science setting will depend on establishing higher-quality training data and deeper understanding of the biases and variance of the labeling process.The CNN predictions provide some useful feedback on these subjective labeling decisions: consider the micrograph, annotation, and predictions in supplemental Figure S1.6 a, e, and i. In the bottom half of this micrograph (and in the other micrographs in this validation set), the annotator neglected to label the metal matrix surrounding the Widmanstätten lath as such, while the CNN consistently includes some matrix predictions associated with Widmanstätten predictions. This subjective labeling decision can be mitigated with higher-fidelity labeling of individual carbide particles -at much greater labeling expense. A high quality dataset might be obtained via crowd-sourcing (e.g. students in a microstructure analytics course), generation of realistic synthetic datasets through e.g. phase field modeling, or through the substantial expense of high-resolution elemental mapping with SEM+EDS (Energy-dispersive spectrometry). A large dataset might also be collected in a semi-supervised fashion through the development of smart microscopes with integrated microstructure recognition features. Furthermore, it is critical to benchmark microstructure-specific tasks against other popular CNN architectures for semantic segmentation. Our approach of directly transferring the particle prediction CNN is tenuous, especially due to the disparity in magnification between the general UHCS and specific particle segmentation datasets. Rather than training two separate CNNs, it may be more appropriate train a single CNN in a multi-task setting, so that microstructures are mapped to a common numerical representation before the respective microconstituent and particle classification tasks.Finally, microstructure data science is extremely data-limited in comparison to most general computer vision tasks. Though outside the scope of the present report, a detailed follow-on study to fully characterize the training data requirements of deep learning based microstructure segmentation models would be a valuable tool to enable experimental planning before significant investment for industrial application. In parallel, collaboration with computer scientists working on low-data deep learning, semi-supervised, and unsupervised techniques could also open the door to applicability in many more microstructure systems, especially where pixel-level annotations are expensive or difficult to consistently obtain.

Conclusions
We demonstrate microstructural segmentation and quantitative analysis at a high level of abstraction by applying an off-the-shelf deep neural network architecture for pixel-wise prediction tasks. We also present two new open microstructure segmentation benchmark datasets featuring the microstruc-tures in ultra-high carbon steel at different length scales. This data-driven approach to microstructure segmentation expands the reach of traditional quantitative microstructure characterization to more complex industriallyrelevant microstructure features that have, until now been, difficult to treat in an automated fashion. Combined with emerging automated microscopy capabilities, data-driven microstructure segmentation systems will enable future applications in high-throughput microstructure studies, including investigations of structure/processing relationships, microstructure design and optimization, and microstructure-based material qualification.

Footnote
3 : We used the medical image annotation system MITK[33].