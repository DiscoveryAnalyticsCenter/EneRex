What is Holding Back Convnets for Detection?

Abstract
Convolutional neural networks have recently shown excellent results in general object detection and many other tasks. Albeit very effective, they involve many user-defined design choices. In this paper we want to better understand these choices by inspecting two key aspects "what did the network learn?", and "what can the network learn?". We exploit new annotations (Pascal3D+), to enable a new empirical analysis of the R-CNN detector. Despite common belief, our results indicate that existing state-of-the-art convnet architectures are not invariant to various appearance factors. In fact, all considered networks have similar weak points which cannot be mitigated by simply increasing the training data (architectural changes are needed). We show that overall performance can improve when using image renderings for data augmentation. We report the best known results on the Pascal3D+ detection and viewpoint estimation tasks.

Introduction
In the last years convolutional neural networks (convnets) have become "the hammer that pounds many nails" of computer vision. Classical problems such as general image classification  #b16 , object detection  #b11 , pose estimation  #b3 , face recognition  #b29 , object tracking  #b19 , keypoint matching  #b9 , stereo matching  #b41 , optical flow  #b8 , boundary estimation  #b39 , and semantic labelling  #b20 , have now all top performing results based on a direct usage of convnets. The price to pay for such versatility and good results is a limited understanding of why convnets work so well, and how to build & train them to reach better results.In this paper we focus on convnets for object detection. For many object categories convnets have almost doubled over previous detection quality. Yet, it is unclear what exactly enables such good performance, and critically, how to further improve it. The usual word of wisdom for better detection with convnets is "larger networks and more data". But: how should the network grow; which kind of additional data will be most helpful; what follows after fine-tuning an ImageNet pre-trained model on the classes of interest? We aim at addressing such questions in the context of the R-CNN detection pipeline  #b11  ( §2).Previous work aiming to analyse convnets have either focused on theoretical aspects  #b1 , visualising some specific patterns emerging inside the network  #b17  #b30  #b32  #b21 , or doing ablation studies of working systems  #b11  #b2  #b0 . However, it remains unclear what is withholding the detection capabilities of convnets.Contributions This paper contributes a novel empirical exploration of R-CNNs for detection. We use the recently available Pascal3D+  #b38  dataset, as well as rendered images to analyze R-CNNs capabilities at a more detailed level than previous work. In a new set of experiments we explore which appearance factors are well captured by a trained R-CNN, and which ones are not. We consider factors such as rotation (azimuth, elevation), size, category, and instance shape. We want to know which aspects can be improved by simply increasing the training data, and which ones require changing the network. We want to answer both "what did the network learn?" ( §5) and "what can the network learn?" ( §6 and §7). Our results indicate that current convnets (AlexNet  #b16 , GoogleNet  #b34 , VGG16  #b31 ) struggle to model small objects, truncation, and occlusion and are not invariant to these factors. Simply increasing the training data does solve these issues. On the other hand, properly designed synthetic training data can help pushing forward the overall detection performance.

Related work
Understanding convnets The tremendous success of convnets coupled with their black-box nature has drawn much attention towards understanding them better. Previous analyses have either focused on highlighting the versatility of its features  #b27  #b26 , learning equivariant mappings  #b18 , training issues  #b4  #b15 , theoretical arguments for its expressive power  #b1 , discussing the brittleness of the decision boundary  #b35  #b13 , visualising specific patterns emerging inside the network  #b17  #b30  #b32  #b21 , or doing ablation studies of working systems  #b11  #b2  #b0 . We leverage the recent Pascal3D+ annotations  #b38  to do a new analysis complementary to previous ones. Rather than aiming to explain how does the network work, we aim at identifying in which cases the network does not work well, and if training data is sufficient to improve these issues. While previous work has shown that convnet representations are increasingly invariant with depth, here we show that current architectures are still not overall invariant to many appearance factors.Synthetic data The idea of using rendered images to train detectors has been visited multiple times. Some of the strategies considered include video game renderings  #b40  (aiming at photo-realism), CAD model wire-frame renderings  #b33  #b24  (focusing on object boundaries), texture-mapped CAD models  #b28  #b22 , or augmenting the training set by subtle deformations of the positive samples  #b6  #b25 .Most of these works focused on DPM-like detectors, which can only make limited use of large training sets  #b42 . In this paper we investigate how different types of renderings (wire-frame, materials, and textures) impact the performance of a convnet. A priori convnets are more suitable to ingest larger volumes of data.

The R-CNN detector
The remarkable convnet results in the ImageNet 2012 classification competition  #b16  ignited a new wave of neural networks for computer vision. R-CNN  #b11  adapts such convnets for the task of object detection, and has become the defacto architecture for state-of-the-art object detection (with top results on Pascal VOC  #b7  and ImageNet  #b5 ) and is thus the focus of attention in this paper. The R-CNN detector is a three stage pipeline: object proposal generation  #b37 , convnet feature extraction, and one-vs-all SVM classification. We refer to the original paper for details of the training procedure  #b11 . Different networks can be used for feature extraction (AlexNet  #b16 , VGG  #b2 , GoogleNet  #b34 ), all pre-trained on ImageNet and fine-tuned for detection. The larger the network, the better the performance. The SVM gains a couple of final mAP points compared to logistic regression used during fine-tuning (and larger networks benefit less from it  #b10 ). In this work we primarily focus on the core ingredient: convnet fine-tuning for object detection. We consider fine-tuning with various training distributions, and analyse the performance under various appearance factors. Unless otherwise specified reported numbers include the SVM classification stage, but not the bounding box regression.

Pascal3D+ dataset
Our experiments are enabled by the recently introduced Pascal3D+ [39] dataset. It enriches PASCAL VOC 2012 with 3D annotations in the form of aligned 3D CAD models for 11 classes (aeroplane, bicycle, boat, bus, car, chair, diningtable, motorbike, sof a, train, and tv monitor) of the train and val subsets. The alignments are obtained through human supervision, by first selecting the visually most similar CAD model for each instance, and specifying the correspondences between a set of 3D CAD model keypoints and their image projections, which are used to compute the 3D pose of the instance in the image. The rich object annotations include object pose and shape, and we use them as a test bed for our analysis. Unless otherwise stated all presented models are trained on the Pascal3D+ train set and evaluated on its test set (Pascal VOC 2012 val).

Synthetic images
Convnets reach high classification/detection quality by using a large parametric model (e.g. in the order of 10 7 parameters). The price to pay is that convnets need a large training set to reach top performance. We want to explore whether the performance scales as we increase the amount of training data. To that end, we explore two possible directions to increase the data volume: data augmentation and synthetic data generation.Data augmentation consists of creating new training samples by simple transformations of the original ones (such as scaling, cropping, blurring, subtle colour shifts, etc.), and it is a common practice during training on large convnets  #b16  #b2 . To generate synthetic images we rely on CAD models of the object classes of interest. Rendering synthetic data has the advantage that we can generate large amounts of training data in a controlled setup, allowing for arbitrary appearance factor distributions. For our synthetic data experiments we use an extended set of CAD models, and consider multiple types of renderings ( §4.1).  Extended Pascal3D+ CAD models Although the Pascal3D+ dataset  #b38  comes with its own set of CAD models, this set is rather small and it comes without material information (only polygonal mesh). Thus the Pascal3D+ models alone are not sufficient for our analysis. We extend this set with models collected from internet resources. We use an initial set of ∼ 40 models per class. For each Pascal3D+ training sample we generate one synthetic version per model using a "plain texture" rendering (see next section) with the same camera-toobject pose. We select suitable CAD models by evaluating the R-CNN (trained on Pascal 2007 train set) on the rendered images, and we keep a model if it generates the highest scoring response (across CAD models) for at least one training sample. This procedure makes sure we only use CAD models that generate somewhat realistic images close to the original training data distribution, and makes it easy to prune unsuitable models. Out of ∼ 440 initial models, ∼ 275 models pass the selection process (∼25 models per class).

Rendering types
A priori it is unclear which type of rendering will be most effective to build or augment a convnet training set. We consider multiple options using the same set of CAD models. Note that all rendering strategies exploit the Pascal3D+ data to generate training samples with a distribution similar to the real data (similar size and orientation of the objects). See Fig. 1 for example renderings.Wire-frame Using a white background, shape boundaries of a CAD model are rendered as black lines. This rendering reflects the shape (not the mesh) of the object, abstracting its texture or material properties and might help the detector to focus on the shape aspects of the object.Plain texture A somewhat more photo-realistic rendering considers the material properties (but not the textures), so that shadows are present. We considered using a blank background, or an environment model to generate plausible backgrounds. We obtain slightly improved results using the plausible backgrounds, and thus only report these results. This rendering provides "toy car" type images, that can be considered as middle ground between "wire frame" and "texture transfer" rendering.Texture transfer All datasets suffer from bias  #b36 , and it is hard to identify it by hand. Ideally, synthetic renderings should have the same bias as the real data, while injecting additional diversity. We aim at solving this by generating new training samples via texture transfer. For a given annotated object on the Pascal3D+ dataset, we have both the image it belongs to and an aligned 3D CAD model. We create a new training image by replacing the object with a new 3D CAD model, and by applying over it a texture coming from a different image. This approach allows to generate objects with slightly different shapes, and with different textures, while still adequately positioned in a realistic background context (for now, our texture transfer approach ignores occlusions). This type of rendering is close to photo-realistic, using real background context, while increasing the diversity by injecting new object shapes and textures. As we will see in §7, it turns out that any of our renderings can be used to improve detection performance. However the degree of realism affects how much improvement is obtained.5 What did the network learn from real data?In this section we analyze R-CNNs detection performance in an attempt to understand what have the models actually learned. We first explore models performance across different appearance factors ( §5.1), going beyond the usual perclass detection performance. Second, we dive deeper and aim at understanding what have the network layers actually learned ( §5.2).

Detection performance across appearance factors
To analyze the performance across appearance factors we split each factor into equi-spaced bins. We present a new evaluation protocol where for each bin only the data falling in it are actually considered in the evaluation and the rest are ignored. This allows to dissect the detection performance across different aspects of an appearance factor. The original R-CNN  #b11  work includes a similar analysis based on the toolkit from  #b14 . Pascal3D+ however enables a more fine-grained analysis. Our experiments report results for AlexNet (51.2 mAP)  #b16 , GoogleNet (56.6 mAP)  #b34 , VGG16 (58.8 mAP)  #b31  and their combination (62.4 mAP).Appearance factors We focus the evaluation on the following appearance factors: rotation (azimuth, elevation), size, occlusion and truncation as these factors have strong impact on objects appearance. Azimuth and elevation refer to the angular camera position w.r.t. the object. Size refers to the bounding box height. Although the Pascal3D+ dataset comes with binary occlusion and truncation states, using the aligned CAD models and segmentation masks we compute level of occlusion as well as level and type of truncation. While occlusion and truncation levels are expressed as object area percentage, we distinguish between 4 truncation types: bottom (b), top (t), left (l) and right (r) truncation.Analysis Fig. 2 reports performance across the factors. The results point to multiple general observations. First, there is a clear ordering among the models. VGG16 is better than GoogleNet on all factor bins, which in turn consistently  outperforms AlexNet. The combination of the three models (SVM trained on concatenated features) consistently outperforms all of them suggesting there is underlying complementarity among the networks. Second, the relative strengths and weaknesses across the factors remain the same across models. All networks struggle with occlusions, truncations, and objects below 120 pixels in height. Third, for each factor the performance is not homogeneous across bins, suggesting the networks are not invariant w.r.t. the appearance factors.It should be noted that there are a few confounding factors in the results. First such factor is the image support (pixel area) of the object, which is strongly correlated with performance. Whenever the support is smaller e.g. small sizes, large occlusions/truncations or frontal views the performance is lower. Second confounding factor is the training data distribution. For a network with a finite number of parameters, it needs to decide to which cases it will allocate resources. The loss used during training will push the network to handle well the most common cases, and disregard the rare cases. Typical example is the elevation, where the models learn to handle well the near 0 • cases (most represented), while they all fail on the outliers: upper (90 • )and lower (−90 • ) cases. We explore precisely this aspect in section 6, where we investigate performance under different training distributions.Conclusion There is a clear performance ordering among the convnets which all have similar weaknesses, tightly related to data distribution and object area. Occlusion, truncation, and small size objects are clearly weak points of the R-CNN detectors (arguably harder problems by themselves). Given similar tendencies next sections focus on AlexNet.

Appearance vector disentanglement
Other than just the raw detection quality, we are interested in understanding what did the network learn internally. While previous work focused on specific neuron activations  #b12 , we aim at analyzing the feature representations of individual layers. Given a trained network, we apply it over positive  test samples, and cluster the feature vectors at a given layer. We then inspect the cluster entropy with respect to different appearance factors, as we increase the number of clusters. The resulting curves are shown in Fig. 3. Lower average entropy indicates that at the given layer the network is able to disentangle the considered appearance factor. Disentanglement relates to discriminative power, invariance, and equivariance. (Related entropy based metric is reported in  #b0 , however they focus on individual neurons).Analysis From Fig. 3a we see that classes are well disentangled. As we go from the lowest conv1 layer to the highest fc7 layer the disentanglement increases, showing that with depth the network layers become more variant w.r.t. category. This is not surprising as the network has been trained to distinguish classes. On the other hand for azimuth, elevation and shape (class-specific disentanglement) the disentanglement across layers and across cluster number stays relatively constant, pointing out that the layers are not as variant to these factors. We also applied this evaluation over plain texture renderings (see §4 and §7) to evaluate the disentanglement of CAD models, the result is quite similar to Fig. 3a.Conclusion We make two observations. First, convnet representations at higher layers disentangle object categories well, explaining its strong recognition performance. Second, network layers are to some extent invariant to different factors.6 What could the network learn with more data?Section 5 inspected what the network learned when trained with the original training set. In this section we explore what the network could learn if additional data is available. We will focus on size ( §6.1), truncation ( §6.2), and occlusion ( §6.3) cases since these are aspects that R-CNNs struggle to handle. For each case we consider two general approaches: changing the training data distribution, or using additional supervision during training. For the former we use data augmentation to generate additional samples for specific size, occlusion, or truncation bins. Augmenting the training data distribution helps us realize if adding extra training data for a specific factor bin helps improving the performance on that particular bin. When using additional supervision, we leverage the annotations to train a separate model for each bin. Providing an explicit signal during training forces the network to distinguish among specific factor bins. The experiments involve fine-tuning the R-CNN only (no SVM on top) as we are interested in convnet modelling capabilities.  More data The "original" bars correspond to the results in Fig. 2. "Up & downscale" corresponds to training with a uniform size distribution across bins by up/down-scaling all training samples to all bins. As upscaled images will be blurry, "downscale only" avoids such blur, resulting in a distribution with more small size training samples than larger sizes. Results in Fig. 4 indicate that data augmentation across sizes can provide a couple of mAP points gain for small objects, however the network still struggles with small objects, thus it is not invariant w.r.t. size despite the uniform training distribution.Bin-specific models The right side bars of Fig. 4 show results for binspecific networks. Each bar corresponds to a model trained and tested on that size range. Both augmentation methods outperform the original data distribution on all size bins (e.g. at 195 pixels, "up & downscale" improves by 5.2 mAP). In "comb size" we combine the "up & downscale" size specific models via an SVM trained on their concatenated features. This results in superior overall performance (54.0 mAP) w.r.t. the original data (51.2 mAP with SVM).

Conclusion
We presented new results regarding the performance and potential of the R-CNN architecture. Although higher overall performance can be reached with deeper convnets (VGG16), all the considered state-of-the-art networks have similar weaknesses; they underperform for truncated, occluded and small objects ( §6). Additional training data does not solve these weak points, hinting that structural changes are needed. Despite common belief, our results suggest these models are not invariant to various appearance factors. Increased training data, however, does improve overall performance, even when using synthetic image renderings ( §7).In future work, we would like to extend the CAD model set in order to cover more categories. Understanding which architectural changes will be most effective to handle truncation, occlusion, or small objects remains an open question.

Truncation handling
More data Fig. 5 shows that generating truncated samples from non-truncated ones, respecting the original data distribution, help improve (1.5 mAP points) handling objects with minimal truncation; but does not improve medium or large truncation handling (trend for top, left and right is similar to the shown bottom case). Using biased distributions provided worse results.Bin-specific models Similar to the "more data" case, training a convnet for each specific truncation cases only helps for the low truncation cases, but is ineffective for medium or large truncations.Conclusion These results are a clear indication that training data do not help per-se handling this case. Architectural changes to the detector seem required to obtain a meaningful improvement.

Occlusion handling
Similar to the truncation case, Fig. 6 shows that specialising a network for each occlusion case is only effective for the low occlusion case. Medium/high occlusion cases are a "distraction" for training non-occluded object detection.Conclusion For truncation and occlusion, it seems that architectural changes are needed to obtain significant improvements. Simply adding training data or focusing the network on sub-tasks seems insufficient. We have seen that convnets have weak spots for object detection, and adding data results in limited gains. As convnets are data hungry methods, the question remains what happens when more data from the same training distribution is introduced. Obtaining additional annotated training data is expensive, thus we consider the option of using renderings. Our results with renderings (see §4) are summarised in Tab. 1. Again we focus on finetuning convnets only. All renderings are done using a similar data distribution as the original one, aiming to improve on common cases.Analysis From Tab. 1 we observe that using synthetic data alone (0:1 ratio) under-performs compared to using real data, showing there is still room for improvement on the synthetic data itself. That being said, we observe that even the arguably weak wire-frame renderings do help improve detections when used as an extension of the real data. We empirically chose data ratio of 1:2 between real and synthetic as that seemed to strike good balance among the two data sources. As expected, the detection improvement is directly proportional to the photo-realism (see Tab. 1). This indicates that further gains can be expected as photo-realism is improved. Our texture transfer approach is reasonably effective, with a 4 mAP points improvement. Wire-frame renderings inject information from the extended CAD models. The plain texture renderings additionally inject information from the material properties and the background images. The texture transfer renderings use Pascal3D+ data, which include some ImageNet images too. If we add these images directly to the training set (instead of doing the texture transfer) we obtain 50.6 mAP (original to additional ImageNet images ratio is 1:3). This shows that the increased diversity of our synthetic samples further help improving results. Plain textures provide 2 mAP points improvement, and texture transfer 4 mAP points. In comparison,  #b10   Conclusion Synthetic renderings are an effective mean to increase the overall detection quality. Even simple wire-frame renderings can be of help.

All-in-one
In Tab. 2 we show results when training the SVM on top of the concatenated features of the convnets fine-tuned with real and mixed data. We also report joint object localization and viewpoint estimation results (AAVP  #b23  measure). As in  #b23 , for viewpoint prediction we rely on a regressor trained on convnet features finetuned for detection. We observe that the texture renderings improve performance on all models (e.g. VGG16 58.8 to 61.9 mAP). Combining these three models further improves the detection performance and achieves state-of-the-art viewpoint estimation. Adding size specific VGG16 models (like in §6.1) further pushes the results, improving (up to 5 mAP points) on small/medium sized objects. Adding bounding box regression, our final combination achieves 67.2 mAP, the best reported detection result on Pascal3D+.