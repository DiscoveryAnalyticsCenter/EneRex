Classification under Streaming Emerging New Classes: A Solution using Completely Random Trees

Abstract
This paper investigates an important problem in stream mining, i.e., classification under streaming emerging new classes or SENC. The common approach is to treat it as a classification problem and solve it using either a supervised learner or a semi-supervised learner. We propose an alternative approach by using unsupervised learning as the basis to solve this problem. The SENC problem can be decomposed into three sub problems: detecting emerging new classes, classifying for known classes, and updating models to enable classification of instances of the new class and detection of more emerging new classes. The proposed method employs completely random trees which have been shown to work well in unsupervised learning and supervised learning independently in the literature. This is the first time, as far as we know, that completely random trees are used as a single common core to solve all three sub problems: unsupervised learning, supervised learning and model update in data streams. We show that the proposed unsupervised-learningfocused method often achieves significantly better outcomes than existing classification-focused methods.


and changes gradually. In the streaming classification problem, some new classes are likely to emerge as the environment changes. The predictive accuracy of a previously trained classifier will be severely degraded if it is used to classify instances of a previously unseen class in the data stream. Ideally, we would like instances of a new class to be detected as soon as they emerge in the data stream; and only instances which are likely to belong to known classes are passed to the classifier to predict their classes.It is assumed that true class labels are not available throughout the entire process, except a training set of known classes which is used to train a classifier (and a detector for new classes) at the beginning of the data stream. After the deployment of the classifier (and the detector), any future updates of the models must rely on the unlabelled instances as they appear in the data stream. Note that this assumption does not prevent the proposed method from using true class labels when they are available. It sets the hardest condition in the SENC problem.An illustrative example is provided in Figure 1 which shows a news image classifier system making predictions in a data stream. Assume that a classifier about news content is built in early 2014, which starts with two classes (money and airplane); then some new classes (football and phone) emerge in two later periods in the data stream. The system must have the ability to detect those new classes and update itself timely in order to maintain the predictive accuracy.Conceptually, the SENC problem can be decomposed into three sub problems: detecting emerging new classes, classifying known classes, and updating models to enable classification of instances of the new classes and detection of more emerging new classes. For every test instance in a data stream, the detector acts as a filter to determine whether it is likely to belong to a known class.If it is, the instance is passed on to the classifier to produce a class prediction. Otherwise, the instance is declared a new class and placed in a buffer which stores candidates of previously unseen class. When the candidates have reached the buffer size, they are used to update both the classifier and the detector. The process repeats in the data stream after the models are updated.The overall aim of the task is to maintain high classification accuracy continuously in a data stream. Thus, the challenges in the SENC problem are to detect emerging new classes and classify instances of known classes with high accuracy, and to perform model update efficiently in data streams. In order to maintain the model complexity to a reasonable size, model components related to currently inactive classes must be eliminated from the current model.We show that these challenges can be met by using completely random trees, and the proposed method often achieves significantly better outcomes than existing more complicated methods.The proposed method has the following distinguishing features:• The proposed method employs an unsupervised learning method as the basis to solve the SENC problem, and has a single common core which acts as distinct unsupervised learner and supervised learner. In contrast, most existing methods treat this problem as a classification problem and employ a supervised or semi-supervised learning approach  #b20  #b10  to solve it.• The method explicitly differentiates anomalies of known classes from instances of emerging new classes using an unsupervised learning anomaly detection approach.• The model is updated without the initial training set because the proposed method does not need to train new models for every future model updated. In contrast, most existing methods must keep this training set in order to train new models (e.g., LACU-SVM  #b10 .)Note that most of the existing methods mentioned above are designed to solve part of the SENC problem only. Details are provided in Section 2.Our main contribution is the proposal to shift the focus of treating SENC as a classification problem to one based on unsupervised anomaly detection problem. In other words, the focus is shifted from the second sub problem to the first sub problem which is more critical in solving the entire problem. This shift brings about an integrated approach to solve all three sub problems in SENC. No such solution exists in the current classification-focused approaches, as far as we know.The rest of this paper is organized as follows: Section 1 describes the intuition of the proposed algorithm. Section 2 reviews the related work. Section 4 and 5 describe related definitions and the details of the proposed algorithm. We report the experimental results in Section 6. The conclusion is provided in the last section.

The intuition


Detecting emerging new classes
The intuition is that anomalies of known classes are at the fringes of the data cloud of known classes, and instances of any emerging new classes are far from the known classes. To detect emerging new classes, we propose to treat instances of any new class as "outlying" anomalies which are significantly different from both instances and anomalies of the known classes.The anomaly detector for the SENC problem must be able to differentiate between these two types of anomalies. The assumption is that anomalies of the known classes are more "normal" than the "outlying" anomalies. This is a reasonable assumption in this context because only instances of the known classes are available to train the anomaly detector.An anomaly detector often categorises the feature space into two types of regions: anomaly and normal. Following the above idea, we propose to further subdivide each anomaly region into two sub regions: "outlying" anomaly sub region and anomaly sub region: (1) The instances in anomaly sub region is closer to the region of normal instances than instances from emerging new classes as the anomalies and normal instances are generated from the same distribution. (2) "Outlying" anomaly sub region is further away from the normal region and anomaly sub region.A test instance is regarded as belonging to an emerging new class if it falls in the "outlying" anomaly sub region.  Figure 2 illustrates the normal and anomaly regions constructed by an anomaly detector. The anomaly region is further partitioned into two sub regions. The sub region outside the anomaly sub region is the "outlying" anomaly sub region.The construction of "outlying" anomaly sub regions assumes that anomaly regions can be identified. We show in Section 4.2 that this can be easily achieved using a threshold of the anomaly scores provided by an anomaly detector to categorise all regions into two types: anomaly and normal.

Classification and efficient model update
If we treat the second sub problem, i.e., classification, as having no relation to the first sub problem for detecting emerging new classes, then any classifier can be applied. However, in order to facilitate efficient model update that enables classification of newly detected class and detection of more emerging new classes in data streams, we suggest an integrated approach which has a single common core for both the detection and classification tasks.An unsupervised learner iForest  #b18 , which induces completely random trees, has enabled us to implement the integrated approach with ease. This is because previous works  #b11  #b16  have shown that, ensemble of completely random trees  #b27 Chap.3.5], as an extreme case of variable-random trees  #b17 , can be successfully applied as a powerful classifier. We use exactly the same completely random trees, generated for the purpose of anomaly detection, for classification. This can be easily achieved by simply recording the class labels (provided in the training set) in each leaf. This is the only additional step that needs to be done in the training process to produce an ensemble of completely random trees that will act as both an unsupervised learner (to detect emerging new classes) and an supervised learner (to classify known classes) in data streams.As the single core for both tasks is completely random trees only, they can be updated easily when a sufficient number of instances of emerging new classes have been detected. The single core also facilitates to maintain the model complexity in a reasonable size by using effective model retiring mechanism and growing mechanism in the model update process.In a nutshell, we introduce a simple and unique method to solve the SENC problem and show that the proposed method can detect emerging new classes and classify known classes with high accuracy, and perform model update efficiently in data streams. Our empirical evaluation shows that it often performs significantly better than existing more complex methods.

Related work
The SENC problem has the following challenges:1. In the extreme case, no true labels except in the initial training set, i.e, true labels are not available after the model deployment.2. A prediction must be made immediately for each incoming instance in the stream.3. Store no data permanently from the data stream. 4. Fast model update.Note that, as far as we know, there is no an algorithm that using one single core to conquer the whole SENC challenges. We review the related work with respect to these challenges as following.Class-incremental learning (C-IL)  #b26  is a branch of incremental learning which modifies a previously trained classifier to deal with emerging new classes. It has been found to be useful in various applications, e.g., detecting bots  #b8 , face recognition [HAY + 07] and video concept detection  #b25 . C-IL problems includes open set recognition  #b23 , Learning with Augmented Class (LAC)  #b10 . All of these works are in the batch mode setting. The SENC problem is a C-IL problem in the data stream context.In addition, many existing methods treat the SENC problem as a classification problem. This is the reason why they have employed supervised learning or semi-supervised learning approaches.Moreover, most of these studies assume that instances of an emerging new class are identified by some other mechanism and focuses on methods to train and incorporate classifiers which can classify new classes incrementally with previously trained classifiers  #b10  #b15 . As a result, no existing methods in C-IL meet the four challenges mentioned above. is a batch-mode method that requires to store all training data. Thus, it is not suitable in data streams and does not meet the four challenges.The aim of novel class detection is to identify new data which are not previously seen by a machine learning system during training. This is the first sub problem of SENC. An example of this work in Bioinformatics  #b22  employs an one-class SVM approach to detect novel classes. It is interesting to note that this approach does not make a distinction between novel class detection and anomaly detection (or outlier detection)  #b6 , which is the identification of items, events or observations which do not conform to an expected pattern in a data set in batch mode. It thus also does not meet the four SENC challenges in data streams.The goal of change point detection is to detect changes in the generating distributions of the timeseries. Many works have been conducted to tackle this problem  #b5  which include parametric methods  #b9  and non-parametric methods  #b2 . This problem is equivalent to the first sub problem in SENC, without addressing the classification and model update issues. Yet, others have focused on classification in data streams [BHP + 09, JA03, KM07], without addressing the emerging new classes problem.Another related work, ECSMiner, [MGK + 11] tackles the novel class detection and classification problems by introducing time constraints for delayed classification. ECSMiner assumes that true labels of new emerging class can be obtained after some time delay; otherwise, models cannot be updated. In contrast, our proposed method assumes that no labels are available for the entire duration of a data stream.The SENC problem can be solved by treating the first two sub problems independently by using existing methods, i.e., a new class detector and a known classes classifier. To detect emerging new class, existing anomaly detectors (such as LOF  #b4 , iForest  #b18  and one-class SVM  #b20 ) can be employed; and multi-class SVM  #b7 ) can be used as an the classifier for known classes. In addition, existing supervised or semi-supervised batch classification methods can be adapted to solve the SENC problem, e.g., One-vs-rest SVM  #b21  and LACU-SVM  #b10 .However, all these algorithms do not solve the SENC problem satisfactorily. Table 1 summarizes the ability of these algorithms and the proposed SENCForest to meet the four challenges. 

Algorithm
Challenge1 2 3 4 LOF+SVM × × × 1SVM+SVM × × × One-vs-rest SVM × × × LACU-SVM × × × iForest+SVM × × ECSMiner × ×

SENCForest
Details about those algorithms implemented and the proposed SENCForest are provided in following sections.SENCForest is the only one which can meet all four challenges. Only ECSMiner, among existing algorithms, can meet Challenge #3. Note that all existing algorithms assume that true labels are made available after the model deployment at some points in time-unable to meet Challenge #1.

Terminology Definition
Before introducing the detail of our proposed algorithm, we will give the formal definitions of many important concepts used in this paper.  The SENC problem can have different variations. The hardest condition is when true class labels are not available throughout the entire process, except that the initial training set of known classes is used to train a classifier (and a detector for new classes) at the beginning of the data stream.D = {(x i , y i )} L i=1 , where x i ∈ R d is a training instance and y i ∈ Y = {1, 2, . . . , K} is the associated class label. A streaming data S = {(x ′ t , y ′ t )} ∞ t=1 , where x ′ ∈ R d , y ′ ∈ Y ′ = {1A relaxation of this condition produces easier SENC problems. For example, true class labels are available at some intervals in streaming data S. In this paper, we show that the proposed method can deal with the hardest condition (in Section 5.2) as well as some easier conditions (in Section 5.3).Definition 3.2 Scores for test instances: Model f yields a score for a test instance x, which determines x as belonging to either a known class or an emerging new class (i.e., an "outlying" anomaly.)  

The Proposed Algorithm
In this section, we propose an efficient algorithm to deal with the SENC problem named SENC-Forest which is composed of SENCTrees and assigns each instance, as it appears in a data stream, a class label: Emerging New Class or one of the known classes. Instead of treating it as a classification problem, we formulate it as a new class detection problem and solve it using an unsupervised anomaly detector as the basis to build SENCForest which will finally act as both unsupervised learner and supervised learner.We provide an overview of the procedure in section 4.1. The pertinent details in the procedure are then provided in the following three sections.

SENCForest: An Overview
SENCForest has four major steps:1. Train a detector for emerging new classes. Given the initial training set of known classes D, an unsupervised anomaly detector SENCForest is trained, ignoring the class information, as follows:1. Build an iForest  #b18 .2. Determine the path length  #b18  thresholdτ . The anomaly sub region outside the ball is the "outlying" anomaly sub region.

Within each region
2. Using known class information to build a classifier from a detector. Once the above new class detector is constructed, class distributions based on known class labels are recorded in each K or B region. Each region with class distribution acts as a classifier that outputs the majority class as the classification result for a test instance which fall into the region.The training set is discarded once the training process is completed.

Deployment in data stream.
SENCForest is now ready to be deployed in a data stream, and it is assumed that no true class labels are available for model updated throughout the entire data stream. An instance in the data stream is given a class prediction by SENCForest if it falls into K or B region; otherwise, it is identified as an instance from an emerging new class and placed in a buffer of size s.4. Model update. The model update process in SENCForest is simple. It begins when the buffer is full. Using instances from the buffer, the same tree growing process is then applied to each leaf of every existing tree until the stopping criterion is satisfied. The rest of the model update process follows the same steps from 1.2 onwards, as described above. Note that the update largely involves newly grown subtrees, i.e., replacing leaf nodes which have the number of instances more than a set limit after taking new instances from the buffer into consideration.Thus, the whole process can be completed quickly. To maintain model size, mechanisms to retireSENCForest are also employed in the model update process. 

SENCForest: Training process
The training procedure to build an SENCForest with both detection and classification functions is detailed in Algorithms 1 and 2. These are the combined step to build iForest  #b18  and to produce a classifier from a detector. The trees are then used to determine the path length threshold and to construct "outlying" anomaly regions described following respectively. Note that the procedure is the same as in building iForest, except in line 2 of Algorithm 2. As the trees constructed are not exactly iTrees, we name the trees with the new classification capability,

SENCTrees.
Build an iForest. The unsupervised anomaly detector iForest  #b18  is an ensemble of Isolation Tree (iTrees). "Isolation" is a unique concept in anomaly detection, as each iTree is built to isolate every instance from the rest of the instances in the training set. The idea is based on the fact that since anomalies are 'few' and 'different', they are more susceptible to isolation than normal instances. Hence, an anomaly can be isolated using fewer partitions in an iTree than a normal instance.Liu et. al.  #b18  show that iTrees can be created using a completely random process to achieve the required isolation. Given a random subsample of size ψ, a partition is produced by randomly selecting an attribute and its cut-point between the minimum and maximum values in the subsample. To produce an iTree, the partitioning process is repeated recursively until every instance in the subsample is isolated. An iForest is an ensemble of z iTrees, each generated using a subsample randomly selected from the given training set.In the testing process, an instance having a short path length, which is the number of edges it traversed from the root node to a leaf node of an iTree, is more like to be an anomaly. The average path length from all iTrees is used as the anomaly score for each test instance.For both instances of emerging new class and anomalies of known classes, iForest will produce short path lengths because they all are individually 'few' and 'different' from the known classes.In order words, they are all in the regions with short path length in iTrees. We called this type of region, anomaly region A to differentiate them from normal region K which have long path length.In order to detect emerging new class, we first need to determine a path length threshold to differentiate A from K. Then, build a sub region B in each A region which covers all training instances in the region. As these instances are from known classes, they are anomalies of known classes. These two processes are described in the following paragraph.Determine the path length threshold. As each region in iTree has its own path length, and anomaly regions A are expected to have shorter path length than that from normal regions K,we employ the following method to determine the path length threshold to separate these two types of regions.We produce a list L which orders all path lengths representing all regions in an iTree in ascending order. A threshold τ in this list yields two sub-lists L l and L r . To find the best threshold, we use the following criterion which minimises the difference in standard deviations σ(.):τ = arg min τ |σ(L r ) − σ(L l )|The thresholdτ is used to differentiate anomaly regions A from normal regions K, where the former has low path length and the latter has long path length.Using a tree, Figure 4 shows an example of cumulative distribution for list L and its SD diff(= |σ(L r ) − σ(L l )|) curve.Note that the minimum SD diff point separates into two clear regions:anomaly and normal regions.Note that (i) because thresholdτ is determined automatically, no additional parameter is introduced; and (ii) this process does not require training data.Construct "outlying" anomaly sub regions. Afterτ is determined, a ball B is constructed using all training instances in every region A of a tree, according to Definitions 3.4 and 3.5.When balls B have been built for all A regions in every SENCTree, the SENCForest has the first function as an unsupervised detector and is ready to detect instances of emerging new classes. A test instance which falls into A but outside B is an "outlying" anomaly, i.e., an instance of an emerging new class.Produce a classifier from a detector To incorporate the second function of being a classifier into SENCForest, all we have to do is to record class distribution F [j] in each region from K and B using the training subsample, where F [j] denotes the number of class j instances in a region.Note that this is the only step class labels are required.Once the above training steps are completed, SENCForest is ready to be deployed to a data stream.Algorithm 1 Build SENCForest Input: D -input data, z -number of trees, ψ -subsample size. let Q be a list of attributes in X 5:randomly select an attribute q ∈ Q 6:randomly select a split point p from max and min values of attribute q in X 7:X L ← f ilter(X, q ≤ p) 8:X R ← f ilter(X, q > p) 9:return inNode{Left ← SENCTree(X L ),

10:
Right ← SENCTree(X R ),

11:
SplittAtt ← q, if |X ′ | > 0 then 8:X ← Pseudo instances from Tree.LeafNode j 9:X ′ ← X ′ ∪ X 10:Tree.LeafNode j ← SENCTree(X ′ )  If there are some instances which fall into a leaf node, a subtree needs to be grown as follows.As the previous training set is not stored, pseudo instances are generated for the leaf node which node 1 is replaced with this new subTree. Every leaf node goes through the same process.Note that the update process retains the original tree structure, and all pseudo instances in a leaf node will still be placed into a single leaf node of the newly grown subtree. Thus, the predictions for the known classes are not altered in the model update process.Once each tree has completed the model update,τ is recalculated as described in Section 4.2.Growing multiple SENCForests. When the number of classes in a SENCForest reaches ρ, itsSENCTrees will stop growing for any emerging new class. A new SENCForest is grown instead for the next ρ emerging new classes. This user-defined parameter is set based on the memory space available.

Prediction using Multiple SENCForests
In a model with multiple SENCForests, the final prediction is resolved as follows. For a given x, SENCForest i yields prediction y i and probability p i = Number of SENCTrees predicting y i Total number of SENCTreesThe final prediction is N ewClass only if all SENCForests predict x as belonging to N ewClass.Otherwise, the final prediction is the known class which has the highest p i . This procedure is given in Algorithm 5.

Algorithm 5 Final Prediction from E SENCForests
Input:x -an instance in the data stream 

Retiring Mechanism
A mechanism to retire SENCForest is required as the data stream progresses. A SENCForest is retired under the following scenarios:1. When a SENCForest is not used for predicting known classes for a certain period of time, it is eliminated for any future predictions. In other words, a SENCForest outputs "NewClass" for a long time, this SENCForest will be retired The known classes at each duration (ti -ti+1) are denoted as b1,b2,b3, and b4. The details of the two methods, iForest+SVM and None+SVM, are described in Table 2.2. In the event that the number of SENCForests has reached the preset limit ρ and no SENC-Forest can be retired based on (1), then the least used SENCForest in the last period is chosen to retire.The number of known class predictions is recorded for each SENCForest in data stream. The one which has made the minimum number of predictions for known classes is identified to be the least used SENCForest.

Experiment
This section reports the empirical evaluation we have conducted to assess the performance of SENCForest in comparison with several state-of-the-art methods.

Experimental Setup
Data Stream: To simulate emerging new classes in a data stream, we assume that an initial training set with two known classes are available to train the initial models. When the trained models are deployed at the beginning of a data stream, instances of the two known classes and an emerging new class appear in the first period of the data stream with uniform distribution. It is assumed that the method employed will update its models sometime within the first period.In the second period, instances of the three classes seen in the first period and another emerging new class appear with uniform distribution. Instances appear one at a time, and the deployed method is expected to make a prediction for each instance before processing the next, i.e., each instance is predicted as belonging to either an emerging new class or one of the known classes thus far.No true class labels for all instances are available throughout the entire data stream. 1 Model update is based on the instances of the emerging new class identified at the time the model update is triggered. Figures 6 and 7 show example data streams using the KDDCUP 99 data set and the MNIST data set. The class composition in the two distinct periods in the data stream are described as follows:Known classes New class In the first period, all instances of the emerging new class identified by a method is placed in a buffer B of size s. When the buffer is full (marked as t 1 ), the method updates its model before processing the next instance. Note that t 1 differs for different methods as their detection rates for the new class are different, as shown in Figures 6(b) and 6(c) for iForest+SVM and SENCForest (so as in Figures 7(b) and 7(c).) The buffer is reset to be empty when the model of a method has been updated. Note that after the model is updated, the new class in t 0 -t 1 becomes a known class b 3 of the updated model in t 1 -t 2 , as shown in the table above.First period: t 0 -t 2 t 0 -t 1 b 1 ,b 2 t 1 -t 2 b 1 ,b 2 ,b 3 × Second period: t 2 -t 4 t 2 -t 3 b 1 ,b 2 ,b 3 t 3 -t 4 b 1 ,b 2 ,b 3 ,b 4 ×Similarly, in the second period between t 2 and t 4 , t 3 is the time when the buffer is full and the model of a method is updated for the second time. The new class in t 2 -t 3 becomes a known class b 4 of the updated model in t 3 -t 4 . Figure 8 shows the information of the evolving SENCForest at three different times in the data stream on two data sets.Evaluation measures: To evaluate the predictive accuracy of algorithms in the SENC problem, we introduce EN Accuracy in a fixed window size. Let N be the total number of instances in a window; A n be the total number of emerging class instances identified correctly; and A o be the  In the experiments reported in Section 5.2, the difference in performance between two methods is considered to be significance on paired t-tests at 95% significance level in our paperEN Accuracy = A n + A o N

Contenders:
The complete list of the methods used for new class detection, classification and model update methods is shown in Table 2. As some of these methods can act as a new class detector only, a state-of-the-art classifier, i.e., multi-class SVM  #b7 , is employed to classify instances of known classes. Note that three types of information, additional to that was provided to SENCForest, are required for other methods. First, true labels must be provided at each model update. Otherwise, no models could be updated. ECSMiner assumes that true labels are given at the end of a fixed interval (T l ) in order to update model. Other existing methods requires all instances in B must be given the true labels. Second, LACU-SVM needs to have additional unlabelled data before training at each model update. Third, the initial training set must be stored and incorporated at each model update. SENCForest is the only method which does not require (i) true labels during the entire data stream after training, (ii) to store the initial training set, and (iii) unlabelled training set.A brief description of each of the methods used in the experiment is given as follows:1. LOF or Local Outlier Factor [BKNS00] is a density-based anomaly detector which employs k-nearest neighbour procedure to estimate density. Model update can only occurs if true labels are available within some fixed duration.6. iForest  #b18  is an unsupervised anomaly detector which builds a model to isolate each training instance from the rest of the training set.In the experiments, all methods were executed in the MATLAB environment. The following implementations are used: SVM in the LIBSVM package  #b7 ; LACU-SVM and iForest were the codes as released by the corresponding authors; and LOF is in the outlier detection toolbox. 2The ECSMiner code is completed based on the authors' paper [MGK + 11]. We set the max size of each tree to 300, which avoids to the worst case that growing infinitely by random partition.The parameter settings used for these algorithms are provided in Table 5 in Appendix A.Data sets: Five data sets are used to assess the performance of all methods , including Synthetic, KDDCup 99 3 , Forest Cover 4 , MHAR and MNIST 5 . For KDDCup 99 data set, we use the four largest classes, i.e., normal, neptune, smurf and back. For Forest Cover data set, we use 10 attributes, and all binary attributes are removed. A description for Synthetic and MHAR data sets are provided in Appendix B. A summary of the data characteristics is provided in Table 3.Simulation: In the following experiment, each data set is used to simulate a data stream over ten trials. In each trial, the initial training set has two classes, and the emerging new class in each period is a class different from the known classes. These classes are randomly selected from the available classes. The instances in the initial training set and the data sequence in the data stream are randomly selected from the given data set, but following uniform class distribution.For all real-world data sets, the data size of the initial training set D is 500 per class; the buffer size |B| = 250; and the total number of instances which have appeared in the data stream at the end of the first period at t 2 is 1000; and the second period (t 2 -t 4 ) has a total of 1500 instances.As we can afford to generate more data in the synthetic data set, D, B, and the data size at each period are double to examine the effect of larger data sizes. The average result of ten trials is reported.The following sections will give related evaluation results. Section 5.2 describes the empirical evaluation under the condition that no true labels are available after the data stream has started. Section 5.3 reports results under the long streams situation. Section 5.4 describes using SENC-Forest under the condition that emerging multiple new classes in a period.

Empirical results
The results for the five data sets are shown in Figure 9.In terms of new class detection, SENCForest produced the highest F-measure in all data sets.Recall that SENCForest+SVM uses SENCForest only for new class detection; thus both SENC-Forest and SENCForest+SVM have the same F-measure performance.The closest contenders are LACU-SVM and 1R-SVM, each had the second or third highest Fmeasure in three data sets. SENCForest was significantly better than all contenders, except in MNIST (wrt LACU-SVM) and Forest Cover (wrt ECSMiner). In terms of EN Accuracy, SENCForest and SENCForest+SVM produced the highest performance in all data sets. This result shows that (i) the accurate detection of emerging new class leads directly to high classification accuracy; and (ii) SENCForest as a classifier is competitive to SVM.LACU-SVM was the closest contender which had the second highest accuracy in three data sets.Beside SENCForest+SVM, SENCForest performed significantly better than the other contenders in three data sets. The two exceptions are wrt to LACU-SVM (in MNIST and Synthetic) and1R-SVM (in Synthetic).An analysis is provided below:• LOF and one-class SVM: the poor detection performance of these two methods wrt to iForest is likely to be due to the parameter search, i.e., a search for a wider range of values may improve their performance. However, such search is a computationally expensive process, and this makes them unsuitable for data stream applications.• iForest performed worse than SENCForest in all data sets, and the differences were significance in four data sets. This shows that an unsupervised anomaly detector can be successfully used in the SENC problem if anomaly regions are reshaped (as described inSections 4.2) to detect emerging new classes.• While One-vs-rest SVM performed reasonably well in classification, it is not a good choice for detection of emerging new classes, in comparison with SENCForest.• LACU-SVM is the only method which requires additional unlabelled instances in training the initial model and in every model update. While obtaining unlabelled instances may not be a problem in real applications, it is important to note that its detection performance is highly depended on the existence of a new class in the set of unlabelled instances. Insufficient instances of the new class will severely limit LACU-SVM's ability to detect the new class.In the experiment, LACU-SVM was provided a set of unlabelled instances in t 0 , t 1 and t 3 , in addition to those instances in the initial training set and the buffer, in order to update its model. This additional data set was not available to all other methods. Despite this additional training information, LACU-SVM still performed significantly worse thanSENCForest in four data sets in terms of F-measure.• ECSMiner is the only algorithm which was provided with true labels in order to train a new classifier in each fixed interval, which occurs more often than at each model update, over the entire data stream. Despite this advantage, it still performed significantly worse than SENCForest in four out of five data sets in both measures. 6• The result of None+SVM clearly shows that not using a detector is not an option in the SENC problem.• SENCForest is the best choice detector and a competitive classifier in the SENC problem.6 ECSMiner [MGK + 11] had employed the KDD CUP 99 and Forest Cover datasets in their evaluation. OurECSMiner results are compatible with theirs in these two datasets. However, ECSMiner performed poorly in the other three datasets.While it is possible that a more sophisticated classifier may yield a higher accuracy in classifying known classes, it often comes at a high computational cost in an extensive parameter search.• While using SVM, in addition to SENCForest, could potentially produce a better accuracy than that from SENCForest alone, this comes with a computational cost which is usually too expensive in the data streams context. Note that to achieve the performance of SENCForest+SVM presented in Figure 9, it needs to store all instances thus far, which is impossible in data streams. In contrast, SENCForest achieves comparable result as SENC-Forest+SVM without the need to store any data.

SENCForest in long data streams
The aims of this section are to examine the ability of SENCForest to (i) maintain good performance using limited memory in long data streams; and (ii) make use of true class labels when they are available.We simulate a long data stream using the MNIST data set. This stream has twelve emerging new classes 7 . The initial training data set has 2 classes, and every subsequent period has 1000 instances from one emerging new class and two known classes. The maximum number of classes which can be handled by each SENCForest is set to 3. Other settings are the same as used in the last section. In addition, true class labels are assumed to be available in Q percentage of instances in the buffer before a model update. SENCForest with Q = 0%, 50% and 100% are compared with LACU-SVM in the experiment. Recall that, as in the previous experiment, LACU-SVM is given 100% true labels at each model update and an additional set of unlabelled instances; andECSMiner is also provided with 100% true labels at each model update. Figure 11 shows the average number of leaves of each SENCForest at the start of each time period. Note that a new SENCForest was produced at periods 2, 5, 8, 11, and the first twoSENCForests, A and B, were retired at periods 8 and 11, respectively. Table 4 shows the further 7 Classes are reused in the simulation when they are no more in use in the current period. Because this simulation needs a number of classes, that is why only the MNIST dataset, out of the five datasets, can be used in the long stream simulation. information about SENCForests(Q = 0%) at the start of each time period. The first three rows provide the overall information; and the last three lines show the detailed information of the only evolving SENCForest at each time period, e.g., periods 2, 3, 4 for SENCForest B , periods 5, 6, 7for SENCForest C and so on. Note that the number of leaves in anomaly regions may decrease asSENCForest grows. This happens when instances of new classes fall into few leaves only.The number of SENCForests is maintained at a preset memory limit through retiring not-in-useSENCForests. Note that the model size is constrained within the set limit of three SENCForests which allows the proposed method to deal with infinite data streams. In contrast, LACU-SVM continues to demand larger and larger memory size to accommodate larger training set size as the stream progresses.The result in Figure 10 (a) shows that SENCForest with Q = 0% maintains good predictive accuracy over the long stream. SENCForest is able to make use of true class labels to improve its performance along the stream. The extent of the improvement increases as Q increases. In contrast, the predictive accuracy of ECSMiner and LACU-SVM continued to decrease as the stream progressed.As a result, as shown in Figure 10(b), its training time continued to grow as the stream progressed.ECSMiner has the least model update time, because k-nearest neighbor is as base learner, which only spends time in building the clusters in buffer. But, that means it needs to save the cluster summary of each cluster into memory.  

Multiple new classes in a period
The emergence of multiple new classes in a period is a challenge in the SENC problem. AlthoughSENCForest is designed to deal with one emerging new class in each period, it can still perform well by treating these emerging classes in a period as a single new class. Figure 12 shows that SENCForest performs as well when every period has two emerging new classes. In this stream, there are three periods; each period has 2000 instances and 4 classes (i.e., two emerging new classes and two known classes).In the event that it is important to identify each class in each period, a clustering algorithm  #b0  can be used to achieve this aim before proceeding to do the model update.

Conclusions and future work
This paper contributes to decompose the SENC problem into three sub problems and posits that the ability to tackle the first sub problem of detecting emerging new classes effectively is crucial for the whole problem. The difficulty of the SENC problem is highlighted by the inability of existing methods to solve it satisfactorily.We show that the unsupervised-anomaly-detection-focused approach, coupled with an integrated method using completely random trees, provides a complete solution for the entire SENC problem. The current classification-focused approach has failed to provide one thus far.The strength of SENCForest is its ability to detect new class with high accuracy. The use of an unsupervised anomaly detector, incorporated with the new ability to differentiate between anomalies of known classes and instances of new classes, underlines the source of the strength.Existing supervised and semi-supervised methods are unable to achieve the same level of detection accuracy because the focus was on the second sub problem: classification, rather than the first sub problem: emerging new class detection.The fact that the unsupervised learner consists of completely random trees facilitate the use of a common core which can be converted to an effective classifier with ease. The common core also makes model updates in data streams to be a simple model adjustment, rather than training a completely new model as in most existing methods. Like in previous work, we show that the completely random trees are a classifier competitive to state-of-the-art classifiers, especially in the data stream context which demands fast model update and classification time.Our empirical evaluation shows that SENCForest outperforms eight existing methods, despite the fact that it was not given the true class labels in the entire data stream; and other methods were given the true class labels at each model update. In addition, it works effectively in long stream with emerging new classes under the limited memory environment. No existing methods have the capability to work under the same condition, as far as we know.In the future, we plan to improve the proposed method to deal with concept drift and to differentiate two or more emerging new classes before model updates. From a broader perspective, the proposed method is the first implementation of the unsupervised-anomaly-detection-focused approach to the SENC problem. We intend to explore other implementations of the same approach.

appendices


Parameter settings
The parameter settings of all algorithms used in the experiments are provided in Table 5. A 10-fold cross-validation on the training set is used in the parameter search to determine the final settings for all SVM algorithms. The parameter search for LOF is as described in  #b10 .ECSMiner employs K-means and K is set to 5 in the experiment.  MHAR: This data set [AGO + 12] is collected from 30 volunteers wearing a smart phone on the waist and performing 6 activities (walking, upstairs, downstairs, standing, sitting, laying). The embedded 3D-accelerometer and 3D-gyroscope of a Samsung Galaxy S2 smart phone were used to collect data at a constant rate of 50 Hz. This data set has 6 classes, 10299 instances and 561attributes.

Footnote
1 : This is a more stringent condition than previous studies (e.g., [MGK + 11]) which assume that true labels are available for model update, after some time delay.
2 : https://goker.wordpress.com/2011/12/30/outlier-detection-toolbox-in-matlab/ 3 http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html 4 https://kdd.ics.uci.edu/databases/covertype/covertype.data.html 5 http://cis.jhu.edu/ sachin/digit/digit.html