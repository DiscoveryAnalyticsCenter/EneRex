Multi-view Anomaly Detection via Probabilistic Latent Variable Models

Abstract
We propose a nonparametric Bayesian probabilistic latent variable model for multi-view anomaly detection, which is the task of finding instances that have inconsistent views. With the proposed model, all views of a non-anomalous instance are assumed to be generated from a single latent vector. On the other hand, an anomalous instance is assumed to have multiple latent vectors, and its different views are generated from different latent vectors. By inferring the number of latent vectors used for each instance with Dirichlet process priors, we obtain multi-view anomaly scores. The proposed model can be seen as a robust extension of probabilistic canonical correlation analysis for noisy multi-view data. We present Bayesian inference procedures for the proposed model based on a stochastic EM algorithm. The effectiveness of the proposed model is demonstrated in terms of performance when detecting multi-view anomalies and imputing missing values in multi-view data with anomalies.

Introduction
There has been great interest in multi-view learning, in which data are obtained from various information sources  #b6  #b5  #b27 . In a wide variety of applications, data are naturally comprised of multiple views. For example, an image can be represented by color, texture and shape information; a web page can be represented by words, images and URLs occurring on in the page; and a video can be represented by audio and visual features. In this paper, we consider the task of finding anomalies in multi-view data. The task is called horizontal anomaly detection  #b12 , or multi-view anomaly detection  #b16 . Anomalies in multi-view data are instances that have inconsistent features across multiple views. Multi-view anomaly detection is different from standard (singleview) anomaly detection. Single-view anomaly detection finds instances that do not conform to expected behavior  #b7 ).  Figure 1: A multi-view anomaly 'M' and a single-view anomaly 'S' in a two-view data set. Each letter represents an instance, and the same letter indicates the same instance. W d is a projection matrix for view d. Figure 1 (a,b) shows the difference between a multi-view anomaly and a single-view anomaly in a two-view data set. 'M' is a multi-view anomaly since 'M' belongs to different clusters in different views ('A-D' cluster in View 1 and 'E-J' cluster in View 2) and views of 'M' are not consistent. 'S' is a single-view anomaly since 'S' is located far from other instances in each view. However, both views of 'S' have the same relationship with the others (they are far from the other instances), and then 'S' is not a multi-view anomaly. Single-view anomaly detection methods, such as one-class support vector machines  #b23 , consider that 'S' is anomalous. On the other hand, we would like to develop a multi-view anomaly detection method that detects 'M' as anomaly, but not 'S'.Multi-view anomaly detection can be used for many applications, such as information disparity management  #b10 , purchase behavior analysis  #b12 , malicious insider detection  #b16 , and user aggregation from multiple databases. In information disparity management, multiple views can be obtained from documents written in different languages such as Wikipedia. Multi-view anomaly detection tries to find documents that contain different information across different languages. In purchase behavior analysis, multiple views for each item can be defined as its genre and its purchase history, i.e. a set of users who purchased the item. Multi-view anomaly detection can find movies inconsistently purchased by users based on the movie genre.We propose a probabilistic latent variable model for multi-view anomaly detection. With the proposed model, there is a latent space that is shared across all views. We assume that all views of a non-anomalous (normal) instance are generated using a single latent vector. On the other hand, an anomalous instance is assumed to have multiple latent vectors, and its different views are generated using different latent vectors, which indicates inconsistency across different views of the instance. Figure 1 (c) shows an example of a latent space shared by the two-view data. Two views of every non multi-view anomaly can be generated from a latent vector using view-dependent projection matrices. On the other hand, since two views of multi-view anomaly 'M' are not consistent, two latent vectors are required to generate the two views using the projection matrices.Since the number of latent vectors for each instance is unknown, we automatically infer it from the given data by using Dirichlet process priors. The inference of the proposed model is based on a stochastic EM algorithm. In the E-step, a latent vector is assigned for each view of each instance using collapsed Gibbs sampling while analytically integrating out latent vectors. In the M-step, projection matrices for mapping latent vectors into observations are estimated by maximizing the joint likelihood. By alternately iterating E-and M-steps, we infer the number of latent vectors used in each instance and calculate its anomaly score from the probability of using more than one latent vector.The proposed model can be seen as a robust extension of probabilistic canonical correlation analysis (PCCA)  #b4  #b32 . PCCA assumes that each instance has a latent vector, and the estimated parameters suffer effects from anomalies. In contrast, by preparing multiple latent vectors for anomalies, the proposed model can infer latent vectors and projection matrices properly without influence of multi-view anomalies.

Related Work
Anomaly detection has had a wide variety of applications, such as credit card fraud detection  #b0 , intrusion detection for network security  #b19 , and analysis for healthcare data  #b2 . However, most existing anomaly detection techniques assume data with a single view, i.e. a single observation feature set.A number of anomaly detection methods for two-view data have been proposed  #b11  #b25  #b28  #b29  #b33 ). However, they cannot be used for data with more than two views.  #b12  proposed a HOrizontal Anomaly Detection algorithm (HOAD) for finding anomalies from multi-view data. With HOAD, all views of all instances are simultaneously embedded in a low-dimensional space with the constraints that different views of the same instance are embedded close together using a spectral framework  #b17 . Then, anomalies are detected according to distance between the embedded locations of different views of each instance. In HOAD, there are hyperparameters including a weight for the constraint that require the data to be labeled as anomalous or not for tuning, and the performance is sensitive to the hyperparameters. On the other hand, the parameters with the proposed model can be estimated from the given multiview data without label information by maximizing the likelihood. In addition, because the proposed model is a probabilistic generative model, we can extend it in a probabilistically principled manner, for example, for handling missing data and combining with other probabilistic models.  #b16  proposed multi-view anomaly detection methods using consensus clustering. They found anomalies based on the inconsistency of clustering results across multiple views. Therefore, they cannot find inconsistency within a cluster.  #b9  proposed a method for filtering instances that are corrupted by background noise from multi-view data. The multi-view anomalies considered in this paper include not only instances corrupted by background noise but also instances categorized into different foreground classes across views, and instances with inconsistent views even if they belong to the same cluster.The proposed model is related to probabilistic canonical correlation analysis (PCCA). When every instance has only one latent vector, the proposed model can be seen as PCCA. PCCA, or canonical correlation analysis (CCA), can be used for multi-view anomaly detection. With PCCA, a latent vector that is shared by all views for each instance and a linear projection matrix for each view are estimated by maximizing the likelihood, or minimizing the reconstruction error of the given data. The reconstruction error for each instance can be used as an anomaly score. However, the reconstruction errors are not reliable because they are calculated from parameters that are estimated using data with anomalies by assuming that all of the instances are non-anomalous. On the other hand, because the proposed model simultaneously estimates the parameters and infers anomalies, the estimated parameters are not contaminated by the anomalies. With some CCA-related methods, each latent vector is factorized into shared and private components across different views  #b3  #b14  #b22  #b31 . They assume that every instance has shared and private parts that are the same dimensionality for all instances, and they cannot be used for multi-view anomaly detection. In contrast, the proposed model can detect anomalies by assuming that nonanomalous instances have only shared latent vectors, and anomalies have private latent vectors. Recently,  #b1  proposed a multi-view anomaly detection method. However, since the method is based on clustering, it cannot find anomalies when there are no clusters in the given data.

Proposed Model
Suppose that we are given N instances with D viewsX = {X n } N n=1 , where X n = {x nd } D d=1is a set of multi-view observation vectors for the nth instance, and x nd ∈ R M d is the observation vector of the dth view. The task is to find anomalous instances that have inconsistent observation features across multiple views.We propose a probabilistic latent variable model for this task. The proposed model assumes that each instance has potentially a countably infinite number of latent vectors Z n = {z nj } ∞ j=1 , where z nj ∈ R K . Each view of an instance x nd is generated depending on a view-specific projection matrix W d ∈ R M d ×K and a latent vector z ns nd that is selected from a set of latent vectors Z n . Here, s nd ∈ {1, · · · , ∞} is the latent vector assignment of x nd . When the instance is non-anomalous and all its views are consistent, all of the views are generated from a single latent vector. In other words, the latent vector assignments for all views are the same, s n1 = s n2 = · · · = s nD . When it is an anomaly and some views are inconsistent, different views are generated from different latent vectors, and some latent vector assignments are different, i.e. s nd = s nd ′ for some d = d ′ .Specifically, the proposed model is an infinite mixture model, where the probability for the dth view of the nth instance is given byp(x nd |Z n , W d , θ n , α) = ∞ j=1 θ nj N (x nd |W d z nj , α −1 I),(1)where θ n = {θ nj } ∞ j=1 are the mixture weights, θ nj represents the probability of choosing the jth latent vector, α is a precision parameter, N (µ, Σ) denotes the Gaussian distribution with mean µ and covariance matrix Σ, and I is the identity matrix. We use a Dirichlet process for the prior of mixture weight θ n . Its use enables us to automatically infer the number of latent vectors for each instance from the given data.The complete generative process of the proposed model for multi-view instances X is as follows, s nd ∼ Discrete(θ n ) ii. Draw an observation vector x nd ∼ N (W d z ns nd , α −1 I)Here, Stick(γ) is the stick-breaking process  #b24 ) that generates mixture weights for a Dirichlet process with concentration parameter γ, and r is the relative precision for latent vectors. α is shared for observation and latent vector precision because it makes it possible to analytically integrate out s x z r θ γ w a b α D ∞ NS = {{s nd } D d=1 } N n=1 is given by p(X, S|W , a, b, r, γ) = p(S|γ)p(X|S, W , a, b, r),(2)whereW = {W d } D d=1. Because we use conjugate priors, we can analytically integrate out mixture weights Θ = {θ n } N n=1 , latent vectors Z, and precision parameter α. Here, we use a Dirichlet process prior for multinomial parameter θ n , and a Gaussian-Gamma prior for latent vector z nj . By integrating out mixture weights Θ, the first factor is calculated byp(S|γ) = N n=1 γ Jn Jn j=1 (N nj − 1)! γ(γ + 1) · · · (γ + D − 1) ,(3)where N nj represents the number of views assigned to the jth latent vector in the nth instance, and J n is the number of latent vectors of the nth instance for which N nj > 0. By integrating out latent vectors Z and precision parameter α, the second factor of (2) is calculated byp(X|S, W , a, b, r) = (2π) − N d M d 2 r K n Jn 2 b a b ′a ′ Γ(a ′ ) Γ(a) N n=1 Jn j=1 |C nj | 1 2 , (4) where a ′ = a + N D d=1 M d 2 ,(5)b ′ = b + 1 2 N n=1 D d=1 x ⊤ nd x nd − 1 2 N n=1 Jn j=1 µ ⊤ nj C −1 nj µ nj ,(6)µ nj = C nj d:s nd =j W ⊤ d x nd ,(7)C −1 nj = d:s nd =j W ⊤ d W d + rI.(8)The posterior for the precision parameter α is given byp(α|X, S, W , a, b) = Gamma(a ′ , b ′ ),(9)and the posterior for the latent vector z nj is given byp(z nj |X, S, W , r) = N (µ nj , α −1 C nj ).(10)The proposed model is a generalization of either probabilistic principal component analysis (PPCA)  #b30  or probabilistic canonical correlation analysis (PCCA). When all views are generated from different latent vectors for every instance, the proposed model corresponds to PPCA that is performed independently for each view. When all views are generated from a single latent vector for every instance, the proposed model corresponds to PCCA with spherical noise.

Inference
We describe inference procedures for the proposed model based on a stochastic EM algorithm, in which collapsed Gibbs sampling of latent vector assignments S and the maximum joint likelihood estimation of projection matrices W are alternately iterated while analytically integrating out the latent vectors Z, mixture weights Θ and precision parameter α. By integrating out latent vectors, we do not need to explicitly infer the latent vectors, leading to a robust and fast-mixing inference.Let ℓ = (n, d) be the index of the dth view of the nth instance for notational convenience. In the E-step, given the current state of all but one latent assignment s ℓ , a new value for s ℓ is sampled from {1, · · · , J n\ℓ + 1} according to the following probability,p(s ℓ = j|X, S \ℓ , W , a, b, r, γ) ∝ p(s ℓ = j, S \ℓ |γ) p(S \ℓ |γ) · p(X|s ℓ = j, S \ℓ , W , a, b, r) p(X \ℓ |S \ℓ , W , a, b, r) ,(11)where \ℓ represents a value or set excluding the dth view of the nth instance. The first factor is given byp(s ℓ = j, S \ℓ |γ) p(S \ℓ |γ) = N nj\ℓ D−1+γ if j ≤ J n\ℓ γ D−1+γ if j = J n\ℓ + 1,(12)using (3), where j ≤ J n\ℓ is for existing latent vectors, and j = J n\ℓ + 1 is for a new latent vector. By using (4), the second factor is given byp(X|s ℓ = j, S \ℓ , W , a, b, r) p(X \ℓ |S \ℓ , W , a, b, r) = (2π) − M d 2 r I(j=J n\ℓ +1) K 2 b ′a ′ \ℓ \ℓ b ′a ′ s ℓ =j s ℓ =j Γ(a ′ s ℓ =j ) Γ(a ′ \ℓ ) |C j,s ℓ =j | 1 2 |C j\ℓ | 1 2 ,(13)where I(·) represents the indicator function, i.e. I(A) = 1 if A is true and 0 otherwise, and subscript s ℓ = j indicates the value when x ℓ is assigned to the jth latent vector as follows,a ′ s ℓ =j = a ′ ,(14)b ′ s ℓ =j = b ′ \ℓ + 1 2 x ⊤ ℓ x ℓ + 1 2 µ ⊤ nj\ℓ C −1 nj\ℓ µ nj\ℓ − 1 2 µ ⊤ nj,s ℓ =j C −1 nj,s ℓ =j µ nj,s ℓ =j , (15) µ nj,s ℓ =j = C nj,s ℓ =j (W ⊤ d x ℓ + C −1 nj\ℓ µ nj\ℓ ),(16)C −1 nj,s ℓ =j = W ⊤ d W d + C −1 nj\ℓ .(17)Intuitively, if the current view cannot be modeled well by existing latent vectors, a new latent vector is used, which indicates that the view is inconsistent with the other views.In the M-step, the projection matrices W are estimated by maximizing the logarithm of the joint likelihood (2). We can maximize the likelihood by using a gradient-based numerical optimization method such as the quasi-Newton method  #b18 . The gradient of the joint log likelihood is calculated by∂ log p(X, S|W , a, b, r, γ) ∂W d = −W d N n=1 Jn j=1 C nj − a ′ b ′ N n=1 W d µ ns nd µ ⊤ ns nd − x nd µ ⊤ ns nd .(18)When we iterate the E-step that samples the latent vector assignment s nd by employing (11) for each view d = 1, . . . , D in each instance n = 1, . . . , N , and the M-step that maximizes the joint likelihood using (18) with respect to the projection matrix W d for each view d = 1, . . . , D, we obtain an estimate of the latent vector assignments and projection matrices.For an anomaly score, we use the probability that the instance uses more than one latent vector. It is estimated by using the samples obtained in the inference as follows,v n = 1 H H h=1 I(J (h) n > 1),(19)where J (h) n is the number of latent vectors used by the nth instance in the hth iteration of the Gibbs sampling after the burn-in period, and H is the number of the iterations.We can use cross-validation to select an appropriate dimensionality for the latent space K. With cross-validation, we assume that some features are missing from the given data, and infer the model with a different K. Then, we select the smallest K value that has performed the best at predicting missing values.

Experiments


Data
We evaluated the proposed model quantitatively by using 11 data sets, which we obtained from the LIBSVM data sets  #b8 . We generated multiple views by randomly splitting the features, and anomalies were added by swapping views of two randomly selected instances as  #b12  did in their experiments. Splitting data does not generate anomalies. Therefore, we can evaluate methods while controlling the anomaly rate properly. By swapping, although single-view anomalies cannot be created since the distribution for each view does not change, multi-view anomalies are created.

Comparing methods
We compared the proposed model with probabilistic canonical correlation analysis (PCCA), horizontal anomaly detection (HOAD)  #b12 , consensus clustering based anomaly detection (CC)  #b16 , and one-class support vector machine (OCSVM)  #b23 . For PCCA, we used the proposed model in which the number of latent vectors was fixed at one for every instance. The anomaly scores obtained with PCCA were calculated based on the reconstruction errors. HOAD requires to select an appropriate hyperparameter value for controlling the constraints whereby different views of the same instance are embedded close together. We ran HOAD with different hyperparameter settings {0.1, 1, 10, 100}, and show the results that achieved the highest performance for each data set. For CC, first we clustered instances for each view using spectral clustering  #b17 . We set the number of clusters at 20, which achieved a good performance in preliminary experiments. Then, we calculated anomaly scores by the likelihood of consensus clustering when an instance was removed. OCSVM is a representative method for single-view anomaly detection. To investigate the performance of a single-view method for multi-view anomaly detection, we included OCSVM as a comparison method. For OCSVM, multiple views are concatenated in a single vector, then use it for the input. We used Gaussian kernel. In the proposed model, we used γ = 1, a = 1, and b = 1 for all experiments. The number of iterations for the Gibbs sampling was 500, and the anomaly score was calculated by averaging over the multiple samples.

Multi-view anomaly detection
For the evaluation measurement, we used the area under the ROC curve (AUC). A higher AUC indicates a higher anomaly detection performance. Figure 3 shows AUCs with different rates of anomalies using two-view data sets, which are averaged over 50 experiments. For the dimensionality of the latent space, we used K = 5 for the proposed model, PCCA, and HOAD. In general, as the anomaly rate increases, the performance decreases. The proposed model achieved the best performance with eight of the 11 data sets. This result indicates that the proposed model can find anomalies effectively by inferring a number of latent vectors for each instance. The performance of CC was low because it assumes that there are clusters for each view, and it cannot find anomalies within clusters. The AUC of OCSVM was low, because it is a singleview anomaly detection method, which considers instances anomalous that are different from others within a single view. Multi-view anomaly detection is the task to find instances that have inconsistent features across views, but not inconsistent features within a view. The computational time needed for PCCA was 2 sec, and that needed for the proposed model was 35 sec with wine data. Figure 4 shows AUCs with different numbers of views. For many cases with different latent dimensionalities and different numbers of views, the proposed model achieved the best performance. Figure 5 shows AUCs with different dimensionalities of latent vectors using data sets whose anomaly rate is 0.4. When the dimensionality was very low (K = 1 or 2), the AUC was low in most of the data sets, because lowdimensional latent vectors cannot represent the observation vectors well. With all the methods, the AUCs were relatively stable when the latent dimensionality was higher than four.

Single-view anomaly detection
We illustrated that the proposed model does not detect single-view anomalies using synthetic single-view anomaly data. With the synthetic data, latent vectors for single-view anomalies were generated from N (0, √ 10I), and those for non-anomalous instances were generated from N (0, I). Since each of the anomalies has only one single latent vector, it is not a multi-view anomaly. The numbers of anomalous and non-anomalous instances were 5 and 95, respectively. The dimensionalities of the observed and latent spaces were five and three, respectively. Table 1 shows the average AUCs with the single-view anomaly data, which are averaged over 50 different data sets. The low AUC of the proposed model indicates that it does not consider single-view anomalies as anomalies. On the other hand, the AUC of the one-class SVM (OCSVM) was high because OCSVM is a single-view anomaly detection method, and it leads to low multi-view anomaly detection performance.  

Missing value imputation
We can use the proposed model for missing value imputation in multi-view data with anomalies. The proposed model was evaluated by comparison with PCCA and Average, which estimates missing values by averaging the observa- tion features over instances. We randomly selected 5% of the values as missing. Table 2 shows the mean squared errors for missing value imputation with data sets whose anomaly rate is 0.4. For all of the data sets, the proposed model achieved the lowest error. This result shows the effectiveness of the proposed model for imputing missing values in multi-view data with anomalies.

Latent dimensionality estimation
We also evaluated the performance of the proposed model to estimate the latent dimensionality from the given data. For this purpose, we used synthetic data sets, where the true latent dimensionality was known. Synthetic data sets with two views were generated according to the following procedure. First, we sampled a latent vector for each non-anomalous instance, and two latent vectors for each anomaly from a K ⋆ -dimensional Gaussian distribution with mean 0 and covariance I. Here, K ⋆ is the true latent dimensionality, and we used K ⋆ = 5. Second, we generated projection matrices W where each element was drawn from a Gaussian distribution with mean 0 and variance 1. Finally, we generated observations using the latent vectors and projection matrices with Gaussian noise. Figure 6 shows the mean squared errors for missing value im- putation using synthetic data sets with different anomaly rates when the latent dimensionality of the proposed model was changed, K ∈ {2, · · · , 10}. With the proposed model, the mean squared errors decreased rapidly when K < 5, and they did not vary greatly when K ≥ 5, in all of the data sets with different anomaly rates. This result indicates that the proposed model can estimate the latent dimensionality from multi-view data with anomalies. In contrast, with PCCA, the mean squared errors decreased even when K > 5 especially in data sets with high anomaly rates. This implies that PCCA requires a higher latent dimensionality for modeling data with anomalies than the true latent dimensionality. When the anomaly rate is small, the proposed model and PCCA perform similarly as shown in Figure 6 (a). Because we use Bayesian inference for the proposed model and PCCA based on Markov chain Monte Carlo in our experiments, we can avoid overfitting when the latent dimensionality is higher than the true dimensionality as shown in Bayesian probabilistic matrix factorization  #b21 .

Application to movie data
For an application of multi-view anomaly detection, we analyzed inconsistency between movie rating behavior and genre in MovieLens data  #b13 ). An instance corresponds to a movie, where the first view represents whether the movie is rated or not by users, and the second view represents the movie genre. Both views consists of binary features. We used 338 movies, 943 users and 19 genres. Table 3 shows high and low anomaly score movies when we analyzed the movie data by the proposed method with K = 5. 'The Full Monty' and 'Liar Liar' were categorized in 'Comedy' genre. They are rated by not only users who likes 'Comedy', but also who likes 'Romance' and 'Action-Thriller'. 'The Professional' was anomaly because it was rated by two different user groups, where a group prefers 'Romance' and the other prefers 'Action'. Since 'Star Trek' series are typical Sci-Fi and liked by specific users, its anomaly score was low.  

Conclusion
We proposed a generative model approach for multi-view anomaly detection, which finds instances that have inconsistent views. In the experiments, we confirmed that the proposed model could perform much better than existing methods for detecting multi-view anomalies, and for imputing missing values in multi-view data with anomalies. There are several avenues that can be pursued for future work. The proposed model assumes the linearity of observations with respect to their latent vectors. We can relax this assumption by using Gaussian processes  #b15  #b20  #b26 . We would like to evaluate our proposed model for other real applications. CCA is successfully used for a wide variety of real multi-view data, such as multilingual data and image-annotation data. This indicates that the proposed model based on CCA can handle the same wide variety of real data.