Title-Guided Encoding for Keyphrase Generation

Abstract
Keyphrase generation (KG) aims to generate a set of keyphrases given a document, which is a fundamental task in natural language processing (NLP). Most previous methods solve this problem in an extractive manner, while recently, several attempts are made under the generative setting using deep neural networks. However, the state-of-the-art generative methods simply treat the document title and the document main body equally, ignoring the leading role of the title to the overall document. To solve this problem, we introduce a new model called Title-Guided Network (TG-Net) for automatic keyphrase generation task based on the encoderdecoder architecture with two new features: (i) the title is additionally employed as a query-like input, and (ii) a titleguided encoder gathers the relevant information from the title to each word in the document. Experiments on a range of KG datasets demonstrate that our model outperforms the state-ofthe-art models with a large margin, especially for documents with either very low or very high title length ratios.

Introduction
Keyphrases are short phrases that can quickly provide the main information of a given document (the terms "document", "source text" and "context" are interchangeable in this study, and all of them represent the concatenation of the title and the main body.). Because of the succinct and accurate expression, keyphrases are widely used in information retrieval  #b13 , document categorizing  #b11 , opinion mining  #b1 , etc. Due to the huge potential value, various automatic keyphrase extraction and generation methods have been developed. As shown in Figure 1, the input usually consists of the title and the main body, and the output is a set of keyphrases.Most typical automatic keyphrase extraction methods  #b36  #b24  #b26  focus on extracting present keyphrases like "relevance profiling" in Figure 1, which are the exact phrases appearing in the source text. The main ideas among them are identifying candidate phrases first and then ranking algorithms. However, these methods ignore the semantic meaning underlying the context content and cannot generate absent keyphrases like "interactive information retrieval", which do not appear in the source text.To overcome the above drawbacks, several encoderdecoder based keyphrase generation methods have been proposed including CopyRNN  #b25  and Copy-CNN  #b38 . First, these methods treat the title and the main body equally and concatenate them as the only source text input. Then, the encoder maps each source text word into a hidden state vector which is regarded as the contextual representation. Finally, based on these representations, the decoder generates keyphrases from a predefined vocabulary regardless of the presence or absence of the keyphrases. A serious drawback of these models is that they ignore the leading role of the title and consequently fail to sufficiently utilize the already summarized information in it.It is a widely agreed fact that the title can be viewed as a high-level summary of a document and the keyphrases provide more details of the key topics introduced in the document  #b19 . They play a similar and complementary role with each other. Therefore, keyphrases should have close semantic meaning to the title  #b19 . For example, as shown in Figure 1, the title contains most of the salient points reflected by these keyphrases including "re-trieval", "profiling", and "evaluation". Statistically, we study the proportion of keyphrases related to the title on the largest KG dataset and show the results in Table 1. For simplicity, we define a TitleRelated keyphrase as the keyphrase containing at least one common non-stop-word with the title. From Table 1, we find that about 33% absent keyphrases are Ti-tleRelated. For present keyphrases, the TitleRelated percentage is up to around 60%. By considering the fact that the length of a title is usually only 3%-6% of the corresponding source text, we can conclude that the title, indeed, contains highly summative and valuable information for generating keyphrases. Moreover, information in the title is also helpful in reflecting which part of the main body is essential, such as the part containing the same or related information with the title. For instance, in Figure 1, the point "evaluation" in the title can guide us to focus on the part "... task-oriented, comparative evaluation ..." of the main body, which is highly related to the absent keyphrase "task-oriented evaluation".To sufficiently leverage the title content, we introduce a new title-guided network by taking the above fact into the keyphrase generation scenario. In our model, the title is additionally treated as a query-like input in the encoding stage. First, two bi-directional Gated Recurrent Unit (GRU)  #b2  layers are adopted to separately encode the context and the title into corresponding contextual representations. Then, an attention-based matching layer is used to gather the relevant title information for each context word according to the semantic relatedness. Since the context is the concatenation of the title and the main body, this layer implicitly contains two parts. The former part is the "title to title" self-matching, which aims to make the salient information in the title more important. The latter part is the "main body to title" matching wherein the title information is employed to reflect the importance of information in the main body. Next, an extra bi-directional GRU layer is used to merge the original contextual information and the corresponding gathered title information into the final title-guided representation for each context word. Finally, the decoder equipped with attention and copy mechanisms utilizes the final title-guided context representation to predict keyphrases. We evaluate our model on five real-world benchmarks, which test the ability of our model to predict present and absent keyphrases. Using these benchmarks, we demonstrate that our model can effectively exploit the title information and it outperforms the relevant baselines by a significant margin: for present (absent) keyphrase prediction, the improvement gain of F1-measure at 10 (Recall at 50) score is up to 9.4% (19.1%) compared to the best baseline on the largest dataset. Besides, we probe the performance of our model and a strong baseline CopyRNN on documents with different title length ratios (i.e., the title length over the context length). Experimental results show that our model consistently improves the performance with large gains, especially for documents with either very low or very high title length ratios.Our main contributions consist of three parts: • A new perspective on keyphrase generation is explored, which sufficiently employs the title to guide the keyphrase prediction process. • A novel TG-Net model is proposed, which can effectively leverage the useful information in the title. • The overall empirical results on five real-world benchmarks show that our model outperforms the state-ofthe-art models significantly on both present and absent keyphrase prediction, especially for documents with either very low or very high title length ratios.

Related Work Automatic Keyphrase Extraction
Most of the automatic keyphrase extraction methods consist of two steps. Firstly, the candidate identification step obtains a set of candidate phrases such as phrases with some specific part-of-speech (POS) tags ( (2010) proposes a graph-based ranking algorithm which initializes the importance score of title phrases as one and the others as zero and then propagates the influence of title phrases iteratively. The biggest difference between Li et al. (2010) and our method is that our method utilizes the contextual information of the title to guide the context encoding, while their model only considers the phrase occurrence in the title. Liu et al. (2011) models keyphrase extraction process as a translation operation from a document to keyphrases. The title is used as the target output to train the translator. Compared with our model, one difference is that this method still cannot handle semantic meaning of the context. The other is that our model regards the title as an extra query-like input instead of a target output. , we regard the title as an extra query-like input to guide the source context encoding. Consequently, we propose a TG-Net model to explicitly explore the useful information in the title. In this paper, we focus on how to incorporate a title-guided encoding into the RNN-based model, but it is also convenient to apply this idea to the CNN-based model in a similar way.

Problem Definition
We denote vectors with bold lowercase letters, matrices with bold uppercase letters and sets with calligraphy letters. We denote Θ as a set of parameters and W as a parameter matrix.Keyphrase generation (KG) is usually formulated as follows: given a context x, which is the concatenation of the title and the main body, output a set of keyphrasesY = {y i } i=1,...,M where M is the keyphrase number of x.Here, the context x = [x 1 , . . . , x Lx ] and each keyphrase y i = [y i 1 , . . . , y i L y i ] are both word sequences, where L x is the length (i.e., the total word number) of the context and L y i is the length of the i-th produced keyphrase y i .To adapt the encoder-decoder framework, M contextkeyphrase pairs {(x, y i )} i=1,...,M are usually split. Since we additionally use the title t = [t 1 , . . . , t Lt ] with length L t as an extra query-like input, we split M context-titlekeyphrase triplets {(x, t, y i )} i=1,...,M instead of contextkeyphrase pairs to feed our model. For conciseness, we use (x, t, y) to represent such a triplet, where y is one of its target keyphrases.

Our Proposed Model


Title-Guided Encoder Module
As shown in Figure 2, the title-guided encoder module consists of a sequence encoding layer, a matching layer, and a! " ! # ! $ ! % ! & ' " ' # ' $ ' % ' & Title Main Body ( " ( # Title . . .

Sequence Encoding Layer
Matching LayerMerging Layer[' $ ; + $ ]The source context -The title .' $  merging layer. First, the sequence encoding layer reads the context input and the title input and learns their contextual representations separately. Then the matching layer gathers the relevant title information for each context word reflecting the important parts of the context. Finally, the merging layer merges the aggregated title information into each context word producing the final title-guided context representation.

Attention-based Matching
/ ! $ ⊕ ' $ / ! " ⊕ ' " / ! # ⊕ ' # / ! % ⊕ ' % / ! & ⊕ ' & ' " ' # ' % ' & -" -# -$ -% -& . " . # ( " ( # .Sequence Encoding Layer At first, an embedding lookup table is applied to map each word within the context and the title into a dense vector with a fixed size d e . To incorporate the contextual information into the representation of each word, two bi-directional GRUs  #b2 ) are used to encode the context and the title respectively:− → u i = GRU 11 (x i , − → u i−1 ), (1) ← − u i = GRU 12 (x i , ← − u i+1 ), (2) − → v j = GRU 21 (t j , − → v j−1 ), (3) ← − v j = GRU 22 (t j , ← − v j+1 ),(4) where i = 1, 2, . . . , L x and j = 1, 2, . . . , L t . x i and t j are the d e -dimensional embedding vectors of the i-th context word and the j-th title word separately.− → u i , ← − u i , − → v j , and ← − v j are d/2-dimensional hidden vectors where d is the hidden dimension of the bi-directional GRUs. The concatenations u i = [ − → u i ; ← − u i ] ∈ R d and v j = [ − → v j ; ← − v j ]∈ R d are used as the contextual vectors for the i-th context word and the j-th title word respectively.

Matching Layer
The attention-based matching layer is engaged to aggregate the relevant information from the title for each word within the context. The aggregation operationc i = attn(u i , [v 1 , v 2 , . . . , v Lt ]; W 1 ) is as follows: c i = Lt j=1 α i,j v j ,(5)α i,j = exp(s i,j )/ Lt k=1 exp(s i,k ),(6)s i,j = (u i ) T W 1 v j ,(7)where c i ∈ R d is the aggregated information vector for the i-th word of x. α i,j (s i,j ) is the normalized (unnormalized) attention score between u i and v j .Here, the matching layer is implicitly composed of two parts because the context is a concatenation of the title and the main body. The first part is the "title to title" selfmatching part, wherein each title word attends the whole title itself and gathers the relevant title information. This part is used to strengthen the important information in the title itself, which is essential to capture the core information because the title already contains much highly summative information. The other part is the "main body to title" matching part wherein each main body word also aggregates the relevant title information based on semantic relatedness. In this part, the title information is employed to reflect the importance of information in the main body based on the fact that the highly title-related information in the main body should contain core information. Through these two parts, this matching layer can utilize the title information much more sufficiently than any of the previous sequence to sequence methods.Merging Layer Finally, the original contextual vector u i and the aggregated information vector c i are used as the inputs to another information merging layer:− → m i = GRU 31 ([u i ; c i ], − → m i−1 ), (8) ← − m i = GRU 32 ([u i ; c i ], ← − m i+1 ),(9)m i = λu i + (1 − λ)[ − → m i ; ← − m i ],(10)where [u i ; c i ] ∈ R 2d , − → m i ∈ R d/2 , ← − m i ∈ R d/2 , [ − → m i , ← − m i ] ∈ R d , and m i ∈ R d . The u i in Eq.(10) is a residual connection, and λ ∈ (0, 1) is the corresponding hyperparameter. Eventually, we obtain the title-guided contextual representation of the context (i.e., [ m 1 , m 2 , . . . , m Lx ]), which is regarded as a memory bank for the later decoding process.

Decoder Module
After encoding the context into the title-guided representation, we engage an attention-based decoder  #b22  incorporating with copy mechanism (See, Liu, and  #b31  to produce keyphrases. Only one foward GRU is used in this module:h t = GRU 4 ([e t−1 ;h t−1 ], h t−1 ),(11)c t = attn(h t , [ m 1 , m 2 , . . . , m Lx ]; W 2 ),(12)h t = tanh(W 3 [ĉ t ; h t ]),(13)where t = 1, 2, . . . , L y , e t−1 ∈ R de is the embedding of the (t − 1)-th predicted word wherein e 0 is the embedding of the start token,ĉ t ∈ R d is the aggregated vector for h t ∈ R d from the memory bank [ m 1 , m 2 , . . . , m Lx ], andh t ∈ R d is the attentional vector at time step t. Consequently, the predicted probability distribution over the predefined vocabulary V for current step is computed by:P v (y t |y t−1 , x, t) = softmax(W vht + b v ),(14)where y t−1 = [y 1 , . . . , y t−1 ] is the previous predicted word sequence, and b v ∈ R |V| is a learnable parameter vector.Before generating the predicted word, a copy mechanism is adopted to efficiently exploit the in-text information and to strengthen the extraction capability of our model. We follow See, Liu, and Manning (2017) and first calculate a soft switch between generating from the vocabulary and copying from the source context x at time step t:g t = σ(w T ght + b g ),(15)where w g ∈ R d is a learnable parameter vector and b g is a learnable parameter scalar. Eventually, we get the final predicted probability distribution over the dynamic vocabulary V ∪ X , where X are all words appearing in the source context. For simplicity, we use P v (y t ) and P f inal (y t ) to denote P v (y t |y t−1 , x, t) and P f inal (y t |y t−1 , x, t) respectively:P f inal (y t ) = (1 − g t )P v (y t ) + g t i:xi=ytα t,i ,(16)whereα t,i is the normalized attention score between h t and m i . For all out-of-vocabulary (OOV) words (i.e., y t / ∈ V), we set P v (y t ) as zero. Similarly, if word y t does not appear in the source context x (i.e., y t / ∈ X ), the copy probability i:xi=ytα t,i is set as zero.

Training
We use the negative log likelihood loss to train our model:L = − Ly t=1 logP f inal (y t |y t−1 , x, t; Θ),(17)where L y is the length of target keyphrase y and y t is the t-th target word in y, and Θ represents all the learnable parameters.

Experiment Settings
The keyphrase prediction performance is first evaluated by comparing our model with the popular extractive methods and the state-of-the-art generative methods on five realworld benchmarks. Then, comparative experiments of different title length ratios are performed on our model and CopyRNN for further model exploration. Finally, an ablation study and a case study are conducted to better understand and interpret our model. The experiment results lead to the following findings: • Our model outperforms the state-of-the-art models on all the five benchmark datasets for both present and absent keyphrase prediction. • Our model consistently improves the performance on various title length ratios and obtains relative higher improvement gains for both very low and very high title length ratios. • The title-guided encoding part and the copy part are consistently effective in both present and absent keyphrase prediction tasks. We implement the models using PyTorch 

Training Dataset
Because of the public accessibility, many commonly-used scientific publication datasets are used to evaluate the explored KG methods. This study also focuses on generating keyphrases from scientific publications. For all the generative models (i.e. our TG-Net model as well as all the encoder-decoder baselines), we choose the largest publicly available keyphrase generation dataset KP20k constructed by  #b25  as the training dataset. KP20k consists of a large amount of high-quality scientific publications from various computer science domains. Totally 567,830 articles are collected in this dataset, where 527,830 for training, 20,000 for validation, and 20,000 for testing. Both the validation set and testing set are randomly selected. Since the other commonly-used datasets are too small to train a reliable generative model, we only train these generative models on KP20k and then test the trained models on all the testing part of the datasets listed in Table 2. As for the traditional supervised extractive baseline, we follow Meng et al. (2017) and use the dataset configuration shown in Table 2. To avoid the out-of-memory problem, for KP20k, we use the validation set to train the traditional supervised extractive baseline.

Testing Datasets
Besides KP20k, we also adopt other four widely-used scientific datasets for comprehensive testing, including Inspec  #b12  

Implementation Details
For all datasets, the main body is the abstract, and the context is the concatenation of the title and the abstract. During preprocessing, various operations are performed including lowercasing, tokenizing by CoreNLP  #b23 , and replacing all the digits with the symbol digit . We define the vocabulary V as the 50,000 most frequent words. We set the embedding dimension d e to 100, the hidden size d to 256, and λ to 0.5. All the initial states of GRU cells are set as zero vectors except that h 0 is initialized as[ − → m Lx ; ← − m 1 ].We share the embedding matrix among the context words, the title words, and the target keyphrase words. All the trainable variables including the embedding matrix are initialized randomly with uniform distribution in [-0.1, 0.1]. The model is optimized by Adam  #b16  with batch size = 64, initial learning rate = 0.001, gradient clipping = 1, and dropout rate = 0.1. We decay the learning rate into the half when the evaluation perplexity stops dropping. Early stopping is applied when the validation perplexity stops dropping for three continuous evaluations. During testing, we set the maximum depth of beam search as 6 and the beam size as 200. We repeat the experiments of our model three times using different random seeds and report the averaged results.We do not remove any predicted single-word phrase in the post-processing for KP20k during testing, which is different from Meng et al. (2017), since our model is trained on this dataset and it can effectively learn the distribution of the single-word keyphrases. But for other testing datasets, we only keep the first predicted single-word phrase following Meng et al. (2017).

Baseline Models and Evaluation Metric
For present keyphrase predicting experiment, we use two unsupervised models including TF-IDF and Tex-tRank  #b26 , and one supervised model Maui (Medelyan, Frank, and Witten 2009) as our traditional extraction baselines. Besides, we also select Copy-RNN  #b25  and CopyCNN (Zhang, Fang, and Weidong 2017), the two state-of-the-art encoder-decoder models with copy mechanism  #b9 , as the baselines for present keyphrase prediction task. As for absent keyphrase prediction, since traditional extraction baselines cannot generate such keyphrases, we only choose CopyRNN and CopyCNN as the baseline models. For all baselines, we use the same setups as Meng et al. (2017) and Zhang, Fang, and Weidong (2017).The recall and F-measure (F 1 ) are employed as our metrics for evaluating these algorithms. Recall is the number of correctly predicted keyphrases over the total number of target kayphrases. F 1 score is computed based on the Recall and the Precision, wherein Precision is defined as the number of correctly predicted keyphrases over the total predicted keyphrase number. Following Meng et al. (2017) and  #b38 , we also employ Porter Stemmer for preprocessing when determining whether two keyphrases are matched.

Results and Analysis Present Keyphrase Predicting
In this section, we compare present keyphrase prediction ability of these models on the five real-world benchmark datasets. The F-measures at top 5 and top 10 predictions of each model are shown in Table 3.From this table, we find that all the generative models significantly outperforms all the traditional extraction baselines. Besides, we also note that our TG-Net model achieves the best performance on all the datasets with significant margins. For example, on KP20k dataset, our model improves 9.4% (F 1 @10 score) than the best generative model Copy-CNN. Compared to CopyRNN which also applies an RNN-

Model


Inspec
Krapivin   NUS SemEval KP20k F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10

Absent Keyphrase Predicting
In this setting, we consider the absent keyphrase predicting ability which requires the understanding of the semantic meaning of the context. Only the absent target keyphrases and the absent predictions are preserved for this evaluation. Generally, recalls at top 10 and top 50 predictions are engaged as the metrics to evaluate how many absent target keyphrases are correctly predicted. The performance of all models is listed in Table 4. It is observed that our TG-Net model consistently outperforms the previous sequence-to-sequence models on all the datasets. For instance, our model exceeds 19.1% (R@50 score) on KP20k than the state-of-the-art model CopyCNN. Overall, the results indicate that our model is able to capture the underlying semantic meaning of the context content much better than these baselines, as we have anticipated.

Keyphrase Predicting on Various Title Length Ratios
To find out how our title incorporation influences the prediction ability, we compare the keyphrase predicting ability of two RNN-based models (i.e., our model and Copy-RNN) on different title length ratios. The title length ratio is defined as the title length over the context length. This analysis is based on the KP20k testing dataset. In view of the title length ratio, we preprocess the testing set into five groups (i.e., <3%, 3%-6%, 6%-9%, 9%-12% and >12%). Then, the present keyphrase prediction results (F1@5 measure) and the improvement gain on each group are depicted in Figure 3.In Figure 3(a), we notice that both CopyRNN and our TG-Net model generally perform better when the title length ra-  tio is higher. One possible explanation is that when the title is long, it conveys substantial salient information of the abstract. Therefore, the chance for the models to attend to the core information is enhanced, which leads to the observed situation. This figure also shows that both TG-Net and CopyRNN get worse performance on >12% group than 9%-12% group. The main reason is that there exist some data with a short abstract in >12% group, which leads to the lack of enough context information for correctly generating all keyphrases.In Figure 3(b), we find that our TG-Net consistently improves the performance with a large margin on five testing groups, which again indicates the effectiveness of our model. In a finer perspective, we note that the improvement gain is higher on the lowest (i.e., <3%) and the highest (i.e., Title: Exponential stability of switched stochastic delay systems with non-linear uncertainties Abstract: This article considers the robust exponential stability of uncertain switched stochastic systems with time-delay. Both almost sure (sample) stability and stability in mean square are investigated. Based on Lyapunov functional methods and linear matrix inequality techniques, new criteria for exponential robust stability of switched stochastic delay systems with non-linear uncertainties are derived in terms of linear matrix inequalities and average dwell-time conditions. Numerical examples are also given to illustrate the results.  >12%) title length ratio groups. In >12% group, the title plays a more important role than in other groups, and consequently our model benefits more by not only explicitly emphasizing the title information itself, but also utilizing it to guide the encoding of information in the main body. As for <3% group, the effect of such a short title is small on the latter part of the context in CopyRNN because of the long distance. However, our model explicitly employs the title to guide the encoding of each context word regardless of the distance, which utilizes the title information much more sufficiently. Consequently, our model achieves much higher improvement in this group. While we only display the results of present keyphrase prediction, the absent keyphrase predicting task gets the similar results.

(a) Present Keyphrases


Ablation Study
We also perform an ablation study on Krapivin for better understanding the contributions of the main parts of our model. For a comprehensive comparison, we conduct this study on both present keyphrase prediction and absent keyphrase prediction.As shown in Table 5, after we remove the title-guided part and only reserve the sequence encoding for the context (i.e., -title), both the present and absent keyphrase prediction performance become obviously worse, indicating that our title-guided context encoding is consistently critical for both present and absent keyphrase generation tasks. We also investigate the effect of removing the copy mechanism (i.e., -copy) from our TG-Net. From Table 5, we notice that the scores decrease dramatically on both present and absent keyphrase prediction, which demonstrates the effectiveness of the copy mechanism in finding important parts of the context.

Case Study
A keyphrase prediction example for a paper about the exponential stability of uncertain switched stochastic delay systems is shown in Figure 4. To be fair, we also only compare the RNN-based models (i.e., TG-Net and CopyRNN). For present keyphrase, we find that a present keyphrase "non-  linear uncertainties", which is a title phrase, is correctly predicted by our TG-Net, while CopyRNN fails to do so. As for absent keyphrase, we note that CopyRNN fails to predict the absent keyphrase "time-delay systems". But our TG-Net can effectively utilize the title information "stochastic delay systems" to locate the important abstract information "stochastic systems with time-delay" and then successfully generate this absent keyphrase. These results exhibit that our model is capable of capturing the title-related core information more effectively and achieving better results in predicting present and absent keyphrases.

Conclusion
In this paper, we propose a novel TG-Net for keyphrase generation task, which explicitly considers the leading role of the title to the overall document main body. Instead of simply concatenating the title and the main body as the only source input, our model explicitly treats the title as an extra query-like input to guide the encoding of the context. The proposed TG-Net is able to sufficiently leverage the highly summative information in the title to guide keyphrase generation. The empirical experiment results on five popular real-world datasets exhibit the effectiveness of our model for both present and absent keyphrase generation, especially for a document with very low or very high title length ratio.One interesting future direction is to explore more appropriate evaluation metrics for the predicted keyphrases instead of only considering the exact match with the human labeled keyphrases as the current recall and F-measure do.