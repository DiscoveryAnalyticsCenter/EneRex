Inducing Regular Grammars Using Recurrent Neural Networks

Abstract
Grammar induction is the task of learning a grammar from a set of examples. Recently, neural networks have been shown to be powerful learning machines that can identify patterns in streams of information. In this work we investigate their effectiveness in inducing a regular grammar from data, without any assumptions about the grammar. We train a recurrent neural network to distinguish between strings that are in or outside a regular language, and utilize an algorithm for extracting the learned finite-state automaton. We apply this method to several regular languages and find unexpected results regarding the connections between the network's states that may be regarded as evidence for generalization.

Introduction
Grammar induction is the task of learning a grammar from a set of examples, thus constructing a model that captures the patterns within the observed data. It plays an important role in scientific research of sequential phenomena, such as human language or genetics. We focus on the most basic level of grammars -regular grammars. That is, the set of all languages that can be decided by a Deterministic Finite Automaton (DFA).Inducing regular grammars is an old and extensively studied problem (De la Higuera, 2010). However, most suggested methods involve prior assumptions about the grammar being learned. In this work, we aim to induce a grammar from examples that are in or outside a language, without any assumption on its structure. * Authors equally contributed to this work.Recently, neural networks were shown to be powerful learning models for identifying patterns in data, including language-related tasks  #b9  #b8 . This work investigates how good neural networks are at inducing a regular grammar from data. More specifically, we investigate whether RNNs, a neural network that specializes in processing sequential streams, can learn a DFA from data.RNNs are suitable for this task since they resemble DFAs. At each time step the network has a current state, and given the next input symbol it produces the next state. Formally, let s t be the current state and x t+1 the next input symbol, then the RNN computes the next state s t+1 = δ(s t , x t+1 ), where δ is the function learned by the RNN. Consequently, δ is actually a transition function between states, similar to a DFA.This analogy between RNNs and DFAs suggests a way to understand RNNs. It enables us to "open the black box" and analyze the network by converting it into the corresponding DFA and examining the learned language.Inspired by that, we explore a method for grammar induction. Given a labeled dataset of strings that are in and outside a language, we wish to train a network to classify them. If the network succeeds, it must have learned the latent patterns underlying the data. This allows us to extract the states used by the network and reconstruct the grammar it had learned.There is one major difference between the states of DFAs and RNNs. While the former are discrete and finite, the latter are continuous. In theory, this difference makes RNNs much more powerful than DFAs  #b13 . However in practice, simple RNNs 1 are not strong enough to deal with languages beyond the regular domain  #b3 .Similar ideas have already been investigated in the 1990s  #b0  #b4  #b2  #b11  #b10  #b14  2 . These works also presented techniques to extract a DFA from a trained RNN. However, most of them included a priori quantization of the RNNs continuous state space, yielding exponential number of states even for simple grammars. Instead, several works used clustering techniques for quantizing the state space, which yielded much smaller number of states  #b15 . However, they fixed the number of clusters in advance. In this work, we introduce a novel technique for extracting a DFA from an RNN using clustering without the need to know the number of cluster. For that purpose, we present a heuristic to find the most suitable number of states for the DFA, making the process much more general and unconstrained.

Problem Statement
Given a regular language L and a labeled dataset{(X i , y i )} of strings X i with binary labels y i = 1 ⇔ X i ∈ L, the goal is to output a DFA A such that A accepts L, i.e., L(A) = L.Since the target language L is unknown, we relax this goal to A(X i ) =ȳ i . Namely, A accepts or rejects correctly on a test set {(X i ,ȳ i )} that has not been used for training. Accordingly, the accuracy of A is defined as the proportion of strings classified correctly by A.

Method
The method consists of the following main steps. 3 1. Data Generation -creating a labeled dataset of positive and negative strings: 15, 000 strings for training the network, a validation set of 10, 000 strings for constructing the DFA, and a test set of 10, 000 strings for evaluating the results. 2. Learning -training an RNN within 15 epochs to classify the dataset with high accuracy-in almost all the conducted experiments (fully described in the next section), a nearly perfect validation accuracy is achieved, approximately 99%.RnnInduceRegularGrammar 3. DFA Construction -extracting the states produced by the RNN, quantizing them and constructing a minimized DFA.

Data Generation
The following method was used to create a balanced dataset of positive and negative strings. Given a regular expression we randomly generate sequences out of it. As for the negative strings, two different methods were used. The first method was to generate random sequences of words from the vocabulary, such that their length distribution is identical to the positive ones. The other approach was to generate ungrammatical strings which are almost identical to the grammatical ones, making the learning more difficult for the RNN. We generated the negative strings by applying minor modifications such as word deletions, additions or movements. Both methods did not yield any significant difference in the results, thus we show only results for the first method.

Learning
We used the most basic architecture of RNNs, with one layer and cross entropy loss. In more detail, the RNN's transition function is a single fullyconnected layer given by s t = tanh (Ws t−1 + Ux t + v) .Prediction is made by another fully-connected layer which gets the RNN's final state s n as an input and returns a predictionŷ ∈ [0, 1] by, y = σ(Aσ(Bs n + c) + d).where σ stands for the sigmoid function, W, U, A, B are learned matrices and v, c, d are learned vectors. The loss used for training is cross entropy,l = − 1 n n i=1 y i logŷ i + (1 − y i ) log (1 −ŷ i ) ,where y 1 , . . . , y n are the true labels andŷ 1 , . . . ,ŷ n are the network's predictions. To optimize our loss, we employ the Adam algorithm  #b7  Our goal is to reach perfect accuracy on the validation set, in order to make sure the network succeeded in generalizing and inducing the regular grammar underlying the dataset. This assures that the DFA we extract later is reliable as much as possible.

DFA Construction
When training is finished, we extract the DFA learned by the network. This process consists of the following four steps.Collecting the states First, we collect the RNN's continuous state vectors by feeding the network with strings from the validation set and collecting the states it outputs while reading them.Quantization After collecting the continuous states, we need to transform them into a discrete set of states. We can achieve this by simply using any conventional clustering method with the Euclidean distance measure. More specifically, we use the K-means clustering algorithm, where K is taken to be the minimal value such that the quantized graph's classifications match the network's ones with high rate. That is, for each K we build the quantized DFA (as we describe later) and count the number of matches between the DFA's classifications and the network's over a validation set. We return the minimal K that exceeds 99% matches. It should be noted that the initial state is left as is and is not associated with any of the clusters.Building the DFA Given the RNN's transition function δ and the clustering method c, we use the following algorithm to build the DFA.

Algorithm 1 DFA Construction
V, E ← φ, φ for each sequence X i do s t−1 , v t−1 ← s 0 , c(s 0 ) for each symbol x t+1 ∈ X i do s t ← δ(s t−1 , x t ) v t ← c(s t ) Add v t to V Add (v t−1 , x t ) → v t to E s t−1 , v t−1 ← s t , v t end forMark v t as accept ifŷ i = 1 end for return V, E Finally, we use the Myhill-Nerode algorithm in order to find the minimal equivalent DFA (Downey and Fellows, 2012).

Experiments
To demonstrate our method, we applied it on the following few regular expressions. 

Simple Binary Regexes
The resulting DFAs for the two binary regexes (01) * and (0|1) * 100 are shown in Figure 1. It can be observed that the method produced perfect DFAs that accept exactly the given languages. The DFAs accuracy was indeed 100%.A finding worth mentioning is the emergence of cycles within the continuous states transitions. That is, the RNN before quantization mapped new states into the exact same state it has already seen before. This is surprising because if we think of the continuous states as random vectors, the probability to see an exact vector twice is zero. This finding, which was reproduced for several regexes, may be an evidence for generalization as we discuss later.

Part of Speech Regex
To test our model on a more complicated grammar, we created a synthetic regex inspired by natural language. The regex we used describes a simplified part-of-speech grammar, DET? ADJ * NOUN VERB (DET? ADJ * NOUN)?The resulting DFA is shown in Figure 2.The DFA's accuracy was 99.6%, i.e. the learned language is not exactly the target one, but is very close. For example, it accepts sentences likeThe nice boy kissed a beautiful lovely girl and rejects sentences like The boy nice However, by examining the DFA we can find sentences that the network misclassifies. For example, it accepts sentences like The the boy standsInspecting the DFA's errors might be meaning- ful also for training, as a technique for targeted data augmentation. By the pumping lemma, each of the states where the network is wrong stands for an infinite class of sequences that end at the same state. In other words, those states are actually a "formula" for generating as much data as we want, such that the network is wrong. This way, the network can be re-trained on its own errors.

Discussion
LearningThe network reached 100% accuracy quickly on the synthetic datasets. This may indicate that deciding a regular language is a reasonable task for an RNN, and illustrates the similarity between RNNs and DFAs discussed earlier.

Emergence of cycles
The emergence of cycles within the continuous states, mentioned in experiment 4.1 , might be understood as an evidence for generalization. Having learned those cycles means that for an infinite set of sequences the network will always traverse the same path and predict the same label. In other words, the model has generalized for sequences of arbitrary length before quantization. It should be noted that such cycles have also been noticed in  #b14 . Nevertheless, in their work the learning objective was predicting the next state rather than classifying the whole sentence. As a result, the emergence of cycles is expected, since the network was forced to learn them by the supervision. This differs from the case of classification, as learning the states and the cycles is not supervised.

Quantization
The process used for quantizing the states may introduce conflicts, if two different states in the same cluster lead to two different clusters for the same input. However, all of our experiments did not yield any conflict. This means that the clusters do reflect well the RNN's different states. This is reasonable due to the continuity of the RNN's transition function, which maps "close" states into "close" states in terms of Euclidean distance. Thus, states within the same cluster will have similar transitions and therefore be mapped into the same cluster.Another way to confirm the clusters validity is to check their compatibility with the network's decisions, i.e., whether accepting or rejecting states are clustered together. Figre 3 presents the continuous vectors for the binary regex (0|1) * 100 4 . Clearly, the states are divided into five distinct clusters and only one of them is accepting. 

Conclusions
We investigated a method for grammar induction using recurrent neural networks. This method gives some insights about RNNs as a learning model, and raises several questions, for example regarding the cycles within the continuous states.Quantization via clustering proved itself to reflect well the true states of the network. Identifying an infinite set of vectors as one state may reduce the network's sensitivity to noise. Thus, states quantization during or after training may be considered as a technique for injecting some robustness to the model and reducing overfitting.Finally, this method may serve as a tool for scientific research, by finding regular patterns within a real-world sequence. For example, it would be interesting to use this method for natural language, more specifically to induce grammar rules of phonology, which is claimed to be regular  #b6 . Another example is to use it to find regularity within the structure of DNA, which is also regarded as regular  #b5 .

Footnote
1 : Without the aid of additional memory such as in LSTMs (Hochreiter andSchmidhuber, 1997)   
2 : For a more thorough survey of relevant papers, see section 5.1
4 : To reduce the vectors dimensions we used PCA.
3 :  Python code is available at https://github.com/acrola/