Automated proof synthesis for propositional logic with deep neural networks

Abstract
This work explores the application of deep learning, a machine learning technique that uses deep neural networks (DNN) in its core, to an automated theorem proving (ATP) problem. To this end, we construct a statistical model which quantifies the likelihood that a proof is indeed a correct one of a given proposition. Based on this model, we give a proof-synthesis procedure that searches for a proof in the order of the likelihood. This procedure uses an estimator of the likelihood of an inference rule being applied at each step of a proof. As an implementation of the estimator, we propose a proposition-to-proof architecture, which is a DNN tailored to the automated proof synthesis problem. To empirically demonstrate its usefulness, we apply our model to synthesize proofs of propositional logic. We train the proposition-to-proof model using a training dataset of proposition-proof pairs. The evaluation against a benchmark set shows the very high accuracy and an improvement to the recent work of neural proof synthesis.

INTRODUCTION
Theorem proving is an essential activity in formal reasoning. Needless to say, mathematics has become the reliable foundation of modern natural science, including several branches of theoretical computer science, by justifying theorems with proofs. The importance of correct proofs leads to the study of software called proof assistants  #b35 Norell 2009;The Coq Development Team 2017], which allow users to state theorems and their proofs formally in the form of certain programming languages and automatically check that the proofs correctly prove the theorems. The realm of the areas that rely on theorem proving is expanding beyond mathematics; for example, it is being applied for system verification [Klein et al. 2009; #b30 , where one states the correctness of a system as a theorem and justifies it in the form of proofs.Automated theorem proving (ATP)  #b5  #b14  #b37 ] is a set of techniques that prove logical formulas automatically. We are concerned with the following form of ATP called automated proof synthesis (APS): Given a logical formula P, if P holds, return a proof M of P. In the light of the importance of theorem proving, APS serves as a useful tool for activities based on formal reasoning. For example, from the perspective of the aforementioned system verification, APS serves for automating system verification; indeed, various methods for (semi)automated static program verification  #b3  #b8  #b13  can be seen as APS procedures.We also remark another important application of APS: automated program synthesis. An APS algorithm can be seen as an automated program synthesis procedure via the Curry-Howard isomorphism  #b44 , in which M can be seen as a program and P can be seen as a specification. Not only is APS interesting from the practical viewpoint, it is also interesting from the theoretical perspective in that it investigates the algorithmic aspect of theorem proving.Traditionally, the main weapon from the programming-language community to tackle APS has been symbolic methods; an APS algorithm inspects the syntactic structure of the formula P and, using the obtained information, tries Authors' addresses: Taro Sekiyama, National Institute of Informatics, Japan, sekiyama@nii.ac.jp; Kohei Suenaga, Kyoto University, Japan, ksuenaga@kuis. kyoto-u.ac.jp, JST PRESTO, Japan, ksuenaga@kuis.kyoto-u.ac.jp.

2018.
to construct a proof derivation of P. A seminal work in this regard is by Ben-Yelles [1979]; they proposed a sound and complete APS algorithm for an implicational fragment of the propositional logic.This paper tackles the APS problem using another emerging technology: statistical machine learning. In particular,we explore an application of deep neural networks (DNN)  #b17 . DNNs have seen a great success in recent years for solving various tasks; to name a few, image recognition  #b19 ], speech recognition , and natural language processing  #b49 . To this end, we propose a novel DNN architecture named proposition-to-proof model 1 tailored to the APS problem.Concretely, we statistically model the APS problem in terms of probabilities. This statistical model serves for quantifying how a partially constructed proof is likely to lead to a correct proof of the given proposition P. Based on this statistical model, we define a proof-synthesis procedure that searches for a proof of given proposition P in the order of the likelihood. This proof synthesis procedure requires a function to estimate the likelihood of an inference rule being applied at a specific step of a proof (or, equivalently, a specific position of a partially constructed proof). For this estimation, we use a DNN based on the proposition-to-proof architecture that we propose. We empirically evaluate the performance of our network, which reveals that it can predict the inference rules that fill the rest of a partially constructed proof of a propositional-logic formula with 96.79% accuracy.This work is not the first one that applies DNNs to APS. Among them,  #b42  reports an application of DNN equipped with long-short term memory (LSTM) to the APS problem. The idea in their work is to view the APS problem as a machine translation problem from the language of logical formulas to the language of proofs. Based on this view, they applied an off-the-shelf neural machine translation framework to APS. They report that their network, which is a vanilla one for neural machine translation, proved around 50% of the propositional-logic formulas in the benchmark they used. In contrast to their approach of trying to synthesize an entire proof at once, we designed our proof-synthesis procedure so that it gradually constructs a proof based on the likelihood of each inference rule.The contributions of this work are summarized as follows.• We construct a statistical model for the APS problem in terms of probability. This model formally quantifies the likelihood of a proof being a correct one of a given proposition. Applying the laws of probability to this model, we derive how the computation of the likelihood of an entire proof is reduced to the successive computations of the likelihood of inference rules. • Based on this statistical model, we design a proof-synthesis procedure that searches for a proof of a given proposition in the descending order of the likelihood. This algorithm gradually constructs a proof by repeating the estimation of the likelihood of occurrences of inference rules in the proof.• We propose a novel DNN architecture which we call proposition-to-proof model that estimates the above likelihood of inference rules. This network takes a proposition P, the position in a partially constructed proof to be filled, and contextual information as input and outputs the likelihood of inference rules being applied at the position to be filled.• We implemented the proof-synthesis procedure with a trained proposition-to-proof model and empirically confirmed its effectiveness compared to  #b42 . In addition to measuring the accuracy of the trained proposition-to-proof model, we conducted in-depth analyses of the model. We confirmed that our model estimates the proof constructor with 96.79% accuracy.Types P, Q, R ::= a | P → Q | P × Q | P + Q Terms L, M, N ::= [ ] | x | λx.M | M N | (M, N ) | case M of (x, y) → N | Left M | Right M | case L of { Left x → M; Right y → N }Typing contexts Γ ::= ∅ | Γ, x:P Currently, we do not claim that our procedure outperforms the state-of-the-art APS method for propositional logic.Rather, our contribution consists in the statistical reformulation of the APS problem and application of deep learning, which exposes superhuman performance in many areas. We believe that deep learning is also useful in the APS problem possibly in combination with symbolic methods and that the present work opens up a new research direction in this regard.The rest of this paper is organized as follows: Section 2 defines the logic and the proof system that we use in this paper; Section 3 reviews statistical machine learning briefly; Section 4 defines the proof-synthesis algorithm; Section 5 gives a brief introduction to deep learning and introduces the proposition-to-proof architecture; Section 6 describes the result of the experiments; Section 7 discusses related work; and Section 8 concludes.We assume the readers' familiarity to the Curry-Howard isomorphism  #b44 . We sometimes abuse the terminologies in the simply typed lambda-calculus for those of the proof theory of the propositional logic. We also assume that the readers are familiar with the probability theory although we do not deal with measure-theoretic discussions in the present paper.

THE SIMPLY TYPED LAMBDA CALCULUS AS PROPOSITIONAL LOGIC
In this work, we identify the simply typed lambda calculus with the intuitionistic propositional logic via the Curry-Howard isomorphism  #b44 ]. This view is indeed beneficial for us: (1) a term of the simply typed lambda calculus is the concise representation of a derivation tree, which is essentially the proof of a proposition and (2) we can express a partially constructed proof as a term with holes, which denote positions in a proof that needs to be filled. In the rest of this section, we introduce the simply typed lambda calculus extended with product types and sum types. The Curry-Howard isomorphism allows us to identify a product type with the conjunction of propositions and a sum type with the disjunction. Figure 1 shows the syntax of the simply typed lambda calculus. Types (or propositions) are represented by the metavariables P, Q, and R; terms (or proofs) are represented by the metavariables L, M, and N ; and typing contexts (or collections of assumptions) are represented by the metavariable Γ. The definition of types is standard: they consist of type variables (or propositional variables), function types P → Q, product types P × Q, and sum types P + Q. We use the metavariables a, b, c, and d for type variables. The syntax of terms is that of the simply typed lambda calculus. Products are constructed by (M, N ) and destructed by case M of (x, y) → M; sums are constructed by Left M and Right N anddestructed by case L of { Left x → M; Right y → N }.The term syntax is equipped with a hole [ ] to express partially constructed terms. A hole denotes a position in a term that needs to be filled (see below). We use metavariables x, y, and z for term variables. The notions of free variables, bound variables, and substitution for terms are defined as usual. λx.M binds x in M; case M of (x, y) → N binds x and y in N ; and case L of { Left x → M; Right y → N } binds x in M and y in N , respectively. Types have no binders. We write FV (M) for the set of term variables that occur freely in M. We write [N /x] M for the capture-avoiding substitution of N for x in M. We say two terms are α-equivalent if they are different only in the use of bound variable names. We identify two α-equivalent terms.Γ ⊢ M : P Γ ⊢ [ ] : P Hole x:P ∈ Γ Γ ⊢ x : P Var Γ, x:P ⊢ M : Q Γ ⊢ λx.M : P → Q Abs Γ ⊢ M : P → Q Γ ⊢ N : P Γ ⊢ M N : Q App Γ ⊢ M : P Γ ⊢ N : Q Γ ⊢ (M, N ) : P × Q Pair Γ ⊢ M : P × Q Γ, x:P, y:Q ⊢ N : R Γ ⊢ case M of (x, y) → N : R CasePair Γ ⊢ M : P Γ ⊢ Left M : P + Q Left Γ ⊢ M : Q Γ ⊢ Right M : P + Q Right Γ ⊢ L : P + Q Γ, x:P ⊢ M : R Γ, y:Q ⊢ N : R Γ ⊢ case L of { Left x → M; Right y → N } : R CaseSumA term that contains holes represents a partially constructed proof. Our proof-synthesis procedure introduced in Section 4 maintains a set of partially constructed terms and fills a hole inside a term in the set at each step. We assume that holes in a term are uniquely identified by natural numbers. We write [ ] i for a hole with number i. We write M[N ] i for the term obtained by filling the hole [ ] i in M with N .We also define the typing relation Γ ⊢ M : P as the least relation that satisfies the inference rules in Figure 2. This relation means that term M is typed at P under Γ or, equivalently, M is a proof of P under assumptions Γ. We write Γ ⊬ M : P to denote that Γ ⊢ M : P does not hold. The rules in Figure 2 are standard except for the rule Hole for holes. This rule allows any type to be given to a hole. We call the inference rules except for the rule Hole proof inference rules.We say that M is a (complete) proof of P if ∅ ⊢ M : P is derived and M has no holes. M is said to be partial or partially constructed if ∅ ⊢ M : P but M contains holes.To define the notion of normal forms, we introduce β-reduction (−→ β ) and η-reduction (−→ η ), which are the least compatible relations satisfying the rules in Figure 3; the last η-reduction rule for sums is given by  #b15 . Term M is a βη normal form when there does not exist N such that M −→ β N nor M −→ η N .

BACKGROUND: STATISTICAL MACHINE LEARNING
In order to make the present paper self-contained, we explain basic concepts on statistical machine learning and probabilities that appear in this paper. The exposition about machine learning in this section is not intended to be exhaustive; for detail, see the standard textbooks, e.g.,  #b6 .Machine learning is a generic term for a set of techniques to make software "learn" how to behave from data without being explicitly programmed. The machine-learning task in this paper is of a type called supervised learning. In this type of tasks, a learner needs to synthesize a function f : X → Y for certain sets X and Y . 2 In a typical setting, the learner is given a set D := {(x 1 , y 1 ), . . . , (x n , y n )} of sample input-output pairs of f as a hint for this learning task; the set D is called a training dataset.M −→ β N β-reduction (λx.M) N −→ β [N /x] M case (L, M) of (x, y) → N −→ β [L/x, M/y] N case (Left L) of { Left x → M; Right y → N } −→ β [L/x] M case (Right L) of { Left x → M; Right y → N } −→ β [L/y] N M −→ η N η-reduction (λx.M x) −→ η M (x FV (M)) (case L of (x, y) → x, case L of (x, y) → y) −→ η L case M of { Left x → [Left x/z] N ; Right y → [Right y/z] N } −→ η [M/z] N (x, y FV (N ))A popular strategy to tackle this problem is to use statistics. In order to explain application of statistics in machine learning, we fix notation about probabilities. We designate a random variable, say B x , that evaluates to an element of Y following a certain probabilistic distribution parameterized by an element x ∈ X ; we use the bold face for random variables in this paper. For this random variable, one can consider, for example, the probability p(B x = y) of B x being evaluated to y. More generally, given a predicate φ(x, B x ) over x and B x , one can define the probability p(φ) that φ holds for x and a value of B x . 3 Notice that the truth value of the predicate φ is a random variable; it may hold or may not hold depending on the result of the evaluation of B x in general. We can also define the probability p(φ 1 | φ 2 ) for given two predicates φ 1 and φ 2 , which is the probability of φ 1 holding under the condition that φ 2 is true. A probability distribution of a random variable B x conditioned on φ, written p(B x | φ), is a function that maps an element y ∈ Y to the probability p(B x = y | φ).We view the statistical machine learning in the following way. Ideally, it is desirable to discover the "true" probability distribution, especially probability distribution d x behind the random variable B x , that explains how the training dataset D is generated. If we have this distribution, then we can guess a highly probable output y to x as one that maximizes the value of d x (i.e., arg max y ∈Y d x (y)). However, it is actually hard to identify d x precisely. An alternative promising way is to approximate d x by a parameterized function F x,w 1 , ...,w n over parameters w 1 , . . . , w n that maps an element of Y to the (approximated) probability of B x being evaluated to y. The parameters are tuned using numerical optimization so that the probability of D being generated is maximized. Then, the function f is synthesized so that f (x) = arg max y ∈Y F x,w 1 , ...,w n (y).The performance of f synthesized as above depends on several factors, including: (1) whether the set {F x,w 1 , ...,w n | w 1 , . . . , w n are possible parameters} contains a function that is sufficiently "close" to the true distribution d x and (2)x → F x,w 1 , ...,w n is not overfitted to D and performs well for unobserved data. Deep learning, as seen in Section 5.1, is a promising methodology to address these issues.

AUTOMATED PROOF SYNTHESIS WITH STATISTICAL MODELING
This section starts with making a statistical model for the APS problem by rephrasing the concepts introduced in Section 3 using the terms of our setting. Based on this model, we define a proof-search procedure that takes the likelihood of a proof term into account. We show that we can decompose the probability distribution p(M P |P) for APS to the multiplication of easier-to-approximate fine-grained probability distributions in terms of term constructors that occur in proof terms. We show the derivation of these fine-grained distributions and then proceed to the definition of the procedure of proof synthesis.

Automated proof synthesis, statistically
In our setting, the training dataset D is a set of proposition-proof pairs{(P ′ 1 , M ′ 1 ), . . . , (P ′ n , M ′ n )}, where M ′ i is a complete proof of P ′ i for each i.We are to synthesize a function f that takes a proposition P and returns its proof M for as many propositions P as possible, that is, our f has to satisfy ∅ ⊢ f (P) : P for many propositions P.To statistically model the APS problem, we designate several random variables. P is a random variable that evaluates to a type. 4 M P is a family of random variables indexed by type P. Then, for a predicate φ(P) on P, the probability distribution p(M P | φ) is a distribution on the set of proof terms conditioned by the predicate φ(P). As we outlined in Section 3, if we have a good approximation of the conditional probability distribution p(M P | P = P), then we obtain a highly probable proof term of P by computing arg max M p(M P = M | P = P). Therefore, a function that maps P to arg max M p(M P = M | P = P) is expected to be a good proof synthesizer under this model.

Derivation of fine-grained distributions
We could directly learn p(M P | P = P) using a certain machine leaning technique along with the training dataset D;this is the strategy taken by  #b42 . However, we found that such monolithic approximation of the probabilistic distribution often leads to a bad approximation; indeed, the accuracy of the automated proof synthesizer by Sekiyama et al. was around 50% at best. In this paper, we instead convert p(M P | P = P) using the laws of probabilities so that the learning task is reduced to a set of fine-grained ones. Under this strategy, one can compute an approximation of the probability distribution by combining several easier-to-approximate distributions. We discover that the combination leads to a better proof synthesizer than learning M monolithically.In order to derive the fine-grained distributions, we first introduce several notions that enable us to specify occurrences of term constructors in proofs.Definition 4.1 (One-depth contexts). The set of one-depth contexts is defined by the following BNF:C ∈ Ctx ::= λx. [ ] | [ ] [ ] | ([ ] , [ ]) | case [ ] of (x, y) → [ ] | Left [ ] | Right [ ] | case [ ] of { Left x → [ ] ; Right y → [ ]}.We assume that each hole in a one-depth context is equipped with a unique identifier. We write C[M i ] i for the term obtained by filling holes [ ] 0 , ..., [ ] n in C with terms M 0 , ..., M n , respectively.

Definition 4.2 (Paths).
A path ρ is a finite sequence of pairs (C, i) where i is a natural number that identifies a hole in C. We write ⟨ρ, (C, i)⟩ for the path obtained by postpending (C, i) to path ρ.A one-depth context represents a term constructor other than variables. Using one-depth contexts,ρ = ⟨(C 0 , i 0 ), (C 1 , i 1 ), . . . , (C n , i n )⟩specifies a path in a term, whose top-level constructor is identical to C 0 , from its root node in the following way: C 0 ;the hole in C 0 with the identifier i 0 ; C 1 ; the hole in C 1 with the identifier i 1 ; and so on. For example, let M be a term λx.case x of (y, z) → (z, y). Then, a path from the root of M to the reference to variable y is represented by the path⟨(λx. [ ] 0 , 0), (case [ ] 0 of (y, z) → [ ] 1 , 1), (([ ] 0 , [ ] 1 ), 1)⟩.We show that the probability p(M P = M | P = P) is equal to ϕ(M, ⟨⟩) if every subterm M ′ of M is annotated with its type (which we write typeof(M ′ )), where ϕ is defined by induction on the structure of M:ϕ (x, ρ) = p(x = x | P = P, Q = typeof(x), ρ = ρ) ϕ (C[M i ] i , ρ) = p(C = C | P = P, Q = typeof(C[M i ] i ), ρ = ρ) × i ϕ (M i , ⟨ρ, (C, i)⟩).(1)The function ϕ (M, ρ) computes p(M = M |P = P, Q = typeof(M), ρ = ρ), the probability of M being a subterm of typeof(M) at the position specified by ρ within a proof of P, by induction on the structure of M using two auxiliaryprobabilities: p(x = x | P = P, Q = Q, ρ = ρ) and p(C = C | P = P, Q = Q, ρ = ρ).In the definition, we use the following random variables: x evaluates to a term variable; C evaluates to a one-depth context; Q evaluates to the type to be proved by M; and ρ evaluates to a path that specifies the position where x or C is placed. Note that P evaluates to the type that is supposed to be proved by the root node, not by M. The conditional-probability expression p(x = x | P = P, Q = Q, ρ = ρ) quantifies the probability of x being a proof of Q under the condition that it appears at the position specified by ρ; and p(C = C | P = P, Q = Q, ρ = ρ) is the probability of C being the top-level constructor of a proof term of Q if it appears at the position specified by ρ. We will explain how to model type annotations typeof(M) later.If M is a variable x, then ϕ uses the value of the former probability as the answer. If M is not a variable, then it can be written in the form of C[M i ] i using some one-depth context C and terms M 1 , . . . , M n . The definition argues that this probability is the multiplication of (1) the likelihood of C conditioned by P = P, Q = typeof(M), and ρ = ρ; and (2) = C[M i ] i . We start from p(M = C[M i ] i | P = P, Q = typeof (M), ρ = ρ). This probability is equal to the following probability:p C = C, M 1 = M 1 , . . . , M n = M n P = P, Q = typeof (M), ρ = ρ .Here, each M i is the random variable that evaluates to the term to be filled in the i-th hole in C. By using the chain rule of conditional probabilities  #b28  this probability is equal to the following.p (C = C | P = P, Q = typeof(M), ρ = ρ) × p (M 1 = M 1 , . . . , M n = M n | C = C, P = P, Q = typeof(M), ρ = ρ) .(2)We decompose the second expression in Equation 2. Let us first refine the condition of this expression with Q i , a family of the random variables that evaluate to the type of the i-th hole in C, and ρ i , a family of random variables that evaluate to the paths of the i-th hole in C. For short, let us write− → Q i = typeof( − → M i ) for Q 1 = typeof(M 1 ), ..., Q n = typeof(M n ) and − → ρ i = − → ρ i for ρ 1 = ⟨ρ, (C, 1)⟩, ..., ρ n = ⟨ρ, (C, n)⟩. Then, with these random variables, the second expression in Equation 2 is equal to p M 1 = M 1 , . . . , M n = M n C = C, P = P, Q = typeof(M), ρ = ρ, − → Q i = typeof( − → M i ), − → ρ i = − → ρ i because (1) the condition − → Q i = typeof( − → M i ) doesp M j = M j C = C, P = P, Q = typeof(M), ρ = ρ, − → Q i = typeof( − → M i ), − → ρ i = − → ρ i .In the condition part of this expression, noting that (1) p M j = M j P = P, Q j = typeof(M j ), ρ j = ρ j .By the induction hypothesis, this is equal to 1≤j ≤n ϕ(M j , ⟨ρ, (C, j)⟩); substituting this to Equation 2, we have Equation 1.We back up the aforementioned observation that is a key to decompose the second expression by an example. Let Cbe ([ ] 0 , [ ] 1 ) under a path that binds f to a → b, д to a × b → a,x to a, and y to b. Suppose we know that [ ] 0 should be filled with a term of type a → b and [ ] 1 with a term of type a. Obviously, in our context of proof synthesis, the fact that f is filled in [ ] 0 does not add any restriction on the set of possible terms in [ ] 1 , since any term of the type of each hole works as a proof of each type. This observation can be generalized to an arbitrary case in our type system. 6

Proof synthesis procedure
Based on the discussion in Section 4.2, we design a proof-synthesis procedure. Procedure 1 shows the definition of our procedure ProofSynthesize, which takes proposition P to be proved. This procedure maintains a priority queue Q of partially constructed terms. The priority associated with M by Q denotes the likelihood of M forming a proof of P. In each iteration of Lines 4-15, ProofSynthesize picks a term M with the highest likelihood and fills a hole in M with a one-depth context. It returns a proof if it encounters a correct proof of P. We write p * (φ 1 | φ 2 ) for an approximation ofp(φ 1 | φ 2 ).Before going into the detail, we remark a gap between the procedure ProofSynthesize and the statistical model in Section 4.2. In that statistical model, we defined the likelihood of a variable p(x | P = P, Q = Q, ρ = ρ) and that of a one-depth context p(C | P = P, Q = Q, ρ = ρ) as separate probability distributions. Although this separation admits the Procedure 1 Proof synthesis 1: procedure ProofSynthesize(P) 2:Initialize priority queue Q that contains partial proofs constructed so far.3:Push [ ] to Q with priority 1.0.

4:
while Q is not empty do 5:Pop M with the highest priority P from Q.

6:
Let ρ = arg max ρ ∈hole (M) max r p * (r = r | ρ = ρ).7:for each C x ∈ Ctx ∪ BV (M, ρ) such that ∅ ⊢ M[C x ] ρ : P do 8: if hole (M[C x ] ρ ) = ∅ then 9: return M[C x ] ρ 10: else 11:Let Q be a proof obligation to be discharged at [ ] ρ .

12:
PushM[C x ] ρ to Q with priority p * (r = r C x | P = P, ρ = ρ, Q = Q) P 13: end if 14:end for 15:end while 16: end procedure inductive definition of the function ϕ, it is not necessarily plausible from the viewpoint of proof synthesis since, in filling a hole, we do not know whether it should be filled with a variable or with a one-depth context.In order to solve this problem, we assume that we have an approximation of the likelihood of an proof inference rule that should be applied to a hole. Concretely, we assume that we can approximate the probability distribution p(r | P, ρ, Q), where r is a random variable that evaluates to the name of an proof inference rule in Figure 2. This assumption requires that we estimate the likelihood of Var being applied for a hole, which can be done in the same way as estimation of those of other inference rules.Let us explain the inside of the procedure in more detail. A proof is synthesized by the while loop, where the procedure fills the hole [ ] ρ pointed by ρ in the partial proof M that has the highest likelihood P (Lines 4-15). We write hole (M) for the set of paths to holes in M. We select path ρ such that the inference rule applied at the position pointed by the path has the highest probability. After finding the hole to be filled, we replace it with C x , which denotes one-depth contexts or variables. BV (M, ρ) is the set of bound variables that can be referred to at [ ] ρ and M[C x ] ρ is the term obtained by filling [ ] ρ in M with C x . Note that Ctx are the set of all one-depth contexts. If M[C x ] ρ is a proof of P, which can be checked using an off-the-shelf type checker, then the procedure returns it as the synthesis result (Line 9).Otherwise, M[C x ] ρ is added to Q with priority p * (r = r C x | P = P, ρ = ρ, Q = Q) P, which is the likelihood of M[C x ] ρforming a proof (Line 12). r C x is the proof inference rule corresponding to C x . Q is a proof obligation at [ ] ρ ; how to find it is discussed in Section 6.4.2.We make a few remarks about the procedure:• In the current implementation, we have not implemented the approximator of p(x | P, ρ, Q); instead, if the procedure decides to fill a hole with a variable, we assume that p(x | P, ρ, Q) is the uniformly distribution on the set of variables that are available at this scope. Although this may look like a naive strategy, our implementation still works quite well for many propositions; see Section 6.4.2. The problem of estimating the likelihood of a variable is similar to the premise selection problem, for which various work has been done  #b23  #b31  #b48 . Combining our synthesizer with such a technique is an interesting future direction.• In the current implementation, we assume that the type checking conducted in Line 7 infers the type of each subexpression of M[C x ] ρ and annotates these types to them; this is indeed how we handle the typeof (M) in Section 4.2. This is a reasonable assumption as far as we are concerned with the propositional logic. For more expressive logics, we may need some auxiliary methods to guess the type of each expression.• The procedure ProofSynthesize is not an algorithm. If it is fed with an unsatisfiable proposition, then it does not terminate. Even if it is fed with a valid proposition, it may not be able to discover a proof of the proposition depending on the performance of the estimator of p * .

NEURAL PROPOSITION-TO-PROOF MODEL
In order to implement ProofSynthesize, we are to approximate the probability distribution p(r | P, ρ, Q) that produces the likelihood of a proof inference rule being applied at a given position in a proof. To this end, we design a new DNN model, which we call a proposition-to-proof model, tailored to the classification task of inference rules. We start with a brief review of deep learning for making this paper self-contained; see, e.g.,  #b17  for the details.Then we describe a basic architecture of the proposition-to-proof model.

Deep learning, briefly
Deep learning is a generic term for machine learning methods based on deep neural networks. Usually, as other machine learning technologies, the first common step of deep learning is to build a vector space of features and a mapping from a datum to the space of the features. An element of the vector space of the features is called a feature vector. It is a multidimensional vector each dimension of which is a numerical value that represents certain information of the datum.By embedding data to a vector space and working on this space, we can apply various numerical optimization techniques of machine learning such as support vector machine and random forest. The design of the feature representation is known to have a great influence on the performance of a machine learning method. However, developing feature representations, known as feature-engineering, requires deep expertise in the application domains; it is in general difficult to generalize a design of a feature representation to other tasks.Deep learning allows us to avoid hard feature-engineering. In contrast with other methods, deep learning can learn the feature representation from data without manual engineering, which makes it possible to find a good feature representation with less effort. Deep learning is also very expressive in the sense that it can approximate any continuous function f with a given degree of accuracy by appropriately tuning the set of parameters  #b22 ]. Furthermore, it is known that deep learning tends to generalize to unseen data without overfitting to a given training dataset if the set is sufficiently large. 7 Models in deep learning are represented by (artificial) neural networks, which consist of an input layer that converts data to a feature vector; an output layer that converts a feature vector to a human-readable format; and one or more hidden layers that learn feature representations. Neural networks with multiple hidden layers are especially called deep neural networks (DNNs). The basic building block of hidden layers is a perceptron, which is a function over multidimensional vectors with learnable parameters. Given an n-dimensional vector v x = [x 1 , ..., x n ] in the vector space R n on reals, a perceptron produces an m-dimensional vector v y = [y 1 , ..., y m ] ∈ R m such that:y j = n i=1 W j,i x i + b jwhere W j,i ∈ R, a matrix called a weight, is a learnable coefficient parameter; and b j ∈ R, a vector called a bias, is a learnable parameter independent of the input. We simply write the behavior of a perceptron as the following linearoperation: v y = W v x + bwhere W is a real matrix in R m×n , b is a real vector in R m , W v x is the matrix product of W and the transpose of v x , and + is the element-wise addition. Since a perceptron is a linear function, any multilayer perceptron is also represented by a single perceptron since the composition of several linear maps is also linear. To give DNN models the ability to approximate any nonlinear function, each hidden layer postpends the application of a nonlinear function f , called activation, and produces the result v z of an element-wise application of v y to f :v z = f (v y ).A hidden layer of this type is called a fully connected layer. We write W and b in it as W fc and b fc for clarification.Hidden layers are supposed to extract abstract features of data and a use of multiple layers makes DNNs powerful.However, training a model with an excessive number of layers requires expensive learning cost. This is one of the reasons why many variants of DNNs have been studied for effective learning; this work can be seen as a new DNN model tailored to proof synthesis.As other machine learning methods, deep learning requires training to tune the learnable parameters. The parameters are tuned so that a model approximates the distribution of a given dataset as closely as possible using numerical optimization techniques such as stochastic gradient descent  #b38 . These numerical optimization methods adjust learnable parameters so that the difference between an expected output and the actual response from the model is minimized; this difference is called a loss value and a function to calculate loss values is called a loss function.Splitting a training dataset into multiple small collections called mini-batches is a popular strategy in the training of a DNN. In this strategy, the numerical optimization is applied to each mini-batch to update the learnable parameters.After the training, one evaluates the performance of the trained model by using a validation dataset, which is different but supposed to origin from the same distribution as the training dataset.

Proposition-to-proof model
We design a DNN model that takes three arguments, proposition P to be proven, path ρ pointing to the hole to be filled, and proof obligation Q a term of which should be placed in the hole, and approximates the probability of a proof inference rule being applied at the position specified by ρ in a proof of P. Following the standard manner in deep learning, the model represents features of the three arguments as real vectors and then approximates the likelihood of each proof inference rule with them. We first explain how we learn feature representations of propositions P and Q.The features of the path are obtained by extracting those of P along the path. We finally integrate all features into a single feature vector and use it to estimate a proof inference rule that should be applied. propositional variable. We first give a simple feature to each node in the AST and then design a new layer to learn an effective feature representation of the AST. In what follows, we suppose that each node in an AST is associated with a feature vector.One possible way to provide vectors that distinguish nodes of an AST is to use one-hot vectors, which are used broadly in natural language processing and represent a word as an n-dimensional vector (n is the number of unique words considered) that only the element corresponding to the word has scalar value 1 and the others have 0. In this work, the information of proposition constructor is embedded into a vector as in one-hot vectors, while propositional variables embed their numerical scale values into a fixed element in the vector. We expect that this encoding of nodes is more informative, especially, for propositional variables that do not occur in a training dataset-we call such variables unknown-than one-hot vectors. How to handle unknown entities is a common issue also in natural language processing, which addresses the issue by a workaround that maps all unknown words to a special symbol "unknown". However, this workaround has the problems that (1) the feature vector for the "unknown" is not related to propositional variables at the training phase since the "unknown" does not occur in the training dataset and (2) all unknown propositional variables are mapped to a single symbol "unknown" and so theyTree convolution v 4 v 2 v 3 v 6 v 1 v 8 v 9 v 10 v 11 v 5 v 7 Tree convolution … v 4 v 2 v 3 v 6 v 1 v 8 v 9 v 10 v 11 v 5 v 7 ' ' ' ' ' ' ' ' ' ' ' Aggregation v P Fig. 5. Encoder.are not distinguished. Fortunately, we know as a domain knowledge that "unknown" comes from only propositional variables and we can assign unique positive numbers to all propositional variables, which should make feature vectors of unknown propositional variables more informative. We expect the encoding by Enc to be helpful for the issue of unknown propositional variables.After giving a vector to each node by Enc, we obtain features of P by two steps ( Figure 5). The first step gains a feature representation of each node from nodes around it by using AST convolution layers. The second step aggregates feature vectors of nodes into a single vector by an aggregation layer.AST convolution layer. An AST convolution layer updates a feature vector of each node t in an AST by using vectors of nodes around t. Suppose that parent (t) is the parent and child (t, i) is the i-th child of t. Let v t be an n-dimensional feature vector of t. Then, the AST convolution layer updates all vectors of nodes in a given AST simultaneously as follows. Let ς t be a class of node t, that is, a proposition constructor (→, ×, or +) or a class to denote propositional variables.v t ← F conv i W conv ς t ,i v child (t,i) + W conv ς t v t + W conv ς t ,p v parent (t) + b conv ς t(3)where W conv ς t ,i ∈ R m×n is a weight parameter which is a coefficient of the feature vector of the i-th child, W conv ς t ∈ R m×n is for t, W conv ς t ,p ∈ R m×n is for the parent, b conv ς t ∈ R m is a bias parameter for ς t , and F conv is an activation function. Each parameter is shared between nodes with the same ς t . If t is the root node, then v parent (t) denotes the zero vector. We use multiple AST convolution layers to learn features of a node.The update (3) is inspired by tree-based convolution proposed by  #b33 , but there are a few differences. This view is useful when one deals with ASTs where a node may have an arbitrary number of children. However, a different tree representation may affect a feature representation learned by DNNs-especially, it may not preserve the locality of the original AST representation. Thus, instead of binary trees, we deal with ASTs as they are. Fortunately, the syntax of propositions is defined rigorously and the number of children of each node is fixed. Hence, we can fix the number of learnable weight parameters for children: two for each proposition constructor.Aggregation layer. An aggregation layer integrates features of nodes in an AST to a single vector.Definition 5.2 (Aggregation layer). Let t be a node of an AST where nodes are augmented with n-dimensional vectors.Function Agg (t) produces an n-dimensional vector from t as follows:Agg (t) = F agg i W agg ς t ,i Agg (child (t, i)) + W agg ς t v t + b agg ς twhere W agg ς t and W agg ς t ,i ∈ R n×n are weight parameters which are coefficients of vectors of t and its i-th child, respectively, b agg ς t ∈ R n is a bias for ς t , and F agg is an activation function. Each parameter is shared between nodes with the same ς t .Another way to produce a single feature vector from an AST is a max-pool  #b33 ], which, for each dimension, takes the maximum scalar value among all feature vectors in the AST. While max-pools are used by usual convolutional neural networks  #b29 , it is not clear that gathering only maximum values captures features of the whole of the AST. By contrast, an aggregation layer can be considered as "fold" on trees with feature vectors, and we expect that Agg (t) learns a feature representation of the AST because it takes not only maximum values but also the other elements of feature vectors of all nodes into account.In what follows, we write v P for the feature vector of P that is achieved by applying Enc, multiple AST convolution layers, and an aggregation layer sequentially.

Path encoder.
To achieve good performance, we have to know what assumptions are available at the position for which an inference rule is estimated. For example, if a variable of type a → b can be referred to, we expect the variable to be useful to prove b. We can access to information of assumptions via proposition P and path ρ. The proposition-to-proof model thus extracts features of assumptions from the feature vector v P of P along the given ρ.

Definition 5.3 (Extraction)
. Extract (ρ, v) extracts features in the position to which ρ points from v.Extract (⟨⟩, v) = W ext v + b ext Extract (⟨(C, i), ρ⟩, v) = Extract (ρ, v ′ ) where v ′ = F ext W ext C,i v + b ext C,iwhere W ext , W ext C,i ∈ R n×n and b ext , b ext C,i ∈ R n are learnable parameters and F ext is an activation function. ⟨(C, i), ρ⟩ is the addition of (C, i) to path ρ at the beginning. Figure 6 illustrates the process of computing Extract (ρ, v), where features are extracted along the path. We write v P, ρ for Extract (ρ, v P ). The weight parameters in Definition 5.3 have a role of extracting features necessary to capture assumptions from v P . The biases are expected to capture information of the context around the node to which the path points.

Classification.
We estimate what proof inference rule is most likely to be applied by using two feature vectors v P, ρ , the extracted features from P along ρ, and v Q , the features of proof obligation Q. For that, as usual, we concatenate v P, ρ and v Q and apply multiple fully connected layers to the concatenation result so that the number of dimensions of the final output v o is equal to that of proof inference rules, that is, eight. Using v o , we approximate the likelihood of a proof inference rule r being applied by softmax. For vector v ∈ R n , we write v[i] for the real number of the i-th dimension of v. Let n r ∈ {1, ..., 8} be an index corresponding to proof inference rule r in v o . Then, the approximation probability p * (r = r | P = P, ρ = ρ, Q = Q) is calculated by:exp(v o [n r ]) 8 j=1 exp(v o [j]) v v v 1 v v 1 v 2 v v 1 v 2 v 3 v v 1 v 2 v 3 v P,ρthe node pointed by the path 

EXPERIMENTS
This section reports the performance of our proposition-to-proof model and the proof synthesis procedure with it. We train the model on a dataset that contains pairs of a proposition and its proof by supervised learning. After explaining the detailed architecture of our model (Section 6.1), we detail creation of the dataset (Section 6.2). We evaluate the trained model on the basis of accuracy, that is, we check, given a proposition, a partially constructed proof, and a hole from a validation dataset, how accurately the model estimates the inference rule to be applied at the hole; we also conduct an in-depth analysis of the model to confirm how influential depths of hole positions are on the accuracy (Section 6.4.1). Finally, we evaluate the proof synthesis procedure given in Section 4.3 (Section 6.4.2).We implemented the procedure ProofSynthesize and our model on Python 3 (version 3.6.3) with the deep learning framework Chainer  #b47 ] (version 2.1.0). We use the Haskell interpreter GHCi (version 8.0.1) for a type checker in ProofSynthesize. All experiments are conducted on a machine equipped with 12 CPU cores (Intel i7-6850K 3.60GHz), 32 GB RAM, and NVIDIA GPUs (Quadro P6000). Figure 7 shows the architecture of our proposition-to-proof model in the experiments. We use three AST convolution layers to encode proposition P to be proven and one for proof obligation Q. The concatenation result of v P, ρ from Extract and v Q from Agg 2 is fed to three fully connected layers. The detailed specification of each layer is shown in Table 1. We use a rectified linear unit (ReLU)  #b16 ] as activation functions throughout the architecture.

Network configuration


Dataset
The power of deep learning rests on datasets used to train DNN models. In this work, we need a dataset of pairs of a proposition and its proof. We make dataset D all by generating small proofs exhaustively and large proofs at random.Small proofs are generated by Procedure 2, which produces a set D of pairs of a proposition and its proof the size of which is equal to or less than s.  Fig. 7. The architecture of our DNN model. P is a proposition to be proven, ρ is a path specifying the hole to be filled, and Q is a proof obligation to be discharged at the hole.

Layer
Learnable parameters Number of dimensions of output vectors Table 1. Learnable parameters and the number of dimensions of vectors in the output for each layer. ς is a class of a node in a proposition AST.AST conv 1 W conv ς ,W conv ς,i ,W conv ς,p ∈ R 200×4 200 b conv ς ∈ R 200 AST conv 2 W conv ς ,W conv ς,i ,W conv ς,p ∈ R 500×200 500 b conv ς ∈ R 500 AST conv 3 W conv ς ,W conv ς,i ,W conv ς,p ∈ R 1000×500 1000 b conv ς ∈ R 1000 Agg 1 W agg ς ,W agg ς,i ∈ R 1000×1000 b agg ς ∈ R 1000 1000 Extract W ext , W ext C,i ∈ R 1000×1000 1000 b ext , b ext C,i ∈ R 1000 AST conv 4 W conv ς ,W conv ς,i ,W conv ς,p ∈ R 16×4 16 b conv ς ∈ R 16 Agg 2 W agg ς ,W agg ς,i ∈ R 16×16 b agg ς ∈ R 16 16 FC 1 W fc ∈ R 1016×1016 b fc ∈ R 1016 1016 FC 2 W fc ∈ R 1016×1016 b fc ∈ R 1016 1016 FC 3 W fc ∈ R 8×1016 b fc ∈ R 8 8which is smaller (Lines 10-11) in order to decrease the number of estimations performed by ProofSynthesize-the error by approximation becomes larger as more estimations are performed. We call P principal when, for any Q such that ∅ ⊢ M : Q, there exists some map from propositional variables to propositions such that f (P) = Q  #b32 ]; well-typed terms in the simply typed lambda calculus have principal types. Following  #b42 ,we have constructed only βη normal forms. The dataset D all that we use in this work includes a dataset produced by SmallProofGen(9).Procedure 2 Small proof generation 1: procedure SmallProofGen(s) 2:Initialize D with the empty set and S with the empty queue 3:Push [ ] to S 4:while S is not empty do  Generating large proofs is not so easy due to the huge space to be searched. We generate a large proof efficiently, as follows. Suppose that a lower bound l and an upper bound u of the size of a proof generated are given and let M be a βη normal proof partially constructed so far. We start with M = [ ]. We gradually fill holes in M with term constructors chosen randomly and keep M to be the partial proof produced last. If M becomes a complete proof with a smaller size than l, we restart the proof generation from the beginning with M = [ ]. If the size of M becomes larger than l, we preferentially choose variables as term constructors substituted for holes to finish the proof generation as soon as possible. If M becomes a complete proof with size s such that l ≤ s ≤ u, we produce M as the result. If the size of M becomes large than u, we restart the proof generation. This approach may appear rather ad-hoc, but we could generate many large proofs by it. For (l, u) ∈ {(10, 30), (20, 40), (30, 50)}, we generate 15000, 15000, and 10000 proofs, respectively.The dataset D all contains proofs shown in Table 2. Since our DNN model feeds a proposition P, a path ρ, and a proof obligation Q to be discharged at the hole specified by ρ and estimates an inference rule r that should be applied at the hole, we make quadruples (P, Q, ρ, r) from D all and split them into training dataset D t and validation dataset D v . D t contains 90% of quadruples generated from D all (1731998 quadruples) and D v does the remaining 10% (193108 ones).   Table 4. Validation accuracy of the trained model for each inference rule per depth. The column "#" shows the number of validation data and "All" does the accuracy for all inference rules. "N/A" means that there are no validation data.

Training
We train the proposition-to-proof model with the architecture given in Section 6.1 on dataset D t by stochastic gradient descent with a mini-batch size of 1000 for 20 epochs. 9 Weights in each layer of the model are initialized by the values independently drawn from the Gaussian distribution with mean 0 and standard deviation 1 n where n is the number of dimensions of vectors in the input to the layer. The biases are initialized with 0. We use the softmax cross entropy as the loss function. As an optimizer, we use Adam [Kingma and Ba 2014] with parameters α = 0.001, β 1 = 0.9, β 2 = 0.999, and ϵ = 10 −8 . We lower α, which controls the learning rate, by 10 times when the training converges. We regularize our model by a weight decay with penalty rate λ = 0.0001.

Evaluation
6.4.1 Accuracy. Table 4 shows the accuracy of the trained model on the validation dataset D v . The bottom row in the table reports the summarized accuracy and presents that the trained model achieves total accuracy 96.79%. Looking 9 Epoch is the unit that means how many times the dataset is scanned during the training.at results per inference rule, we achieve the very high accuracy for Var, Abs, Pair, Left, and Right. It is interesting that the train model chooses either of Left or Right appropriately according to problem instances. It means that, given proposition P + Q, the proof synthesis procedure with this trained model can select whichever of P and Q should be proven with high probability. The accuracy for App, CasePair, and CaseEither is not so bad, but the estimation of these rules is more difficult than that of other rules. This may be due to the training dataset. As shown in Table 3, the numbers of training data for App, CasePair, and CaseEither are much smaller than those of other rules. Since the model is trained so that inference rules that often occur in the training dataset are more likely to be estimated in order to minimize the loss, the trained model may prefer to choose inference rules other than App, CasePair, andCaseEither. Furthermore, it may be possible that the training data for those rules are insufficient to learn feature representation of the likelihood of them being applied. In either case, data augmentation would be useful, though we need to establish effective augmentation of proofs.Our model is supposed to access the assumptions via the P and ρ. Since ρ becomes larger as the position of the hole does deeper, the depth of the hole is expected to affect the performance of the model. We thus investigate the accuracy of the trained model for each depth of holes in the validation data, which is shown in Table 4. Seeing the column "All", we can find that the accuracy at a greater depth tends to be lower. The accuracy of Abs, Pair, Left, and Right is still high even if holes are at deep positions. We consider that this is because, rather than assumptions, proof obligations play an important role to choose those inference rules. By contrast, the accuracy of App, CasePair, and CaseEither is not high, especially, when holes are at very deep positions. Since these rules need information about assumptions to judge whether they should be applied, their accuracy may be improved by representing features of assumptions better.The accuracy of Var is very high at any depth, though whether we can apply Var should depend on assumptions.This may be due to the large number of training data for Var (Table 3) Table 5.Compared with Table 4, the accuracy of the obligation-free model is lower than that of the proposition-to-proof model for all inference rules, especially, at great depth. The use of proof obligations thus improves the performance of the DNN model. 6.4.2 Proof synthesis. This section evaluates ProofSynthesize (Procedure 1) with the trained proposition-to-proof model. We make two test datasets for evaluation by choosing 500 propositions from D v respectively. One dataset D small consists of propositions that are generated by SmallProofGen(9), that is, the sizes of their proofs can be equal to or lower than 9. The other dataset D large includes propositions that are generated at random so that the sizes of their proofs are larger than 9. We abort the proof synthesis if a proof is not generated within three minutes. We use the principal proposition for a proof obligation that is required by ProofSynthesize.We compare our procedure with an existing method of APS with deep learning by  #b42 . They view proof generation as a translation task from a proposition language to a proof language and apply a so-called sequence-to-sequence model  #b45 , a popular DNN model in machine translation, in order to produce a token sequence expected to be a proof from a token sequence of a proposition. They find that, though the response from the DNN model may not be a proof of the proposition, the response is often "close" to a correct proof and, based on this observation, propose a proof synthesis procedure that uses the response from the DNN model as a guide of proof search. We train the sequence-to-sequence model on D t for 200 epochs in the same way as Sekiyama et al. and apply their proof synthesis procedure to propositions in D small and D large . Table 6 shows the number of propositions that succeed in generation of proofs by each procedure and the average of elapsed times taken by the procedure when proofs are generated successfully (the unit is second). Both procedures succeed in generating proofs for all propositions in D small , which indicates that they work well, at least, for propositions that have small proofs. This direction of enhancing the existing provers is orthogonal to our present work. Although our goal is to generate proofs directly with deep learning, rather than focusing on specific subproblems that are important in theorem proving, we expect (as we discussed in Section 4.3) that the combination of our approach with these techniques is also beneficial to our technique.7.1.2 Formula proving. Solving the Boolean satisfiability (SAT) problem by encoding problem instances into neural networks has been attempted in early days  #b24 ]. Recent work uses DNNs as a binary classifier of Boolean logical formulas.  #b7  represent a Boolean formula in conjunctive normal form (CNF) as a graph where variable nodes are connected to nodes that represent disjunctive clauses referring to the variables and apply a graph neural network  #b40 ] to classify the satisfiability of the formula. Similarly NeuroSAT  #b43 ] regards CNF formulas as graphs, but it adopts a message passing model and can often (not always) produce a Boolean assignment, which makes it possible to check that the formula is truly satisfied.  #b12  tackles the entailment problem in the propositional logic, that is, whether a propositional conjecture can be proven under considered assumptions. They also develop a new DNN model that classifies whether a given entailment holds. These lines of work do not guarantee the correctness of the solution. Our work, although the procedure may not terminate, guarantees the correctness of the returned proof.  #b42  applied deep learning to proof synthesis. Their key idea is that the task of proof synthesis can be seen as a translation task from propositions to proofs. Based on this idea, they use a sequence-to-sequence model  #b45 , which is widely used in machine translation with deep learning, in order to translate a proposition to its proof. As shown in Section 6.4.2, our proposition-to-proof model outperforms their model from the perspectives of (1) the number of propositions that are successfully proved and (2) the time spent by the proof-synthesis procedures.

Neural program synthesis
Synthesizing proofs from propositions can be regarded as synthesizing programs from types via the Curry-Howard isomorphism  #b44 . Program synthesis is one of the classical AI problems, and synthesis with deep learning, dubbed neural program synthesis, has been studied recently. A typical task of the neural program synthesis is to produce programs satisfying given input-output examples  #b2  #b10  #b36  #b50 . Although it appears to be difficult to transfer their approaches to proof synthesis directly since the task of proof synthesis represents a specification of a program by types (i.e., propositions), not input-output pairs, there are similarities between them. For example, the AST decoder based on the syntax of the target language by  #b50  is similar to that used in our work in that we also construct an AST of a proof gradually, whereas our proposition-to-proof model effectively uses the proof obligations which do not appear in neural proof synthesis.We expect that the ideas in neural program synthesis work in proof synthesis as well to achieve better performance.

Deep neural networks for tree structures
Propositions and proofs have variable sizes, and a major way to handle such variable-length data, especially, in natural language processing is to deal with them as sequences. However, such sequence representation collapses the structural information contained in inputs. Indeed, our work takes advantage of the fact that proofs can be interpreted as derivation trees, which makes it possible to synthesize proofs gradually. Besides propositions and proofs, many objects are treestructured-e.g., parse trees, hierarchical dependency graphs, and index structures in databases-and recently there are many studies on tree generation with DNNs.  propose tree long short-term memory (TreeLSTM) to construct tree structures. TreeLSTM relates a parent and its children by a dependency path, which connects a child node to the parent via the siblings. This representation of node relationships needs more steps to pass encoding features to a node. As shown in Section 6.4.1, it would cause degradation of the performance.  #b11  generate tree-structured logical formulas from natural sentences by a top-down decoder. Their decoding method provides special nodes that link a parent to its children, whereas our work does not need such nodes because we know whether a node has children by looking at the inference rule of it.  #b0  also study a decoder for generation of trees where each node has an arbitrary number of children. Their decoder performs two predictions: one is whether a node has a child; and the other is whether it has a sibling. Unlike the task that they address, the number of children of an AST node in the propositional logic is fixed and it is enough to predict the kind of a node.  #b33  develop a tree-based convolutional neural network (TBCNN), which calculates a feature vector of a node by using vectors of nodes near it. While it is similar to the AST convolution and the aggregation layer in our work, there is a difference for each. First, the AST convolution refers to all adjacent nodes including the parent, whereas the TBCNN considers only children. Second, the aggregation layer can be seen as "fold" on trees with feature vectors and the produced single vector should contain features of all nodes in a tree. The TBCNN uses a max-pool to integrate feature vectors of nodes into a single vector, that is, it produces a vector each dimension of which has the value maximum among the corresponding dimensions of feature vectors of the nodes. Although max-pools are commonly used in usual (not tree-based) convolutional neural networks  #b29 , it is unclear that gathering only maximum values does not drop any important feature of nodes in a tree.

CONCLUSION
We present an approach to applying deep learning to the APS problem. We statistically formulate the APS problem in terms of probabilities so that we can quantify the likelihood of a term being a correct proof of a proposition. From this formulation, we show that this likelihood can be calculated by using the likelihood of an inference rule being applied at a specified position in a proof, which enables us to synthesize proofs gradually. To approximate this likelihood,we develop a DNN that we call a proposition-to-proof model. Our DNN model encodes the tree representation of a proposition and decodes it to estimate an inference rule to be applied by using the proof obligation to be discharged effectively. We train the proposition-to-proof model on a dataset of automatically generated proposition-proof pairs and confirmed that the trained model achieves 96.79% accuracy in the inference-rule estimation, though there is still room for improvement. We also develop a proof synthesis procedure with the trained DNN model and show that it can synthesize many proofs of a proposition in short time.Our exploration of APS along with deep learning is still at the early stage; there are many challenging tasks to be addressed. One of the important challenges is to extend the target logic to more expressive ones such as first-order logic and higher-order logic. For example, first-order logic introduces the notions of predicates and quantification.To learn a feature representation of a predicate, we may need a DNN model that takes the "meaning" of a predicate into account. Quantification not only makes formulas complicated but also requires us to deal with the problem of instantiation. Another important notion that we need to deal with is the induction principle. With the extension of the logic, it is expected that a problem with datasets happen. One promising way to address it is, as done by , making a dataset from publicly available proofs. Furthermore, the creation of a benchmark collecting challenging tasks related to APS is crucial for the development of APS with deep learning, as ImageNet  #b39  contributes to the advance of image processing.Another future direction is improvement of a model. Our model is expected to have access to assumptions via the feature vector of a given proposition. However, it may be more useful to encode a set of assumptions directly, as we encoded proof obligations in this work. A problem with it is that the number of assumptions is not fixed; DNNs are good at handling objects with a fixed size but require efforts to deal with variable-sized data. Another possible issue is the vanishing gradient problem; gradients in very deep neural networks often vanish, which makes learning difficult.Since our tree-structured model can be considered to have variable-length nonlinear layers and become deeper as propositions and/or proofs are larger, that problem would be more serious when we deal with larger propositions and proofs than the present work. We expect that the recent progress in research to address this problem works well also in our settings; especially, residual blocks  #b19  and LSTMs  #b21  are promising workarounds.

ACKNOWLEDGMENTS
This work is partially supported by JST PRESTO Grant Number JPMJPR15E5, Japan.

Footnote
1 : In the community of neural network research, there is a habit to call a trained DNN "model". Following this convention, we abuse the word "model" for a trained DNN.
2 : We only consider the case where X and Y are discrete in this paper. 
4 : We could actually build a statistical model that does not treat P as a random variable as long as we focus on the contents of this paper. However, in order to easily extend our framework to one that consider nontrivial probabilistic distributions over propositions in future, we model P as a random variable.
5 : In order to formally prove this fact, we need to define the random variables and the probability distributions in this paper in more formal style, which we decide to defer to future work.
6 : We also expect this observation to be generalized to various type systems. This observation essentially comes from the fact that the interfacing by a type separates certain dependency between a context and a term, which is often true for many type systems.
7 : The reason why a deep learning model tends not to overfit is still not fully understood; seeNeyshabur et al. [2017] for recent remarkable development on this issue.
8 : They did not clarify what binary tree is considered, though.
3 :  Strictly speaking, we need to define the structure of measurable sets on Y and argue that {y ∈ Y | φ(x, y)} is measurable on this structure to formally define p(φ). We do not discuss such measure-theoretic issues in this paper.