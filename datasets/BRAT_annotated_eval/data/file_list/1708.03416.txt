Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose Estimation

Abstract
Hand pose estimation from single depth images is an essential topic in computer vision and human computer interaction. Despite recent advancements in this area promoted by convolutional neural networks, accurate hand pose estimation is still a challenging problem. In this paper we propose a novel approach named as Pose guided structured Region Ensemble Network (Pose-REN) to boost the performance of hand pose estimation. Under the guidance of an initially estimated pose, the proposed method extracts regions from the feature maps of convolutional neural network and generates more optimal and representative features for hand pose estimation. The extracted feature regions are then integrated hierarchically according to the topology of hand joints by tree-structured fully connections to regress the refined hand pose. The final hand pose is obtained by an iterative cascaded method. Comprehensive experiments on public hand pose datasets demonstrate that our proposed method outperforms state-of-the-art algorithms.

Introduction
Accurate 3D hand pose estimation is one of the most important techniques in human computer interaction and virtual reality  #b0 , since it can provide fundamental information for interacting with objects and performing gestures  #b1  #b2 . Hand pose estimation from single depth images has attracted broad research interests in recent years  #b3  #b4  #b5  #b6  #b7  #b8  #b9  #b10  thanks to the availability of depth cameras  #b11  #b12  #b13  #b14 , such as Microsoft Kinect, Intel Realsense Camera etc. However, hand pose estimation is an extremely challenging problem due to the severe selfocclusion, high complexity of hand articulation, noises and holes in depth image, large variation of viewpoints and self-similarity of fingers etc.Hand pose estimation has achieved great advancements by convolutional neural networks (CNNs). CNN-based data-driven methods either predict heatmaps of hand joints  #b4  #b15  and infer hand pose from heatmaps, or directly regress the 3D coordinates of hand joints  #b16  #b17  #b6  #b18  #b9  #b19 . In either ways, features are critical for the performance of hand pose estimation. Prior works mainly focused on incorporating prior knowledge into CNN  #b16  #b19  or using error feedback  #b17  and spatial attention design  #b6 . However, few of prior works have paid attentions to extracting more $ Email addresses: chen-xh13@mails.tsinghua.edu.cn (X. Chen), guohengkaighk@gmail.com (H. Guo), zcr17@mails.tsinghua.edu.cn (C. Zhang). Work was done when H. Guo was with Dept. of EE, Tsinghua University. * Corresponding author: wangguijin@tsinghua.edu.cn (G. Wang) optimal and representative features of CNN. Ye et al.  #b6  used spatial attention module to select and transform features to a canonical space. Guo et al.  #b8  #b51  proposed the region ensemble network (REN) that divides the feature maps of last convolutional layer into several spatial regions and integrates them in fully connected layers. All aforementioned works haven't fully exploit optimal features of CNN for hand pose estimation.In this paper, we propose a novel method called pose guided structured region ensemble network (Pose-REN) to boost the performance of hand pose estimation, as shown in Figure 1. Upon an iterative refinement procedure, our proposed method takes a previously estimated pose as input and predicts a more accurate result in each iteration. We present a novel feature extraction method under the guidance of previous predicted hand pose to get optimal and representative features for hand pose estimation. Furthermore, inspired by hierarchical recurrent neural network  #b20 , we present a hierarchical method to fuse features of different joints according to the topology of hand. Features from joints that belong to the same finger are integrated in the first layer and features from all fingers are fused in the following layers to predict the final hand pose.We evaluate our proposed method on three public hand pose benchmarks  #b21  #b4  #b22 . Compared with state-of-theart methods, our method has achieved the best performance. Extensive ablation analyses illustrate the contributions of different components of the framework and robustness of our proposed method.The remainder of this paper is organized as follows. In Section 2, we review prior works that are highly related  to our proposed method. In Section 3, we present details about our proposed pose guided structured region ensemble network. Evaluations on public datasets and ablation studies are provided in Section 4. Section 5 gives a brief conclusion of this paper.

Related Work
In this section we briefly review related works of our proposed method. Firstly we will review recent algorithms for depth based hand pose estimation. Since our method basically builds upon cascaded framework, we will introduce the cascaded methods for hand pose estimation. Finally, we will review related works about the hierarchical structure of neural network, as the hierarchical structured connections are utilized in our method.

Depth-based Hand Pose Estimation
Recent approaches of hand pose estimation are generally categorized into three classes: discriminative methods  #b4  #b21  #b16  #b22  #b23  #b9  #b15  #b24  #b25  #b10  #b8 , generative methods  #b26  #b27  #b28  #b29  and hybrid methods  #b30  #b5  #b31  #b32  #b33  #b6  #b19 . Comprehensive review and analysis on depth based 3D hand pose estimation can be found in  #b3 .Generative methods fit a predefined hand model to the input data using optimization algorithms to obtain the optimized hand pose, such as PSO (particle swarm optimization)  #b33 , ICP (Iterative Closest Point)  #b26  and their combination (PSO-ICP)  #b35 . Hand-crafted energy functions that describe the distance between the hand model and input image are utilized in prior works, such as golden energy  #b33  and silver energy  #b5 . Several kinds of hand model have been adopted, including sphere model  #b35 , sphere-meshes model  #b27 , cylinder model  #b26  and mesh model  #b33 . Generative methods are robust for self-occlusive areas or missing areas and ensure to output plausible hand pose. However, they need a complex and time-consuming optimizing procedure and are likely to trap into local optimizations.Discriminative methods directly learn a predictor from the labelled training data. The predictor either predicts the probability maps (heatmaps) of each hand joints  #b4  #b15  or directly predicts the 3D hand joint coordinates  #b16  #b8 . The most frequently used methods for predictor are random forest  #b36  #b21  #b37  #b5  #b22  and convolutional neural network  #b4  #b16  #b8  #b9  #b10 . Discriminative methods do not require any complex hand model and are totally datadriven, which are fast and appropriate for real-time applications. Guo et al.  #b8  #b51  proposed a region ensemble network (REN) that greatly promoted the performance of hand pose estimation based on a single network. Region ensemble network divides the feature maps of last convolutional layer into several spatial regions and integrates them in fully connected layers. However, REN extracts the feature regions using a uniform grid and all features are treated equally, which is not optimal to fully incorporate the spatial information of feature maps and obtain highly representative features.Hybrid methods try to combine the discriminative and generative methods to achieve better hand pose estimation performance. Some works adopted the generative methods after obtaining initial results by discriminative methods  #b30  #b32  #b33 . Zhou et al.  #b19  proposed to incorporate a hand model into the CNN, which exploits the constraints of the hand and ensures the geometric validity of the estimated pose. However, hybrid methods have to predefine the properties of the hand model, such as the length of bones. Oberweger et al.  #b17  proposed a data-driven hybrid method, which learns to generate a depth image from hand pose. However, the generation of depth images is likely affected by the errors of annotations.Our proposed method basically falls into the category of discriminative method and does not rely on any predefined hand model. Compared with prior CNN-based discriminative methods, our proposed method directly predicts the 3D locations of hand pose using a cascaded framework without any postprocessing procedure. What's more, our proposed pose guided structured region ensemble network (Pose-REN) can learn better features for hand pose estimation by incorporating guided information of previously estimated hand pose into the feature maps and improve the performance of our method.Although our proposed Pose-REN follows the idea of feature region ensemble as REN  #b8 , there are several essential differences between Pose-REN and REN  #b8 : 1) Different from REN that uses grid region feature extraction, the proposed Pose-REN fully exploits an initially estimated hand pose as the guided information to extract more representative features from CNN, which is shown to have a large impact for hand pose estimation problem, as discussed in Section 4.4.2. 2) Instead of simple feature fusion as adopted in REN, our Pose-REN presents a structured region ensemble strategy that better models the connections and constraints between different joints in the hand.3) The Pose-REN is a common framework that can easily be compatible with any existing methods (for example, Feedback  #b17 , DeepModel  #b19  etc.) by using them to produce initial estimations for Pose-REN.

Cascaded Method
The cascaded framework has been widely used in face alignment  #b38  #b39  #b40 , human pose estimation  #b41  #b42  and has also shown good performances in the problem of hand pose estimation  #b22  #b17  #b6 .Sun et al.  #b22  proposed a method to iteratively refine the hand pose using hand-crafted 3D pose index features that are invariant to viewpoint transformation. Oberweger et al.  #b16  proposed a post-refinement method to refine each joint independently using multiscale input regions centered on the initially estimated hand joints. These works have to train multi models for refinement and independently predict different parts of hand joints while our proposed needs only one model to iteratively improve the estimated hand pose.Oberweger et al.  #b17  presented a feedback loop framework for hand pose estimation. One discriminative network is used to produce initial hand pose. A depth image is then generated from the initial hand pose using a generative CNN and an updater network improves the hand pose by comparing the synthetic depth image and input depth image. However, the depth synthetic network is highly sensitive to the annotation errors of hand poses.Ye et al.  #b6  integrated cascaded and hierarchical regression into a CNN framework using spatial attention mechanism. The partial hand joints are iteratively refined using transformed features generated by spatial attention module. In their method, the features in cascaded framework are generated by a initial CNN and remain unchanged in each refinement stage except for the spatial transformation. In our proposed method, feature maps are updated in each cascaded stage using an end-to-end framework, which will help to learn more effective features for hand pose estimation.Our Pose-REN also adopts the cascaded framework. Different from the above prior methods, we present a novel feature extraction method under the guidance of previous predicted hand pose to get optimal and representative features from CNN. What's more, Pose-REN explicitly models the constraints and relations between different hand joints using structured region ensemble strategy, which is a novel method to improve the robustness and performance of hand pose estimation.

Hierarchical Structure of Neural Network
Du et al.  #b20  proposed a hierarchical recurrent neural network (RNN) for skeleton-based human action recognition. The whole skeleton is divided into five parts and fed into different branches of the RNN. Different parts of skeleton are hierarchically fused to generated higher-level representations. Madadi et al.  #b18  proposed a tree-shape structure of CNN which regresses local poses at different branches and fuses all features in the last layer. In their structure, features of different partial poses are learned independently except for sharing features in very early layers. In contrast, our method shares features in the convolutional layers for all joints and hierarchically fuses different regions from feature maps to finally estimate the hand pose. The shared features enables better representation of hand pose and the hierarchical structure of feature fusion can better model the correlation of different hand joints.

Pose Guided Structured Region Ensemble Network
In this section, we first give an overview of Pose-REN in Section 3.1. After that we will provide detailed elaboration about extracting regions from the feature maps under the guidance of a hand pose in Section 3.2. In Section 3.3 we present the details of fusing feature regions using hierarchically structured connection. Finally, the training strategy and implementation details are given in Section 3.4 and Section 3.5. 

Overview
The framework of our proposed method is depicted in Figure 1. A simple CNN (denoted as Init-CNN) predicts an initial hand pose pose 0 , which is used as the initialization of the cascaded framework. The proposed framework takes a previously estimated hand pose pose t−1 and the depth image as input. The depth image is fed into a CNN to generate feature maps. Feature regions are extracted from these feature maps under the guidance of the input hand pose pose t−1 . The insight of our proposed method is that features around the location of a joint contribute more while other features like corner regions are less important. Afterwards, features from different joints are hierarchically integrated using the structured connection to regress the refined hand pose pose t . The images in dash rectangles show the close-up results of pose t−1 and pose t . It can be seen that the network refines the hand pose gradually.Our method aims to estimate the 3D hand pose from a single depth image in a cascaded framework. Specifically, given a depth image D, the 3D locations P = {p i = (p xi , p yi , p zi )} J i=1 of J hand joints are inferred. Given a previously estimated hand pose result P t−1 in stage t − 1, our method uses the learned regression model R to refine the hand pose in stage t.P t = R(P t−1 , D)(1)After T stages, we get the final estimated hand pose P T for the input depth image D.P T = R(P T −1 , D)(2)It should be noted that only one same model R is used in every stage of refinement in the inference phase, see Section 3.4 for details.

Pose Guided Region Extraction
We first use a standard convolutional neural network (CNN) with residual connections to generate feature maps. The backbone architecture of CNN for generating feature maps used in our method is the same as the baseline network in  #b8 , with 6 convolutional layers and 2 residual connections. Each convolutional layer is followed by a Rectified Linear Unit (ReLU)  #b43  as the activation function and every 2 convolutional layers are followed by a max pooling layer. The residual connections are added between max pooling layers.Denote feature maps from the last convolutional layer as F and the estimated hand pose from previous stage asP t−1 = {(p t−1 xi , p t−1 yi , p t−1 zi )} J i=1. We use P t−1 as the guidance to extract feature regions from F. Specifically, for the i th hand joint, We first project the real-world coordinates into the image pixel coordinates using the intrinsic parameters of the depth camera, as shown in Eq. 3.(p t−1 ui , p t−1 vi , p t−1 di ) = proj(p t−1 xi , p t−1 yi , p t−1 zi )(3)The feature region for this joint is then cropped using a rectangular window which can be defined by a tuple(b t ui , b t vi , w, h), where b t ui and b tvi is the coordinates of topleft corner, w and h is the width and height of the cropped feature region. The coordinates of the rectangular window are calculated by normalizing and converting the originalcoordinates (p t−1 ui , p t−1 vi , p t−1 di ) into coordinates in feature maps.The extracted feature region for hand joint i is then obtained by cropping the feature maps within the rectangular window: Figure 2 gives an example of pose guided region extraction. The left image is a feature map from the last convolutional layer of the CNN. It should be noted the feature maps usually contains multiple channels, we only use one channel of them to depict how to crop a region guided by a joint. The green dot and red dot indicate two joints (palm center joint and Metacarpophalangeal joint for middle finger respectively) from the previously estimated hand pose. The green and red rectangles are the corresponding cropped windows. The images in middle and right columns show the extracted feature regions for these two joints.F t i = crop(F; b t ui , b t vi , w, h) (4) where the function crop(F; b u , b v , w, h) means extracting the region specified by a rectangular window (b u , b v , w, h) from F.

Structured Region Ensemble
In the previous section we have described how to extract feature regions from the feature maps for each joint using the guidance of previously estimated hand pose. One intuitional way to fuse these feature regions is to connect each region with fully connected (f c) layers respectively and then fuse these layers to regress the final hand pose, which is adopted in REN  #b8 .Human hand is a highly complex articulated object. Therefore, there are many constraints and correlations between different joints  #b44  #b45 . Independently connecting feature regions with f c layers and fusing them in the last layer can not fully adopt these constraints. Inspired by hierarchical recurrent neural network  #b20 , in this paper we  adopt hierarchically structured region ensemble strategy to better model the constraints of hand joints, as shown in Figure 3. First a set of feature regions {F t j } M j=1 are fed into f c layers respectively.h l1 j = f c(F t j ), j = 1, . . . , M(5)Where M is the number of regions extracted from the feature maps. Next, {h l1 j } M j=1 are integrated hierarchically according the topology structure of hand. Specifically, denote the indices of joints that belong to the i th finger as {I i j } Mi j=1 , where M i is the number of joints that belong to the i th finger. All joints that belong to the same finger are concatenated (denote as concate) and then fed into a f c layer, as shown in Eq. 6 and Eq. 7.h l1 i = concate({h l1 I i j } Mi j=1 ), i = 1, . . . , 5(6)h l2 i = f c(h l1 i ), i = 1, . . . , 5(7)Afterwards, features from different fingers {h l2 i } 5 i=1 are concatenated and fed into a f c layer to regress the final hand pose P t ∈ R 3×J .h l2 = concate({h l2 i } 5 i=1 )(8)P t = f c(h l2 )(9)Each f c layer in Eq. 5 and Eq. 7 has a dimension of 2048 nodes. They are followed by ReLU layers and dropout layers with dropout rate of 0.5. The last f c layer output a 3 × J vector P t which represents the 3D locations of hand pose.

Training
Denote the original training set asT 0 = {(D i , P 0 i , P gt i )} N T i=1(10)where N T is the number of training samples, D i is the depth image, P 0 i is the initially estimated hand pose and P gt i is the corresponding ground truth of hand pose. In stage t, a regression model R t is trained using T t−1 . Using this model, we can obtain the refined hand pose for each sample in training set.P t i = R t (P t−1 i , D)(11)we add the refined samplesT t = {(D i , P t i , P gt i )} N T i=1to the training set, generating an augmented training set T t .T t = T t−1 T t(12)Again, we train a model R t+1 in stage t + 1 using T t and iteratively repeat this process until reaching the maximum iteration T . The trained model R T is the final model used in the inference phase to refine the initial hand pose iteratively, as described in Eq. 1 and Eq. 2.

Implementation Details
We implemented our proposed method using Caffe  #b46 . RoI Pooling layer  #b47  was used to facilitate the implementation of pose guided region extraction.We used the baseline network in  #b8  as the Init-CNN to produce initial poses for our method. Generally speaking, any existing hand pose estimation algorithms can be adopted as the initialization method of Pose-REN. We will further discuss the effect of different initializations in Section 4.4.4, including generalization of our pre-trained model to other initializations in inference phase and robustness of Pose-REN to other initializations.Preprocessing. Similar to previous methods  #b17  #b8 , we extracted a fix-sized cube from the input depth image. The center of the cube was determined by calculating the centroid of mass of the hand region. The extracted cube was then resized into a patch with size of 96 × 96 and the depth values within it were normalized into [−1, 1]. Besides, depth values that were outside the cube were truncated according to the size of cube, providing robustness to invalid depth values. The idea of extracting a fix-sized cube is to ensure invariance of the hand size to the distance to the camera.Training. We first trained the Init-CNN to obtain initial hand pose. After that, we used the weights of trained Init-CNN to initialize Pose-REN and train the network. The whole network was trained using stochastic gradient descent (SGD) with a batch size of 128 and a momentum of 0.9. A weight decay of 0.0005 was also adopted for the network. The learning rate was set to 0.001 and divided by 10 after every 25 epochs. The model was trained for 100 epochs for each stage and totally trained for two stages.We followed several good practices that have been proved to be quite effective for hand pose estimation  #b8 , including random data augmentation, smooth L 1 loss. For data In our experiments, the size of extracted region was set to (w, h) = (7, 7). In inference phase, the number of iterations was set to T = 3, which will be further discussed in Section 4.4.1.

Experiments
In this section, we will first introduce the datasets and evaluation metrics in the experiments. Afterwards we will evaluation our proposed method on three challenging public datasets: ICVL Hand Posture Dataset  #b21 , NYU Hand Pose Dataset  #b4  and MSRA Hand Pose Dataset  #b22 . Finally we conduct extensive experiments for ablation study to discuss the effectiveness and robustness of different components of our proposed method.

Datasets
ICVL Hand Posture Dataset  #b21 . This dataset was collected from 10 different subjects using Intel's Creative Interactive Gesture Camera  #b48 . In-plane rotations are applied to the collected samples and the final dataset contains 330k samples for training. There are totally 1596 samples in the testset, including 702 samples for test sequence A and 894 samples for test sequence B. The annotation of hand pose contains 16 joints, including 3 joints for each finger and 1 joint for the palm.NYU Hand Pose Dataset  #b4 . The NYU hand pose dataset was collected using three Kinects from different views. The training set contains 72757 frames from 1 subject and the testing set contains 8252 frames from 2 subjects, while one of the subjects in testing set doesn't appear in training set. The annotation of hand pose contains 36 joints. Following the protocol of previous works  #b4  #b16  #b17  #b19  #b8 , we only use frames from the frontal view and 14 out of 36 joints in evaluation.  #b22 . The MSRA hand pose dataset contains 76500 frames from 9 different subjects captured by Intel's Creative Interactive Camera. The leave one subject out cross validation strategy is utilized for evaluation. The annotation of hand pose consists of 21 joints, with 4 joints for each finger and 1 joint for the palm. This dataset has large viewpoint variation, which makes it a rather challenging dataset.

MSRA Hand Pose Dataset


Evaluation Metric
There are two evaluation metrics widely used in hand pose estimation: per-joint errors and success rate. Denote {p ij } as the predicted joint locations of test frames, where i is the index of frame and j is the index of joint. {p gt ij } is the corresponding groundtruth label. N is the number of test frames and J is the number of joints in a frame.Per-joint Errors. Average euclidean distance between predicted joint location and groundtruth for each joint over all test frames. The error for the j th joint is calculated by:err j = i ( p ij − p gt ij ) N(13)Average joint error err = j errj J is also used to evaluate the overall performance of hand pose estimation.Success Rate. The fraction of good frames. A frame is considered as good if the maximum joint error of this frame is within a distance threshold τ . The success rate for distance threshold τ is calculated as Eq. 14.rate τ = i 1(max j ( p ij − p gt ij ) ≤ τ ) N(14)where 1(cond) is an indicate function that equals to one if cond is true and equals to zero otherwise.     Table 1: Quantitative evaluation of different methods on the benchmark NYU dataset for hand pose estimation task. We report 2D average pixel errors and 3D average joint errors in mm.

Methods
3D error (mm) 2D error (pixels) HandsDeep  #b16  19.73 9.81 Feedback  #b17  15.97 8.20 DeepModel  #b19  16.90 8.76 Mask R-CNN  #b49  27.61 8.25 JTSC  #b50  16.80 8.02 Madadi et al.  #b18  15.60 -Lie-X  #b7  14.51 7.48 REN (4x6x6)  #b8  13.39 6.78 REN (9x6x6)  #b51  12.69 6.32 Ours 11.81 5.53

Comparison with State-of-the-Arts
To demonstrate the effectiveness of our proposed method, we compare it against several state-of-the-art methods, including latent random forest (LRF)  #b21 , DeepPrior with refinements (HandsDeep)  #b16 , cascaded hand pose regression (Cascaded)  #b22 , feedback loop (Feedback)  #b17 , deep hand model (DeepModel)  #b19 , Lie group based method (Lie-X)  #b7 , multi-view CNN (Multiview)  #b15 , 3D-CNN based method (3DCNN)  #b10  , CrossingNets  #b9 , local surface normals (LSN)  #b23 , occlusion aware method (Occlusion)  #b52 , JTSC  #b50 , global to local CNN (Madadi et al.)  #b18  and region ensemble network with 9 × 6 × 6 region setting (REN-9x6x6)  #b51 .It should be noted that some reported results of stateof-the-art methods are calculated using the predicted labels that are available online  #b21  #b17  #b19  #b8  #b7  #b51  and others  yaw angle (deg)  $b5    are estimated from the figures and tables of the original papers  #b22  #b23  #b15  #b10  #b9  #b52  #b18 . We also compare our method with Mask R-CNN  #b49  due to its impressive performance on RGB human pose estimation. For fair comparison, we first crop the depth images and resize them into 96 × 96, which is the same preprocessing as our proposed method. We use similar setting with human pose estimation task in  #b49  that exploits ResNet-50-FPN as the backbone network. To adopt Mask R-CNN for depth-based hand pose estimation, we first use Mask R-CNN to detect 2D hand pose in image coordinates and then infer depth values from the original depth images to recover 3D hand pose. To alleviate the impact of noises and holes in depth images, the inferred depth values are constrained within the 3D cube of hand and valid depth values from 9-neighbours are averaged to get the final depth coordinate.On NYU dataset, we compare our proposed method with  #b16  #b17  #b19  #b53  #b7  #b9  #b15  #b10  #b51  #b49 . The success rate with respect to the worse case criteria and per-joint errors are given in Figure 5. As shown in the figure, our proposed outperforms all state-of-the-art methods. We further compare the overall 2D and 3D mean joint error in Table 1.Our method obtain 0.88mm 3D error decrease compared with existing best performance by REN  #b51 . Mask R-CNN performs 2D keypoint detection and the post-processing is used to lift 2D pose to 3D pose. It achieves comparable 2D error with prior methods. Nevertheless, our Pose-REN outperforms Mask R-CNN and reduces the 2D error by 2.7 pixels.On ICVL dataset, we compare our proposed method against  #b21  #b19  #b16  #b9  #b23  #b51 . Results in Figure 6 demonstrate that our proposed method outperforms all other methods with a large margin. Compared with REN  #b51 , our method reduces the mean error by 0.514mm, which is a 7.04% relative improvement.On MSRA dataset, we compare with several state-ofthe-art methods  #b22  #b15  #b10  #b23  #b52  #b51 . The success rate with respect to maximum allowed threshold and per-joint errors are shown in Figure 7. Our method achieves the best performance among all evaluated methods. Following the protocol of previous works  #b22 , we also report the mean joint errors distributed over yaw and pitch viewpoint angles, as shown in Figure 8. Our method achieves the smallest errors in almost all angles. It should be noted that the LSN  #b23  get slightly smaller errors when the yaw or pitch angle is relatively small. However, the performance of LSN decreases rapidly when the viewpoint becomes larger. These results demonstrate that our method is much more robust to viewpoint changes, which is a quite challenging problem in hand pose estimation.The fraction of good frames of our method decreases slightly compared with REN  #b51  when the errors are larger than around 30mm. This is mainly due to worse initial pose for these challenging samples. When regarding to the per-joint errors, our method achieves the best performance among all compared methods.

Ablation Study
In this section we will provide extensive experiments to discuss the contributions of different components of our method and the effect of some parameters.

Effect of the Number of Iteration T
First we will discuss how the number of iteration T affects the performance. The average joint errors on NYU dataset with using the different number of iterations are shown in Figure 10. The error for iteration 0 is the result of the initialization. After one iteration, the error drops rapidly. As the iteration increases, the error becomes stable and finally converges. To better balance the computation complexity and performance, we choose the number of iteration as T = 3.

Effect of Pose Guided Region Extraction
One of the contributions of our proposed method is to extract feature regions under the guidance of hand pose from previous stage. We will show whether this strategy helps to improve the performance of hand pose estimation. In REN  #b8 , feature regions are extracted using a uniformly distributed grid. We report the performances of our method that only adopts one iteration and sets the number of regions and the size of regions the same as REN-4x6x6  #b8  and REN-9x6x6  #b51 . Under such experimental settings, the number of parameters of our method and REN are the same, which ensures fair comparison. The first number in the suffix indicates the number of regions and the last two numbers represent the size of regions. Specifically, we use the palm joint, the root joint of thumb, middle, pinky finger in 4x6x6 setting (denoted Table 2: Comparing average joint errors of our method with and without structured region ensemble strategy on three datasets. The numbers in the brackets indicate the percentages of error reduction.

Dataset
Ours w/o structure Ours (mm) (mm) NYU  #b4  11.869 11.811(−0.5%) ICVL  #b21  6.932 6.793(−2.0%) MSRA  #b22  8.728 8.649(−0.9%)as Our-4x6x6) and use all joints except for two joints in thumb finger and the tip joint of pinky finger in 9x6x6 setting (denoted as Our-9x6x6). The success rate curve and per-joint errors on NYU dataset are shown in Figure 9.With different region settings, our method both performs better than REN that adopts grid region ensemble, indicating the contributions of pose guided region extraction strategy.

Effect of Structured Region Ensemble
We will demonstrate the effectiveness of another component of our proposed method: the hierarchically structured region ensemble. We compare our method with a network (denoted as Ours w/o structure) that use two simple f c layers as is adopted in REN  #b8  instead of hierarchical f c layers. For fair comparison, we set the dimensions of the two f c layers as 2304 and 2048 respectively to ensure the similar number of parameters between our method and Ours w/o structure. The mean joint errors on NYU, ICVL and MSRA dataset are shown in Table 2. It can be seen that our method performs better than Ours w/o structure, which illustrates the effectiveness of the hierarchically structured region ensemble strategy.

Effect of the Initialization
In this section we will demonstrate the robustness of our proposed method over different initializations. Our proposed method builds upon the cascaded framework, which takes an initial hand pose as input and iteratively refine the results. To explore the impact of initialization for our methods, we conduct several experiments on NYU dataset with different initializations.Firstly we will discuss the impact of initialization in inference phase. Specifically, we chose four methods as initialization: Init-CNN (which is proposed in  #b8  as a baseline network and also adopted as the initialization of our method), DeepPrior  #b16 , Feedback  #b17 , DeepModel  #b19 . The results of different initializations and refined results (denoted as, e.g. Ours init deepprior) are shown in Figure 11. We can observe that our method can considerably boost the performances of the initializations. Even with some rather rough initialization (e.g. DeepPrior), the refined results boosted by our Pose-REN are quite competitive. With other better initializations (Feedback, Deep-Model), the final results are similar to our method, even if their initializations are slightly worse than ours. These Fraction of frames within distance (%) REN  results indicate the robustness over initializations of our method. It should be noted that the model used above were trained using the samples with our initialization (Init-CNN). We used different initializations in inference to get the results above. Therefore, the results above also demonstrate the generalization of our model. Furthermore, we consider the case that uses a natural pose (denoted as meanpose) as initialization and discuss which performance can be expected. We used the model that was trained on our initialization to refine the hand pose with meanpose as the initialization. As shown in Figure 12, the initial meanpose is very poor and the results are boosted by adopting our method. We empirically find that the performance converges after 10 stages (Ours init meanpose), resulting the average joint error of 17.708mm, which is comparable with some state-of-the-are methods, as shown in Table 1. We further trained a model using the meanpose as initialization and report the refined results (Ours init meanpose train) in Figure 12. It can be seen that the results are quite close to those of our method that uses a better initialization, which indicates that our proposed method is robust to different initializations.As discussed above, the model trained on our initialization greatly generalize to other initializations. Furthermore, for a very poor initialization, our proposed method can still obtain satisfying results by training a model using this initialization. Figure 13 shows some examples of the iterative process on NYU dataset. The first column shows the results of the initialize hand pose, the second to fourth columns show the refined results on stage 1 − 3. The rightmost column is the groundtruth annotation. Our method gradually improves the estimated hand pose and obtains accurate results after several iterations.

Qualitative Results
Some qualitative results on three datasets can be seen in Figure 14. For each dataset, the first row represents the results of REN-9x6x6  #b51 , the second row shows the results of our proposed method and the third row is the groundtruth. It can be seen that our method performs better than REN even in some challenging samples.

Conclusion
In this paper we propose a novel method called pose guided structured region ensemble network (Pose-REN) for accurate 3D hand pose estimation from a single depth image. Our method extracts regions from the feature maps under the guidance of an initially estimated hand pose to attain more optimal and representative features. Feature   regions are then integrated hierarchically by adopting a tree-like structured connection that models the topology of hand joints. Our method iteratively refines the hand pose to obtain the final estimated results. Experiments on public hand pose datasets demonstrate that our proposed method outperforms all state-of-the-art methods. In our future work, we intend to further improve our method for robust and accurate 3D hand pose estimation when hands are interacting with other hands or objects. We would like to research on integrating hand detection and hand pose estimation into a unified framework, based on Faster R-CNN  #b54  or Mask R-CNN  #b49  etc. It will also be interesting to apply our proposed method for more articulated pose estimation tasks, like human pose estimation and face alignment.