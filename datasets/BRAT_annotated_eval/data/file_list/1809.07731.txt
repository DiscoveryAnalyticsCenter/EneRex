Benchmarking Reinforcement Learning Algorithms on Real-World Robots

Abstract
Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning 1 .

Introduction
In recent years, policy learning algorithms such as trust region policy optimization (TRPO, Schulman et al. 2015), proximal policy optimization (PPO, Schulman et al. 2016), and deep deterministic policy gradient (DDPG, Lillicrap et al. 2015) methods have gained popularity due to their success in various simulated robotic tasks (Duan et al. 2016). A large body of works has been built on these algorithms to address different challenges in reinforcement learning including policy learning (Haarnoja et al. 2017), hierarchical learning (Klissarov et al. 2017, transfer learning  #b8 , and emergence of complex behavior  #b1 . Deep learning software such as Theano and Tensorflow as well as the availability of source code of learning algorithms (e.g., Duan et al. 2016 #b3 ) and benchmark simulated environments (e.g., Brockman et al. 2016, Machado et al. 2018 #b5  contributed to this advancement.It is natural to expect that successes in simulations would inspire similar engagement within the reinforcement learning community toward policy learning with physical robots. But this engage- ment so far has been limited. Some notable works show success when the learning algorithms are supported with one or more of (a) sufficient state information or auxiliary task-specific steps and knowledge (e.g. Levine et al. 2016, (b) preparation in simulation (e.g.  #b1 ) (c) collaborative learning (e.g.  #b9 , and (d) learning from demonstrations (e.g. Hester et al. 2017). However, reinforcement learning research with real-world robots is yet to fully embrace and engage the purest and simplest form of the reinforcement learning problem statement-an agent maximizing its rewards by learning from its first-hand experience of the world. This lack of engagement indicates the difficulties in carrying forward the successes and enthusiasm found in simulation-based works to the real world. Due to the lack of benchmark tasks, it is hard to analyze these difficulties and address them as a community. Mahmood et al. (2018) recently brought to attention some of the difficulties of real-world robot learning and showed that learning performance can be highly sensitive to different elements of the task setup such as the action space, the action cycle time defined by the time between two subsequent actions, and system delays (also see Riedmiller 2012). Therefore, reproducing and utilizing existing results can be hard when the details of these task setup elements are omitted. Moreover, without careful task setups, learning with physical robots can be insurmountably difficult.To study and alleviate these difficulties, we introduce six reinforcement learning tasks based on three commercially available robots. Most of these tasks require no additional hardware installation apart from the basic robot setup. On these tasks, we compare and benchmark four reinforcement learning algorithms for continuous control: TRPO, PPO, DDPG, and Soft Q-learning (Haarnoja et al. 2017).The main contributions of this work are 1) introducing benchmark tasks for physical robots to share across the community, 2) setting up the tasks to be conducive to learning, and 3) providing the first extensive empirical study of multiple policy learning algorithms on multiple physical robots.

Robots
We use three commercially available robots (see Figure 1) as a basis for defining learning tasks.

UR5:
The UR5, shown in Figure 1 (left) is a collaborative industrial arm with six joints produced by Universal Robots. The sensory packets from UR5 include angles, velocities, target accelerations, and currents for each joint. The control interface offers low-level position and velocity control commands. We use UR5 to develop two tasks called UR-Reacher-2 and UR-Reacher-6 based on the tasks developed by Mahmood et al. (2018).Dynamixel MX-64AT: The Dynamixel (DXL) series of programmable Direct-Current actuators, manufactured by Robotis, are popular for custom robots ranging from robot arms to humanoids. We use single DXL MX-64AT actuators, shown in Figure 1 (middle), which complies with high torque and load requirements. The MX series actuators are controlled by digital packets via a half duplex asynchronous serial communication protocol, that is, we can read and write to the motor but not simultaneously. The protocol allows a control computer to send position, velocity or current control commands (referred to as torque control in the manual) to the actuator as well as poll sensorimotor information including position, velocity, temperature, current and load. We develop two tasks based on this actuator, which we call DXL-Reacher and DXL-Tracker.Create 2: The Create 2, shown in Figure 1 (right), is a hobbyist version of iRobot's Roomba vacuum robot. The Create 2 has two actuated wheels and many sensors including six front-facing infrared wall sensors, charging sensor, one omni-directional and two directional infrared sensors for docking, two physical bump sensors, and a directed distance sensor for the forward direction. The software interface allows the control computer to access sensory packets in a streaming mode as well as send to the robot target speeds (mm/s) for its two wheels. We develop two tasks with it, called Create-Mover and Create-Docker.Appendix A.1 contains additional details of the hardware setups.

Tasks
In a reinforcement learning (RL) task  #b4 , an agent interacts with its environment at discrete time steps, where at each step t, the environment provides the agent its state information S t ∈ S and a scalar reward signal R t ∈ R. The agent uses a stochastic policy π with a probability distribution π(a|s) def == Pr {A t = a|S t = s} to select an action A t ∈ A. In response, the environment transitions to a new state S t+1 and produces a new reward R t+1 at the next time step t+1 using a transition probability distribution: p(s , r|s, a)def == Pr {S t+1 = s , R t+1 = r|S t = s, A t = a}.The goal of the agent is typically to find a policy that maximizes the expected return defined as the future accumulated rewards G t def == ∞ k=t γ k−t R k+1 , where γ ∈ [0, 1] is a discount factor. In practice, the agent observes the environment's state partially through a real-valued observation vector o t instead of receiving the state information fully.UR-Reacher-2: We use the Reacher task with UR5 developed by Mahmood et al. (2018), which is designed analogously to OpenAI-Gym Reacher (Brockman et al. 2016). We modify the reward function and call the task UR-Reacher-2. In Gym Reacher, the agent's objective is to reach arbitrary target positions by exercising low-level control over a two-joint robotic arm. In UR-Reacher-2, we actuate the second and the third joints from the base by sending angular speeds between [−0.3, +0.3] rad/s. The observation vector consists of joint angles, joint velocities, the previous action, and the vector difference between the target and the fingertip coordinates. The reward function is defined as:R t = −d t + exp(−100d 2 t ),where d t is the Euclidean distance between the target and the fingertip positions. The second term of the reward function, which we call the precision reward, incentivizes the algorithm to learn to get to the target with a high precision. We defined episodes to be 4 seconds long to allow adequate exploration. At each episode, the target position is generated randomly within a 0.7m × 0.5m boundary, while the arm always starts from the middle of the boundary.UR-Reacher-6: The second task with UR5, which we call UR-Reacher-6, is analogous to UR-Reacher-2 with the exceptions that all six joints are actuated and the target is drawn from a 0.7m × 0.5m×0.4m 3D space. The higher dimensionality of the action and observation spaces and physical limitations of reaching locations from various configurations of the arm joints in 3D space result in a much more complex policy space and substantially increase the learning problem difficulty.DXL-Reacher: We design a Reacher task similar to UR-Reacher-2 with current control of the DXL actuator, which we call DXL-Reacher. The action space is one-dimensional current control signals between [−100, 100] mA, making the task simpler than UR-Reacher-2. The reward function is defined as: R t = −d t . The observation vector includes the actuator position (in radians), moving speed, target position, and the previous action. Each episode is 2 seconds long to allow adequate time for reaching distant target positions. At each episode, the target position is chosen randomly within a certain boundary of angular positions, and the actuator starts from the center of it.DXL-Tracker: We develop a second task using the DXL actuator, which we call DXL-Tracker. The objective here is to precisely track a moving target position with current control signals between [−50, 50] mA. The observation vector includes the actuator position (in radians), moving speed, current target position, target position from 50 milliseconds in the past and the previous action. The reward function is same as that of DXL-Reacher. Each episode is 4 seconds long to allow adequate time to catch up with the target and subsequently track it. At each episode, the starting position of the target is chosen uniformly randomly from a certain range, and the actuator starts from the center of that range. In addition, we also randomly choose the direction of the moving target. The speed of the target is set in such a way that the target always arrives at a certain fixed position at the end of the episode. Thus, the speed is different for different episodes.Create-Mover: We develop a task with Create 2 where the agent needs to move the robot forward as fast as possible within an enclosed arena. We call it Create-Mover. A 3f t × 2.5f t arena is built using white shelving boards for the walls and a white hardboard for the floor. The action space is [−150mm/s, 150mm/s] 2 for actuating the two wheels with speed control. The observation vector is composed of 6 wall-sensors values and the previous action. For the wall sensors, we always take the latest values received within the action cycle and use Equation 1 by (Benet et al. 2002) to convert the incoming signals to approximate distances. The reward function is the summation of the directed distance values over 10 most recent sensory packets. An episode is 90 seconds long but ends earlier if the agent triggers one of its bump sensors. When an episode terminates, the position of the robot is reset by moving backward to avoid bumping into the wall immediately. We use two Create 2 robots and two identical arenas for our experiments. Among the two robots, one of them has two faulty wall sensors always receiving value zero, with four other sensors oriented symmetrically. To make comparisons fair, each algorithm was run using both robots the same number of times.Create-Docker: In this task the objective is to dock to a charging station attached to the middle of one of the wider walls of the Create-Mover arena. The reward function is a large positive number for successful docking with penalty for bumping and encouragement for moving forward and facing the charging station perpendicularly. More details of the task is provided in Appendix A.2.All of these tasks are implemented following the computational model for real-time reinforcement learning tasks described by Mahmood et al. (2018). We improve on that model by running environment and agent computations on two different processes, which we found to reduce execution delays compared to the use of Python threads. The action cycle time is 150ms for Create-Mover, 45ms for Create-Docker and 40ms for the rest of the tasks. The robot read-write cycle time is set to 8ms for UR5 tasks, 10ms for DXL tasks and 15ms for Create tasks. The reward is scaled by the action cycle time in all cases. The action space is normalized between -1 and +1 for each dimension. For the Create tasks, all the observations are also normalized between -1 and +1.

Reinforcement learning algorithms
We select four continuous control policy learning algorithms. TRPO, PPO and DDPG are among the most popular of their kind. On the other hand, Soft-Q is a new algorithm with promising results. We use the OpenAI Baselines implementations for TRPO, and PPO, Rllab implementation for DDPG, and the implementation of Soft Q-learning by the original authors.Trust region policy optimization (TRPO): TRPO  #b2 ) is a policy optimization algorithm that constrain the change in the policy at each learning update. The policy is optimized by iteratively solving the following constrained optimization problem:maximize θ E s,a∼π θ old [r θ (a|s)A θold (s, a)] , s.t. E s,a∼π θ old [D KL (π θ (·|s) π θold (·|s))] ≤ δ,where A θold is the advantage function, r θ (a|s) = π θ (a|s) π θ old (a|s) is the ratio of a target policy probability to the policy probability used to generate data, D KL is a Kullback-Leibler divergence, and δ is a "step-size" parameter. TRPO uses the conjugate-gradient algorithm to solve the above problem.Proximal policy optimization (PPO): PPO (Schulman et al. 2016) attempts to control the policy change during learning updates by replacing the KL-divergence constraint of TPRO in the optimization problem with a penalty term realized by a clipping in the objective function:L CLIP θ = E s,a∼π θ old [min(r θ (a|s)A θold (a, s), clip(r θ (a|s), 1 − ε, 1 + ε)A θold (s, a))] ,where A θold is the advantage function, and ε is a parameter, usually on the order of 0.1. The optimization is done by running several epochs of stochastic gradient ascent at each update.Soft Q-learning (Soft-Q): Soft-Q (Haarnoja et al. 2017) defines a policy as an energy based probability distribution: π(a|s) ∝ exp(−E(a, s)), where the energy function E corresponds to a "soft" action-value function, obtained by optimizing the maximum entropy objective. The soft action-value function is represented by deep neural networks, and therefore the policy energy model can represent complex multi-modal behaviors. This model provides natural exploration mechanism without the need to introduce artificial sources of exploration noise, such as additive Gaussian noise. ∇ θ L θ = 1 N i ∇ a Q φ (s i , a)| a=µ θ (si) ∇ θ µ θ (s i )where N is the batch size. In the Rllab implementation, the exploration of DDPG is addressed by augmenting the policy output µ θ (s) with additive noise from an independent noise model.

Experiment Protocol
We run the four learning algorithms on the six robotic tasks to investigate different characteristics such as hyper-parameter and network initialization sensitivities within tasks, hyper-parameter consistency across tasks, and overall learning effectiveness of the algorithms in all tasks.To analyze the hyper-parameter sensitivity within tasks and consistency across tasks, we perform a random search (Bergstra & Bengio 2012) of seven hyper-parameters of each algorithm on UR-Reacher-2 and DXL-Reacher. For each of these hyper-parameters, we predetermine the range of values to search and draw 30 independent hyper-parameter configurations from that range uniformly randomly in the logarithmic scale. The ranges of hyper-parameter values we use in this experiment are given in Appendix A.3. Each of these hyper-parameter configurations is used to run experiments using a different random initialization of neural networks. Each algorithm is run using the same set of hyper-parameter configurations on both tasks.To know the statistical significance of the comparative performance of each hyper-parameter configuration, we need to run each of them with different randomization seeds that will determine network initialization, random target positions, and random action selections. Instead, we run each hyperparameter configuration of the random search with a single randomly drawn network initialization.To determine the effect of the network initialization, we redraw four hyper-parameter configurations uniformly randomly from our original 30 sets. Each of these four sets was rerun with five randomly drawn network initializations. We use the same five network initializations for all four sets of chosen hyper-parameter values on both tasks.To analyze the overall effectiveness of the algorithms across tasks, we choose the best-performing hyper-parameter configurations of each algorithm from UR-Reacher-2 and use them to run experiments on the four held-out tasks: UR-Reacher-6, DXL-Tracker, Create-Mover, and Create-Docker.To understand the qualitative performance of learned policies, we also run some non-learning scripted agents and compute their average returns using the same experimental setup we use for the learning agents. These scripted agents are described in Appendix A.4.Each run is 150,000 steps long or about 3 hours of wall time for UR-Reacher-2, 200,000 steps long or about 4 hours of wall time for UR-Reacher-6, 50,000 steps long or about 45 minutes of wall time for DXL-Reacher, 150,000 steps long or about 2 hours 15 minutes of wall time for DXL-Tracker, 40,000 steps long or about 2 hours of wall time for Create-Mover, and 300,000 steps long for Create-Docker. All wall times include resets. The resets of Create-Docker is dependent on performance.

Experimental results and discussion
First, we demonstrate the reliability of the experiments by repeating some of them multiple times with the same randomization seed using TRPO. Details are given in Appendix A.5. Figure 2 shows the results for all tasks except Create-Docker. The variation in performance between different runs was small and did not diverge over time except on Create-Mover, where the sequences of experience became dissimilar over time across runs. These results are a testament to the tight control over system delays achieved in our tasks by using the computational model of Mahmood et al. (2018).To illustrate the sensitivity of each algorithm's performance to their hyper-parameters, we show in Figure 3 the performance of all algorithms on both UR-Reacher-2 (left) and DXL-Reacher (right) using Tukey's box plots  #b7  Figure 2: Repeatability of learning on five robotic tasks: The plots show the returns over time of multiple learning experiments that would be identical for the same color if they had been run in simulation. The robot hardware introduces some non-determinism, but not enough to significantly impact repeatability in the natural ups and downs of exploration and learning, except in Create-Mover, where the physical location of the robot can diverge over time across the runs. the worst, having the least median performance on both tasks. The rest of the algorithms achieved good performance with many configurations on both tasks. Among them, TRPO's performance was the least sensitive to hyper-parameter variations with the smallest interquartile range on both tasks.Overall, these results show that hyper-parameter choices are important, as they may make a much bigger difference than the choice of the algorithm. Blue crosshairs in Figure 4 show the performance for each of the 30 hyper-parameter configurations of all four algorithms in the descending order, also shown in Appendix A.6 for easier comparison. The values of all configurations, distributions of individual hyper-parameters and their correlations with performance are given in Appendix A.7.The box plots in Figure 4 show the effect of variations in network initialization with four randomly chosen hyper-parameter configurations. Except in one case of DDPG, the interquartile ranges of performance due to variations in network initializations were smaller than those due to variations in hyper-parameter choices shown in Figure 3. Except for TRPO on UR-Reacher-2, DDPG on DXL-Reacher and Soft-Q on both tasks, the medians of performance also retained the relative rank order of the configurations in the original experiment with single network initializations.We show in Figure 5 how each hyper-parameter configuration ranks on both tasks according to average returns. Each plot corresponds to an algorithm and contains two columns of colored dots, each of which represents one of the 30 randomly chosen hyper-parameter configurations. The gray lines connect identical hyper-parameter configurations on both tasks. The correlations of performance between the tasks with corresponding p-values are given above the plots. The correlations were positive for all algorithms, and significantly large for PPO, Soft-Q and DDPG. This result indicates that although hyper-parameter optimization is likely necessary for best performance on a new task, a good configuration based on one task can still provide a good baseline performance for another. learning curves of TRPO on Create-Docker. The average returns and standard errors of scripted agents are also shown for each task. For the mean curves, we used four independent runs of each algorithm on Create-Mover, and five runs on the rest. DDPG performed poorly on all UR5 and DXL tasks. We did not run it on the two Create tasks. The rest of the algorithms showed learning improvements on all tasks they were run. Among these algorithms, the final performance of TRPO was never substantially worse compared to the best in each task. Soft-Q had the fastest learning rate on all UR5 and DXL tasks. On Create-Mover, TRPO, PPO and Soft-Q learned an effective forwardmoving behavior of the robot, which turned as it approached the wall, as shown in the companion video 2 . On Create-Docker, TRPO learned to dock successfully quite often although the movement was not smooth. Overall, RL solutions were outperformed by scripted solutions, by a large margin in some tasks, where such solutions were well established or easy to script. But in Create-Docker where a scripted solution is not obvious or easy, RL solutions appeared more competitive.Working with real-world systems did create some challenges. Soft-Q on DXL tasks for many of its hyper-parameter configurations resulted in frequent overheating and failed during overnight experiments due to more-aggressive exploration. We could not run un-attended experiments with Create 2 when the robots were tethered to stationary external computers as their cables needed periodic untangling. We were able to overcome this problem by using an on-board external computer, which we used for one of the two Create-Docker runs. Two wall sensors of one of the Create 2s were faulty; surprisingly, learning performance did not appear to be affected, possibly due to four other symmetrically oriented wall sensors being available. Of all three robots, we were most pleased with our experience with UR5, which we were able to use constantly for days without interventions.    Our experiments also revealed some limitations of the learning algorithms and their implementations, such as the sequential computations of the agent's learning updates and forward policy passes. Learning updates of these algorithms are expensive, and our choice of moderately large action cycle times minimized the number of samples affected by these sequential learning updates. However, to learn finer policies with faster action cycle times, moving toward efficient ordering of computations  #b6 , Mahmood et al. 2018) together with inexpensive incremental updates (Mahmood 2017) or asynchronous off-policy updates (Gu et al. 2017) would be essential.

Conclusions
In this work, we provided the first extensive experimental study of multiple policy learning algorithms, namely TRPO, PPO, DDPG, and Soft-Q on multiple commercially-available physical robots. We found that the performance of all algorithms was highly sensitive to their hyper-parameter values, requiring retuning on new tasks for the best performance. Nevertheless, some algorithms achieved effective learning performance across tasks for a wide range of hyper-parameter values. This effectiveness indicates the reliability of our task setups as well as the applicability of these algorithms and implementations in diverse physical environments. Benchmarking more learning algorithms on these tasks as well as upgrading the existing algorithms to allow higher sample efficiency and faster action cycle times are promising directions for future work.We ran more than 450 independent experiments which took over 950 hours of robot usage in total. Most of the experiments were highly repeatable, and many of them resulted in effective learning performance. This study strongly indicates the viability of reinforcement learning research extensively based on real-world experiments, which is essential to understand the difficulties of learning with physical robots and mitigate them to achieve fast and reliable learning performance in dynamic environments. The benchmark tasks and the supporting source code enable the necessary steps for such understanding and easy adoption of physical robots in reinforcement learning research.

Appendix


A.1 Additional details of the robots
Here, we provide additional details on the hardware setup between the robots and the control computers. All of our setups use wired connections. The UR5 arm controller is communicated with the control computer over a TCP/IP connection. We use an Xevelabs USB2AX controller to interface between the MX-64AT actuators and a control computer via USB. A 12V, 5A DC power adapter is used to power the actuators. The Create 2 robot is interfaced with a control computer via serial port using iRobot's specified Open Interface. The robot is communicated in the streaming mode where the internal controller streams a data packet every 15ms, which is the rate the internal controller uses to update data.

A.2 Additional details of Create-Docker
In Create-Docker, the objective is to dock to a charging station attached to the middle of one of the wider walls of the Create-Mover arena. When the robot is at the charging station in such a way that the binary charging signal is active, the internal robot controller switches the operating mode to Passive, in which the actuation commands for the wheels from the external computer are ignored. Being in this state with an active charging signal is considered a successful docking. The internal controller does not switch to the Passive mode right away after an active charging signal. Therefore, it is possible to have an active charging signal momentarily but still not dock successfully due to a high speed in the backward direction or bouncing back from the station. Moreover, it is extremely difficult to activate the charging signal properly if the robot approaches the charging station at an angle. Therefore, to learn how to dock, it is important to approach the charging station perpendicularly and learn to slow down or stop when the charging signal is active.The action space for Create-Docker is the same as for Create-Mover, that is, [−150mm/s, 150mm/s] 2 for actuating the two wheels with speed control. The observation vector is 20-dimensional, consisting of a single binary charging signal, six infrared wall signals, two bump signals, two action components from the previous time step, and nine additional components processed based on the three infrared bytes from the charging station. Each infrared byte informs whether the left buoy, the right buoy and the force field of the dock beam configuration can be seen, from which we obtain nine binary values. Each of these binary values are then averaged over last 20 packets streamed from the robot's internal controller every 15ms.The reward function is a large positive number for successful docking with penalty for bumping and encouragement for moving forward and facing the charging station perpendicularly. The reward function for every time-step is defined as follows:R t = τ (aX t + bY t + cZ t + dV t ) ,(1)where τ is the action cycle time, X t is the docking reward, Y t is bumping penalty, Z t is the moving bonus and V t is the bonus for aligning to the charging station. These components are defined as follows:X t = 2 n(n + 1) n i=1 (n − i + 1)p t (charging, i) (2) Y t = − 2 k=1 n i=1 p t (bump k , i) (3) Z t = 1 n n i=1 p t (distance, i)(4)V t = 1 20 20 i=1 9 k=1 w k p t [ir dock k , i].(5)Here, p t [x, i] stands for the ith most-recent data packet available at time step t for sensor x, where charging stands for charging sensor, bump k stands for kth bump sensor, distance stands for the distance sensor and ir dock k stands for the kth sensor value for the infrared receiver for docking. The weights used for ir dock are w = [1.0, 0.5, 0.05, 0.65, 0.15, 0.65, 0.05, 0.5, 1.0], which are chosen in such a way that the left receiver (first three values) focuses on the left buoy, the right receiver (last three values) focuses on the right buoy and the omni-directional receiver (middle three values) focuses on both buoy equally. The value of n is the ratio between action cycle time and the read-write cycle time, that is, n = τ 0.015 . The weights of the different reward components are a = 150, b = 10, c = 5, and d = 4. They are chosen in such a way that the maximums of the penalty and the two bonuses are of the same magnitude scale and the docking reward is much larger than the auxiliary reward components. If docking is successful, the robot stays at the charging station for the rest of the episode and continues to receive the docking reward. This encourages docking as soon as possible.An episode is always 30 seconds long. We designed the reset between episodes in such a way that docking is relatively easier for the initial policy if the previous episode is unsuccessful and it is relatively more difficult if the previous episode is successful, that is, the episode terminated while the robot is docked. To achieve this, the reset procedure first invokes the internal seek-dock routine of Open Interface to dock to the station if the previous episode is unsuccessful. After the robot docks using seek-dock or a time-out of 20 seconds, the robot moves backward for 3.25 seconds. If the seek-dock succeeds, the robot is always facing the charging station after reset and can dock successfully by learning to move straight in the forward direction. However, if the seek-dock routine does not succeed, then the robot may start from a difficult starting position, for example, at one of the corners of the arena facing the wall. If the previous episode is successful, then the reset procedure makes the robot move backward for 0.75 seconds and then sends uniform random speeds for 2.5 seconds to the two wheels independently between [−250, −50] to move backward further rotationally. This last phase ensures that the robot is likely not facing the charging station perpendicularly and displacement is required to achieve alignment. Here, N = T /τ , where T is the total length of an episode in time and τ is the action cycle time. We restricted total number of weights in the network to be no larger than 100,000, and the upper limit of a hidden size X was determined based on sampled number of hidden layers to respect this limit. 

A.4 Details of scripted agents
For UR5 tasks, we use the movej command of URScript, where we specify the joint angles for the target positions and set the time to reach to 2 seconds. For DXL tasks, we implement a PID controller. We do not constrain the current control values as we do for the learning agent, and we chose the optimal PID gain value for each task separately. For Create-Mover, we use a simple script that applies action [−150mm/s, +150mm/s] whenever the normalized signal value of either of the two front wall sensors has its value above 0.55 and otherwise, moves straight forward with action [+150mm/s, +150mm/s]. For Create-Docker, we use the seek-dock routine of Open Interface. During this routine, the robot moves back and forth to perceive the vertical plane perpendicular to the wall at the charging station using the infrared bytes from the charging station, adjusts its position to align its principle axis with the perpendicular plane, and moves slowly toward the charging station.

A.5 Details of repeatability experiments with TRPO
To generate the plots in Figure 2, we run the same experiment with the same seed four times on five different tasks using TRPO. There are different kinds of randomization in each task. For the agent, randomization is used to initialize the network and sample actions. For the environment, randomization is used to generate targets and resets. By using the same randomization seed across multiple experiments in this set of experiments, we ensure that the environment generates the same sequence of targets and resets, the agent is initialized with the same network, and it generates the same or similar sequence of actions for a particular task. We use the same hyper-parameter values of TRPO used in the experiments by Henderson et al. (2017).

A.6 Relative performance of different algorithms in random search
In the figure below, we show the relative performance of four algorithms across 30 random hyperparameters configurations ordered by performance. Note that for different algorithms, parameter configurations with the same index generally correspond to different parameter values.  In the figure below, we show the best 5 (in red) and the worst 5 (in blue) hyper-parameter values out of 30 random configurations of TRPO on UR-Reacher-2. On each plot the x axis represents parameter values and the y axis represents average returns obtained during the corresponding run. For each hyper-parameter, we also show correlations between log-parameter values and corresponding returns.          

Average Returns


UR-Reacher


Footnote
1 : Source code for all tasks available at https://github.com/kindredresearch/SenseAct 2nd Conference on Robot Learning (CoRL 2018), Zrich, Switzerland.