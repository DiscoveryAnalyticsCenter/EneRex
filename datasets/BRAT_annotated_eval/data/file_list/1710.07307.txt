Interpretable Transformations with Encoder-Decoder Networks

Abstract
Deep feature spaces have the capacity to encode complex transformations of their input data. However, understanding the relative feature-space relationship between two transformed encoded images is difficult. For instance, what is the relative feature space relationship between two rotated images? What is decoded when we interpolate in feature space? Ideally, we want to disentangle confounding factors, such as pose, appearance, and illumination, from object identity. Disentangling these is difficult because they interact in very nonlinear ways. We propose a simple method to construct a deep feature space, with explicitly disentangled representations of several known transformations. A person or algorithm can then manipulate the disentangled representation, for example, to re-render an image with explicit control over parameterized degrees of freedom. The feature space is constructed using a transforming encoder-decoder network with a custom feature transform layer, acting on the hidden representations. We demonstrate the advantages of explicit disentangling on a variety of datasets and transformations, and as an aid for traditional tasks, such as classification. *

Introduction
We seek to understand and exploit the deep feature-space relationship between images and their transformed versions. Different feature spaces are illustrated in Figure 1, and support different use-cases: separability helps discriminate between categories such as identity, while invariance improves robustness to nuisance variables during data capture. Taking head pose as an example, what is a nuisance for one task could be the focus of another. Therefore, we propose deep features with transformation-specific interpretability, which combine both (1) discriminative and (2) robustness properties, with the further benefits of (3) a user-guided parameterized space for controlling image synthesis through interpolation.Learning such a feature space is difficult. In image data, transformations of objects usually couple in complex nonlinear ways, leading to an entangling of transformations. The reverse process of disentangling is then especially hard. An obvious post hoc Figure 1. Three alternative feature spaces and how each encodes images of the same person. (Left) A feature space that is hard to interpret, similar to one learned by a typical CNN. While transformation information is present, it is not obvious how to extract that directly from the feature space. (Middle) A transformation-invariant feature space. (Right) An interpretable feature-space, where ordered transformations of the input subject relate to ordered, structured features. This is like a learned metric space, but also allows for image synthesis. Images of another person are not shown, but would ideally project similarly, albeit elsewhere in each feature space. solution is to learn disentangling transformations using a regressor  #b34 , but this is a time-consuming and inexact process. We cannot assume that the change in representation of a chair and its rotated twin is necessarily the same as the change in representation between a banana and its equally rotated twin. We propose disentangling as an end-to-end supervised learning problem. Some image variations are hard to quantify or explain. But others, for instance 2D and 3D warps or color appearance changes, allow ready access to pre-and post-warp image pairs, along with their ground-truth transformation parameters. These easier transformations, we find, lend themselves to smooth parameterization in feature space, and therefore interpretability. One could argue that it is nicer to learn everything only from raw data, but the transformation parameter labels considered here are obtained with little or no human effort. We therefore pre-define the feature-space structures that encode basic transformations, and train neural networks that map into and out of this feature-space.We take our motivation from considering the feature space structure, introduced by convolutional neural networks  #b33  (CNNs). CNNs owe their success to two differences from the older and more general multilayer perceptrons  #b39 : 1) the receptive field of deep neurons is localized to a small neighborhood, typically not greater than 7×7 pixels from the layer below, and 2) incoming weights are tied between all translated neurons. The motivation behind translational weight-tying is that correlations in the activations are invariant under translation. The side-effect of enforcing such a structure on the weights of a neural network is that integer pixel translations of the image input induce proportional integer pixel translations of the deep feature maps. This phenomenon is called equivariance, meaning the feature-representation of a shifted input is the same, save for its location. We explore continuous transformation equivariance for CNNs, and for the first time, for fully connected models.In this paper, we consider rotations in 2D and 3D, out-ofplane rotations, small translations, stretchings, uniform scalings and changes in lighting direction. For these transformations CNNs do not generally display the equivariance property; although, there are a number of works, which do tackle the problem of rotation  #b7  #b11  #b44  #b13  #b17  #b31  #b54  #b16  #b61 . The main problem with all these approaches (which we detail in the next section) is that the equivariance properties are handcrafted, and suffer from unmodeled oversights in the design process. For instance, all but  #b54  consider equivariance to discretely sampled rotations, when real world rotations are in fact continuous. Given that we can simulate many image-space transformations, it seems only natural to simply acquire equivariance through learning.We now cover related work and theory, followed by Section 3 where we introduce our method and the new feature transform layer, and Section 4 where we test our framework on de-renderre-render problems and for view independent features.

Related Work and Theory
Here we outline basic concepts for us to formalize the task of encoding interpretable transformations, and break down a list of related works into categories of handcrafted or learned equivariance in traditional vision and deep learning.Definition 1 A function f : X → Y is equivariant [54]under a set of transformations Θ if for any transformation T :Θ×X →X of the input, we can associate a transformation F :Θ×Y →Y of the output such thatF θ [f(x)]=f(T θ [x]),(1)for all θ ∈ Θ. Transformations T θ and F θ represent the same underlying transformation but in different spaces, denoted θ. Equivariance is desirable, because it reveals to us a direct relationship between image-space and feature-space transformations, which for deep neural networks are usually elusive  #b34 . Note that invariance is a special case of equivariance, where F θ =I is the identity for all input transformations.

Definition 2
We define an interpretably equivariant featurespace to be an equivariant feature-space as in Equation 1, where the transformation functions F θ and T θ are quantitatively known and can be implemented for all θ, x and f.At an abstract level, an equivariant function is one where some level of structure is preserved between the input and output. Interpretability is the added requirement that for a given θ we know how to apply F θ and T θ . It may be the case that one of these transformations is complicated and cannot be written down as a mathematical expression in closed form (e.g., the rendering equation), but as long we are able to simulate it that is enough. As we show in Section 3.2, one way of preserving the structure of transformations across a feature mapping is via a condition called the homomorphism property. In all of the subsequent related works, equivariance to transformations is the central theme.Handcrafted methods In the 1980s, Crowley and Parker  #b10  studied scale-space representations. These are formed by convolving images with scaled versions of a filter. Scale-space methods exhibit interpretable equivariance. They can be extended to invertible transformations by transforming the filters  #b38  #b1  but has computational complexity exponential in the number of degrees of freedom (DOF) of the transformation. Furthermore, we can only convolve with a finite number of filters, when in reality many transformations are continuous. Freeman and Adelson  #b14  and Lenz  #b36  simultaneously solved the continuity problem, through orientation steerable filters w θ . These can be synthesized at any continuous orientation θ. These are formed as a linear combination of fixed basis filters φ n :w θ (x)= N n=1 α n (θ)φ(x).(2) α n (θ) are known as the interpolation functions. These are still band-limited but unlike scale-space the frequency characteristics are easier to design. Steerable filters were extended to most transformations with one DOF (one-parameter subgroups)  #b51  #b49 , for instance, 1D translations, 2D rotations, scalings, shears, and stretches. For these transformations, there is a function ρ, under which transformation θ becomes a shift, so I(x)T θ → I(ρ −1 (ρ(x) − t θ )),where t θ is the shift. Meanwhile, Perona  #b45  showed that in practical situations some transformations cannot be enacted exactly using steerable functions, for instance scale and affine transformations (specifically those which do not have compact group structure). He showed these can be approximated well with very few basis functions, computed from the singular value decomposition of a matrix of transformed versions of a template patch. This is limited by template choice, SVD efficiency, and figuring out the interpolation functions for steering. More recently Hasegawa  #b18  and Koutaki  #b29  used a variant of this method to learn an affine-equivariant feature detector.Invariance to 1 DOF transformations can be gained via the Fourier Transform (FT) Modulus method  #b28 . This uses the time-shifting property of the FT w(x−t)F T ⇐⇒ e iωt W(ω), where W(ω) is the FT of w(x).The FT modulus |e iωt W(ω)| = |W(ω)| is independent of the shift t. As noted in Scattering Networks  #b5 , this operation removes excessive localization information and is unstable to high-frequency deformations noise. They instead take the modulus of the response to a bank of discretely rotated and scaled wavelets, repeatedly in a deep fashion. This is perhaps the most successful version of a handcrafted deep equivariant feature map.Neural Networks Equivariance in deep learning has very deep roots as far back as the early 1990s. Barnard and Casasent [4] split the main approaches to transformation invariance into three categories: 1) Data augmentation: This is effective and simple to implement, but lacks interpretability. 2) Preprocessing: This is effective, but cannot be applied to geometric transformations. 3) Structured weight networks: These are numerous in the literature. CNNs  #b33  are the most famous example. Pixel-wise integer shifts of an input image will induce proportional pixel-wise shifts in the deep feature space. For partial translation invariance, there is the Global Average Pooling layer  #b37 . For rotations there are two major approaches for discrete rotations: rotate the filters  #b7  #b9  #b17  #b44  #b16  #b61  and rotate the input/feature maps  #b11  #b13  #b31 . Continuous rotations were recently proposed by  #b54 . They restrict their filters and architectures so that the convolutional response is equivariant to continuously rotated inputs. Beyond rotation,  #b20  warp the input, so that general transformations are globally linearized, facilitating the application of CNNs. This requires prior knowledge of the type of transformation and where it is applied in the image.  #b9  can deal with multiple transformations, but these are restricted to group-theoretic structures.  #b24  are able to explicitly transform feature maps with the spatial transformer layer, but do not transform features in the channel dimension. In contrast to the above methods, our method is general and does not require extensive architectural engineering. We can also disentangle confounding factors such as out-of-plane rotation and lighting direction.Deeply Learned Equivariance Some have sought to learn equivariance directly from data. These broadly split into purely generative, purely discriminative and auto-encoded methods. Discriminative:  #b35  regress affine equivariant feature-descriptors directly using supervised data. Their framework is easy to implement, but restricted to group-theoretic transformations. Generative:  #b12  generate views of 3D chairs by regressing appearance with a CNN from an embedding space. In InfoGAN,  #b6  instead used a mutual information maximizing criterion for unsupervised learning of the 'natural' transformations in a training set. This mostly manages to disentangle transformation, but unlike  #b12  is non-interpretable. Auto-encoded:  #b30  presented the deep convolutional inverse graphics network (DC-IGN), a partially supervised variational auto-encoder  #b27 , equivariant to out-of-plane rotation and relighting. Their model is impressive but requires a complicated training procedure, is partially interpretable, and unlike us does not fully exploit known supervised information about transformations.  #b42  #b21  #b60  instead reconstruct transformed versions of an image, given the image and transformation parameters as input. These are similar to our method, but cannot be used to extract interpretable transformation equivariants, which we can do.  #b8  does learn interpretable equivariance to manipulate images of 3D objects from 2D images, but this is only demonstrated on 3D rotations.  #b46  also does learn interpretable equivariance for 3D volumes from 2D images, but their representation space is entire 3D volumes. This is impressive, but it is computationally expensive to represent entire volumes in memory, when sometimes it may not be necessary.

Method
CNNs are interpretably equivariant to pixel-wise translations of their input up to boundary effects, but not to transformations such as 2D and out-of-plane rotations, uniform scalings, stretches, relighting, flips, etc. In this section we design a neural network to learn an interpretable transformation equivariant feature-space. Our method can cope with continuous transformations on intervals, for example, uniform scalings and stretches, and continuous transformations on circles, such as, geometric rotation and relighting, but not discrete transformations, like vertical flips. In Section 3.1 we outline our general framework and in Section 3.2 we introduce the feature transform layer, a channel-wise analogue of the spatial transformer, which can also be applied to fully-connected layers.

Problem Setup
We assume that we are given a training setD = {(x 1 ,x 1 θ i , θ 1 ), ..., (x N ,x N θ i , θ N )} containing pairs of views of transformed examples (x i ,x i θ i )and relative transformation vectors θ i . The relative transformations may be the result of a sensor measurement, or they may be the result of artificial data augmentation, in which case the training set is potentially infinite. The task is to predictx i θ i given x i and θ i (from now on we just write θ for short). We use relative transformation information instead of absolute transformations, because there is no canonical pose, which generalizes across object classes, where alignment between, say, a banana and an airplane does not make sense.Many images x ∈ X are formed from capturing an object o∈O in the 3D world projected via a function Π:O →X onto a 2D canvas. To transform image x intox θ we have to invert Π to find o, perform the world-space transformation and re-project back into image space, sõx θ =Π[T θ [o]]=Π T θ Π −1 [x] .(3)The problem with this approach is that Π is in usually non-invertible. Our solution is to infer the 3D object o given x via statistical methods. CNNs are good at this kind of task (e.g.,  #b30 ), so we opt to use a CNN. Now storing a full volumetric representation like in  #b46  is costly, so we instead opt to use a compressed feature encoding e(x) to approximately represent o, this requires we also have a feature-space representation of the transformation, F θ -see Section 3.2 for details. In our case the feature space is partially learnable, with pre-defined structure imposed by F θ . Our basic model is shown in Figure 2, it is an encoder-decoder network. Loosely speaking Figure 2. We enforce equivariance by minimizing the loss between reconstruction of transformed features d θ and a transformed target x θ . Given just x, the encoder-decoder network does not have enough information to produce a transformed output, thus supplying the missing information θ via the feature transform layer (FTL) forces the network to learn a mapping in and out of the FTL. Critically, whereas other approaches, such as transforming auto-encoders  #b21  and InfoGAN  #b6 , learn the reconstruction to be sensitive to feature transformation information, we can simultaneously learn to map from images to transformation equivariant features.e(•) approximates Π −1 [•], F θ is the feature space equivalent to T θ , d(•) approximates Π[•],

Decoder FTL Encoder
where we have written Π −1 [•] to mean inversion of the projection if possible, or approximation of it. We train the weights of the encoder and decoder by minimizing a summed reconstruction loss , whereL(D)= i d F θ i e(x i ) ,x i θ .(4)In our experiments we use a diverse set of losses, namely, L1 loss, SSIM, and balanced cross-entropy. Note that since we define F θ the feature space of encodings e(x) is interpretable by Definition 2. In Section 3.2, we demonstrate an encoding, which enforces explicit disentangling and from which we can gain approximate transformation invariance 'for free'.

The Feature Transform Layer
The feature-space equivalent of the image-space transform T θ is the feature transform layer F θ . It is an analogue of the spatial transformer  #b24 , but applied to general feature-spaces, not necessarily with spatial dimensions. This means that we can apply it to fully connected layers as well as convolutional layers. It is easiest to describe the feature transform layer via its implementation.Consider a feature vector e, which may be a column of CNN feature channels above a pixel location in an image, or the output of a fully-connected layer. The feature transform layer performs a linear transformation of e via matrix F θ , such that the output y of the layer isy=F θ [e]=F θ e.(5)We only consider linear transformations, whereF θ2θ1 =F θ2 F θ1 .(6)This condition says that if we apply transformation θ 1 to an image, followed by transformation θ 2 , which we have written as θ 2 θ 1 , then in feature space this should be equivalent to applying F θ1 followed by F θ2 . We refer to Equation 6 as the homomorphism property. Abstractly, we can think about it as forcing the neural network to learn a mapping from image-space to feature-space, which preserves the intrinsic structure of the transformations. The homomorphism property implies that (see Supplementary Material)F θ −1 1 =F −1 θ1 .(7)This means that invertible transformations of the input are invertible in feature-space. The homomorphism property is key to ensuring that transformation information is not lost when mapping into feature-space. Examples of F θ are N-dimensional rotation matrices, also known as SO(N), full-rank diagonal matrices, or most generally the group of invertible N × N matrices, known as GL(N). We use rotation matrices, R θ , which have the additional property of being orthogonal or norm-preserving. This means that we can use the feature vector lengths as transformation invariants becauseR θ e 2 2 =e R θ R θ e=e e= e 2 2 ,(8)which shows that R θ e 2 2 is in fact independent of θ. Feature vectors are usually high-dimensional consisting of many channels. We thereform implement the feature transform layer by applying the same rotation matrix on multiple groupings of channels, which we call subvectors of e. We can then define a larger set of invariants, by measuring the relative phase between different subvectors. These are invariant to θ, because if e 1 and e 2 are two subvectors of e, then (R θ e 2 ) R θ e 1 =e 2 R θ R θ e 1 =e 2 e 1 ,which is independent of θ. If e 1 = e 2 , this reduces down to the feature vector length. We denote the concatenation of all subvector dot products as e F . While at first not obvious, we can encode many transformations using rotation matrices, even ones which do not have periodic structure. The trick is to map the domain of the transformation onto the half-circle/sphere, see Figure 3. We prefer to do this rather than using another, perhaps more natural, representation because of the convenience of taking L2-norms and inner products to form invariants.Disentangling We now consider how to disentangle transformations. Since we can model transformations, whose Methodx θ |θx θ |θ,x θ|x CNN MLP Interpretable Supervised Image size DC-IGN  #b30  * † ‡ 150x150 InfoGAN  #b6  64x64 Generating Chairs  #b12  128x128 Transforming AEs  #b21  96x96 Learned Visual Reps.  #b8  96x96 Unsup. 3D from images  #b46  30x30x30 Covariant features  #b35  57x57 Spatial Transformer  #b24  -Any Ours 150x150 Table 1. Comparison of method scopes. In the first 3 columns we display whether a method can generate an imagex θ given just parameters θ,x θ |θ; conditioned on an original imagex θ |θ,x; or infers transformation parameters given an image θ|x. * Qualitative relationship only. †Correspondence between feature dimensions and transformations known, qualitative relationship only. ‡Partial supervision: minibatches grouped into variation of single parameter, but values not given.parameters exist on a circle or interval, we can model each independent transformation DOF by mapping it to a different circle or half-circle. Some transformations, like lighting direction, are more conveniently mapped to the surface of a 3D-sphere. Thus the feature transform layer isF θ e=    R θ1 . . . R θ N   e,(10)with possible tied θ i when we apply a transformation to multiple subvectors. The feature transform layer is simple to implementit is just a matrix multiplication and the block diagonal structure allows efficiency saving via reshapes. In our experiments we found a slow down of just 2%. Furthermore, it can be applied to convolutional features in synchrony with a spatial transformer  #b24  for complete control of both spatial and feature properties.

Experiments, Results, and Discussion
d Below we demonstrate the ability of our system to learn meaningful features on MNIST  #b59 , MNIST-rot  #b32 , the Basel Face Dataset  #b22 , and ModelNet10  #b56 . We choose these datasets because they demonstrate our system's general-purpose usage and performance on 2D and 3D images, for transformations with complex entanglement, and with and without information loss. Our encoder-decoder structure is shown in Figure 5. They are all implemented in TensorFlow.

MNIST: 2D images-2D transformations
This experiment demonstrates our system's ability to disentangle confounding transformations and how it reconstructs an input, after manipulation of the features. The MNIST dataset  #b59  contains 50k training, 10k validation, and 10k grayscale test images of handwritten digits, size 28 × 28. The images are very simple, usually just a pen-stroke. We apply random scalings in the x-and y-directions followed by a random 2D rotation. Due to the simplicity of the images, we use an MLP for both encoder and decoder. Both encoder and decoder have 3 layers, separated by batch normalization  #b23  and leaky ReLU nonlinearities  #b40  apart from the input and output of the feature transform layer, which are linear. All layers except the input and output are 510 neurons wide 1 . The feature transform matrices are a block diagonal composition of three 2D rotation matrices repeated 85 times: rotation R rot , x-scaling R scale-x , and y-scaling R scale-y . We train with the Adam optimizer  #b26  for 200 epochs, with minibatch size 128 and initial learning rate 10 −3 . After training we pass a random digit from the test set through the encoder and transform the code by multiplying by feature transform matrix F θ . In Figure 4 we show random digits from the test set, slowly varying the transformation vectors on an interval. Each row shows a random digit under a combination of rotation, x-, and y-scaling. Notice how the encoder-decoder successfully learns to rotate digits, solely from the feature transformation. Notice also that the scalings are applied in the x-and y-directions of a coordinate system aligned to the canonical pose  $b1  We use this non-standard width because we model three transformations, with each transformation modeled on a separate circle. So feature-space dimensionality must be a multiple of 3×2=6. Furthermore, the value of 510 is close to 512, a common feature-space dimensionality.  of the digit. The system struggles when the images are magnified, nonetheless these results demonstrate clearly that we can learn a feature-space, where we have control over reconstruction transformations. MNIST-rot Next we explore if we could improve classification on the MNIST-rot dataset, with an explicitly rotationally equivariant feature space. We feed the learned transformation invariant subvector relative phases e(x) F into a classifier f ( Figure 5) and use the output f( e(x) F ) for classification. MNIST-rot  #b32  is a specific subset of MNIST split into 10k train, 2k validation, and 55k test images, rotated randomly on the circle. Our results are in Table 2. While we do not achieve state-of-the-art on this benchmark, we do beat standard CNNs trained with data augmentation. All models better than us are designed specifically for rotation. This indicates that in low data scenarios, it pays to exploit our prior knowledge of how transformations affect data. We can use this knowledge to construct meaningful feature spaces, where equivariance and invariance can be utilized. We found that it helps to add a regularization term e(x) F − e(x θ ) F 2 2 to the loss function encouraging transformed encodings of the input to be equal in length

Basel Faces: 2D images-3D transformations
In this experiment, we return to disentangling transformations for superior control in reconstruction. The Basel Face dataset  #b22  contains synthetic face renderings encoded using a PCA model. We can randomly draw faces with vertex positions s and vertex colors t from the model by sampling two 199-dimensional vectors α and β from a unit Gaussian and retrieving the face by  The first set of numbers indicate input tensor shape, the second set of numbers indicate operation (conv: convolution, dcnv: deconvolution, FTL: feature transformer layer, fc: fully-connected). The number trailing the / is the stride. For deconvolution, we first nearest-neighbor upsample, followed by convolution.s(α)=µ s +U s diag(σ s )α, (11) t(β)=µ t +U t diag(σ t )β.(12framework to reorient out-of-plane rotations and relight faces. This is a difficult task, because the encoder-decoder only sees 2D views of a self-occluding 3D scene. The encoder has to learn to decouple the complex interaction between light and 3D surfaces, while inferring missing information, then the decoder has to convert this representation into a faithful 'rendering' of the transformed scene. A key difficulty is to infer occluded surfaces, which may be disoccluded upon out-of-plane rotation of the face. We generate 1000 random RGB faces of size 150 × 150. Both the rotations of the faces and lighting positions can be efficiently encoded using 3D rotation matrices R rot and R light , each with 2 degrees of freedom-azimuth ψ and elevation θ, but no roll. Thus a natural form for the feature transform matrices, which we use is Outside the large green box the encoder-decoder has never seen those transformation parameters. We note the impressive ability of the model to rotate out-of-plane and to relight a 3D surface, when only given a 2D input and a pair of 3D rotation matrices. For unseen transformation parameters, notice that the relighting is of perceptually decent quality, but that the geometric rotations degenerate in quality around the boundaries, such as the nose and chin.F θ = R rot R light(13)R • =   cosθ • sinθ • 1 −sinθ • cosθ •     cosψ • −sinψ • −sinψ • cosψ • 1  (14)We dub it the facial transformer. As basic design principles, we avoid max-pooling, favoring strides, and use batch normalization and leaky ReLUs after all layers, apart from before and after the feature transform layer. For deconvolution we upsample with nearest-neighbor interpolation followed by regular convolution  #b43  #b30 . Inspired by  #b58  #b15  our reconstruction loss is a convex combination of the structural similarity index (SSIM)  #b52  and L1 loss. The L1 loss encourages low-frequency shape information and accurate color matching, and the SSIM encourages high-frequency details, for instance, the shading of the ears. The loss isL face = α N j∈pixels 1−SSIM(x j ,x j ) 2 +(1−α)|x j −x j | (15)where N is number of pixels times 3 color channels. Similarly to  #b58  #b15 , we use the blending coefficient of α = 0.85. We optimize the loss using Adam  #b26 , minibatch size 32, and initial learning rate 10 −4 , dividing by 10 at iteration 30000 and 50000, for a total of 60000 iterations. We train on a single TITAN X Pascal GPU. 1 /4-2 hours is sufficient for good results. Figure  6 shows the results of reoriented and relit faces from a held-out validation set. The input is on the left and the transformed outputs on the right. Top to bottom each row shows a different We emphasize here that the goal of DC-IGN is different to ours, since they learn unsupervised disentangling. We argue to use supervision when the information is accessible. Our use of supervision is evident in that we can quantitatively rotate our faces; whereas, DC-IGN cannot.transformation, namely, lighting azimuth, lighting elevation, rotation azimuth, and rotation elevation. Faces inside the large green box span the transformation parameters seen at training time, those outside were not seen. We note the reconstruction fidelity and impressive ability to reorient out-of-plane rotations, but zooming in shows that the reconstructions lack highfrequency detail to be foolproof replicas of the input and the overall face shape changes slightly. For unseen transformation parameters, notice how faces just outside the green box are of similar quality to inside, but large deviations from the training set degrade. This is especially so for the geometric rotations, where the boundary surfaces (nose and chin in particular) begin to protrude from the face. Surprisingly, the shading of the faces is realistic outside of the box. We also compare against DC-IGN  #b30  in Figure 7. Our superior quality is partially down to better training, but also to improved alignment in feature-space, from supervised transformation information. Interpretability of our features allows for more accurate control over the azimuthal rotation. Feature stability In Figure 8 we test the feature stability under transformations of the input. We take an invariant representation of the data using L2-norms and relative phases, then measure the cosine similarity (top) and L2-distance (bottom) between a face and transformed versions of itself (blue), and we also compute these metrics between transformed versions of a face and a randomly selected face of another identity. There is a clear separation between faces of different identities for medium sized transformations, but this breaks down for large values of the parameters for geometric rotations. This is especially so, when the parameter values are close to the limit of the training data, as would be expected.Real faces For fun, we feed images of real faces into our system, to recognize basic pose, shape, appearance, and lighting. We take internet images, cropping out background and hair. The system makes crude, but convincing enough matches to pose, skin tone, and lighting. The bottom image is particularly hard due to the side pose and lighting. This shows our system has learned a generalizable representation of faces, despite training on artificial data.

Voxelized ShapeNets: 3D Transformations
For this experiment, we use the ModelNet10 subset of the ShapeNet dataset  #b57 . This consists of 3991 CAD models from 10 object categories. Specifically, we use the voxelized ModelNet10 provided by Maturana et al.  #b41 , which is a volumetric binary occupancy grid of size 32x32x32.The encoder-decoder architecture is similar to the variational auto-encoder architecture by Brock et al.  #b4 , with the bottleneck of 200 units with equivariance to rotations about the y-axis. We also employ their variant of the binary cross entropy loss for training:  #b15  where t i are the target values rescaled to [−1, 2], o i is the output of the auto-encoder rescaled to [0.1,0.9999] and γ is set to 0.98 to compensate for the sparseness of volumetric data. We optimize the loss using Adam, minibatch size 16, and learning rate of 10 −4 . See supplementary materials for details on classification.L bce = i∈voxels −γt i log(o i )−(1−γ)(1−t i )log(1−o i ),

Conclusion
We have presented a simple framework to learn deep featurespaces, which disentangle both in-plane and out-of-plane transformations into an interpretable feature space, that also allows smooth interpolation. Our key innovation is the feature transform layer, which can be applied to both convolutional and fully-connected layers. The properties of the feature transform layer give our networks equivariance properties, that can help with generative and discriminative applications.   Figure 8. Pairs of images are compared to each other in feature space and similarity is measured using cosine similarity TOP and L2 distance BOTTOM. Pairs of images with same identity shown in blue, and pairs with different identities shown in red (10 each). Columns show left to right: sweeping of azimuth, elevation, lighting azimuth, and lighting elevation with all other parameters held. Dashed vertical lines show range of transformation values seen at training time. Ideally cosine similarity would be 1 everywhere for the blue lines, indicating perfect transformation invariance. For dissimilar faces, the red curves would be less than 1. We see that invariance to lighting is easy, even beyond the range of training examples (see central box in Figure 6). Elevation is particularly hard, so two features of the same person begin to differ at large elevations. Figure 9. We pass images of real faces through our system re-orienting 50 • from the initial pose, while fixing all other transformation parameters. Despite being trained on artificial data, the system is able to extract basic pose, shape, appearance and illumination. The system struggles to match shape properly, since these are far from the training set.Limitations Our approach is supervised, so labeled examples are needed to span the space of transformations, preferably with little other variety in the images. Also, the feature space needs to be smooth, precluding mirroring.Acknowledgements Support is from Fight for Sight UK, a Microsoft Research PhD Scholarship, NERC NE/P016677/1, and NERC NE/P019013/1. Figure 10. We pass randomly rotated volume of 10 categories from the test set (left) through our system re-orienting it by 0, 60, 120, 180, and 240 degrees from the initial pose. The system struggles to reconstruct thin shapes properly, which is a common problem due to sparseness of the volume occupancy.

Supplementary Material Abstract
Here we present ModelNet10 classification performance and mathematical definition of homomorphism property from the main paper.

A. ShapeNets (ModelNet10) classification accuracy
The ModelNet10 classification task is evaluated on 908 models from the test set. For this task we trained the Modelnet architecture autoencoder with a 2-layer MLP (256-128-10) on the relative phase between all subvectors of the codes.We minimize the sum of two losses: cross-entropy loss for classification and the reconstruction loss. We follow  #b4  for the binary cross-entropy reconstruction loss:L recon = i∈voxels −γt i log(o i )−(1−γ)(1−t i )log(1−o i ),(17)where t i are the target values rescaled to [−1,2], o i is the output of the autoencoder rescaled to [0.1,0.9999] and γ is set to 0.98 to compensate for the sparseness of volumetric data. Thus, the loss is:L=L recon +10L classification(18)We optimize the loss using Adam and minibatch size 16, and learning rate of 10 −4 . We use the augmentation strategy of Maturana et al.  #b41 .We accurately classify 821 models out of 908, with an accuracy of 90.4%.Method Accuracy VRN Ensemble  #b4  97.14% ORION  #b48  93.8% LightNet  #b0  93.39% FusionNet  #b19  93.11% Pairwise  #b25  92.8% GIFT  #b2  92.35% VoxNet  #b41  92% 3D-GAN  #b55  91.00% Ours 90.4% Table 3. State of the Art methods and their classification accuracy on ModelNet10 benchmark.

B. The Homomorphism Property
The homomorphism property (Equation 6) isF θ2θ1 =F θ2 F θ1 .(19)Thus if I ∈Θ is the identity transformation, thenF θ =F Iθ =F I F θ =⇒ F I =I,(20)where I is the identity matrix. This in turn implies the invertability property F θ −1 =F −1 θ , sinceI=F I =F θθ −1 =F θ F θ −1 =⇒ F θ −1 =F −1 θ .(21)