Adaptive Image Denoising by Mixture Adaptation

Abstract
We propose an adaptive learning procedure to learn patch-based image priors for image denoising. The new algorithm, called the Expectation-Maximization (EM) adaptation, takes a generic prior learned from a generic external database and adapts it to the noisy image to generate a specific prior. Different from existing methods that combine internal and external statistics in ad-hoc ways, the proposed algorithm is rigorously derived from a Bayesian hyper-prior perspective. There are two contributions of this paper: First, we provide full derivation of the EM adaptation algorithm and demonstrate methods to improve the computational complexity. Second, in the absence of the latent clean image, we show how EM adaptation can be modified based on pre-filtering. Experimental results show that the proposed adaptation algorithm yields consistently better denoising results than the one without adaptation and is superior to several state-of-the-art algorithms.

I. INTRODUCTION A. Overview
We consider the classical image denoising problem: Given an additive i.i.d. Gaussian noise model,y = x + ε,(1)our goal is to find an estimate of x from y, where x ∈ R n denotes the (unknown) clean image, ε ∼ N (0, σ 2 I) ∈ R n denotes the Gaussian noise vector with zero mean and covariance matrix σ 2 I (where I is the identity matrix), and y ∈ R n denotes the observed noisy image. Image denoising is a long-standing problem. Over the past few decades, numerous denoising algorithms have been proposed, ranging from spatial domain methods  #b0  #b1  #b2  to transform domain methods  #b3  #b4  #b5 , and from local filtering  #b6  #b7  #b8  to global optimization  #b9  #b10 . In this paper, we focus on the Maximum a Posteriori (MAP) approach  #b10  #b11 . MAP is a Bayesian approach which tackles image denoising by maximizing the posterior probability argmax x f (y|x)f (x) = arg minx 1 2σ 2 y − x 2 − log f (x) .Here, the first term is a quadratic function due to the Gaussian noise model. The second term is the negative log of the image prior. The benefit of using the MAP framework is that it allows us to explicitly formulate our prior knowledge about the image via the distribution f (x).The success of an MAP optimization depends vitally on the modeling capability of the prior f (x)  #b12  #b13  #b14 . However, seeking f (x) for the whole image x is practically impossible because of the high dimensionality. To alleviate the problem, we adopt the common wisdom to approximate f (x) using a collection of small patches. Such prior is known as the patch prior, which is broadly attributed to Buades et al. for the nonlocal means  #b15 , and to an independent work of Awate and Whitaker presented at the same conference  #b16 . (See  #b17  for additional discussions about patch priors.) Mathematically, by letting P i ∈ R d×n be a patch-extract operator that extracts the i-th d-dimensional patch from the image x, a patch prior expresses the negative logarithm of the prior as a sum of the logarithms, leading to arg minx 1 2σ 2 y − x 2 − 1 n n i=1 log f (P i x) . (2)The prior thus formed is called the expected patch log likelihood (EPLL)  #b10 .

B. Related Work
Assuming that f (P i x) takes a parametric form for analytic tractability, the question now becomes where to find training samples and how to train the model. There are generally two approaches. The first approach is to learn f (P i x) from the single noisy image. We refer to these types of priors as internal priors, e.g.,  #b18 . The second approach is to learn f (P i x) from a database of images. We call these types of priors as external priors, e.g.,  #b8  #b19  #b20  #b21  #b22 .Combining internal and external priors has been an active direction in recent years. Most of these methods are based on a fusion approach, which attempts to directly aggregate the results of the internal and the external statistics. For example, Mosseri et al.  #b19  used a patch signal-to-noise ratio as a metric to decide if a patch should be denoised internally or externally; Burger et al.  #b20  applied a neural network to weight the internal and external denoising results; Yue et al.  #b23  used a frequency domain method to fuse the internal and external denoising results. There are also some works attempting to use external databases as a guide to train internal priors  #b24  #b25 .When f (P i x) is a Gaussian mixture model, there are special treatments to optimize the performance, e.g., a framework proposed by Awate and Whitaker  #b26  #b27  #b28 . In this method, a simplified Gaussian mixture (using the same weights and shared covariances) is learned directly from the noisy data through an empirical Bayes framework. However, the method primarily focuses on MRI data where the noise is Rician. This is different from the i.i.d. Gaussian noise assumption in our problem. The learning procedure is also different from ours as we use an adaptation process to adapt the generic prior to a specific prior. Our proposed method is inspired by the work of Gauvain and Lee  #b29  with a few important modifications.We should also mention the work of Weissman et al. on universal denoising  #b30  #b31 . Universal denoisers are a general class of denoising algorithms that do not require explicit knowledge about the prior and are also asymptotically optimal. While not explicitly proven, patch-based denoising methods such as non-local means  #b32  and BM3D  #b3  satisfy these properties. For example, the asymptotic optimality of non-local means was empirically verified by Levin et al.  #b33  #b34  with computational improvements by Chan et al.  #b35 . However, we shall not discuss universal denoisers in detail as they are beyond the scope of this paper.

C. Contribution and Organization
Our proposed algorithm is call EM adaptation. Like many external methods, we assume that we have an external database of images for training. However, we do not simply compute the statistics of the external database. Instead, we use the external statistics as a "guide" for learning the internal statistics. As will be illustrated in the subsequent sections, this can be formally done using a Bayesian framework.This paper is an extension of our previous work reported in  #b36 . This paper adds the following two new contributions: 1) Derivation of the EM adaptation algorithm. We rigorously derive the proposed EM adaptation algorithm from a Bayesian hyper-prior perspective. Our derivation complements the work of Gauvain and Lee  #b29  by providing additional simplifications and justifications to reduce computational complexity. We further provide discussion of the convergence. 2) Handling of noisy data. We provide detailed discussion of how to perform EM adaptation for noisy images. In particular, we demonstrate how to automatically adjust the internal parameters of the algorithm using prefiltered images.When this paper was written, we became aware of a very recent work by Lu et al.  #b37 . Compared to  #b37 , this paper provides theoretical results that are lacking in  #b37 . Numerical comparisons can be found in the experiment section.The rest of the paper is organized as follows: Section II gives a brief review of the Gaussian mixture model. Section III presents the proposed EM adaptation algorithm. Section IV discusses how the EM adaptation algorithm should be modified when the image is noisy. Experimental results are presented in Section V.

II. MATHEMATICAL PRELIMINARIES


A. GMM and MAP Denoising
For notational simplicity, we shall denote p i def = P i x ∈ R d as the i-th patch from x. We say that p i is generated from a Gaussian mixture model (GMM) iff (p i | Θ) = K k=1 π k N (p i |µ k , Σ k ),(3)where K k=1 π k = 1 with π k being the weight of the k-th Gaussian component, andN (p i |µ k , Σ k ) (4) def = 1 (2π) d/2 |Σ k | 1/2 exp − 1 2 (p i − µ k ) T Σ −1 k (p i − µ k )is the k-th Gaussian distribution with mean µ k and covarianceΣ k . We denote Θ def = {(π k , µ k , Σ k )} K k=1 as the GMM parameter.With the GMM defined in (3), we can specify the denoising procedure by solving the optimization problem in  #b1 . Here, we follow  #b38  #b39  by using the half quadratic splitting strategy. The idea is to replace (2) with the following minimization arg minx,{vi} n i=1 1 2σ 2 y − x 2 + 1 n n i=1 − log f (v i ) + β 2 P i x − v i 2 ,(5)where {v i } n i=1 are some auxiliary variables and β is a penalty parameter. By assuming that f (v i ) is dominated by the mode of the Gaussian mixture, the solution to (5) is given in the following proposition.Proposition 1:  #b10 . Proposition 1 is a general procedure for denoising images using a GMM under the MAP framework. There are, of course, other possible denoising procedures that also use GMM under the MAP framework, e.g., using surrogate methods  #b40 . However, we will not elaborate on these options. Our focus is on how to obtain the GMM.Assuming f (v i ) is dominated by the k * i -th components, where k * i def = argmax k π k N (v i |µ k , Σ k ), the solu- tion of (5) is x = nσ −2 I + β n i=1 P T i P i −1 nσ −2 y + β n i=1 P T i v i , v i = βΣ k * i + I −1 µ k * i + βΣ k * i P i x . Proof: See

B. EM Algorithm
The GMM parameter Θ = {(π k , µ k , Σ k )} K k=1 is typically learned using the Expectation-Maximization (EM) algorithm. EM is a known method. Interested readers can refer to  #b41  for a comprehensive tutorial. For image denoising, we note that the EM algorithm has several shortcomings as follows: 1) Adaptivity. For a fixed image database, the GMM parameters are specifically trained for that particular database. We call it the generic parameter. If, for example, we are given an image that does not necessarily belong to the database, then it becomes unclear how one can adapt the generic parameter to the image. 2) Computational cost. Learning a good GMM requires a large number of training samples. For example, the GMM in  #b10  is learned from 2,000,000 randomly sampled patches. If our goal is to adapt a generic parameter to a particular image, then it would be more desirable to bypass the computationally intensive procedure. 3) Finite samples. When training samples are few, the learned GMM will be over-fitted; some components will even become singular. This problem needs to be resolved because a noisy image contains much fewer patches than a database of patches. 4) Noise. In image denoising, the observed image always contains noise. It is not clear how to mitigate the noise while running the EM algorithm.

III. EM ADAPTATION
The proposed EM adaptation takes a generic prior and adapts it to create a specific prior using very few samples. Before giving the details of the EM adaptation, we first provide a toy example to illustrate the idea.

A. Toy Example
Suppose we are given two two-dimensional GMMs with two clusters in each GMM. From each GMM, we synthetically generate 400 data points with each point representing a 2D coordinate shown in Figure 1 (a) and (b). Imagine that the data points in (a) come from an external database whereas the data points in (b) come from a clean image of interest.With the two sets of data, we apply EM to learn GMM 1 and GMM 2. Since we have enough samples, both GMMs are estimated reasonably well as shown in (a) and (b). However, if we reduce the number of points in (b) to 20, then learning GMM 2 becomes problematic as shown in (c). Therefore, the question is this: Suppose we are given GMM 1 and only 20 data points from GMM 2, is there a way that we can transfer GMM 1 to the 20 data points so that we can approximately estimate GMM 2? This is the goal of EM adaptation. A result for this example is shown in (d).

B. Bayesian Hyper-prior
As illustrated in the toy example, what EM adaptation does is to use the generic model parameters as a "guide" when learning the new model parameters. Mathematically, suppose { p 1 , . . . , p n } are patches from a single image parameterized by a GMM with a parameter Θ def = {( π k , µ k , Σ k )} K k=1 . Our goal is to estimate Θ with the aid of some generic GMM parameter Θ. However, in order to formally derive the algorithm, we need to explain a Bayesian learning framework.  From a Bayesian perspective, estimation of the parameter Θ can be formulated asΘ = argmax Θ log f ( Θ | p 1 , . . . , p n ) = argmax Θ log f ( p 1 , . . . , p n | Θ) + log f ( Θ) , (6) where f ( p 1 , . . . , p n | Θ) = n i=1 K k=1 π k N ( p i | µ k , Σ k )is the joint distribution of the samples, and f ( Θ) is some prior of Θ. We note that (6) is also a MAP problem. However, the MAP for (6) is the estimation of the model parameter Θ, which is different from the MAP for denoising used in (2). Although the difference seems subtle, there is a drastically different implication that we should be aware of.In  #b5 , f ( p 1 , . . . , p n | Θ) denotes the distribution of a collection of patches conditioned on the parameter Θ. It is the likelihood of observing { p 1 , . . . , p n } given the model parameter Θ. f ( Θ) is a distribution of the parameter, which is called hyper-prior in machine learning  #b42 . Since Θ is the model parameter, the hyper-prior f ( Θ) defines the probability density of Θ.Same as the usual Bayesian modeling, hyper-priors are chosen according to a subjective belief. However, for efficient computation, hyper-priors are usually chosen as the conjugate priors of the likelihood function f ( p 1 , . . . , p n | Θ) so that the posterior distribution f ( Θ | p 1 , . . . , p n ) has the same functional form as the prior distribution. For example; Beta distribution is a conjugate prior for a Bernoulli likelihood function; Gaussian distribution is a conjugate prior for a likelihood function that is also Gaussian, etc. For more discussions on conjugate priors, we refer the readers to  #b42 .C. f ( Θ) for GMMFor GMM, no joint conjugate prior can be found through the sufficient statistic approach  #b29 . To allow tractable computation, it is necessary to separately model the mixture weights and the means/covariances by assuming that the weights and means/covariances are independent.We model the mixture weights as a multinomial distribution so that the corresponding conjugate prior for the mixture weight vector ( π 1 , · · · , π K ) is a Dirichlet densityπ 1 , · · · , π K ∼ Dir(v 1 , · · · , v k ),(7)where v i > 0 is a pseudo-count for the Dirichlet distribution. For mean and covariance ( µ k , Σ k ), the conjugate prior is the normal-inverse-Wishart density so that( µ k , Σ k ) ∼ NIW(ϑ k , τ k , Ψ k , ϕ k ), for k = 1, · · · , K, (8) where (ϑ k , τ k , Ψ k , ϕ k ) are the parameters for the normal- inverse-Wishart density such that ϑ k is a vector of dimension d, τ k > 0, Ψ k is a d × d positive definite matrix, and ϕ k > d − 1.

Remark 1:
The choice of the normal-inverse-Wishart distribution is important here, for it is the conjugate prior of a multivariate normal distribution with unknown mean and unknown covariance matrix. This choice is slightly different from  #b29  where the authors choose a normal-Wishart distribution. While both normal-Wishart and normal-inverse-Wishart can lead to the same result, the proof using normal-inverse-Wishart is considerably simpler for its inverted matrices.Assuming π k is independent of ( µ k , Σ k ), we factorize f ( Θ) as a product of (7) and  #b7 . By ignoring the scaling constants, it is not difficult to show thatf ( Θ) ∝ K k=1 π v k −1 k | Σ k | −(ϕ k +d+2)/2 exp − τ k 2 ( µ k − ϑ k ) T Σ −1 k ( µ k − ϑ k ) − 1 2 tr(Ψ k Σ −1 k ) .(9)The importance of (9) is that it is a conjugate prior of the complete data. As a result, the posterior density f ( Θ| p 1 , . . . , p n ) belongs to the same distribution family as f ( Θ). This can be formally described in Proposition 2.Proposition 2: Given the prior in  #b8 , the posterior f ( Θ| p 1 , . . . , p n ) is given byf ( Θ | p 1 , . . . , p n ) ∝ K k=1 π v ′ k −1 k | Σ k | −(ϕ ′ k +d+2)/2 exp − τ ′ k 2 µ k − ϑ ′ k T Σ −1 k µ k − ϑ ′ k − 1 2 tr(Ψ ′ k Σ −1 k )(10)wherev ′ k = v k + n k , ϕ ′ k = ϕ k + n k , τ ′ k = τ k + n k , ϑ ′ k = τ k ϑ k + n kμk τ k + n k , Ψ ′ k = Ψ k + S k + τ k n k τ k + n k (ϑ k −μ k )(ϑ k −μ k ) T , µ k = 1 n k n i=1 γ ki p i , S k = n i=1 γ ki ( p i −μ k )( p i −μ k ) Tare the parameters for the posterior density.Proof: See Appendix A.

D. Solve for Θ
Solving for the optimal Θ is equivalent to solving the following optimization problem:maximize Θ L( Θ) def = log f ( Θ| p 1 , . . . , p n ) subject to K k=1 π k = 1.(11)The constrained problem  #b10  can be solved by considering the Lagrange function and taking derivatives with respect to each individual parameter. We summarize the optimal solutions in Proposition 3.Proposition 3: The optimal ( π k , µ k , Σ k ) for (11) are π k = n ( K k=1 v k − K) + n · n k n + K k=1 v k − K ( K k=1 v k − K) + n · v k − 1 K k=1 v k − K ,(12)µ k = 1 τ k + n k n i=1 γ ki p i + τ k τ k + n k ϑ k ,(13)Σ k = n k ϕ k + d + 2 + n k 1 n k n i=1 γ ki ( p i − µ k )( p i − µ k ) T + 1 ϕ k + d + 2 + n k Ψ k + τ k (ϑ k − µ k )(ϑ k − µ k ) T .(14)Proof: See Appendix B.

Remark 2:
The results we showed in Proposition 3 are different from  #b29 . In particular, the denominator for Σ k in  #b29  is ϕ k − d + n k whereas ours is ϕ k + d + 2 + n k . However, by using the simplification described in the next subsection, we can obtain the same result for both cases.

E. Simplification of Θ
The results in Proposition 3 are general expressions for any hyper-parameters. We now discuss how to simplify the result with the help of the generic prior. First, since v k −1 K k=1 v k −K is the mode of the Dirichlet distribution, a good surrogate for it is π k . Second, ϑ k denotes the prior mean in the normalinverse-Wishart distribution and thus can be appropriately approximated by µ k . Moreover, since Ψ k is the scale matrix on Σ k and τ k denotes the number of prior measurements in the normal-inverse-Wishart distribution, they can be reasonably chosen as Ψ k = (ϕ k + d + 2)Σ k and τ k = ϕ k + d + 2. Plugging these approximations in the results of Proposition 3, we summarize the simplification results as follows:Proposition 4: Define ρ def = n k n ( K k=1 v k − K) = τ k = ϕ k + d + 2. Let ϑ k = µ k , Ψ k = (ϕ k + d + 2)Σ k , v k − 1 K k=1 v k − K = π k ,and α k = n k ρ+n k , then (12)- (14) becomeπ k =α k n k n + (1 − α k )π k ,(15)µ k =α k 1 n k n i=1 γ ki p i + (1 − α k )µ k ,(16)Σ k = α k 1 n k n i=1 γ ki ( p i − µ k )( p i − µ k ) T + (1 − α k ) Σ k + (µ k − µ k )(µ k − µ k ) T . (17)Remark 3: We note that Reynold et al.  #b43  presented similar simplification results (without derivations) as ours. However, their results are valid only for the scalar case or when the covariance matrices are diagonal. In contrast, our results support full covariance matrices and thus are more general. As will be seen, for our denoising application, since the image pixels (especially adjacent pixels) are correlated, the full covariance matrices are necessary for good performance.Remark 4: Comparing (17) with the work of Lu et al.  #b37 , we note that in  #b37  the covariance isΣ k = α k 1 n k n i=1 γ ki p i p T i + (1 − α k )Σ k .(18)This result, although it looks similar to ours, is generally not valid if we follow the Bayesian hyper-prior approach, unless µ k and µ k are both 0.

F. EM Adaptation Algorithm
The proposed EM adaptation algorithm is summarized in Algorithm 1. EM adaptation shares many similarities with the standard EM algorithm. To better understand the differences, we take a closer look at each step.

E-Step:
The E-step in the EM adaptation is the same as in the EM algorithm. We compute the likelihood of p i conditioned on the generic parameter (π k , µ k , Σ k ) asγ ki = π k N ( p i | µ k , Σ k ) K l=1 π l N ( p i | µ l , Σ l ) .(23)

M-Step:
The M-step is a more interesting step. From  #b19  to  #b21 , ( π k , µ k , Σ k ) are updated through a linear combination of the contributions from the new data and the generic parameters. On one extreme, when α k = 1, the M-step turns exactly back to the M-step in the EM algorithm. On the other extreme, when α k = 0, all emphasis is put onAlgorithm 1 EM adaptation Algorithm Input: Θ = {(π k , µ k , Σ k )} K k=1 , { p 1 , . . . , p n }. Output: Adapted parameters Θ = {( π k , µ k , Σ k )} K k=1 . E-step : Compute, for k = 1, . . . , K and i = 1, . . . , n γ ki = π k N ( p i |µ k , Σ k ) K l=1 π l N ( p i |µ l , Σ l ) , n k = n i=1 γ ki . (19) M-step : Compute, for k = 1, . . . , K π k = α k n k n + (1 − α k )π k ,(20)µ k = α k 1 n k n i=1 γ ki p i + (1 − α k )µ k ,(21)Σ k = α k 1 n k n i=1 γ ki ( p i − µ k )( p i − µ k ) T + (1 − α k ) Σ k + (µ k − µ k )(µ k − µ k ) T .(22)Postprocessing: Normalize { π k } K k=1 so that they sum to 1,and ensure { Σ k } K k=1 is positive semi-definite.the generic parameters. For α k that lies in between, the updates are a weighted averaging of the new data and the generic parameters. Taking the mean as an example, the EM adaptation updates the mean according toµ k = α k 1 n k n i=1 γ ki p i new data + (1 − α k )µ k generic prior .(24)The updated mean in (24) is a linear combination of two terms, where the first term is an empirical data average with the fractional weight γ ki from each data point p i and the second term is the generic mean µ k . Similarly for the covariance update in (22), the first term computes an empirical covariance with each data point weighted by γ ki which is the same as in the M-step of the EM algorithm, and the second term includes the generic covariance along with an adjustment term (µ k − µ k )(µ k − µ k ) T . These two terms are then linearly combined to yield the updated covariance.

G. Convergence
The EM adaptation shown in Algorithm 1 is an EM algorithm. Therefore, its convergence is guaranteed by the classical theory, which we state without proof as follows:Proposition 5: Let L( Θ) = log f ( p 1 , . . . , p n | Θ) be the log-likelihood, f ( Θ) be the prior, and Q Θ| Θ (m)be the Q function in the m-th iteration. IfQ Θ| Θ (m) + log f ( Θ) ≥ Q Θ (m) | Θ (m) + log f Θ (m) , then L( Θ) + log f ( Θ) ≥ L Θ (m) + log f Θ (m) .Proof: See  #b41 .The convergence result states that the EM adaptation converges to a local minimum defined by the Q function. Experimentally, we observe that EM adaptation enters the steady state in a few iterations. To demonstrate this observation, we conduct experiments on different testing images. Figure 2 shows the result of one testing image. For all noise levels (σ = 20 to 100), PSNR increases as more iterations are applied and converges after about four iterations. We also observe that for most testing images, the improvement becomes marginal after a single iteration. 

IV. EM ADAPTATION FOR DENOISING
The proposed Algorithm 1 presented in the previous section works well when the training patches { p 1 , . . . , p n } are drawn from the clean ground-truth image x. In this section, we discuss how to modify the EM adaptation algorithm when the data are drawn from noisy images. We will also discuss methods to improve computational complexity.

A. Adaptation to Noisy Image
In the presence of noise, the Gaussian mixture model is perturbed by the noise covariance. More precisely, if we assume that a clean patch p i is drawn from a Gaussian mixture and if the noise ǫ is i.i.d. Gaussian with distribution N (0, σ 2 I), then the probability of drawing a noisy version p i + ǫ is the convolution of the GMM and the Gaussian noisef ( p i + ǫ) = K k=1 π k ∞ −∞ N ( p i − q | µ k , Σ k )N (q | 0, σ 2 I)dq.Since convolution is linear, it does not influence the weight π k and the mean µ k . The covariance is perturbed by adding σ 2 I to Σ k . Therefore, the final distribution of p i + ǫ isf ( p i + ǫ) = K k=1 π k N ( p i | µ k , Σ k + σ 2 I).The implication of the above analysis is that in the presence of noise, we should compensate the noise present in the observed data. As a result, the EM algorithm can be accordingly modified by changing  #b18  toγ ki = π k N ( p i | µ k , Σ k + σ 2 I) K l=1 π l N ( p l | µ l , Σ l + σ 2 I) ,(25)and (22) toΣ k = α k 1 n k n i=1 γ ki ( p i − µ k )( p i − µ k ) T − σ 2 I + (1 − α k ) Σ k + (µ k − µ k )(µ k − µ k ) T .(26)In other words, we add σ 2 I to γ ki to ensure the correct likelihood of drawing a noisy observation, and subtract σ 2 I from Σ k to ensure the correct mixture model of the clean sample.Remark 5: Our noise handling method shares some similarity with the work of Awate and Whitaker  #b28  by subtracting the noise variance from the observed variance. However, in  #b28  the variance is assumed identical for all mixture components and is a scalar. In our case, we have to estimate a collection of covariance matrices { Σ k } K k=1 , which are dense. Another difference is that we use an adaptation approach by combining the generic prior and the new data. Awate and Whitaker  #b28  do not have generic priors, but they minimize the Kullback-Leibler divergence of the ideal distribution and the estimated distribution.

B. Adaptation by Pre-filtering
A limitation of the above noise handling technique is that when σ is large, it becomes impossible for  #b25  to return the true covariance as we only have limited number of samples for training. Therefore, for practical reasons we consider a pre-filtering step. This idea is similar to many image denoising algorithms such as BM3D  #b3  where a first stage pilot denoising is applied before going to the second stage of the actual denoising. In our problem, we apply an existing denoising algorithm to obtain a pre-filtered image. The adaptation is then applied to the pre-filtered image to generate an adapted prior. In the second stage, we apply the MAP denoising as described in Section II-A to obtain the final denoised image.When a pre-filtering step is employed, we must address the question of what is the influence of a particular prefiltering method. Clearly, a strong pre-filtering will likely lead to better final performance as the pre-filtered image provides more relevant information for the adaptation step. However, as we will see in the experiment section (Section V), using a reasonably advanced denoising algorithm for pre-filtering does not cause much performance difference. The intuition is that pre-filtering is nothing but a black-box method to reduce the noise level σ. Since σ can never be zero for any pre-filtering method, we will have to use  #b24  and (26) by replacing σ with the amount of noise remaining in the prefiltered image. For most state-of-the-art denoising algorithms, the difference in σ is quite small. The real challenge is how to estimate σ.

C. Estimating σ 2
In order to analyze the noise remaining in the pre-filtered image, we let x be the pre-filtered image. The distribution of the residue x − x is unknown, but empirically we observe that it can be approximated by a single Gaussian as in  #b44 . This approximation allows us to model (x−x) ∼ N (0, σ 2 I), where σ 2 def = E x − x 2 is the variance of x.In other words, σ 2 is the mean squared error (MSE) of x compared to x. Therefore, if x is available, estimating σ 2 is trivial as it is just the MSE. However, in the absence of x, we need a surrogate to estimate the MSE and hence σ 2 .The surrogate strategy we use is the Stein's Unbiased Risk Estimator (SURE)  #b45 . SURE provides a way for unbiased estimation of the true MSE. The analytical expression of SURE isσ 2 ≈ SURE def = 1 n y − x 2 − σ 2 + 2σ 2 n div,(27)where div denotes the divergence of the denoising algorithm with respect to the noisy measurements. However, not all denoising algorithms have a closed form for the divergence term. To alleviate this issue, we adopt the Monte-Carlo SURE  #b46  to approximate the divergence. We refer readers to  #b46  for detailed discussions about Monte-Carlo SURE. The key steps are summarized in Algorithm 2.

Algorithm 2 Monte-Carlo SURE for Estimating σ 2
Input: y ∈ R n , σ 2 , δ (typically δ = 0.01). Output: σ 2 . Generate b ∼ N (0, I) ∈ R n . Construct y ′ = y + δb. Apply a denoising algorithm on y and y ′ to get two prefiltered images x and x ′ , respectively.Compute div = 1 δ b T (x ′ − x). Compute σ 2 = SURE def = 1 n y − x 2 − σ 2 + 2σ 2 n div.To demonstrate the effectiveness of Monte-Carlo SURE, we compare the estimates for σ/σ when we use the true MSE and Monte-Carlo SURE. As is observed in Figure 3, over a large range of noise levels, the Monte-Carlo SURE curves are quite similar to the true MSE curves. The pre-filtering method in Figure 3 is EPLL. For other methods, such as BM3D, we have similar observations for different noise levels.

D. Estimating α k
Besides the pre-filtering for noisy images, we should also determine the combination weight α k for the EM adaptation. From the derivation of the algorithm, the combination weight α k = n k n k +ρ is determined by both the probabilistic count n k and the relevance factor ρ. The factor ρ is adjusted to allow different adaptation rates. When there is a good match between training data and target noisy image, we can let the adaptation rely more on the training data; When there is a poor match between training data and target noisy image, we use more of the noisy image. This parameter is user defined, similar to a regularization parameter in optimization. In the application of speaker verification  #b43  #b47 , ρ is set to 16. Experimentally, we find that the performance is insensitive to ρ in the range of 8 and 20.In this paper, ρ is tuned to maximize the PSNR of the denoised image. In Figure 4, we show how PSNR changes in terms of ρ. The PSNR curves indicate that for a testing image of 64 × 64, a large ρ for EM adaptation is better. As the testing images become larger, we observe that the optimal ρ becomes smaller. Empirically, we find that ρ in the range of 1 and 10 works well for a variety of images (over 200 × 200) for different noise levels.Remark 6: For a fixed ρ, it is perhaps tempting to compare the estimated GMM with the ground truth GMM because a good match will likely provide better denoising performance. However, we note that the ground truth GMM is never available even if we use the oracle clean image because a finitesize image does not have adequate patches for estimating the GMM parameters.

E. Computational Improvement
Finally, we comment on a simple but very effective way of improving the computational speed. If we take a closer look at the M-step in Algorithm 1, we observe that π k and µ k are easy to compute. However, Σ k is time-consuming to compute, because updating each of the K covariance matrices requires n time-consuming outer product operations  i=1 γ ki ( p i − µ k )( p i − µ k ) T .Most previous works mitigate the problem by assuming that the covariance is diagonal  #b43  #b47  #b48 . However, this assumption is not valid in our case because image pixels (especially neighboring pixels) are correlated.Our solution to this problem is shown in Proposition 6. The new result is an exact computation of (22) but with significantly less operations. The idea is to exploit the algebraic structure of the covariance matrix.Proposition 6: The full covariance adaptation in  #b21  can be simplified asΣ k = α k 1 n k n i=1 γ ki p i p T i − µ k µ T k + (1 − α k )(Σ k + µ k µ T k ).(28)Proof: See Appendix C. The simplification is very rewarding because computing α k 1 n k n i=1 γ ki p i p T i does not involve µ k and thus can be precomputed for each component, which makes the computation of Σ k much more efficient. This reduces the computational complexity from O(nd 2 ) for (22) down to O(d 2 ) for (28). In Table 1, we list the averaging runtime when computing  #b21  and (28)  V. EXPERIMENTAL RESULTS In this section, we present experimental results for singleand example-based image denoising. Single refers to using the single noisy image for training, whereas example refers to using an external reference image for training.

A. Experiment Settings
For comparison, we consider two state-of-the-art methods: BM3D  #b3  and EPLL  #b10 . For both methods, we run the original codes provided by the authors with the default parameters. The GMM prior in EPLL is learned from 2,000,000 randomly chosen 8 × 8 patches. The number of mixture components is fixed at 200 to match the GMM prior provided by EPLL. Alternatively, we can optimize the number of mixtures by performing a cross validation against the noise distribution as in  #b49  or performing a distribution match against the generic prior as in  #b37 .We consider three versions of EM adaptation: (1) an oracle adaptation by adapting the generic prior to the ground-truth image, denoted as aGMM-clean; (2) a pre-filtered adaptation by adapting the generic prior to the EPLL result, denoted as aGMM-EPLL; (3) a pre-filtered adaptation by adapting the generic prior to the BM3D result, denoted as aGMM-BM3D. In the example-based image denoising, we adapt the generic prior to an example image and denote it as aGMMexample. We set the parameter ρ = 1, and experimental results show that the performance is insensitive to ρ being in the range of 1 and 10. We run denoising experiments on a variety of images and for a large range of noise standard deviations (σ = 20, 40, 60, 80, 100). To reduce the bias due to a particular noise realization, each reported PSNR result is averaged over at least eight independent trials.

B. Single Image Denoising
We use six standard images of size 256 × 256, and six natural images of size 481 × 321 randomly chosen from  #b50  for the single image denoising experiments. Figure 5 shows the denoising results for three standard testing images and three natural testing images. In comparison to the competing methods, our proposed method yields the highest PSNR values. Magnified versions of the images are shown in Figure 5.In Table 2, we report PSNR for different noise variances for the standard images. Two key observations are noted:First, comparing aGMM-EPLL with EPLL, the denoising results from aGMM-EPLL are consistently better than EPLL with an average gain of about 0.3 dB. This validates the usefulness of the adapted GMM through the proposed EM adaptation. For BM3D, we note that the prior model of BM3D (a non-linear transformation followed by a sparsity regularization) is fundamentally different from the EPLL model. Therefore, adapting a Gaussian mixture to a BM3D prefiltered result does not necessarily improve the performance. In fact, aGMM-BM3D is better than BM3D at low noise, but worse at high noise.Second, the quality of the image used for EM adaptation affects the final denoising performance. For example, using the ground-truth clean image for EM adaptation is much better than using the denoised images, such as the EPLL or BM3D denoised image. In some cases, aGMM-BM3D yields larger PSNR values than aGMM-EPLL due to the fact that the denoised image from BM3D is better than that from EPLL. We validate our experiment by repeating 10 Monte Carlo trials. Consequently, we calculate the standard deviation across different noise realizations and conduct a t test with the null hypothesis that aGMM-EPLL has equal performance to the original EPLL. If we reject the null hypothesis, then we conclude that aGMM-EPLL is statistically better than EPLL. Table 3 shows the result of "Peppers" using 10 Monte Carlo trials. Except for the case when σ = 80, all p-values are small, implying that the results by aGMM-EPLL are statistically better than those by EPLL.In addition to comparing EPLL and BM3D, we also compare the performance of the proposed method when there is no generic prior. That is, we train a GMM purely from the noisy image. In this case, we collect the noisy patches from the image and apply EM algorithm to learn a GMM. The results, denoted as GMM-noisy, are shown in Table 4. The performance is undesirable, as we expected. For comparison, we adapt the generic GMM to the noisy image with Equations  #b24  and (26) and set σ 2 = σ 2 . In this case, the performance is improved significantly by the adaptation process, confirming the effectiveness of the method.We compare the result with  #b37  in the last two columns of Table 4. The result indicates that  #b37  performs consistently better than EPLL but slightly worse than the proposed aGMM-EPLL. One reason is that in our method, the pre-filtering has significantly reduced the noise level for better training.In order to visually compare the improvement of the adapted GMM over the generic GMM, we randomly sample 100 patches from each GMM and show the results in Figure  6. The patches we display are the raw un-normalized patches. The gray scale value reflects the actual magnitude of a patch, and the occurrence of the patches indicates the likelihood of drawing that patch. Therefore, a significant portion of the patches are smooth as the GMM has peaks at smooth patches. The results show that the adapted GMM does have improved patch quality compared to the generic one.

C. External Image Denoising
In this subsection, we evaluate the denoising performance when an example image is available for EM adaptation. An example image refers to a clean image and is relevant to the noisy image of interest. In  #b8  #b22 , it is shown that obtaining reference images is feasible in some scenarios such as text images and face images. We consider the following three scenarios for our experiments: 1) Flower image denoising: We use the 102 flowers dataset from  #b51 , which consists of 102 different categories of flowers. We randomly pick one category and then sample two flower images: one as the testing image with additive i.i.d. Gaussian noise, and the other as the example image for the EM adaptation. 2) Face image denoising: We use the FEI face dataset from  #b52 , which consists of 100 aligned and frontal   In Figure 7, we show the denoising results for the three different scenarios. As is shown, the example images in the second column are similar but differ from the testing images. We compare the three denoising methods. The major differ-   ence lies in how the default GMM is adapted: In EPLL, there is no EM adaptation, i.e., the default generic GMM is used for denoising. In aGMM-example, the default GMM is adapted to the example image while in aGMM-clean, the default GMM is adapted to the ground truth image. As observed, the oracle aGMM-clean yields the best denoising performance. aGMM-example outperforms the benchmark EPLL (generic GMM) denoising algorithm both visually and objectively. For example, on average, it is 0.28 dB better in the Flower scenario, 0.78 dB better in the Face scenario, and 1.57 dB better in the Text scenario. In addition to adapting the generic GMM to the example image, we could also apply the EM algorithm on the example image to learn a new GMM. However, we argue that the new GMM would lead to over-fitting and, hence, poor performance for denoising. In Table 5 we show the PSNR results when we use different GMMs for denoising. As is observed, GMMexample, which learns a new GMM from the example image, is even worse than EPLL whose GMM is learned from a generic database. In contrast, our proposed aGMM-example, which adapts the generic GMM to the example image, gives the best performance consistently.  Table 5: External image denoising: PSNR and SSIM in the parenthesis. Two flower images are sampled from  #b51  with one being the testing image and the other being the example image. EPLL uses the generic GMM, GMM-example applies EM algorithm to the example image, and aGMM-example applies adaptation from the generic GMM to the example image.

D. Runtime
Our current implementation is in MATLAB (single thread), and we use an Intel Core i7-3770 CPU with 8 GB RAM. The runtime is about 66 seconds to denoise an image of size 256× 256, where the EM adaptation part takes about 14 seconds, while the MAP denoising part takes about 52 seconds. The EM adaptation utilizes the simplification  #b27  in Section IV-D, which has significant speedup impact to the adaptation. The MAP denoising part has similar runtime as EPLL, which uses an external mixture model for denoising. As pre-filtering is considered, we note that BM3D takes approximately 0.25 seconds, and EPLL takes approximately 50 seconds.

VI. CONCLUSION
We proposed an EM adaptation method to learn effective image priors. The proposed algorithm is rigorously derived from the Bayesian hyper-prior perspective and is further simplified to reduce the computational complexity. In the absence of the latent clean image, we proposed modifications of the algorithm and analyzed how some internal parameters can be automatically estimated. The adapted prior from the EM adaptation better captures the prior distribution of the image of interest and is consistently better than the unadapted generic one. In the context of image denoising, experimental results demonstrate its superiority over some existing denoising algorithms, such as EPLL and BM3D. Future work includes its extended work on video denoising and other restoration tasks, such as deblurring and inpainting.

APPENDIX


A. Proof of Proposition 2
Proof: We first compute the probability that the i-th sample belongs to the k-th Gaussian component asγ ki = π (m) k N ( p i | µ (m) k , Σ (m) k ) K l=1 π (m) l N ( p i | µ (m) l , Σ (m) l ) ,(29)where   #b51 , the face images are from the FEI face dataset  #b52 , and the text images are cropped from randomly chosen documents.{(π (m) k , µ (m) k , Σ(mapproximate log f ( p 1 , . . . , p n )| Θ) in (6) by the Q function as followsQ( Θ| Θ (m) ) = n i=1 K k=1 γ ki log π k N ( p i | µ k , Σ k ) . = n i=1 K k=1 γ ki log π k − 1 2 log | Σ k | − 1 2 ( p i − µ k ) T Σ −1 k ( p i − µ k ) = K k=1 n k (log π k − 1 2 log | Σ k |) − 1 2 K k=1 n i=1 γ ki ( p i − µ k ) T Σ −1 k ( p i − µ k ),(30)where .= indicates that some constant terms that are irrelevant to the parameters Θ are dropped.We further define two notationsµ k def = 1 n k n i=1 γ ki p i , S k def = n i=1 γ ki ( p i −μ k )( p i −μ k ) T .Using the equalityn i=1 γ ki ( p i − µ k ) T Σ −1 k ( p i − µ k ) = n k ( µ k −μ k ) T Σ −1 k ( µ k −μ k ) + tr(S k Σ −1 k ),we can rewrite the Q function as followsQ( Θ| Θ (m) ) = K k=1 n k (log π k − 1 2 log | Σ k |) − n k 2 ( µ k −μ k ) T Σ −1 k ( µ k −μ k ) − 1 2 tr(S k Σ −1 k ) .Therefore, we have f ( Θ| p 1 , . . . , p n ) ∝ exp Q( Θ| Θ (m) ) + log f ( Θ)= f ( Θ) K k=1 π n k k | Σ k | −n k /2 exp − n k 2 ( µ k −μ k ) T Σ −1 k ( µ k −μ k ) − 1 2 tr(S k Σ −1 k ) = K k=1 π v k +n k −1 k | Σ k | −(ϕ k +n k +d+2)/2 exp − τ k + n k 2 ( µ k − τ k ϑ k + n kμk τ k + n k ) T Σ −1 k ( µ k − τ k ϑ k + n kμk τ k + n k ) exp − 1 2 tr((Ψ k + S k + τ k n k τ k + n k (ϑ k −μ k )(ϑ k −μ k ) T ) Σ −1 k ) .(31)Defining v ′ k def = v k + n k , ϕ ′ k def = ϕ k + n k , τ ′ k def = τ k + n k , ϑ ′ k def = τ k ϑ k +n kμk τ k +n k , and Ψ ′ k def = Ψ k + S k + τ k n k τ k +n k (ϑ k −μ k )(ϑ k −μ k ) T , we will getf ( Θ| p 1 , . . . , p n ) ∝ K k=1 π v ′ k −1 k | Σ k | −(ϕ ′ k +d+2)/2 exp − τ ′ k 2 µ k − ϑ ′ k T Σ −1 k µ k − ϑ ′ k − 1 2 tr(Ψ ′ k Σ −1 k ) ,which completes the proof.

B. Proof of Proposition 3
Proof: We ignore some irrelevant terms and get log f ( Θ| p 1 , . . . , p n ) .= K k=1 {(v ′ k − 1) log π k − (ϕ ′ k +d+2) 2 log | Σ k | − τ ′ k 2 ( µ k − ϑ ′ k ) T Σ −1 k ( µ k − ϑ ′ k ) − 1 2 tr(Ψ ′ k Σ −1 k )}.Taking derivatives with respect to π k , µ k and Σ k will yield the following solutions.• Solution to π k .We form the LagrangianJ( π k , λ) = K k=1 (v ′ k − 1) log π k + λ K k=1 π k − 1 ,and the optimal solution satisfies∂J ∂ π k = v ′ k − 1 π k + λ = 0.It is easy to see that λ = − K k=1 (v ′ k − 1), and thus the solution to π k isπ k = v ′ k − 1 K k=1 (v ′ k − 1) = (v k − 1) + n k ( K k=1 v k − K) + n = n ( K k=1 v k − K) + n · n k n + K k=1 v k − K ( K k=1 v k − K) + n · v k − 1 K k=1 v k − K .(32)• Solution to µ k . We let∂L ∂ µ k = − τ ′ k 2 Σ −1 k ( µ k − ϑ ′ k ) = 0,(33)of which the solution isµ k = 1 τ k + n k n i=1 γ ki p i + τ k τ k + n k ϑ k .(34)• Solution to Σ k . We let∂L ∂ Σ k = − ϕ ′ k + d + 2 2 Σ −1 k + 1 2 Σ −1 k Ψ ′ k Σ −1 k + τ ′ k 2 Σ −1 k ( µ k − ϑ ′ k )( µ k − ϑ ′ k ) T Σ −1 k = 0, which yields (ϕ ′ k + d + 2) Σ k = Ψ ′ k + τ ′ k ( µ k − ϑ ′ k )( µ k − ϑ ′ k ) T . (35)Thus, the solution isΣ k = Ψ ′ k + τ ′ k ( µ k − ϑ ′ k )( µ k − ϑ ′ k ) T ϕ ′ k + d + 2 = Ψ k + τ k ( µ k − ϑ k )( µ k − ϑ k ) T ϕ k + d + 2 + n k + n k ( µ k −μ k )( µ k −μ k ) T + S k ϕ k + d + 2 + n k = n k ϕ k + d + 2 + n k 1 n k n i=1 γ ki ( p i − µ k )( p i − µ k ) T + 1 ϕ k + d + 2 + n k Ψ k + τ k (ϑ k − µ k )(ϑ k − µ k ) T .

C. Proof of Proposition 6
Proof: We expand the first term in  #b21 .α k 1 n k n i=1 γ ki ( p i − µ k )( p i − µ k ) T = α k 1 n k n i=1 γ ki ( p i p T i − p i µ T k − µ k p T i + µ k µ T k ) α k 1 n k n i=1 γ ki p i p T i − ( µ k − (1 − α k )µ k ) µ T k − µ k ( µ k − (1 − α k )µ k ) T + α k µ k µ T k = α k 1 n k n i=1 γ ki p i p T i − 2 µ k µ T k + (1 − α k )(µ k µ T k + µ k µ T k ) + α k µ k µ T k ,(36)where holds because α k 1 n k n i=1 γ ki p i = µ k − (1 − α k )µ k from  #b20 . We then expand the second term in  #b21  (1 − α k ) Σ k + (µ k − µ k )(µ k − µ k ) T = (1 − α k )(Σ k + µ k µ T k + µ k µ T k ) − (1 − α k )(µ k µ T k + µ k µ T k ).(37)Combining (36) and (37) completes the proof.